(500, 8)
torch.Size([500, 11])

bounds:tensor([-1.], device='cuda:0')	db:10	Pt_max:10.0
====================================================================================================

epoch:0
Traceback (most recent call last):
  File "g:\CINT\Data_model_drive_direct\HARQ_MSC\train\main.py", line 531, in <module>
    train_(False)
  File "g:\CINT\Data_model_drive_direct\HARQ_MSC\train\main.py", line 286, in train_
    pt = model_pd.train_(
  File "g:\CINT\Data_model_drive_direct\HARQ_MSC\train\model.py", line 476, in train_
    Pout = self.forward(Hx, edge_wight)
  File "g:\CINT\Data_model_drive_direct\HARQ_MSC\train\model.py", line 454, in forward
    action = self.agent.actor.forward(next_state)
  File "g:\CINT\Data_model_drive_direct\HARQ_MSC\train\model.py", line 186, in forward
    a = torch.relu(self.layer_1(state))
  File "F:\Miniconda\envs\HARQ_GCN\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "F:\Miniconda\envs\HARQ_GCN\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "F:\Miniconda\envs\HARQ_GCN\lib\site-packages\torch\nn\modules\linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
