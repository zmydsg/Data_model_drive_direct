
bounds:tensor([-2.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.6167, 2.6493],
        [2.3753, 2.3852, 2.3768],
        [2.3753, 2.9405, 3.3198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.10052523016929626 
model_pd.l_d.mean(): -19.973440170288086 
model_pd.lagr.mean(): -19.872915267944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0018], device='cuda:0')), ('power', tensor([0.9989], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.8181], device='cuda:0')), ('power', tensor([-21.7916], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.10052523016929626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.4888, 2.4992, 2.4904],
        [2.4888, 2.7447, 2.7786],
        [2.4888, 3.0904, 3.4927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.07090787589550018 
model_pd.l_d.mean(): -20.074914932250977 
model_pd.lagr.mean(): -20.00400733947754 
model_pd.lambdas: dict_items([('pout', tensor([1.0036], device='cuda:0')), ('power', tensor([0.9978], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.7560], device='cuda:0')), ('power', tensor([-21.8579], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.07090787589550018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6034, 2.8740, 2.9093],
        [2.6034, 3.2419, 3.6674],
        [2.6034, 2.6144, 2.6051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.03936483711004257 
model_pd.l_d.mean(): -20.15931510925293 
model_pd.lagr.mean(): -20.119949340820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0053], device='cuda:0')), ('power', tensor([0.9967], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.6961], device='cuda:0')), ('power', tensor([-21.9093], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.03936483711004257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7193, 2.7308, 2.7210],
        [2.7193, 3.0046, 3.0414],
        [2.7193, 3.3950, 3.8440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): -0.0003351867198944092 
model_pd.l_d.mean(): -20.22836685180664 
model_pd.lagr.mean(): -20.228702545166016 
model_pd.lambdas: dict_items([('pout', tensor([1.0069], device='cuda:0')), ('power', tensor([0.9956], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.6382], device='cuda:0')), ('power', tensor([-21.9472], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:-0.0003351867198944092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8364, 3.1367, 3.1749],
        [2.8364, 2.8485, 2.8382],
        [2.8364, 3.5497, 4.0224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): -0.06198325380682945 
model_pd.l_d.mean(): -20.283552169799805 
model_pd.lagr.mean(): -20.345535278320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0085], device='cuda:0')), ('power', tensor([0.9945], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5822], device='cuda:0')), ('power', tensor([-21.9729], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:-0.06198325380682945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9547, 3.7062, 4.2028],
        [2.9547, 2.9674, 2.9566],
        [2.9547, 3.2702, 3.3099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): -0.19413495063781738 
model_pd.l_d.mean(): -20.326139450073242 
model_pd.lagr.mean(): -20.520275115966797 
model_pd.lambdas: dict_items([('pout', tensor([1.0100], device='cuda:0')), ('power', tensor([0.9934], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5280], device='cuda:0')), ('power', tensor([-21.9875], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:-0.19413495063781738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0743, 3.8642, 4.3851],
        [3.0743, 3.4051, 3.4463],
        [3.0743, 3.0876, 3.0763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): -0.8391891717910767 
model_pd.l_d.mean(): -20.357223510742188 
model_pd.lagr.mean(): -21.196413040161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0115], device='cuda:0')), ('power', tensor([0.9923], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4754], device='cuda:0')), ('power', tensor([-21.9919], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:-0.8391891717910767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1952, 4.0241, 4.5694],
        [3.1952, 3.2091, 3.1973],
        [3.1952, 3.5415, 3.5841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 1.0135878324508667 
model_pd.l_d.mean(): -20.37778091430664 
model_pd.lagr.mean(): -19.364192962646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0129], device='cuda:0')), ('power', tensor([0.9912], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4243], device='cuda:0')), ('power', tensor([-21.9871], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:1.0135878324508667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3175, 3.3319, 3.3197],
        [3.3175, 3.6795, 3.7236],
        [3.3175, 4.1857, 4.7559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.40985748171806335 
model_pd.l_d.mean(): -20.388673782348633 
model_pd.lagr.mean(): -19.978816986083984 
model_pd.lambdas: dict_items([('pout', tensor([1.0143], device='cuda:0')), ('power', tensor([0.9901], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3745], device='cuda:0')), ('power', tensor([-21.9737], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.40985748171806335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4413, 3.4563, 3.4436],
        [3.4413, 3.8191, 3.8648],
        [3.4413, 4.3493, 4.9446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.2839117646217346 
model_pd.l_d.mean(): -20.390657424926758 
model_pd.lagr.mean(): -20.10674476623535 
model_pd.lambdas: dict_items([('pout', tensor([1.0156], device='cuda:0')), ('power', tensor([0.9890], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3260], device='cuda:0')), ('power', tensor([-21.9523], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.2839117646217346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 4.5151, 5.1358],
        [3.5667, 3.9606, 4.0077],
        [3.5667, 3.5822, 3.5690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.2279852330684662 
model_pd.l_d.mean(): -20.384374618530273 
model_pd.lagr.mean(): -20.156389236450195 
model_pd.lambdas: dict_items([('pout', tensor([1.0169], device='cuda:0')), ('power', tensor([0.9879], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2787], device='cuda:0')), ('power', tensor([-21.9235], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.2279852330684662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6937, 3.7099, 3.6962],
        [3.6937, 4.6831, 5.3296],
        [3.6937, 4.1039, 4.1527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.1957378387451172 
model_pd.l_d.mean(): -20.370370864868164 
model_pd.lagr.mean(): -20.174633026123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0181], device='cuda:0')), ('power', tensor([0.9868], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2324], device='cuda:0')), ('power', tensor([-21.8877], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.1957378387451172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8226, 4.2493, 4.2996],
        [3.8226, 3.8394, 3.8251],
        [3.8226, 4.8534, 5.5260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.17442083358764648 
model_pd.l_d.mean(): -20.34912109375 
model_pd.lagr.mean(): -20.174699783325195 
model_pd.lambdas: dict_items([('pout', tensor([1.0193], device='cuda:0')), ('power', tensor([0.9857], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1871], device='cuda:0')), ('power', tensor([-21.8452], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.17442083358764648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9503, 4.3934, 4.4452],
        [3.9503, 5.0223, 5.7207],
        [3.9503, 3.9677, 3.9529]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.15939699113368988 
model_pd.l_d.mean(): -20.321186065673828 
model_pd.lagr.mean(): -20.161788940429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0205], device='cuda:0')), ('power', tensor([0.9847], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1437], device='cuda:0')), ('power', tensor([-21.7977], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.15939699113368988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0717, 4.5303, 4.5837],
        [4.0717, 5.1827, 5.9058],
        [4.0717, 4.0897, 4.0744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.14854399859905243 
model_pd.l_d.mean(): -20.287830352783203 
model_pd.lagr.mean(): -20.139286041259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0216], device='cuda:0')), ('power', tensor([0.9836], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1038], device='cuda:0')), ('power', tensor([-21.7478], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.14854399859905243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1817, 4.6544, 4.7091],
        [4.1817, 4.2003, 4.1845],
        [4.1817, 5.3282, 6.0734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.14064103364944458 
model_pd.l_d.mean(): -20.251005172729492 
model_pd.lagr.mean(): -20.11036491394043 
model_pd.lambdas: dict_items([('pout', tensor([1.0226], device='cuda:0')), ('power', tensor([0.9825], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0686], device='cuda:0')), ('power', tensor([-21.6991], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.14064103364944458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2761, 4.2951, 4.2790],
        [4.2761, 5.4528, 6.2171],
        [4.2761, 4.7608, 4.8167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.13493435084819794 
model_pd.l_d.mean(): -20.212968826293945 
model_pd.lagr.mean(): -20.078035354614258 
model_pd.lambdas: dict_items([('pout', tensor([1.0237], device='cuda:0')), ('power', tensor([0.9814], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0391], device='cuda:0')), ('power', tensor([-21.6549], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.13493435084819794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3517, 5.5524, 6.3318],
        [4.3517, 4.3710, 4.3546],
        [4.3517, 4.8459, 4.9026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.13092556595802307 
model_pd.l_d.mean(): -20.17585563659668 
model_pd.lagr.mean(): -20.04492950439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0247], device='cuda:0')), ('power', tensor([0.9803], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0161], device='cuda:0')), ('power', tensor([-21.6180], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.13092556595802307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4067, 4.9079, 4.9652],
        [4.4067, 5.6250, 6.4153],
        [4.4067, 4.4263, 4.4097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.12826406955718994 
model_pd.l_d.mean(): -20.14130210876465 
model_pd.lagr.mean(): -20.013038635253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0257], device='cuda:0')), ('power', tensor([0.9792], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9995], device='cuda:0')), ('power', tensor([-21.5903], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.12826406955718994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4414, 4.4611, 4.4443],
        [4.4414, 5.6704, 6.4674],
        [4.4414, 4.9467, 5.0044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.12669283151626587 
model_pd.l_d.mean(): -20.110261917114258 
model_pd.lagr.mean(): -19.98356819152832 
model_pd.lambdas: dict_items([('pout', tensor([1.0267], device='cuda:0')), ('power', tensor([0.9782], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9892], device='cuda:0')), ('power', tensor([-21.5727], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.12669283151626587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4567, 5.6903, 6.4901],
        [4.4567, 4.4765, 4.4597],
        [4.4567, 4.9638, 5.0217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.12602221965789795 
model_pd.l_d.mean(): -20.083030700683594 
model_pd.lagr.mean(): -19.957008361816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0277], device='cuda:0')), ('power', tensor([0.9771], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9847], device='cuda:0')), ('power', tensor([-21.5649], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.12602221965789795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4548, 5.6874, 6.4864],
        [4.4548, 4.9614, 5.0192],
        [4.4548, 4.4746, 4.4578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.12611299753189087 
model_pd.l_d.mean(): -20.05937385559082 
model_pd.lagr.mean(): -19.933259963989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0286], device='cuda:0')), ('power', tensor([0.9760], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9853], device='cuda:0')), ('power', tensor([-21.5661], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.12611299753189087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4380, 4.9422, 4.9997],
        [4.4380, 5.6647, 6.4600],
        [4.4380, 4.4577, 4.4410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.1268644630908966 
model_pd.l_d.mean(): -20.03873634338379 
model_pd.lagr.mean(): -19.9118709564209 
model_pd.lambdas: dict_items([('pout', tensor([1.0296], device='cuda:0')), ('power', tensor([0.9749], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9904], device='cuda:0')), ('power', tensor([-21.5751], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.1268644630908966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4089, 4.4284, 4.4118],
        [4.4089, 5.6257, 6.4147],
        [4.4089, 4.9091, 4.9661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.1282038390636444 
model_pd.l_d.mean(): -20.020374298095703 
model_pd.lagr.mean(): -19.892169952392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0306], device='cuda:0')), ('power', tensor([0.9738], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9991], device='cuda:0')), ('power', tensor([-21.5904], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.1282038390636444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3699, 4.3893, 4.3728],
        [4.3699, 5.5737, 6.3544],
        [4.3699, 4.8648, 4.9212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.1300772726535797 
model_pd.l_d.mean(): -20.00347900390625 
model_pd.lagr.mean(): -19.873401641845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0316], device='cuda:0')), ('power', tensor([0.9728], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0109], device='cuda:0')), ('power', tensor([-21.6104], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.1300772726535797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3236, 4.8122, 4.8680],
        [4.3236, 5.5120, 6.2828],
        [4.3236, 4.3427, 4.3265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.13244113326072693 
model_pd.l_d.mean(): -19.987268447875977 
model_pd.lagr.mean(): -19.854827880859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0327], device='cuda:0')), ('power', tensor([0.9717], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0250], device='cuda:0')), ('power', tensor([-21.6338], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.13244113326072693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2723, 5.4436, 6.2036],
        [4.2723, 4.7540, 4.8091],
        [4.2723, 4.2911, 4.2751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.13525338470935822 
model_pd.l_d.mean(): -19.97105598449707 
model_pd.lagr.mean(): -19.83580207824707 
model_pd.lambdas: dict_items([('pout', tensor([1.0337], device='cuda:0')), ('power', tensor([0.9706], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0408], device='cuda:0')), ('power', tensor([-21.6591], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.13525338470935822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2182, 4.2367, 4.2210],
        [4.2182, 5.3716, 6.1202],
        [4.2182, 4.6927, 4.7470]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.1384643167257309 
model_pd.l_d.mean(): -19.954261779785156 
model_pd.lagr.mean(): -19.815797805786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0348], device='cuda:0')), ('power', tensor([0.9695], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0577], device='cuda:0')), ('power', tensor([-21.6850], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.1384643167257309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1636, 4.6307, 4.6843],
        [4.1636, 4.1818, 4.1663],
        [4.1636, 5.2989, 6.0359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.14200575649738312 
model_pd.l_d.mean(): -19.93647003173828 
model_pd.lagr.mean(): -19.794464111328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0358], device='cuda:0')), ('power', tensor([0.9684], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0749], device='cuda:0')), ('power', tensor([-21.7105], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.14200575649738312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1105, 4.1285, 4.1132],
        [4.1105, 5.2281, 5.9540],
        [4.1105, 4.5705, 4.6234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.1457798182964325 
model_pd.l_d.mean(): -19.91742515563965 
model_pd.lagr.mean(): -19.771644592285156 
model_pd.lambdas: dict_items([('pout', tensor([1.0369], device='cuda:0')), ('power', tensor([0.9673], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0919], device='cuda:0')), ('power', tensor([-21.7346], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.1457798182964325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0609, 5.1620, 5.8774],
        [4.0609, 4.0787, 4.0636],
        [4.0609, 4.5143, 4.5664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.14964954555034637 
model_pd.l_d.mean(): -19.89703369140625 
model_pd.lagr.mean(): -19.747385025024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0380], device='cuda:0')), ('power', tensor([0.9663], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1080], device='cuda:0')), ('power', tensor([-21.7564], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.14964954555034637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0167, 4.0342, 4.0193],
        [4.0167, 5.1029, 5.8088],
        [4.0167, 4.4640, 4.5155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.15343353152275085 
model_pd.l_d.mean(): -19.8753604888916 
model_pd.lagr.mean(): -19.721927642822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0392], device='cuda:0')), ('power', tensor([0.9652], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1226], device='cuda:0')), ('power', tensor([-21.7753], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.15343353152275085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9793, 5.0529, 5.7507],
        [3.9793, 3.9966, 3.9819],
        [3.9793, 4.4214, 4.4724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.15691153705120087 
model_pd.l_d.mean(): -19.85259437561035 
model_pd.lagr.mean(): -19.695682525634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0403], device='cuda:0')), ('power', tensor([0.9641], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1350], device='cuda:0')), ('power', tensor([-21.7910], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.15691153705120087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9500, 5.0135, 5.7048],
        [3.9500, 3.9671, 3.9526],
        [3.9500, 4.3880, 4.4384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.1598462164402008 
model_pd.l_d.mean(): -19.828981399536133 
model_pd.lagr.mean(): -19.66913604736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0414], device='cuda:0')), ('power', tensor([0.9630], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1448], device='cuda:0')), ('power', tensor([-21.8031], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.1598462164402008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9295, 4.9858, 5.6724],
        [3.9295, 3.9466, 3.9321],
        [3.9295, 4.3645, 4.4146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.16201910376548767 
model_pd.l_d.mean(): -19.804794311523438 
model_pd.lagr.mean(): -19.64277458190918 
model_pd.lambdas: dict_items([('pout', tensor([1.0426], device='cuda:0')), ('power', tensor([0.9619], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1518], device='cuda:0')), ('power', tensor([-21.8115], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.16201910376548767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9183, 4.3514, 4.4013],
        [3.9183, 3.9353, 3.9209],
        [3.9183, 4.9703, 5.6540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.1632746458053589 
model_pd.l_d.mean(): -19.780254364013672 
model_pd.lagr.mean(): -19.616979598999023 
model_pd.lambdas: dict_items([('pout', tensor([1.0438], device='cuda:0')), ('power', tensor([0.9608], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1556], device='cuda:0')), ('power', tensor([-21.8163], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.1632746458053589
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9161, 3.9331, 3.9187],
        [3.9161, 4.9667, 5.6493],
        [3.9161, 4.3485, 4.3982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.16355502605438232 
model_pd.l_d.mean(): -19.755516052246094 
model_pd.lagr.mean(): -19.591960906982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0449], device='cuda:0')), ('power', tensor([0.9597], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1565], device='cuda:0')), ('power', tensor([-21.8176], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.16355502605438232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9224, 4.9742, 5.6574],
        [3.9224, 4.3551, 4.4048],
        [3.9224, 3.9393, 3.9249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.16291016340255737 
model_pd.l_d.mean(): -19.73063087463379 
model_pd.lagr.mean(): -19.567720413208008 
model_pd.lambdas: dict_items([('pout', tensor([1.0461], device='cuda:0')), ('power', tensor([0.9586], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1545], device='cuda:0')), ('power', tensor([-21.8157], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.16291016340255737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9360, 4.9915, 5.6769],
        [3.9360, 3.9530, 3.9386],
        [3.9360, 4.3701, 4.4198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.16148138046264648 
model_pd.l_d.mean(): -19.705589294433594 
model_pd.lagr.mean(): -19.54410743713379 
model_pd.lambdas: dict_items([('pout', tensor([1.0472], device='cuda:0')), ('power', tensor([0.9575], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1500], device='cuda:0')), ('power', tensor([-21.8109], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.16148138046264648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9558, 5.0170, 5.7056],
        [3.9558, 4.3919, 4.4418],
        [3.9558, 3.9729, 3.9584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.15946604311466217 
model_pd.l_d.mean(): -19.680328369140625 
model_pd.lagr.mean(): -19.520862579345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0484], device='cuda:0')), ('power', tensor([0.9564], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1434], device='cuda:0')), ('power', tensor([-21.8035], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.15946604311466217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9804, 3.9975, 3.9829],
        [3.9804, 4.4191, 4.4691],
        [3.9804, 5.0486, 5.7416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.1570781171321869 
model_pd.l_d.mean(): -19.6547908782959 
model_pd.lagr.mean(): -19.497713088989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0495], device='cuda:0')), ('power', tensor([0.9554], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1353], device='cuda:0')), ('power', tensor([-21.7942], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.1570781171321869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0081, 5.0845, 5.7823],
        [4.0081, 4.0253, 4.0107],
        [4.0081, 4.4499, 4.5001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.15451796352863312 
model_pd.l_d.mean(): -19.62893295288086 
model_pd.lagr.mean(): -19.474414825439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0506], device='cuda:0')), ('power', tensor([0.9543], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1263], device='cuda:0')), ('power', tensor([-21.7834], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.15451796352863312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0374, 4.0548, 4.0401],
        [4.0374, 4.4826, 4.5329],
        [4.0374, 5.1225, 5.8256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.15195423364639282 
model_pd.l_d.mean(): -19.602758407592773 
model_pd.lagr.mean(): -19.450803756713867 
model_pd.lambdas: dict_items([('pout', tensor([1.0517], device='cuda:0')), ('power', tensor([0.9532], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1167], device='cuda:0')), ('power', tensor([-21.7716], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.15195423364639282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0670, 5.1608, 5.8691],
        [4.0670, 4.0844, 4.0696],
        [4.0670, 4.5154, 4.5660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.14951744675636292 
model_pd.l_d.mean(): -19.57633399963379 
model_pd.lagr.mean(): -19.426816940307617 
model_pd.lambdas: dict_items([('pout', tensor([1.0528], device='cuda:0')), ('power', tensor([0.9521], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1072], device='cuda:0')), ('power', tensor([-21.7596], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.14951744675636292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0954, 4.1129, 4.0980],
        [4.0954, 4.5469, 4.5977],
        [4.0954, 5.1975, 5.9109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.1473015397787094 
model_pd.l_d.mean(): -19.549760818481445 
model_pd.lagr.mean(): -19.4024600982666 
model_pd.lambdas: dict_items([('pout', tensor([1.0539], device='cuda:0')), ('power', tensor([0.9510], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0981], device='cuda:0')), ('power', tensor([-21.7477], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.1473015397787094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1214, 4.5758, 4.6268],
        [4.1214, 4.1391, 4.1241],
        [4.1214, 5.2311, 5.9491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.14536914229393005 
model_pd.l_d.mean(): -19.523176193237305 
model_pd.lagr.mean(): -19.3778076171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0550], device='cuda:0')), ('power', tensor([0.9499], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0898], device='cuda:0')), ('power', tensor([-21.7367], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.14536914229393005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1441, 5.2603, 5.9822],
        [4.1441, 4.6009, 4.6520],
        [4.1441, 4.1619, 4.1468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.14375752210617065 
model_pd.l_d.mean(): -19.496728897094727 
model_pd.lagr.mean(): -19.35297203063965 
model_pd.lambdas: dict_items([('pout', tensor([1.0561], device='cuda:0')), ('power', tensor([0.9488], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0826], device='cuda:0')), ('power', tensor([-21.7271], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.14375752210617065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1628, 5.2842, 6.0091],
        [4.1628, 4.6215, 4.6727],
        [4.1628, 4.1806, 4.1655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.14248472452163696 
model_pd.l_d.mean(): -19.470556259155273 
model_pd.lagr.mean(): -19.32807159423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0572], device='cuda:0')), ('power', tensor([0.9477], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0768], device='cuda:0')), ('power', tensor([-21.7191], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.14248472452163696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1769, 4.6369, 4.6881],
        [4.1769, 4.1947, 4.1796],
        [4.1769, 5.3020, 6.0291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.14155423641204834 
model_pd.l_d.mean(): -19.444778442382812 
model_pd.lagr.mean(): -19.303224563598633 
model_pd.lambdas: dict_items([('pout', tensor([1.0583], device='cuda:0')), ('power', tensor([0.9467], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0724], device='cuda:0')), ('power', tensor([-21.7131], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.14155423641204834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1863, 4.2041, 4.1889],
        [4.1863, 5.3136, 6.0418],
        [4.1863, 4.6469, 4.6981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.14095835387706757 
model_pd.l_d.mean(): -19.419466018676758 
model_pd.lagr.mean(): -19.278507232666016 
model_pd.lambdas: dict_items([('pout', tensor([1.0593], device='cuda:0')), ('power', tensor([0.9456], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0696], device='cuda:0')), ('power', tensor([-21.7093], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.14095835387706757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1908, 5.3188, 6.0472],
        [4.1908, 4.6516, 4.7027],
        [4.1908, 4.2087, 4.1935]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.14068114757537842 
model_pd.l_d.mean(): -19.39464569091797 
model_pd.lagr.mean(): -19.253965377807617 
model_pd.lambdas: dict_items([('pout', tensor([1.0604], device='cuda:0')), ('power', tensor([0.9445], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0682], device='cuda:0')), ('power', tensor([-21.7077], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.14068114757537842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1909, 5.3181, 6.0458],
        [4.1909, 4.2087, 4.1936],
        [4.1909, 4.6512, 4.7021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.14069965481758118 
model_pd.l_d.mean(): -19.370296478271484 
model_pd.lagr.mean(): -19.229597091674805 
model_pd.lambdas: dict_items([('pout', tensor([1.0615], device='cuda:0')), ('power', tensor([0.9434], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0683], device='cuda:0')), ('power', tensor([-21.7081], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.14069965481758118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1870, 5.3121, 6.0382],
        [4.1870, 4.6462, 4.6970],
        [4.1870, 4.2047, 4.1896]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.14098456501960754 
model_pd.l_d.mean(): -19.346357345581055 
model_pd.lagr.mean(): -19.205371856689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0625], device='cuda:0')), ('power', tensor([0.9423], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0696], device='cuda:0')), ('power', tensor([-21.7104], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.14098456501960754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1796, 5.3015, 6.0254],
        [4.1796, 4.6374, 4.6879],
        [4.1796, 4.1973, 4.1822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.14150087535381317 
model_pd.l_d.mean(): -19.32274627685547 
model_pd.lagr.mean(): -19.181245803833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0636], device='cuda:0')), ('power', tensor([0.9412], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0720], device='cuda:0')), ('power', tensor([-21.7143], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.14150087535381317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1695, 4.6255, 4.6757],
        [4.1695, 4.1871, 4.1721],
        [4.1695, 5.2873, 6.0085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1422078013420105 
model_pd.l_d.mean(): -19.299360275268555 
model_pd.lagr.mean(): -19.15715217590332 
model_pd.lambdas: dict_items([('pout', tensor([1.0647], device='cuda:0')), ('power', tensor([0.9401], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0753], device='cuda:0')), ('power', tensor([-21.7195], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.1422078013420105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1575, 4.1750, 4.1601],
        [4.1575, 4.6115, 4.6614],
        [4.1575, 5.2707, 5.9887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.14305895566940308 
model_pd.l_d.mean(): -19.27608299255371 
model_pd.lagr.mean(): -19.133024215698242 
model_pd.lambdas: dict_items([('pout', tensor([1.0658], device='cuda:0')), ('power', tensor([0.9391], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0792], device='cuda:0')), ('power', tensor([-21.7255], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.14305895566940308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1445, 4.1619, 4.1471],
        [4.1445, 4.5963, 4.6459],
        [4.1445, 5.2527, 5.9673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.14400264620780945 
model_pd.l_d.mean(): -19.252826690673828 
model_pd.lagr.mean(): -19.108823776245117 
model_pd.lambdas: dict_items([('pout', tensor([1.0669], device='cuda:0')), ('power', tensor([0.9380], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0835], device='cuda:0')), ('power', tensor([-21.7319], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.14400264620780945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1313, 4.5809, 4.6302],
        [4.1313, 4.1486, 4.1339],
        [4.1313, 5.2344, 5.9455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.14498333632946014 
model_pd.l_d.mean(): -19.2294979095459 
model_pd.lagr.mean(): -19.084514617919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0679], device='cuda:0')), ('power', tensor([0.9369], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0878], device='cuda:0')), ('power', tensor([-21.7384], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.14498333632946014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1187, 4.5661, 4.6151],
        [4.1187, 5.2169, 5.9247],
        [4.1187, 4.1360, 4.1213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.14594347774982452 
model_pd.l_d.mean(): -19.206043243408203 
model_pd.lagr.mean(): -19.060100555419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0690], device='cuda:0')), ('power', tensor([0.9358], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0919], device='cuda:0')), ('power', tensor([-21.7445], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.14594347774982452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1074, 4.5529, 4.6016],
        [4.1074, 5.2011, 5.9059],
        [4.1074, 4.1246, 4.1100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.14682632684707642 
model_pd.l_d.mean(): -19.182415008544922 
model_pd.lagr.mean(): -19.03558921813965 
model_pd.lambdas: dict_items([('pout', tensor([1.0701], device='cuda:0')), ('power', tensor([0.9347], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0957], device='cuda:0')), ('power', tensor([-21.7501], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.14682632684707642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0981, 4.1152, 4.1006],
        [4.0981, 4.5418, 4.5902],
        [4.0981, 5.1878, 5.8900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.1475798338651657 
model_pd.l_d.mean(): -19.158594131469727 
model_pd.lagr.mean(): -19.011014938354492 
model_pd.lambdas: dict_items([('pout', tensor([1.0712], device='cuda:0')), ('power', tensor([0.9336], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0988], device='cuda:0')), ('power', tensor([-21.7548], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.1475798338651657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0911, 4.1081, 4.0936],
        [4.0911, 4.5333, 4.5815],
        [4.0911, 5.1777, 5.8777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.14816047251224518 
model_pd.l_d.mean(): -19.134580612182617 
model_pd.lagr.mean(): -18.986419677734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0723], device='cuda:0')), ('power', tensor([0.9325], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1012], device='cuda:0')), ('power', tensor([-21.7584], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.14816047251224518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0867, 5.1711, 5.8694],
        [4.0867, 4.1037, 4.0893],
        [4.0867, 4.5279, 4.5759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.14853741228580475 
model_pd.l_d.mean(): -19.110374450683594 
model_pd.lagr.mean(): -18.961837768554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0734], device='cuda:0')), ('power', tensor([0.9314], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1027], device='cuda:0')), ('power', tensor([-21.7609], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.14853741228580475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0852, 4.5256, 4.5734],
        [4.0852, 4.1021, 4.0877],
        [4.0852, 5.1682, 5.8653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.14869479835033417 
model_pd.l_d.mean(): -19.08599281311035 
model_pd.lagr.mean(): -18.937297821044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0745], device='cuda:0')), ('power', tensor([0.9304], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1033], device='cuda:0')), ('power', tensor([-21.7621], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.14869479835033417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0864, 5.1688, 5.8654],
        [4.0864, 4.1033, 4.0890],
        [4.0864, 4.5264, 4.5740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.14863312244415283 
model_pd.l_d.mean(): -19.061445236206055 
model_pd.lagr.mean(): -18.912811279296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0756], device='cuda:0')), ('power', tensor([0.9293], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1030], device='cuda:0')), ('power', tensor([-21.7622], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.14863312244415283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0902, 5.1729, 5.8693],
        [4.0902, 4.1071, 4.0927],
        [4.0902, 4.5301, 4.5776]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.14836831390857697 
model_pd.l_d.mean(): -19.0367488861084 
model_pd.lagr.mean(): -18.88838005065918 
model_pd.lambdas: dict_items([('pout', tensor([1.0767], device='cuda:0')), ('power', tensor([0.9282], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1019], device='cuda:0')), ('power', tensor([-21.7611], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.14836831390857697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0962, 4.5363, 4.5837],
        [4.0962, 4.1131, 4.0987],
        [4.0962, 5.1799, 5.8767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14792969822883606 
model_pd.l_d.mean(): -19.011924743652344 
model_pd.lagr.mean(): -18.863994598388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0778], device='cuda:0')), ('power', tensor([0.9271], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1001], device='cuda:0')), ('power', tensor([-21.7590], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.14792969822883606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1040, 4.1209, 4.1066],
        [4.1040, 4.5445, 4.5918],
        [4.1040, 5.1893, 5.8868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.14735540747642517 
model_pd.l_d.mean(): -18.986984252929688 
model_pd.lagr.mean(): -18.839628219604492 
model_pd.lambdas: dict_items([('pout', tensor([1.0789], device='cuda:0')), ('power', tensor([0.9260], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0977], device='cuda:0')), ('power', tensor([-21.7562], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.14735540747642517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1132, 5.2004, 5.8989],
        [4.1132, 4.5542, 4.6014],
        [4.1132, 4.1301, 4.1157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.14668916165828705 
model_pd.l_d.mean(): -18.961959838867188 
model_pd.lagr.mean(): -18.815271377563477 
model_pd.lambdas: dict_items([('pout', tensor([1.0800], device='cuda:0')), ('power', tensor([0.9249], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0948], device='cuda:0')), ('power', tensor([-21.7527], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.14668916165828705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1231, 5.2125, 5.9121],
        [4.1231, 4.1400, 4.1256],
        [4.1231, 4.5647, 4.6119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.1459759771823883 
model_pd.l_d.mean(): -18.936874389648438 
model_pd.lagr.mean(): -18.7908992767334 
model_pd.lambdas: dict_items([('pout', tensor([1.0811], device='cuda:0')), ('power', tensor([0.9238], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0918], device='cuda:0')), ('power', tensor([-21.7489], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.1459759771823883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1332, 4.1501, 4.1357],
        [4.1332, 5.2249, 5.9256],
        [4.1332, 4.5755, 4.6226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.14525842666625977 
model_pd.l_d.mean(): -18.911758422851562 
model_pd.lagr.mean(): -18.76650047302246 
model_pd.lambdas: dict_items([('pout', tensor([1.0822], device='cuda:0')), ('power', tensor([0.9227], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0886], device='cuda:0')), ('power', tensor([-21.7449], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.14525842666625977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1430, 5.2369, 5.9387],
        [4.1430, 4.5860, 4.6330],
        [4.1430, 4.1600, 4.1456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.14457447826862335 
model_pd.l_d.mean(): -18.88665771484375 
model_pd.lagr.mean(): -18.742082595825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0833], device='cuda:0')), ('power', tensor([0.9217], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0856], device='cuda:0')), ('power', tensor([-21.7411], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.14457447826862335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1521, 4.1691, 4.1546],
        [4.1521, 4.5956, 4.6426],
        [4.1521, 5.2479, 5.9507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.14395573735237122 
model_pd.l_d.mean(): -18.861604690551758 
model_pd.lagr.mean(): -18.717649459838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0844], device='cuda:0')), ('power', tensor([0.9206], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0828], device='cuda:0')), ('power', tensor([-21.7376], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.14395573735237122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1600, 4.6039, 4.6508],
        [4.1600, 5.2574, 5.9609],
        [4.1600, 4.1770, 4.1626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.14342646300792694 
model_pd.l_d.mean(): -18.836627960205078 
model_pd.lagr.mean(): -18.693201065063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0855], device='cuda:0')), ('power', tensor([0.9195], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0804], device='cuda:0')), ('power', tensor([-21.7345], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.14342646300792694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1665, 4.6106, 4.6574],
        [4.1665, 4.1835, 4.1691],
        [4.1665, 5.2650, 5.9689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.1430032104253769 
model_pd.l_d.mean(): -18.81175994873047 
model_pd.lagr.mean(): -18.66875648498535 
model_pd.lambdas: dict_items([('pout', tensor([1.0865], device='cuda:0')), ('power', tensor([0.9184], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0785], device='cuda:0')), ('power', tensor([-21.7322], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.1430032104253769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1714, 4.1884, 4.1740],
        [4.1714, 4.6155, 4.6622],
        [4.1714, 5.2705, 5.9745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.14269496500492096 
model_pd.l_d.mean(): -18.78702163696289 
model_pd.lagr.mean(): -18.64432716369629 
model_pd.lambdas: dict_items([('pout', tensor([1.0876], device='cuda:0')), ('power', tensor([0.9173], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0770], device='cuda:0')), ('power', tensor([-21.7305], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.14269496500492096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1747, 4.6186, 4.6650],
        [4.1747, 5.2738, 5.9775],
        [4.1747, 4.1916, 4.1772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.14250366389751434 
model_pd.l_d.mean(): -18.76241683959961 
model_pd.lagr.mean(): -18.61991310119629 
model_pd.lambdas: dict_items([('pout', tensor([1.0887], device='cuda:0')), ('power', tensor([0.9162], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0761], device='cuda:0')), ('power', tensor([-21.7296], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.14250366389751434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1763, 5.2750, 5.9781],
        [4.1763, 4.6198, 4.6661],
        [4.1763, 4.1931, 4.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.142424538731575 
model_pd.l_d.mean(): -18.73794174194336 
model_pd.lagr.mean(): -18.595518112182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0898], device='cuda:0')), ('power', tensor([0.9151], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0757], device='cuda:0')), ('power', tensor([-21.7295], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.142424538731575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1764, 5.2742, 5.9764],
        [4.1764, 4.1932, 4.1789],
        [4.1764, 4.6193, 4.6654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.1424470841884613 
model_pd.l_d.mean(): -18.713590621948242 
model_pd.lagr.mean(): -18.571144104003906 
model_pd.lambdas: dict_items([('pout', tensor([1.0908], device='cuda:0')), ('power', tensor([0.9141], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0758], device='cuda:0')), ('power', tensor([-21.7300], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.1424470841884613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1753, 5.2717, 5.9729],
        [4.1753, 4.6174, 4.6633],
        [4.1753, 4.1921, 4.1778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.14255553483963013 
model_pd.l_d.mean(): -18.689329147338867 
model_pd.lagr.mean(): -18.54677391052246 
model_pd.lambdas: dict_items([('pout', tensor([1.0919], device='cuda:0')), ('power', tensor([0.9130], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0763], device='cuda:0')), ('power', tensor([-21.7311], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.14255553483963013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1732, 4.6145, 4.6602],
        [4.1732, 5.2680, 5.9679],
        [4.1732, 4.1900, 4.1757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.14273014664649963 
model_pd.l_d.mean(): -18.66514015197754 
model_pd.lagr.mean(): -18.522409439086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0930], device='cuda:0')), ('power', tensor([0.9119], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0771], device='cuda:0')), ('power', tensor([-21.7327], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.14273014664649963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1705, 4.1872, 4.1730],
        [4.1705, 4.6108, 4.6563],
        [4.1705, 5.2635, 5.9619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.1429484784603119 
model_pd.l_d.mean(): -18.640993118286133 
model_pd.lagr.mean(): -18.498044967651367 
model_pd.lambdas: dict_items([('pout', tensor([1.0941], device='cuda:0')), ('power', tensor([0.9108], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0780], device='cuda:0')), ('power', tensor([-21.7345], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.1429484784603119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1676, 4.1842, 4.1701],
        [4.1676, 5.2586, 5.9556],
        [4.1676, 4.6069, 4.6522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.14318645000457764 
model_pd.l_d.mean(): -18.616849899291992 
model_pd.lagr.mean(): -18.473663330078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0952], device='cuda:0')), ('power', tensor([0.9097], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0791], device='cuda:0')), ('power', tensor([-21.7365], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.14318645000457764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1647, 5.2539, 5.9493],
        [4.1647, 4.6030, 4.6481],
        [4.1647, 4.1813, 4.1672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.1434197723865509 
model_pd.l_d.mean(): -18.592695236206055 
model_pd.lagr.mean(): -18.449275970458984 
model_pd.lambdas: dict_items([('pout', tensor([1.0962], device='cuda:0')), ('power', tensor([0.9086], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0801], device='cuda:0')), ('power', tensor([-21.7384], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.1434197723865509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1623, 4.5997, 4.6445],
        [4.1623, 4.1788, 4.1647],
        [4.1623, 5.2497, 5.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.1436259150505066 
model_pd.l_d.mean(): -18.568500518798828 
model_pd.lagr.mean(): -18.424875259399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0973], device='cuda:0')), ('power', tensor([0.9075], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0810], device='cuda:0')), ('power', tensor([-21.7402], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.1436259150505066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1605, 5.2463, 5.9391],
        [4.1605, 4.1770, 4.1630],
        [4.1605, 4.5970, 4.6417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.14378544688224792 
model_pd.l_d.mean(): -18.544252395629883 
model_pd.lagr.mean(): -18.400466918945312 
model_pd.lambdas: dict_items([('pout', tensor([1.0984], device='cuda:0')), ('power', tensor([0.9064], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0817], device='cuda:0')), ('power', tensor([-21.7416], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.14378544688224792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1596, 5.2441, 5.9358],
        [4.1596, 4.1761, 4.1621],
        [4.1596, 4.5954, 4.6398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.14388307929039001 
model_pd.l_d.mean(): -18.519943237304688 
model_pd.lagr.mean(): -18.376060485839844 
model_pd.lambdas: dict_items([('pout', tensor([1.0995], device='cuda:0')), ('power', tensor([0.9054], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0821], device='cuda:0')), ('power', tensor([-21.7426], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.14388307929039001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1597, 4.1762, 4.1622],
        [4.1597, 5.2433, 5.9341],
        [4.1597, 4.5949, 4.6391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.1439093053340912 
model_pd.l_d.mean(): -18.495561599731445 
model_pd.lagr.mean(): -18.351652145385742 
model_pd.lambdas: dict_items([('pout', tensor([1.1006], device='cuda:0')), ('power', tensor([0.9043], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0822], device='cuda:0')), ('power', tensor([-21.7432], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.1439093053340912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1609, 5.2438, 5.9339],
        [4.1609, 4.1773, 4.1633],
        [4.1609, 4.5955, 4.6396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.14386038482189178 
model_pd.l_d.mean(): -18.471113204956055 
model_pd.lagr.mean(): -18.327253341674805 
model_pd.lambdas: dict_items([('pout', tensor([1.1016], device='cuda:0')), ('power', tensor([0.9032], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0819], device='cuda:0')), ('power', tensor([-21.7433], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.14386038482189178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1631, 5.2457, 5.9352],
        [4.1631, 4.1795, 4.1655],
        [4.1631, 4.5974, 4.6412]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.14373862743377686 
model_pd.l_d.mean(): -18.446596145629883 
model_pd.lagr.mean(): -18.302858352661133 
model_pd.lambdas: dict_items([('pout', tensor([1.1027], device='cuda:0')), ('power', tensor([0.9021], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0814], device='cuda:0')), ('power', tensor([-21.7429], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.14373862743377686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1662, 4.1825, 4.1686],
        [4.1662, 4.6002, 4.6439],
        [4.1662, 5.2488, 5.9380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.14355185627937317 
model_pd.l_d.mean(): -18.42201805114746 
model_pd.lagr.mean(): -18.278465270996094 
model_pd.lambdas: dict_items([('pout', tensor([1.1038], device='cuda:0')), ('power', tensor([0.9010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0805], device='cuda:0')), ('power', tensor([-21.7421], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.14355185627937317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1701, 4.6039, 4.6475],
        [4.1701, 5.2529, 5.9419],
        [4.1701, 4.1864, 4.1725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.14331215620040894 
model_pd.l_d.mean(): -18.39739418029785 
model_pd.lagr.mean(): -18.25408172607422 
model_pd.lambdas: dict_items([('pout', tensor([1.1049], device='cuda:0')), ('power', tensor([0.8999], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0794], device='cuda:0')), ('power', tensor([-21.7410], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.14331215620040894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1745, 5.2577, 5.9467],
        [4.1745, 4.1908, 4.1769],
        [4.1745, 4.6083, 4.6516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.14303499460220337 
model_pd.l_d.mean(): -18.37273406982422 
model_pd.lagr.mean(): -18.229698181152344 
model_pd.lambdas: dict_items([('pout', tensor([1.1060], device='cuda:0')), ('power', tensor([0.8988], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0781], device='cuda:0')), ('power', tensor([-21.7396], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.14303499460220337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1793, 5.2630, 5.9520],
        [4.1793, 4.6130, 4.6562],
        [4.1793, 4.1956, 4.1817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.14273735880851746 
model_pd.l_d.mean(): -18.348054885864258 
model_pd.lagr.mean(): -18.205318450927734 
model_pd.lambdas: dict_items([('pout', tensor([1.1070], device='cuda:0')), ('power', tensor([0.8977], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0767], device='cuda:0')), ('power', tensor([-21.7380], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.14273735880851746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1841, 4.2004, 4.1865],
        [4.1841, 5.2683, 5.9573],
        [4.1841, 4.6178, 4.6608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.142436683177948 
model_pd.l_d.mean(): -18.3233699798584 
model_pd.lagr.mean(): -18.180932998657227 
model_pd.lambdas: dict_items([('pout', tensor([1.1081], device='cuda:0')), ('power', tensor([0.8967], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0753], device='cuda:0')), ('power', tensor([-21.7364], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.142436683177948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1888, 4.6224, 4.6653],
        [4.1888, 5.2735, 5.9625],
        [4.1888, 4.2050, 4.1912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.14214900135993958 
model_pd.l_d.mean(): -18.298694610595703 
model_pd.lagr.mean(): -18.156545639038086 
model_pd.lambdas: dict_items([('pout', tensor([1.1092], device='cuda:0')), ('power', tensor([0.8956], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0740], device='cuda:0')), ('power', tensor([-21.7348], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.14214900135993958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1931, 4.2093, 4.1955],
        [4.1931, 4.6266, 4.6693],
        [4.1931, 5.2781, 5.9670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.14188842475414276 
model_pd.l_d.mean(): -18.274049758911133 
model_pd.lagr.mean(): -18.13216209411621 
model_pd.lambdas: dict_items([('pout', tensor([1.1103], device='cuda:0')), ('power', tensor([0.8945], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0727], device='cuda:0')), ('power', tensor([-21.7334], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.14188842475414276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1968, 4.2131, 4.1993],
        [4.1968, 5.2821, 5.9708],
        [4.1968, 4.6301, 4.6727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.14166571199893951 
model_pd.l_d.mean(): -18.249441146850586 
model_pd.lagr.mean(): -18.10777473449707 
model_pd.lambdas: dict_items([('pout', tensor([1.1113], device='cuda:0')), ('power', tensor([0.8934], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0717], device='cuda:0')), ('power', tensor([-21.7323], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.14166571199893951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2000, 5.2852, 5.9736],
        [4.2000, 4.6330, 4.6754],
        [4.2000, 4.2161, 4.2024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.14148801565170288 
model_pd.l_d.mean(): -18.224882125854492 
model_pd.lagr.mean(): -18.08339500427246 
model_pd.lambdas: dict_items([('pout', tensor([1.1124], device='cuda:0')), ('power', tensor([0.8923], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0708], device='cuda:0')), ('power', tensor([-21.7314], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.14148801565170288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2023, 5.2873, 5.9752],
        [4.2023, 4.6350, 4.6772],
        [4.2023, 4.2185, 4.2047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.14136075973510742 
model_pd.l_d.mean(): -18.20037269592285 
model_pd.lagr.mean(): -18.059011459350586 
model_pd.lambdas: dict_items([('pout', tensor([1.1135], device='cuda:0')), ('power', tensor([0.8912], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0702], device='cuda:0')), ('power', tensor([-21.7309], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.14136075973510742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2039, 4.6361, 4.6782],
        [4.2039, 4.2200, 4.2063],
        [4.2039, 5.2884, 5.9757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.14128649234771729 
model_pd.l_d.mean(): -18.17591094970703 
model_pd.lagr.mean(): -18.034624099731445 
model_pd.lambdas: dict_items([('pout', tensor([1.1145], device='cuda:0')), ('power', tensor([0.8901], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0698], device='cuda:0')), ('power', tensor([-21.7308], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.14128649234771729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2047, 5.2885, 5.9751],
        [4.2047, 4.6365, 4.6783],
        [4.2047, 4.2208, 4.2071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.1412627100944519 
model_pd.l_d.mean(): -18.151504516601562 
model_pd.lagr.mean(): -18.010242462158203 
model_pd.lambdas: dict_items([('pout', tensor([1.1156], device='cuda:0')), ('power', tensor([0.8891], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0697], device='cuda:0')), ('power', tensor([-21.7310], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.1412627100944519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2049, 5.2878, 5.9735],
        [4.2049, 4.6360, 4.6777],
        [4.2049, 4.2209, 4.2072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.14128242433071136 
model_pd.l_d.mean(): -18.127134323120117 
model_pd.lagr.mean(): -17.985851287841797 
model_pd.lambdas: dict_items([('pout', tensor([1.1167], device='cuda:0')), ('power', tensor([0.8880], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0697], device='cuda:0')), ('power', tensor([-21.7315], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.14128242433071136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2045, 4.2206, 4.2069],
        [4.2045, 5.2864, 5.9711],
        [4.2045, 4.6351, 4.6765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.1413346827030182 
model_pd.l_d.mean(): -18.10279083251953 
model_pd.lagr.mean(): -17.961456298828125 
model_pd.lambdas: dict_items([('pout', tensor([1.1178], device='cuda:0')), ('power', tensor([0.8869], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0700], device='cuda:0')), ('power', tensor([-21.7323], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.1413346827030182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2039, 5.2846, 5.9684],
        [4.2039, 4.2199, 4.2063],
        [4.2039, 4.6338, 4.6750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.1414061337709427 
model_pd.l_d.mean(): -18.07846450805664 
model_pd.lagr.mean(): -17.937057495117188 
model_pd.lambdas: dict_items([('pout', tensor([1.1188], device='cuda:0')), ('power', tensor([0.8858], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0703], device='cuda:0')), ('power', tensor([-21.7331], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.1414061337709427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2032, 4.6324, 4.6735],
        [4.2032, 5.2828, 5.9655],
        [4.2032, 4.2192, 4.2056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.14148221909999847 
model_pd.l_d.mean(): -18.054136276245117 
model_pd.lagr.mean(): -17.912654876708984 
model_pd.lambdas: dict_items([('pout', tensor([1.1199], device='cuda:0')), ('power', tensor([0.8847], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0706], device='cuda:0')), ('power', tensor([-21.7341], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.14148221909999847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2027, 5.2811, 5.9629],
        [4.2027, 4.6312, 4.6721],
        [4.2027, 4.2186, 4.2050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.1415482759475708 
model_pd.l_d.mean(): -18.0297908782959 
model_pd.lagr.mean(): -17.888242721557617 
model_pd.lambdas: dict_items([('pout', tensor([1.1210], device='cuda:0')), ('power', tensor([0.8836], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0709], device='cuda:0')), ('power', tensor([-21.7349], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.1415482759475708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2025, 5.2800, 5.9608],
        [4.2025, 4.2184, 4.2048],
        [4.2025, 4.6304, 4.6711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.14159144461154938 
model_pd.l_d.mean(): -18.005416870117188 
model_pd.lagr.mean(): -17.86382484436035 
model_pd.lambdas: dict_items([('pout', tensor([1.1220], device='cuda:0')), ('power', tensor([0.8825], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0711], device='cuda:0')), ('power', tensor([-21.7356], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.14159144461154938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2028, 4.2186, 4.2051],
        [4.2028, 4.6301, 4.6706],
        [4.2028, 5.2794, 5.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.14160151779651642 
model_pd.l_d.mean(): -17.98101043701172 
model_pd.lagr.mean(): -17.83940887451172 
model_pd.lambdas: dict_items([('pout', tensor([1.1231], device='cuda:0')), ('power', tensor([0.8814], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0711], device='cuda:0')), ('power', tensor([-21.7360], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.14160151779651642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2036, 5.2796, 5.9590],
        [4.2036, 4.2195, 4.2060],
        [4.2036, 4.6305, 4.6708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.1415719985961914 
model_pd.l_d.mean(): -17.956560134887695 
model_pd.lagr.mean(): -17.814987182617188 
model_pd.lambdas: dict_items([('pout', tensor([1.1242], device='cuda:0')), ('power', tensor([0.8804], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0709], device='cuda:0')), ('power', tensor([-21.7362], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.1415719985961914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2051, 4.2209, 4.2075],
        [4.2051, 4.6316, 4.6718],
        [4.2051, 5.2807, 5.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.14150071144104004 
model_pd.l_d.mean(): -17.932065963745117 
model_pd.lagr.mean(): -17.790565490722656 
model_pd.lambdas: dict_items([('pout', tensor([1.1252], device='cuda:0')), ('power', tensor([0.8793], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0706], device='cuda:0')), ('power', tensor([-21.7360], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.14150071144104004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2072, 4.6333, 4.6734],
        [4.2072, 4.2230, 4.2095],
        [4.2072, 5.2825, 5.9609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.1413896232843399 
model_pd.l_d.mean(): -17.907535552978516 
model_pd.lagr.mean(): -17.766145706176758 
model_pd.lambdas: dict_items([('pout', tensor([1.1263], device='cuda:0')), ('power', tensor([0.8782], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0700], device='cuda:0')), ('power', tensor([-21.7356], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.1413896232843399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2098, 4.6357, 4.6755],
        [4.2098, 5.2850, 5.9631],
        [4.2098, 4.2255, 4.2121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.14124475419521332 
model_pd.l_d.mean(): -17.882970809936523 
model_pd.lagr.mean(): -17.74172592163086 
model_pd.lambdas: dict_items([('pout', tensor([1.1274], device='cuda:0')), ('power', tensor([0.8771], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0693], device='cuda:0')), ('power', tensor([-21.7349], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.14124475419521332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2127, 4.6384, 4.6781],
        [4.2127, 4.2285, 4.2151],
        [4.2127, 5.2881, 5.9659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.14107486605644226 
model_pd.l_d.mean(): -17.858381271362305 
model_pd.lagr.mean(): -17.71730613708496 
model_pd.lambdas: dict_items([('pout', tensor([1.1285], device='cuda:0')), ('power', tensor([0.8760], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0685], device='cuda:0')), ('power', tensor([-21.7340], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.14107486605644226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2159, 4.6414, 4.6810],
        [4.2159, 4.2316, 4.2182],
        [4.2159, 5.2914, 5.9690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.1408907175064087 
model_pd.l_d.mean(): -17.833778381347656 
model_pd.lagr.mean(): -17.692888259887695 
model_pd.lambdas: dict_items([('pout', tensor([1.1295], device='cuda:0')), ('power', tensor([0.8749], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0676], device='cuda:0')), ('power', tensor([-21.7331], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.1408907175064087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2192, 4.2349, 4.2215],
        [4.2192, 4.6445, 4.6839],
        [4.2192, 5.2948, 5.9722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.1407034695148468 
model_pd.l_d.mean(): -17.80916976928711 
model_pd.lagr.mean(): -17.668466567993164 
model_pd.lambdas: dict_items([('pout', tensor([1.1306], device='cuda:0')), ('power', tensor([0.8738], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0667], device='cuda:0')), ('power', tensor([-21.7321], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.1407034695148468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2223, 4.6475, 4.6867],
        [4.2223, 5.2980, 5.9753],
        [4.2223, 4.2380, 4.2246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.14052364230155945 
model_pd.l_d.mean(): -17.784570693969727 
model_pd.lagr.mean(): -17.644046783447266 
model_pd.lambdas: dict_items([('pout', tensor([1.1317], device='cuda:0')), ('power', tensor([0.8728], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0658], device='cuda:0')), ('power', tensor([-21.7311], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.14052364230155945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2252, 4.6502, 4.6893],
        [4.2252, 4.2409, 4.2275],
        [4.2252, 5.3010, 5.9780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.1403602659702301 
model_pd.l_d.mean(): -17.759986877441406 
model_pd.lagr.mean(): -17.619626998901367 
model_pd.lambdas: dict_items([('pout', tensor([1.1327], device='cuda:0')), ('power', tensor([0.8717], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0650], device='cuda:0')), ('power', tensor([-21.7302], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.1403602659702301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2277, 5.3034, 5.9801],
        [4.2277, 4.2434, 4.2300],
        [4.2277, 4.6524, 4.6914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.14022156596183777 
model_pd.l_d.mean(): -17.735427856445312 
model_pd.lagr.mean(): -17.59520721435547 
model_pd.lambdas: dict_items([('pout', tensor([1.1338], device='cuda:0')), ('power', tensor([0.8706], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0643], device='cuda:0')), ('power', tensor([-21.7296], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.14022156596183777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2298, 5.3053, 5.9816],
        [4.2298, 4.6542, 4.6930],
        [4.2298, 4.2454, 4.2321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.14011302590370178 
model_pd.l_d.mean(): -17.710893630981445 
model_pd.lagr.mean(): -17.57077980041504 
model_pd.lambdas: dict_items([('pout', tensor([1.1348], device='cuda:0')), ('power', tensor([0.8695], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0638], device='cuda:0')), ('power', tensor([-21.7291], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.14011302590370178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2314, 5.3065, 5.9824],
        [4.2314, 4.6555, 4.6941],
        [4.2314, 4.2470, 4.2337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.14003559947013855 
model_pd.l_d.mean(): -17.686391830444336 
model_pd.lagr.mean(): -17.546356201171875 
model_pd.lambdas: dict_items([('pout', tensor([1.1359], device='cuda:0')), ('power', tensor([0.8684], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0634], device='cuda:0')), ('power', tensor([-21.7289], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.14003559947013855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2325, 4.6562, 4.6947],
        [4.2325, 4.2481, 4.2348],
        [4.2325, 5.3072, 5.9825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.13998660445213318 
model_pd.l_d.mean(): -17.66191864013672 
model_pd.lagr.mean(): -17.52193260192871 
model_pd.lambdas: dict_items([('pout', tensor([1.1370], device='cuda:0')), ('power', tensor([0.8673], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0631], device='cuda:0')), ('power', tensor([-21.7288], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.13998660445213318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2334, 4.6566, 4.6949],
        [4.2334, 5.3074, 5.9821],
        [4.2334, 4.2489, 4.2356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.13996000587940216 
model_pd.l_d.mean(): -17.637468338012695 
model_pd.lagr.mean(): -17.497509002685547 
model_pd.lambdas: dict_items([('pout', tensor([1.1380], device='cuda:0')), ('power', tensor([0.8662], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0630], device='cuda:0')), ('power', tensor([-21.7289], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.13996000587940216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2340, 4.2495, 4.2362],
        [4.2340, 4.6567, 4.6949],
        [4.2340, 5.3074, 5.9813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.13994672894477844 
model_pd.l_d.mean(): -17.613027572631836 
model_pd.lagr.mean(): -17.473081588745117 
model_pd.lambdas: dict_items([('pout', tensor([1.1391], device='cuda:0')), ('power', tensor([0.8652], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0629], device='cuda:0')), ('power', tensor([-21.7292], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.13994672894477844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2345, 5.3073, 5.9806],
        [4.2345, 4.6568, 4.6948],
        [4.2345, 4.2500, 4.2368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.13993646204471588 
model_pd.l_d.mean(): -17.58858871459961 
model_pd.lagr.mean(): -17.448652267456055 
model_pd.lambdas: dict_items([('pout', tensor([1.1402], device='cuda:0')), ('power', tensor([0.8641], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0628], device='cuda:0')), ('power', tensor([-21.7294], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.13993646204471588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2352, 5.3073, 5.9800],
        [4.2352, 4.6570, 4.6948],
        [4.2352, 4.2506, 4.2375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.13991859555244446 
model_pd.l_d.mean(): -17.56414222717285 
model_pd.lagr.mean(): -17.424222946166992 
model_pd.lambdas: dict_items([('pout', tensor([1.1412], device='cuda:0')), ('power', tensor([0.8630], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0627], device='cuda:0')), ('power', tensor([-21.7296], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.13991859555244446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2361, 5.3077, 5.9797],
        [4.2361, 4.6575, 4.6952],
        [4.2361, 4.2515, 4.2384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.13988398015499115 
model_pd.l_d.mean(): -17.539676666259766 
model_pd.lagr.mean(): -17.39979362487793 
model_pd.lambdas: dict_items([('pout', tensor([1.1423], device='cuda:0')), ('power', tensor([0.8619], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0625], device='cuda:0')), ('power', tensor([-21.7297], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.13988398015499115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2374, 4.2528, 4.2397],
        [4.2374, 5.3085, 5.9801],
        [4.2374, 4.6584, 4.6959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.1398262232542038 
model_pd.l_d.mean(): -17.515188217163086 
model_pd.lagr.mean(): -17.375362396240234 
model_pd.lambdas: dict_items([('pout', tensor([1.1434], device='cuda:0')), ('power', tensor([0.8608], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0622], device='cuda:0')), ('power', tensor([-21.7296], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.1398262232542038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2391, 4.2545, 4.2414],
        [4.2391, 4.6598, 4.6971],
        [4.2391, 5.3099, 5.9810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.13974185287952423 
model_pd.l_d.mean(): -17.490676879882812 
model_pd.lagr.mean(): -17.350934982299805 
model_pd.lambdas: dict_items([('pout', tensor([1.1444], device='cuda:0')), ('power', tensor([0.8597], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0618], device='cuda:0')), ('power', tensor([-21.7293], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.13974185287952423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2412, 4.2565, 4.2435],
        [4.2412, 5.3119, 5.9826],
        [4.2412, 4.6616, 4.6988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.13963113725185394 
model_pd.l_d.mean(): -17.466135025024414 
model_pd.lagr.mean(): -17.32650375366211 
model_pd.lambdas: dict_items([('pout', tensor([1.1455], device='cuda:0')), ('power', tensor([0.8586], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0612], device='cuda:0')), ('power', tensor([-21.7288], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.13963113725185394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2437, 4.6638, 4.7009],
        [4.2437, 4.2590, 4.2459],
        [4.2437, 5.3143, 5.9848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.13949726521968842 
model_pd.l_d.mean(): -17.441574096679688 
model_pd.lagr.mean(): -17.30207633972168 
model_pd.lambdas: dict_items([('pout', tensor([1.1465], device='cuda:0')), ('power', tensor([0.8575], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0605], device='cuda:0')), ('power', tensor([-21.7281], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.13949726521968842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2464, 4.6664, 4.7033],
        [4.2464, 5.3171, 5.9874],
        [4.2464, 4.2617, 4.2487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.13934604823589325 
model_pd.l_d.mean(): -17.4169979095459 
model_pd.lagr.mean(): -17.277652740478516 
model_pd.lambdas: dict_items([('pout', tensor([1.1476], device='cuda:0')), ('power', tensor([0.8565], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0598], device='cuda:0')), ('power', tensor([-21.7272], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.13934604823589325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2493, 5.3201, 5.9902],
        [4.2493, 4.2646, 4.2516],
        [4.2493, 4.6691, 4.7059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.13918514549732208 
model_pd.l_d.mean(): -17.392410278320312 
model_pd.lagr.mean(): -17.253225326538086 
model_pd.lambdas: dict_items([('pout', tensor([1.1487], device='cuda:0')), ('power', tensor([0.8554], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0590], device='cuda:0')), ('power', tensor([-21.7263], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.13918514549732208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2523, 4.2675, 4.2545],
        [4.2523, 5.3232, 5.9931],
        [4.2523, 4.6719, 4.7085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.13902238011360168 
model_pd.l_d.mean(): -17.36782455444336 
model_pd.lagr.mean(): -17.228801727294922 
model_pd.lambdas: dict_items([('pout', tensor([1.1497], device='cuda:0')), ('power', tensor([0.8543], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0582], device='cuda:0')), ('power', tensor([-21.7254], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.13902238011360168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2551, 4.6746, 4.7111],
        [4.2551, 4.2703, 4.2573],
        [4.2551, 5.3261, 5.9959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13886579871177673 
model_pd.l_d.mean(): -17.343238830566406 
model_pd.lagr.mean(): -17.20437240600586 
model_pd.lambdas: dict_items([('pout', tensor([1.1508], device='cuda:0')), ('power', tensor([0.8532], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0574], device='cuda:0')), ('power', tensor([-21.7245], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13886579871177673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2577, 5.3287, 5.9982],
        [4.2577, 4.2729, 4.2599],
        [4.2577, 4.6769, 4.7133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.13872796297073364 
model_pd.l_d.mean(): -17.31867218017578 
model_pd.lagr.mean(): -17.17994499206543 
model_pd.lambdas: dict_items([('pout', tensor([1.1518], device='cuda:0')), ('power', tensor([0.8521], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0567], device='cuda:0')), ('power', tensor([-21.7237], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.13872796297073364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2598, 4.6788, 4.7150],
        [4.2598, 5.3307, 6.0000],
        [4.2598, 4.2750, 4.2620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.13861609995365143 
model_pd.l_d.mean(): -17.29413414001465 
model_pd.lagr.mean(): -17.155517578125 
model_pd.lambdas: dict_items([('pout', tensor([1.1529], device='cuda:0')), ('power', tensor([0.8510], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0561], device='cuda:0')), ('power', tensor([-21.7231], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.13861609995365143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2615, 4.2767, 4.2637],
        [4.2615, 4.6802, 4.7163],
        [4.2615, 5.3322, 6.0010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.13853254914283752 
model_pd.l_d.mean(): -17.269620895385742 
model_pd.lagr.mean(): -17.131088256835938 
model_pd.lambdas: dict_items([('pout', tensor([1.1539], device='cuda:0')), ('power', tensor([0.8499], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0557], device='cuda:0')), ('power', tensor([-21.7228], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.13853254914283752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2628, 4.2779, 4.2650],
        [4.2628, 4.6812, 4.7171],
        [4.2628, 5.3331, 6.0015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.13847489655017853 
model_pd.l_d.mean(): -17.245134353637695 
model_pd.lagr.mean(): -17.106658935546875 
model_pd.lambdas: dict_items([('pout', tensor([1.1550], device='cuda:0')), ('power', tensor([0.8489], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0553], device='cuda:0')), ('power', tensor([-21.7226], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.13847489655017853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2638, 5.3336, 6.0014],
        [4.2638, 4.2789, 4.2660],
        [4.2638, 4.6817, 4.7175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.13843637704849243 
model_pd.l_d.mean(): -17.220664978027344 
model_pd.lagr.mean(): -17.08222770690918 
model_pd.lambdas: dict_items([('pout', tensor([1.1560], device='cuda:0')), ('power', tensor([0.8478], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0551], device='cuda:0')), ('power', tensor([-21.7226], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.13843637704849243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2646, 4.6822, 4.7178],
        [4.2646, 5.3339, 6.0012],
        [4.2646, 4.2797, 4.2668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.1384081095457077 
model_pd.l_d.mean(): -17.196205139160156 
model_pd.lagr.mean(): -17.057796478271484 
model_pd.lambdas: dict_items([('pout', tensor([1.1571], device='cuda:0')), ('power', tensor([0.8467], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0550], device='cuda:0')), ('power', tensor([-21.7227], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.1384081095457077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2653, 4.2804, 4.2675],
        [4.2653, 4.6825, 4.7180],
        [4.2653, 5.3341, 6.0008]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.13838507235050201 
model_pd.l_d.mean(): -17.171737670898438 
model_pd.lagr.mean(): -17.03335189819336 
model_pd.lambdas: dict_items([('pout', tensor([1.1582], device='cuda:0')), ('power', tensor([0.8456], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0548], device='cuda:0')), ('power', tensor([-21.7228], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.13838507235050201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2660, 5.3343, 6.0005],
        [4.2660, 4.6829, 4.7182],
        [4.2660, 4.2811, 4.2682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.13836126029491425 
model_pd.l_d.mean(): -17.147253036499023 
model_pd.lagr.mean(): -17.008892059326172 
model_pd.lambdas: dict_items([('pout', tensor([1.1592], device='cuda:0')), ('power', tensor([0.8445], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0547], device='cuda:0')), ('power', tensor([-21.7228], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.13836126029491425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2669, 5.3347, 6.0005],
        [4.2669, 4.2819, 4.2691],
        [4.2669, 4.6834, 4.7186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.13832944631576538 
model_pd.l_d.mean(): -17.122758865356445 
model_pd.lagr.mean(): -16.98443031311035 
model_pd.lambdas: dict_items([('pout', tensor([1.1603], device='cuda:0')), ('power', tensor([0.8434], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0545], device='cuda:0')), ('power', tensor([-21.7228], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.13832944631576538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2679, 5.3355, 6.0008],
        [4.2679, 4.2829, 4.2701],
        [4.2679, 4.6842, 4.7192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.1382821500301361 
model_pd.l_d.mean(): -17.098243713378906 
model_pd.lagr.mean(): -16.9599609375 
model_pd.lambdas: dict_items([('pout', tensor([1.1613], device='cuda:0')), ('power', tensor([0.8423], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0543], device='cuda:0')), ('power', tensor([-21.7227], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.1382821500301361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2693, 5.3366, 6.0016],
        [4.2693, 4.6853, 4.7203],
        [4.2693, 4.2843, 4.2715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.1382143497467041 
model_pd.l_d.mean(): -17.073705673217773 
model_pd.lagr.mean(): -16.93549156188965 
model_pd.lambdas: dict_items([('pout', tensor([1.1624], device='cuda:0')), ('power', tensor([0.8413], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0539], device='cuda:0')), ('power', tensor([-21.7224], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.1382143497467041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2711, 4.6869, 4.7217],
        [4.2711, 5.3383, 6.0030],
        [4.2711, 4.2861, 4.2733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.13812381029129028 
model_pd.l_d.mean(): -17.04914665222168 
model_pd.lagr.mean(): -16.911022186279297 
model_pd.lambdas: dict_items([('pout', tensor([1.1634], device='cuda:0')), ('power', tensor([0.8402], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0534], device='cuda:0')), ('power', tensor([-21.7220], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.13812381029129028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2732, 5.3405, 6.0050],
        [4.2732, 4.2882, 4.2754],
        [4.2732, 4.6888, 4.7236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.13801121711730957 
model_pd.l_d.mean(): -17.024568557739258 
model_pd.lagr.mean(): -16.88655662536621 
model_pd.lambdas: dict_items([('pout', tensor([1.1645], device='cuda:0')), ('power', tensor([0.8391], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0529], device='cuda:0')), ('power', tensor([-21.7213], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.13801121711730957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2756, 4.2906, 4.2778],
        [4.2756, 5.3430, 6.0074],
        [4.2756, 4.6911, 4.7258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.13788044452667236 
model_pd.l_d.mean(): -16.99997329711914 
model_pd.lagr.mean(): -16.862092971801758 
model_pd.lambdas: dict_items([('pout', tensor([1.1655], device='cuda:0')), ('power', tensor([0.8380], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0522], device='cuda:0')), ('power', tensor([-21.7205], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.13788044452667236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2783, 5.3458, 6.0101],
        [4.2783, 4.2932, 4.2805],
        [4.2783, 4.6937, 4.7282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.13773739337921143 
model_pd.l_d.mean(): -16.975372314453125 
model_pd.lagr.mean(): -16.837635040283203 
model_pd.lambdas: dict_items([('pout', tensor([1.1666], device='cuda:0')), ('power', tensor([0.8369], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0515], device='cuda:0')), ('power', tensor([-21.7196], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.13773739337921143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2810, 5.3487, 6.0130],
        [4.2810, 4.6963, 4.7308],
        [4.2810, 4.2959, 4.2832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.13758885860443115 
model_pd.l_d.mean(): -16.950769424438477 
model_pd.lagr.mean(): -16.813180923461914 
model_pd.lambdas: dict_items([('pout', tensor([1.1676], device='cuda:0')), ('power', tensor([0.8358], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0507], device='cuda:0')), ('power', tensor([-21.7186], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.13758885860443115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2837, 5.3516, 6.0158],
        [4.2837, 4.2986, 4.2859],
        [4.2837, 4.6989, 4.7333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.13744193315505981 
model_pd.l_d.mean(): -16.92616844177246 
model_pd.lagr.mean(): -16.788726806640625 
model_pd.lambdas: dict_items([('pout', tensor([1.1687], device='cuda:0')), ('power', tensor([0.8347], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0499], device='cuda:0')), ('power', tensor([-21.7176], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.13744193315505981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2863, 5.3544, 6.0185],
        [4.2863, 4.3012, 4.2885],
        [4.2863, 4.7015, 4.7357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.13730216026306152 
model_pd.l_d.mean(): -16.90158462524414 
model_pd.lagr.mean(): -16.7642822265625 
model_pd.lambdas: dict_items([('pout', tensor([1.1697], device='cuda:0')), ('power', tensor([0.8337], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0492], device='cuda:0')), ('power', tensor([-21.7167], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.13730216026306152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2887, 4.7037, 4.7379],
        [4.2887, 5.3569, 6.0209],
        [4.2887, 4.3036, 4.2909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.13717374205589294 
model_pd.l_d.mean(): -16.877010345458984 
model_pd.lagr.mean(): -16.739835739135742 
model_pd.lambdas: dict_items([('pout', tensor([1.1708], device='cuda:0')), ('power', tensor([0.8326], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0485], device='cuda:0')), ('power', tensor([-21.7159], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.13717374205589294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2909, 4.7058, 4.7398],
        [4.2909, 5.3592, 6.0230],
        [4.2909, 4.3058, 4.2931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.13705860078334808 
model_pd.l_d.mean(): -16.852455139160156 
model_pd.lagr.mean(): -16.715396881103516 
model_pd.lambdas: dict_items([('pout', tensor([1.1718], device='cuda:0')), ('power', tensor([0.8315], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0479], device='cuda:0')), ('power', tensor([-21.7152], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.13705860078334808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2930, 5.3611, 6.0247],
        [4.2930, 4.7076, 4.7415],
        [4.2930, 4.3078, 4.2951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.13695678114891052 
model_pd.l_d.mean(): -16.827917098999023 
model_pd.lagr.mean(): -16.690959930419922 
model_pd.lambdas: dict_items([('pout', tensor([1.1729], device='cuda:0')), ('power', tensor([0.8304], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0474], device='cuda:0')), ('power', tensor([-21.7146], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.13695678114891052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2948, 4.7092, 4.7430],
        [4.2948, 5.3628, 6.0261],
        [4.2948, 4.3096, 4.2969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.136866495013237 
model_pd.l_d.mean(): -16.803388595581055 
model_pd.lagr.mean(): -16.666522979736328 
model_pd.lambdas: dict_items([('pout', tensor([1.1739], device='cuda:0')), ('power', tensor([0.8293], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0469], device='cuda:0')), ('power', tensor([-21.7141], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.136866495013237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2965, 4.7107, 4.7443],
        [4.2965, 5.3644, 6.0274],
        [4.2965, 4.3113, 4.2986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.13678430020809174 
model_pd.l_d.mean(): -16.778873443603516 
model_pd.lagr.mean(): -16.64208984375 
model_pd.lambdas: dict_items([('pout', tensor([1.1750], device='cuda:0')), ('power', tensor([0.8282], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0465], device='cuda:0')), ('power', tensor([-21.7137], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.13678430020809174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2981, 4.7120, 4.7456],
        [4.2981, 4.3129, 4.3002],
        [4.2981, 5.3658, 6.0285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.13670603930950165 
model_pd.l_d.mean(): -16.754365921020508 
model_pd.lagr.mean(): -16.617660522460938 
model_pd.lambdas: dict_items([('pout', tensor([1.1760], device='cuda:0')), ('power', tensor([0.8271], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0460], device='cuda:0')), ('power', tensor([-21.7133], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.13670603930950165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2997, 4.7134, 4.7468],
        [4.2997, 4.3145, 4.3019],
        [4.2997, 5.3673, 6.0297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.1366271823644638 
model_pd.l_d.mean(): -16.729860305786133 
model_pd.lagr.mean(): -16.593233108520508 
model_pd.lambdas: dict_items([('pout', tensor([1.1771], device='cuda:0')), ('power', tensor([0.8261], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0456], device='cuda:0')), ('power', tensor([-21.7129], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.1366271823644638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3014, 4.3162, 4.3036],
        [4.3014, 5.3689, 6.0310],
        [4.3014, 4.7149, 4.7482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.13654349744319916 
model_pd.l_d.mean(): -16.705347061157227 
model_pd.lagr.mean(): -16.568803787231445 
model_pd.lambdas: dict_items([('pout', tensor([1.1781], device='cuda:0')), ('power', tensor([0.8250], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0452], device='cuda:0')), ('power', tensor([-21.7124], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.13654349744319916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3033, 4.3180, 4.3055],
        [4.3033, 4.7165, 4.7497],
        [4.3033, 5.3706, 6.0324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.13645178079605103 
model_pd.l_d.mean(): -16.680831909179688 
model_pd.lagr.mean(): -16.54438018798828 
model_pd.lambdas: dict_items([('pout', tensor([1.1792], device='cuda:0')), ('power', tensor([0.8239], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0447], device='cuda:0')), ('power', tensor([-21.7119], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.13645178079605103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3053, 4.3200, 4.3075],
        [4.3053, 5.3726, 6.0342],
        [4.3053, 4.7184, 4.7514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.13635049760341644 
model_pd.l_d.mean(): -16.656309127807617 
model_pd.lagr.mean(): -16.51995849609375 
model_pd.lambdas: dict_items([('pout', tensor([1.1802], device='cuda:0')), ('power', tensor([0.8228], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0441], device='cuda:0')), ('power', tensor([-21.7113], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.13635049760341644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3075, 4.3222, 4.3097],
        [4.3075, 4.7204, 4.7533],
        [4.3075, 5.3748, 6.0362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.13623908162117004 
model_pd.l_d.mean(): -16.63178253173828 
model_pd.lagr.mean(): -16.495542526245117 
model_pd.lambdas: dict_items([('pout', tensor([1.1812], device='cuda:0')), ('power', tensor([0.8217], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0435], device='cuda:0')), ('power', tensor([-21.7106], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.13623908162117004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3099, 5.3772, 6.0384],
        [4.3099, 4.7226, 4.7554],
        [4.3099, 4.3246, 4.3120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.1361185610294342 
model_pd.l_d.mean(): -16.60724449157715 
model_pd.lagr.mean(): -16.471126556396484 
model_pd.lambdas: dict_items([('pout', tensor([1.1823], device='cuda:0')), ('power', tensor([0.8206], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0429], device='cuda:0')), ('power', tensor([-21.7098], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.1361185610294342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3124, 4.3270, 4.3145],
        [4.3124, 4.7249, 4.7576],
        [4.3124, 5.3798, 6.0409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.13599079847335815 
model_pd.l_d.mean(): -16.58270263671875 
model_pd.lagr.mean(): -16.446712493896484 
model_pd.lambdas: dict_items([('pout', tensor([1.1833], device='cuda:0')), ('power', tensor([0.8195], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0422], device='cuda:0')), ('power', tensor([-21.7090], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.13599079847335815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3149, 4.7274, 4.7599],
        [4.3149, 5.3825, 6.0435],
        [4.3149, 4.3296, 4.3171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.1358586549758911 
model_pd.l_d.mean(): -16.55816078186035 
model_pd.lagr.mean(): -16.42230224609375 
model_pd.lambdas: dict_items([('pout', tensor([1.1844], device='cuda:0')), ('power', tensor([0.8185], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0415], device='cuda:0')), ('power', tensor([-21.7081], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.1358586549758911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3175, 5.3852, 6.0461],
        [4.3175, 4.7298, 4.7623],
        [4.3175, 4.3322, 4.3197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.13572512567043304 
model_pd.l_d.mean(): -16.533618927001953 
model_pd.lagr.mean(): -16.39789390563965 
model_pd.lambdas: dict_items([('pout', tensor([1.1854], device='cuda:0')), ('power', tensor([0.8174], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0408], device='cuda:0')), ('power', tensor([-21.7072], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.13572512567043304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3201, 4.7323, 4.7646],
        [4.3201, 4.3347, 4.3223],
        [4.3201, 5.3879, 6.0487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.13559311628341675 
model_pd.l_d.mean(): -16.509084701538086 
model_pd.lagr.mean(): -16.373491287231445 
model_pd.lambdas: dict_items([('pout', tensor([1.1864], device='cuda:0')), ('power', tensor([0.8163], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0401], device='cuda:0')), ('power', tensor([-21.7063], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.13559311628341675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3227, 4.3373, 4.3248],
        [4.3227, 4.7346, 4.7668],
        [4.3227, 5.3906, 6.0512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.1354648768901825 
model_pd.l_d.mean(): -16.484554290771484 
model_pd.lagr.mean(): -16.349088668823242 
model_pd.lambdas: dict_items([('pout', tensor([1.1875], device='cuda:0')), ('power', tensor([0.8152], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0394], device='cuda:0')), ('power', tensor([-21.7054], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.1354648768901825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3251, 5.3931, 6.0536],
        [4.3251, 4.7369, 4.7690],
        [4.3251, 4.3397, 4.3272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.13534165918827057 
model_pd.l_d.mean(): -16.46003532409668 
model_pd.lagr.mean(): -16.32469367980957 
model_pd.lambdas: dict_items([('pout', tensor([1.1885], device='cuda:0')), ('power', tensor([0.8141], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0387], device='cuda:0')), ('power', tensor([-21.7046], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.13534165918827057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3275, 5.3955, 6.0558],
        [4.3275, 4.7391, 4.7711],
        [4.3275, 4.3420, 4.3296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.13522379100322723 
model_pd.l_d.mean(): -16.43552017211914 
model_pd.lagr.mean(): -16.300296783447266 
model_pd.lambdas: dict_items([('pout', tensor([1.1896], device='cuda:0')), ('power', tensor([0.8130], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0381], device='cuda:0')), ('power', tensor([-21.7038], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.13522379100322723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3298, 4.3443, 4.3319],
        [4.3298, 4.7412, 4.7731],
        [4.3298, 5.3978, 6.0579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.13511055707931519 
model_pd.l_d.mean(): -16.411014556884766 
model_pd.lagr.mean(): -16.275903701782227 
model_pd.lambdas: dict_items([('pout', tensor([1.1906], device='cuda:0')), ('power', tensor([0.8119], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0374], device='cuda:0')), ('power', tensor([-21.7030], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.13511055707931519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3320, 4.7433, 4.7750],
        [4.3320, 5.4001, 6.0600],
        [4.3320, 4.3465, 4.3341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.13500085473060608 
model_pd.l_d.mean(): -16.38651466369629 
model_pd.lagr.mean(): -16.251514434814453 
model_pd.lambdas: dict_items([('pout', tensor([1.1916], device='cuda:0')), ('power', tensor([0.8109], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0368], device='cuda:0')), ('power', tensor([-21.7023], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.13500085473060608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3342, 4.3487, 4.3363],
        [4.3342, 5.4023, 6.0620],
        [4.3342, 4.7453, 4.7769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.13489283621311188 
model_pd.l_d.mean(): -16.362022399902344 
model_pd.lagr.mean(): -16.227128982543945 
model_pd.lambdas: dict_items([('pout', tensor([1.1927], device='cuda:0')), ('power', tensor([0.8098], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0362], device='cuda:0')), ('power', tensor([-21.7016], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.13489283621311188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3364, 5.4045, 6.0640],
        [4.3364, 4.3509, 4.3385],
        [4.3364, 4.7473, 4.7788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.13478481769561768 
model_pd.l_d.mean(): -16.3375301361084 
model_pd.lagr.mean(): -16.20274543762207 
model_pd.lambdas: dict_items([('pout', tensor([1.1937], device='cuda:0')), ('power', tensor([0.8087], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0356], device='cuda:0')), ('power', tensor([-21.7009], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.13478481769561768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3387, 5.4067, 6.0660],
        [4.3387, 4.3532, 4.3408],
        [4.3387, 4.7494, 4.7807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.1346750557422638 
model_pd.l_d.mean(): -16.31304168701172 
model_pd.lagr.mean(): -16.17836570739746 
model_pd.lambdas: dict_items([('pout', tensor([1.1947], device='cuda:0')), ('power', tensor([0.8076], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0350], device='cuda:0')), ('power', tensor([-21.7001], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.1346750557422638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3410, 4.3555, 4.3431],
        [4.3410, 4.7515, 4.7827],
        [4.3410, 5.4090, 6.0682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.13456234335899353 
model_pd.l_d.mean(): -16.288549423217773 
model_pd.lagr.mean(): -16.153987884521484 
model_pd.lambdas: dict_items([('pout', tensor([1.1958], device='cuda:0')), ('power', tensor([0.8065], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0344], device='cuda:0')), ('power', tensor([-21.6994], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.13456234335899353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3434, 4.7537, 4.7848],
        [4.3434, 4.3578, 4.3455],
        [4.3434, 5.4115, 6.0705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.13444584608078003 
model_pd.l_d.mean(): -16.264057159423828 
model_pd.lagr.mean(): -16.12961196899414 
model_pd.lambdas: dict_items([('pout', tensor([1.1968], device='cuda:0')), ('power', tensor([0.8054], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0338], device='cuda:0')), ('power', tensor([-21.6986], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.13444584608078003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3459, 4.7560, 4.7869],
        [4.3459, 4.3603, 4.3480],
        [4.3459, 5.4140, 6.0728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.13432514667510986 
model_pd.l_d.mean(): -16.239566802978516 
model_pd.lagr.mean(): -16.105241775512695 
model_pd.lambdas: dict_items([('pout', tensor([1.1978], device='cuda:0')), ('power', tensor([0.8043], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0331], device='cuda:0')), ('power', tensor([-21.6977], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.13432514667510986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3484, 5.4166, 6.0753],
        [4.3484, 4.3628, 4.3505],
        [4.3484, 4.7584, 4.7892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.13420066237449646 
model_pd.l_d.mean(): -16.215070724487305 
model_pd.lagr.mean(): -16.080869674682617 
model_pd.lambdas: dict_items([('pout', tensor([1.1989], device='cuda:0')), ('power', tensor([0.8033], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0324], device='cuda:0')), ('power', tensor([-21.6968], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.13420066237449646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3510, 4.7608, 4.7915],
        [4.3510, 4.3654, 4.3531],
        [4.3510, 5.4194, 6.0779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.13407301902770996 
model_pd.l_d.mean(): -16.19057846069336 
model_pd.lagr.mean(): -16.05650520324707 
model_pd.lambdas: dict_items([('pout', tensor([1.1999], device='cuda:0')), ('power', tensor([0.8022], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0317], device='cuda:0')), ('power', tensor([-21.6959], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.13407301902770996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3537, 4.7633, 4.7939],
        [4.3537, 5.4221, 6.0806],
        [4.3537, 4.3680, 4.3557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.133943572640419 
model_pd.l_d.mean(): -16.166086196899414 
model_pd.lagr.mean(): -16.032142639160156 
model_pd.lambdas: dict_items([('pout', tensor([1.2009], device='cuda:0')), ('power', tensor([0.8011], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0310], device='cuda:0')), ('power', tensor([-21.6950], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.133943572640419
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3563, 5.4249, 6.0832],
        [4.3563, 4.7658, 4.7962],
        [4.3563, 4.3707, 4.3584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.1338135153055191 
model_pd.l_d.mean(): -16.1415958404541 
model_pd.lagr.mean(): -16.007781982421875 
model_pd.lambdas: dict_items([('pout', tensor([1.2020], device='cuda:0')), ('power', tensor([0.8000], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0302], device='cuda:0')), ('power', tensor([-21.6940], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.1338135153055191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3590, 4.3733, 4.3611],
        [4.3590, 5.4277, 6.0859],
        [4.3590, 4.7683, 4.7986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.13368432223796844 
model_pd.l_d.mean(): -16.117111206054688 
model_pd.lagr.mean(): -15.983427047729492 
model_pd.lambdas: dict_items([('pout', tensor([1.2030], device='cuda:0')), ('power', tensor([0.7989], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0295], device='cuda:0')), ('power', tensor([-21.6930], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.13368432223796844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3616, 4.7708, 4.8010],
        [4.3616, 4.3759, 4.3637],
        [4.3616, 5.4304, 6.0885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.13355684280395508 
model_pd.l_d.mean(): -16.09263038635254 
model_pd.lagr.mean(): -15.959074020385742 
model_pd.lambdas: dict_items([('pout', tensor([1.2040], device='cuda:0')), ('power', tensor([0.7978], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0288], device='cuda:0')), ('power', tensor([-21.6921], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.13355684280395508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3642, 5.4331, 6.0911],
        [4.3642, 4.3785, 4.3663],
        [4.3642, 4.7732, 4.8033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.13343185186386108 
model_pd.l_d.mean(): -16.06815528869629 
model_pd.lagr.mean(): -15.934723854064941 
model_pd.lambdas: dict_items([('pout', tensor([1.2051], device='cuda:0')), ('power', tensor([0.7968], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0281], device='cuda:0')), ('power', tensor([-21.6912], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.13343185186386108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3668, 4.3810, 4.3689],
        [4.3668, 5.4358, 6.0936],
        [4.3668, 4.7756, 4.8055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.1333095282316208 
model_pd.l_d.mean(): -16.04368782043457 
model_pd.lagr.mean(): -15.910378456115723 
model_pd.lambdas: dict_items([('pout', tensor([1.2061], device='cuda:0')), ('power', tensor([0.7957], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0274], device='cuda:0')), ('power', tensor([-21.6903], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.1333095282316208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3693, 4.7780, 4.8077],
        [4.3693, 5.4384, 6.0960],
        [4.3693, 4.3836, 4.3714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.13318967819213867 
model_pd.l_d.mean(): -16.019224166870117 
model_pd.lagr.mean(): -15.88603401184082 
model_pd.lambdas: dict_items([('pout', tensor([1.2071], device='cuda:0')), ('power', tensor([0.7946], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0267], device='cuda:0')), ('power', tensor([-21.6894], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.13318967819213867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3718, 5.4409, 6.0984],
        [4.3718, 4.3860, 4.3739],
        [4.3718, 4.7803, 4.8099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.13307182490825653 
model_pd.l_d.mean(): -15.994769096374512 
model_pd.lagr.mean(): -15.86169719696045 
model_pd.lambdas: dict_items([('pout', tensor([1.2081], device='cuda:0')), ('power', tensor([0.7935], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0260], device='cuda:0')), ('power', tensor([-21.6885], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.13307182490825653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3743, 4.7826, 4.8121],
        [4.3743, 4.3885, 4.3763],
        [4.3743, 5.4434, 6.1007]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.13295505940914154 
model_pd.l_d.mean(): -15.97031307220459 
model_pd.lagr.mean(): -15.837358474731445 
model_pd.lambdas: dict_items([('pout', tensor([1.2092], device='cuda:0')), ('power', tensor([0.7924], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0254], device='cuda:0')), ('power', tensor([-21.6876], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.13295505940914154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3768, 5.4460, 6.1031],
        [4.3768, 4.7849, 4.8142],
        [4.3768, 4.3910, 4.3788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.13283829391002655 
model_pd.l_d.mean(): -15.945862770080566 
model_pd.lagr.mean(): -15.813024520874023 
model_pd.lambdas: dict_items([('pout', tensor([1.2102], device='cuda:0')), ('power', tensor([0.7913], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0247], device='cuda:0')), ('power', tensor([-21.6868], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.13283829391002655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3793, 4.3935, 4.3813],
        [4.3793, 5.4486, 6.1055],
        [4.3793, 4.7872, 4.8164]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.1327204704284668 
model_pd.l_d.mean(): -15.921415328979492 
model_pd.lagr.mean(): -15.788694381713867 
model_pd.lambdas: dict_items([('pout', tensor([1.2112], device='cuda:0')), ('power', tensor([0.7902], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0240], device='cuda:0')), ('power', tensor([-21.6859], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.1327204704284668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3819, 4.7896, 4.8187],
        [4.3819, 4.3960, 4.3839],
        [4.3819, 5.4512, 6.1080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.1326006054878235 
model_pd.l_d.mean(): -15.896967887878418 
model_pd.lagr.mean(): -15.76436710357666 
model_pd.lambdas: dict_items([('pout', tensor([1.2122], device='cuda:0')), ('power', tensor([0.7892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0233], device='cuda:0')), ('power', tensor([-21.6850], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.1326006054878235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3845, 4.3986, 4.3865],
        [4.3845, 5.4539, 6.1106],
        [4.3845, 4.7920, 4.8210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.13247829675674438 
model_pd.l_d.mean(): -15.872520446777344 
model_pd.lagr.mean(): -15.740041732788086 
model_pd.lambdas: dict_items([('pout', tensor([1.2133], device='cuda:0')), ('power', tensor([0.7881], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0226], device='cuda:0')), ('power', tensor([-21.6840], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.13247829675674438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3871, 5.4567, 6.1132],
        [4.3871, 4.4012, 4.3892],
        [4.3871, 4.7945, 4.8233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.13235342502593994 
model_pd.l_d.mean(): -15.848073959350586 
model_pd.lagr.mean(): -15.715720176696777 
model_pd.lambdas: dict_items([('pout', tensor([1.2143], device='cuda:0')), ('power', tensor([0.7870], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0219], device='cuda:0')), ('power', tensor([-21.6830], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.13235342502593994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3898, 4.4039, 4.3919],
        [4.3898, 5.4595, 6.1159],
        [4.3898, 4.7971, 4.8258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.13222630321979523 
model_pd.l_d.mean(): -15.823627471923828 
model_pd.lagr.mean(): -15.691401481628418 
model_pd.lambdas: dict_items([('pout', tensor([1.2153], device='cuda:0')), ('power', tensor([0.7859], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0212], device='cuda:0')), ('power', tensor([-21.6820], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.13222630321979523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3926, 4.4067, 4.3946],
        [4.3926, 5.4624, 6.1187],
        [4.3926, 4.7997, 4.8282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.132097527384758 
model_pd.l_d.mean(): -15.799182891845703 
model_pd.lagr.mean(): -15.667085647583008 
model_pd.lambdas: dict_items([('pout', tensor([1.2163], device='cuda:0')), ('power', tensor([0.7848], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0204], device='cuda:0')), ('power', tensor([-21.6810], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.132097527384758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3954, 5.4654, 6.1215],
        [4.3954, 4.4094, 4.3974],
        [4.3954, 4.8023, 4.8307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.1319677233695984 
model_pd.l_d.mean(): -15.774740219116211 
model_pd.lagr.mean(): -15.642772674560547 
model_pd.lambdas: dict_items([('pout', tensor([1.2173], device='cuda:0')), ('power', tensor([0.7837], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0197], device='cuda:0')), ('power', tensor([-21.6800], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.1319677233695984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3982, 4.8049, 4.8332],
        [4.3982, 4.4122, 4.4002],
        [4.3982, 5.4683, 6.1244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.1318376362323761 
model_pd.l_d.mean(): -15.750301361083984 
model_pd.lagr.mean(): -15.618463516235352 
model_pd.lambdas: dict_items([('pout', tensor([1.2184], device='cuda:0')), ('power', tensor([0.7827], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0189], device='cuda:0')), ('power', tensor([-21.6789], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.1318376362323761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4010, 4.4150, 4.4030],
        [4.4010, 5.4713, 6.1272],
        [4.4010, 4.8076, 4.8358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.13170793652534485 
model_pd.l_d.mean(): -15.725862503051758 
model_pd.lagr.mean(): -15.594154357910156 
model_pd.lambdas: dict_items([('pout', tensor([1.2194], device='cuda:0')), ('power', tensor([0.7816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0182], device='cuda:0')), ('power', tensor([-21.6779], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.13170793652534485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4038, 4.8102, 4.8383],
        [4.4038, 5.4742, 6.1300],
        [4.4038, 4.4178, 4.4058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.13157925009727478 
model_pd.l_d.mean(): -15.701428413391113 
model_pd.lagr.mean(): -15.569849014282227 
model_pd.lambdas: dict_items([('pout', tensor([1.2204], device='cuda:0')), ('power', tensor([0.7805], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0174], device='cuda:0')), ('power', tensor([-21.6768], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.13157925009727478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4065, 5.4771, 6.1328],
        [4.4065, 4.8128, 4.8407],
        [4.4065, 4.4205, 4.4086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.1314520388841629 
model_pd.l_d.mean(): -15.67699909210205 
model_pd.lagr.mean(): -15.545547485351562 
model_pd.lambdas: dict_items([('pout', tensor([1.2214], device='cuda:0')), ('power', tensor([0.7794], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([-21.6758], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.1314520388841629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4093, 4.8154, 4.8432],
        [4.4093, 4.4232, 4.4113],
        [4.4093, 5.4800, 6.1356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.13132637739181519 
model_pd.l_d.mean(): -15.65257740020752 
model_pd.lagr.mean(): -15.52125072479248 
model_pd.lambdas: dict_items([('pout', tensor([1.2224], device='cuda:0')), ('power', tensor([0.7783], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0159], device='cuda:0')), ('power', tensor([-21.6747], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.13132637739181519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4120, 4.8180, 4.8456],
        [4.4120, 5.4828, 6.1383],
        [4.4120, 4.4260, 4.4140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.13120213150978088 
model_pd.l_d.mean(): -15.628158569335938 
model_pd.lagr.mean(): -15.496956825256348 
model_pd.lambdas: dict_items([('pout', tensor([1.2234], device='cuda:0')), ('power', tensor([0.7772], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0152], device='cuda:0')), ('power', tensor([-21.6737], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.13120213150978088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4147, 4.4286, 4.4167],
        [4.4147, 4.8205, 4.8480],
        [4.4147, 5.4857, 6.1410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.13107949495315552 
model_pd.l_d.mean(): -15.603743553161621 
model_pd.lagr.mean(): -15.472663879394531 
model_pd.lambdas: dict_items([('pout', tensor([1.2245], device='cuda:0')), ('power', tensor([0.7762], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0144], device='cuda:0')), ('power', tensor([-21.6727], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.13107949495315552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4174, 4.4313, 4.4194],
        [4.4174, 4.8231, 4.8504],
        [4.4174, 5.4885, 6.1437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.1309582144021988 
model_pd.l_d.mean(): -15.579334259033203 
model_pd.lagr.mean(): -15.448375701904297 
model_pd.lambdas: dict_items([('pout', tensor([1.2255], device='cuda:0')), ('power', tensor([0.7751], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0137], device='cuda:0')), ('power', tensor([-21.6717], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.1309582144021988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4201, 4.8256, 4.8528],
        [4.4201, 4.4340, 4.4221],
        [4.4201, 5.4913, 6.1463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.13083821535110474 
model_pd.l_d.mean(): -15.554925918579102 
model_pd.lagr.mean(): -15.424087524414062 
model_pd.lambdas: dict_items([('pout', tensor([1.2265], device='cuda:0')), ('power', tensor([0.7740], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0130], device='cuda:0')), ('power', tensor([-21.6707], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.13083821535110474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4227, 4.4366, 4.4247],
        [4.4227, 5.4940, 6.1489],
        [4.4227, 4.8281, 4.8551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.1307188868522644 
model_pd.l_d.mean(): -15.530524253845215 
model_pd.lagr.mean(): -15.399805068969727 
model_pd.lambdas: dict_items([('pout', tensor([1.2275], device='cuda:0')), ('power', tensor([0.7729], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0123], device='cuda:0')), ('power', tensor([-21.6697], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.1307188868522644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4254, 5.4968, 6.1515],
        [4.4254, 4.4392, 4.4274],
        [4.4254, 4.8305, 4.8575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.1305997520685196 
model_pd.l_d.mean(): -15.506128311157227 
model_pd.lagr.mean(): -15.375528335571289 
model_pd.lambdas: dict_items([('pout', tensor([1.2285], device='cuda:0')), ('power', tensor([0.7718], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0116], device='cuda:0')), ('power', tensor([-21.6687], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.1305997520685196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4281, 5.4996, 6.1542],
        [4.4281, 4.4419, 4.4301],
        [4.4281, 4.8331, 4.8599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.13048028945922852 
model_pd.l_d.mean(): -15.481733322143555 
model_pd.lagr.mean(): -15.351253509521484 
model_pd.lambdas: dict_items([('pout', tensor([1.2295], device='cuda:0')), ('power', tensor([0.7707], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0109], device='cuda:0')), ('power', tensor([-21.6677], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.13048028945922852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4308, 4.4446, 4.4328],
        [4.4308, 5.5024, 6.1569],
        [4.4308, 4.8356, 4.8623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.13036015629768372 
model_pd.l_d.mean(): -15.457340240478516 
model_pd.lagr.mean(): -15.326979637145996 
model_pd.lambdas: dict_items([('pout', tensor([1.2305], device='cuda:0')), ('power', tensor([0.7697], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0101], device='cuda:0')), ('power', tensor([-21.6667], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.13036015629768372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4335, 5.5053, 6.1596],
        [4.4335, 4.4473, 4.4355],
        [4.4335, 4.8382, 4.8647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.13023895025253296 
model_pd.l_d.mean(): -15.432948112487793 
model_pd.lagr.mean(): -15.302709579467773 
model_pd.lambdas: dict_items([('pout', tensor([1.2315], device='cuda:0')), ('power', tensor([0.7686], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0094], device='cuda:0')), ('power', tensor([-21.6656], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.13023895025253296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4363, 4.4501, 4.4383],
        [4.4363, 4.8408, 4.8672],
        [4.4363, 5.5082, 6.1624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.13011661171913147 
model_pd.l_d.mean(): -15.408562660217285 
model_pd.lagr.mean(): -15.278446197509766 
model_pd.lambdas: dict_items([('pout', tensor([1.2325], device='cuda:0')), ('power', tensor([0.7675], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0087], device='cuda:0')), ('power', tensor([-21.6646], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.13011661171913147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4391, 4.8434, 4.8697],
        [4.4391, 4.4528, 4.4411],
        [4.4391, 5.5111, 6.1652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.1299930214881897 
model_pd.l_d.mean(): -15.384174346923828 
model_pd.lagr.mean(): -15.254180908203125 
model_pd.lambdas: dict_items([('pout', tensor([1.2336], device='cuda:0')), ('power', tensor([0.7664], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0079], device='cuda:0')), ('power', tensor([-21.6635], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.1299930214881897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4419, 5.5141, 6.1680],
        [4.4419, 4.4556, 4.4439],
        [4.4419, 4.8461, 4.8722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.1298685520887375 
model_pd.l_d.mean(): -15.35979175567627 
model_pd.lagr.mean(): -15.229923248291016 
model_pd.lambdas: dict_items([('pout', tensor([1.2346], device='cuda:0')), ('power', tensor([0.7653], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0071], device='cuda:0')), ('power', tensor([-21.6624], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.1298685520887375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4448, 4.4585, 4.4467],
        [4.4448, 5.5171, 6.1709],
        [4.4448, 4.8488, 4.8748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.1297435164451599 
model_pd.l_d.mean(): -15.33541202545166 
model_pd.lagr.mean(): -15.205668449401855 
model_pd.lambdas: dict_items([('pout', tensor([1.2356], device='cuda:0')), ('power', tensor([0.7642], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0064], device='cuda:0')), ('power', tensor([-21.6613], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.1297435164451599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4476, 5.5201, 6.1738],
        [4.4476, 4.8515, 4.8773],
        [4.4476, 4.4613, 4.4496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.12961842119693756 
model_pd.l_d.mean(): -15.311030387878418 
model_pd.lagr.mean(): -15.181411743164062 
model_pd.lambdas: dict_items([('pout', tensor([1.2366], device='cuda:0')), ('power', tensor([0.7632], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0056], device='cuda:0')), ('power', tensor([-21.6602], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.12961842119693756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4505, 4.4641, 4.4524],
        [4.4505, 4.8542, 4.8799],
        [4.4505, 5.5231, 6.1767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12949368357658386 
model_pd.l_d.mean(): -15.28665828704834 
model_pd.lagr.mean(): -15.157164573669434 
model_pd.lambdas: dict_items([('pout', tensor([1.2376], device='cuda:0')), ('power', tensor([0.7621], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0048], device='cuda:0')), ('power', tensor([-21.6591], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12949368357658386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4533, 5.5261, 6.1796],
        [4.4533, 4.8569, 4.8824],
        [4.4533, 4.4670, 4.4553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.12936964631080627 
model_pd.l_d.mean(): -15.262288093566895 
model_pd.lagr.mean(): -15.132918357849121 
model_pd.lambdas: dict_items([('pout', tensor([1.2386], device='cuda:0')), ('power', tensor([0.7610], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0041], device='cuda:0')), ('power', tensor([-21.6579], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.12936964631080627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4562, 4.8595, 4.8850],
        [4.4562, 5.5291, 6.1825],
        [4.4562, 4.4698, 4.4581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.1292465478181839 
model_pd.l_d.mean(): -15.237924575805664 
model_pd.lagr.mean(): -15.108677864074707 
model_pd.lambdas: dict_items([('pout', tensor([1.2396], device='cuda:0')), ('power', tensor([0.7599], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0033], device='cuda:0')), ('power', tensor([-21.6568], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.1292465478181839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4590, 5.5321, 6.1853],
        [4.4590, 4.4726, 4.4609],
        [4.4590, 4.8622, 4.8875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.12912434339523315 
model_pd.l_d.mean(): -15.2135648727417 
model_pd.lagr.mean(): -15.084440231323242 
model_pd.lambdas: dict_items([('pout', tensor([1.2406], device='cuda:0')), ('power', tensor([0.7588], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0026], device='cuda:0')), ('power', tensor([-21.6557], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.12912434339523315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4618, 4.8649, 4.8900],
        [4.4618, 4.4754, 4.4637],
        [4.4618, 5.5351, 6.1882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.12900303304195404 
model_pd.l_d.mean(): -15.189208030700684 
model_pd.lagr.mean(): -15.060205459594727 
model_pd.lambdas: dict_items([('pout', tensor([1.2416], device='cuda:0')), ('power', tensor([0.7577], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0018], device='cuda:0')), ('power', tensor([-21.6546], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.12900303304195404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4646, 4.4782, 4.4666],
        [4.4646, 5.5380, 6.1910],
        [4.4646, 4.8675, 4.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.12888240814208984 
model_pd.l_d.mean(): -15.16485595703125 
model_pd.lagr.mean(): -15.03597354888916 
model_pd.lambdas: dict_items([('pout', tensor([1.2426], device='cuda:0')), ('power', tensor([0.7567], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0011], device='cuda:0')), ('power', tensor([-21.6535], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.12888240814208984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4674, 4.4810, 4.4694],
        [4.4674, 4.8702, 4.8950],
        [4.4674, 5.5410, 6.1938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12876203656196594 
model_pd.l_d.mean(): -15.140508651733398 
model_pd.lagr.mean(): -15.011746406555176 
model_pd.lambdas: dict_items([('pout', tensor([1.2436], device='cuda:0')), ('power', tensor([0.7556], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0003], device='cuda:0')), ('power', tensor([-21.6524], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12876203656196594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4703, 4.8728, 4.8976],
        [4.4703, 4.4838, 4.4722],
        [4.4703, 5.5440, 6.1967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.12864160537719727 
model_pd.l_d.mean(): -15.116164207458496 
model_pd.lagr.mean(): -14.98752212524414 
model_pd.lambdas: dict_items([('pout', tensor([1.2446], device='cuda:0')), ('power', tensor([0.7545], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9995], device='cuda:0')), ('power', tensor([-21.6513], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.12864160537719727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4731, 5.5470, 6.1995],
        [4.4731, 4.8755, 4.9001],
        [4.4731, 4.4866, 4.4750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.12852083146572113 
model_pd.l_d.mean(): -15.091822624206543 
model_pd.lagr.mean(): -14.963301658630371 
model_pd.lambdas: dict_items([('pout', tensor([1.2456], device='cuda:0')), ('power', tensor([0.7534], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9988], device='cuda:0')), ('power', tensor([-21.6502], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.12852083146572113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4760, 5.5500, 6.2024],
        [4.4760, 4.4895, 4.4779],
        [4.4760, 4.8782, 4.9027]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.12839949131011963 
model_pd.l_d.mean(): -15.067483901977539 
model_pd.lagr.mean(): -14.93908405303955 
model_pd.lambdas: dict_items([('pout', tensor([1.2466], device='cuda:0')), ('power', tensor([0.7523], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9980], device='cuda:0')), ('power', tensor([-21.6490], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.12839949131011963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4789, 4.4923, 4.4808],
        [4.4789, 4.8809, 4.9052],
        [4.4789, 5.5531, 6.2054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.12827752530574799 
model_pd.l_d.mean(): -15.043146133422852 
model_pd.lagr.mean(): -14.914868354797363 
model_pd.lambdas: dict_items([('pout', tensor([1.2476], device='cuda:0')), ('power', tensor([0.7512], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9973], device='cuda:0')), ('power', tensor([-21.6479], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.12827752530574799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4818, 4.4952, 4.4837],
        [4.4818, 4.8837, 4.9079],
        [4.4818, 5.5561, 6.2083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.12815497815608978 
model_pd.l_d.mean(): -15.018813133239746 
model_pd.lagr.mean(): -14.890658378601074 
model_pd.lambdas: dict_items([('pout', tensor([1.2486], device='cuda:0')), ('power', tensor([0.7502], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9965], device='cuda:0')), ('power', tensor([-21.6467], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.12815497815608978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4847, 5.5592, 6.2113],
        [4.4847, 4.4981, 4.4866],
        [4.4847, 4.8865, 4.9105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.12803199887275696 
model_pd.l_d.mean(): -14.99448299407959 
model_pd.lagr.mean(): -14.866451263427734 
model_pd.lambdas: dict_items([('pout', tensor([1.2496], device='cuda:0')), ('power', tensor([0.7491], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9957], device='cuda:0')), ('power', tensor([-21.6455], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.12803199887275696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4876, 4.5010, 4.4895],
        [4.4876, 5.5624, 6.2143],
        [4.4876, 4.8892, 4.9131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.1279086470603943 
model_pd.l_d.mean(): -14.970155715942383 
model_pd.lagr.mean(): -14.842247009277344 
model_pd.lambdas: dict_items([('pout', tensor([1.2506], device='cuda:0')), ('power', tensor([0.7480], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9949], device='cuda:0')), ('power', tensor([-21.6443], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.1279086470603943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4906, 4.8920, 4.9158],
        [4.4906, 4.5040, 4.4925],
        [4.4906, 5.5655, 6.2173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.127785325050354 
model_pd.l_d.mean(): -14.945832252502441 
model_pd.lagr.mean(): -14.818046569824219 
model_pd.lambdas: dict_items([('pout', tensor([1.2516], device='cuda:0')), ('power', tensor([0.7469], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9941], device='cuda:0')), ('power', tensor([-21.6432], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.127785325050354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4935, 4.5069, 4.4954],
        [4.4935, 4.8948, 4.9184],
        [4.4935, 5.5686, 6.2204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.12766219675540924 
model_pd.l_d.mean(): -14.921512603759766 
model_pd.lagr.mean(): -14.79384994506836 
model_pd.lambdas: dict_items([('pout', tensor([1.2526], device='cuda:0')), ('power', tensor([0.7458], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9933], device='cuda:0')), ('power', tensor([-21.6420], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.12766219675540924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4965, 4.8976, 4.9211],
        [4.4965, 4.5098, 4.4984],
        [4.4965, 5.5718, 6.2234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.12753936648368835 
model_pd.l_d.mean(): -14.897196769714355 
model_pd.lagr.mean(): -14.769657135009766 
model_pd.lambdas: dict_items([('pout', tensor([1.2536], device='cuda:0')), ('power', tensor([0.7448], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9925], device='cuda:0')), ('power', tensor([-21.6407], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.12753936648368835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4995, 4.9004, 4.9238],
        [4.4995, 5.5749, 6.2264],
        [4.4995, 4.5128, 4.5014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.1274169385433197 
model_pd.l_d.mean(): -14.872886657714844 
model_pd.lagr.mean(): -14.74547004699707 
model_pd.lambdas: dict_items([('pout', tensor([1.2545], device='cuda:0')), ('power', tensor([0.7437], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9918], device='cuda:0')), ('power', tensor([-21.6395], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.1274169385433197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5024, 5.5781, 6.2295],
        [4.5024, 4.5157, 4.5043],
        [4.5024, 4.9033, 4.9264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.12729483842849731 
model_pd.l_d.mean(): -14.848579406738281 
model_pd.lagr.mean(): -14.721284866333008 
model_pd.lambdas: dict_items([('pout', tensor([1.2555], device='cuda:0')), ('power', tensor([0.7426], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9910], device='cuda:0')), ('power', tensor([-21.6383], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.12729483842849731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5054, 5.5812, 6.2325],
        [4.5054, 4.5187, 4.5073],
        [4.5054, 4.9061, 4.9291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.1271730214357376 
model_pd.l_d.mean(): -14.824275016784668 
model_pd.lagr.mean(): -14.697101593017578 
model_pd.lambdas: dict_items([('pout', tensor([1.2565], device='cuda:0')), ('power', tensor([0.7415], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9902], device='cuda:0')), ('power', tensor([-21.6371], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.1271730214357376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5083, 4.9089, 4.9318],
        [4.5083, 5.5844, 6.2355],
        [4.5083, 4.5216, 4.5102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.12705132365226746 
model_pd.l_d.mean(): -14.799978256225586 
model_pd.lagr.mean(): -14.672926902770996 
model_pd.lambdas: dict_items([('pout', tensor([1.2575], device='cuda:0')), ('power', tensor([0.7404], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9894], device='cuda:0')), ('power', tensor([-21.6359], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.12705132365226746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5113, 4.9117, 4.9344],
        [4.5113, 4.5246, 4.5132],
        [4.5113, 5.5876, 6.2386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.1269296258687973 
model_pd.l_d.mean(): -14.775680541992188 
model_pd.lagr.mean(): -14.648751258850098 
model_pd.lambdas: dict_items([('pout', tensor([1.2585], device='cuda:0')), ('power', tensor([0.7393], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9886], device='cuda:0')), ('power', tensor([-21.6347], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.1269296258687973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5143, 5.5907, 6.2417],
        [4.5143, 4.5276, 4.5162],
        [4.5143, 4.9145, 4.9371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.12680774927139282 
model_pd.l_d.mean(): -14.751389503479004 
model_pd.lagr.mean(): -14.624581336975098 
model_pd.lambdas: dict_items([('pout', tensor([1.2595], device='cuda:0')), ('power', tensor([0.7383], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9878], device='cuda:0')), ('power', tensor([-21.6334], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.12680774927139282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5173, 5.5940, 6.2448],
        [4.5173, 4.9174, 4.9398],
        [4.5173, 4.5305, 4.5192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.12668554484844208 
model_pd.l_d.mean(): -14.727102279663086 
model_pd.lagr.mean(): -14.600417137145996 
model_pd.lambdas: dict_items([('pout', tensor([1.2605], device='cuda:0')), ('power', tensor([0.7372], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9870], device='cuda:0')), ('power', tensor([-21.6322], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.12668554484844208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5204, 5.5972, 6.2479],
        [4.5204, 4.5336, 4.5222],
        [4.5204, 4.9203, 4.9426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.1265629231929779 
model_pd.l_d.mean(): -14.7028169631958 
model_pd.lagr.mean(): -14.576253890991211 
model_pd.lambdas: dict_items([('pout', tensor([1.2615], device='cuda:0')), ('power', tensor([0.7361], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9862], device='cuda:0')), ('power', tensor([-21.6309], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.1265629231929779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5234, 4.5366, 4.5253],
        [4.5234, 5.6004, 6.2511],
        [4.5234, 4.9232, 4.9453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.12644006311893463 
model_pd.l_d.mean(): -14.678535461425781 
model_pd.lagr.mean(): -14.552095413208008 
model_pd.lambdas: dict_items([('pout', tensor([1.2625], device='cuda:0')), ('power', tensor([0.7350], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9854], device='cuda:0')), ('power', tensor([-21.6297], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.12644006311893463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5265, 5.6037, 6.2542],
        [4.5265, 4.5396, 4.5283],
        [4.5265, 4.9261, 4.9481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.12631681561470032 
model_pd.l_d.mean(): -14.654255867004395 
model_pd.lagr.mean(): -14.527938842773438 
model_pd.lambdas: dict_items([('pout', tensor([1.2634], device='cuda:0')), ('power', tensor([0.7339], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9846], device='cuda:0')), ('power', tensor([-21.6284], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.12631681561470032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5295, 5.6070, 6.2574],
        [4.5295, 4.9290, 4.9509],
        [4.5295, 4.5427, 4.5314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.12619341909885406 
model_pd.l_d.mean(): -14.629982948303223 
model_pd.lagr.mean(): -14.503789901733398 
model_pd.lambdas: dict_items([('pout', tensor([1.2644], device='cuda:0')), ('power', tensor([0.7329], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9838], device='cuda:0')), ('power', tensor([-21.6271], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.12619341909885406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5326, 4.5457, 4.5345],
        [4.5326, 4.9320, 4.9537],
        [4.5326, 5.6103, 6.2606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.1260698437690735 
model_pd.l_d.mean(): -14.605710983276367 
model_pd.lagr.mean(): -14.47964096069336 
model_pd.lambdas: dict_items([('pout', tensor([1.2654], device='cuda:0')), ('power', tensor([0.7318], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9829], device='cuda:0')), ('power', tensor([-21.6258], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.1260698437690735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5357, 4.9349, 4.9565],
        [4.5357, 4.5488, 4.5376],
        [4.5357, 5.6136, 6.2639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.12594613432884216 
model_pd.l_d.mean(): -14.581445693969727 
model_pd.lagr.mean(): -14.455499649047852 
model_pd.lambdas: dict_items([('pout', tensor([1.2664], device='cuda:0')), ('power', tensor([0.7307], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9821], device='cuda:0')), ('power', tensor([-21.6245], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.12594613432884216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5388, 4.5519, 4.5407],
        [4.5388, 4.9379, 4.9593],
        [4.5388, 5.6170, 6.2671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.12582235038280487 
model_pd.l_d.mean(): -14.557181358337402 
model_pd.lagr.mean(): -14.43135929107666 
model_pd.lambdas: dict_items([('pout', tensor([1.2674], device='cuda:0')), ('power', tensor([0.7296], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9813], device='cuda:0')), ('power', tensor([-21.6232], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.12582235038280487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5420, 5.6203, 6.2704],
        [4.5420, 4.9409, 4.9621],
        [4.5420, 4.5550, 4.5438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.12569844722747803 
model_pd.l_d.mean(): -14.532920837402344 
model_pd.lagr.mean(): -14.407222747802734 
model_pd.lambdas: dict_items([('pout', tensor([1.2683], device='cuda:0')), ('power', tensor([0.7285], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9805], device='cuda:0')), ('power', tensor([-21.6218], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.12569844722747803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5451, 5.6237, 6.2737],
        [4.5451, 4.5581, 4.5469],
        [4.5451, 4.9439, 4.9650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.12557457387447357 
model_pd.l_d.mean(): -14.508665084838867 
model_pd.lagr.mean(): -14.38309097290039 
model_pd.lambdas: dict_items([('pout', tensor([1.2693], device='cuda:0')), ('power', tensor([0.7274], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9796], device='cuda:0')), ('power', tensor([-21.6205], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.12557457387447357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5482, 4.9468, 4.9678],
        [4.5482, 5.6271, 6.2769],
        [4.5482, 4.5612, 4.5501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.12545143067836761 
model_pd.l_d.mean(): -14.484411239624023 
model_pd.lagr.mean(): -14.358960151672363 
model_pd.lambdas: dict_items([('pout', tensor([1.2703], device='cuda:0')), ('power', tensor([0.7264], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9788], device='cuda:0')), ('power', tensor([-21.6192], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.12545143067836761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5513, 4.5643, 4.5532],
        [4.5513, 5.6304, 6.2802],
        [4.5513, 4.9498, 4.9706]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.12532977759838104 
model_pd.l_d.mean(): -14.460165977478027 
model_pd.lagr.mean(): -14.33483600616455 
model_pd.lambdas: dict_items([('pout', tensor([1.2713], device='cuda:0')), ('power', tensor([0.7253], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9780], device='cuda:0')), ('power', tensor([-21.6178], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.12532977759838104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5544, 4.9527, 4.9734],
        [4.5544, 4.5674, 4.5562],
        [4.5544, 5.6337, 6.2834]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.12520980834960938 
model_pd.l_d.mean(): -14.435924530029297 
model_pd.lagr.mean(): -14.310714721679688 
model_pd.lambdas: dict_items([('pout', tensor([1.2723], device='cuda:0')), ('power', tensor([0.7242], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9772], device='cuda:0')), ('power', tensor([-21.6165], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.12520980834960938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5574, 4.9556, 4.9762],
        [4.5574, 4.5704, 4.5593],
        [4.5574, 5.6370, 6.2865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.12509146332740784 
model_pd.l_d.mean(): -14.411687850952148 
model_pd.lagr.mean(): -14.286596298217773 
model_pd.lambdas: dict_items([('pout', tensor([1.2732], device='cuda:0')), ('power', tensor([0.7231], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9764], device='cuda:0')), ('power', tensor([-21.6152], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.12509146332740784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5605, 4.5734, 4.5623],
        [4.5605, 5.6402, 6.2897],
        [4.5605, 4.9585, 4.9789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.12497411668300629 
model_pd.l_d.mean(): -14.387454986572266 
model_pd.lagr.mean(): -14.262480735778809 
model_pd.lambdas: dict_items([('pout', tensor([1.2742], device='cuda:0')), ('power', tensor([0.7220], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9756], device='cuda:0')), ('power', tensor([-21.6139], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.12497411668300629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5635, 4.9614, 4.9817],
        [4.5635, 4.5764, 4.5653],
        [4.5635, 5.6434, 6.2928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.12485706061124802 
model_pd.l_d.mean(): -14.363225936889648 
model_pd.lagr.mean(): -14.23836898803711 
model_pd.lambdas: dict_items([('pout', tensor([1.2752], device='cuda:0')), ('power', tensor([0.7210], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9748], device='cuda:0')), ('power', tensor([-21.6126], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.12485706061124802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5666, 4.9643, 4.9844],
        [4.5666, 4.5795, 4.5684],
        [4.5666, 5.6467, 6.2960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.12473940849304199 
model_pd.l_d.mean(): -14.338998794555664 
model_pd.lagr.mean(): -14.214259147644043 
model_pd.lambdas: dict_items([('pout', tensor([1.2762], device='cuda:0')), ('power', tensor([0.7199], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9740], device='cuda:0')), ('power', tensor([-21.6113], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.12473940849304199
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5697, 4.9673, 4.9873],
        [4.5697, 4.5826, 4.5715],
        [4.5697, 5.6500, 6.2992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.12462013959884644 
model_pd.l_d.mean(): -14.314775466918945 
model_pd.lagr.mean(): -14.190155029296875 
model_pd.lambdas: dict_items([('pout', tensor([1.2771], device='cuda:0')), ('power', tensor([0.7188], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9731], device='cuda:0')), ('power', tensor([-21.6100], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.12462013959884644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5728, 4.9703, 4.9902],
        [4.5728, 4.5857, 4.5746],
        [4.5728, 5.6535, 6.3026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.1244986355304718 
model_pd.l_d.mean(): -14.29055404663086 
model_pd.lagr.mean(): -14.166055679321289 
model_pd.lambdas: dict_items([('pout', tensor([1.2781], device='cuda:0')), ('power', tensor([0.7177], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9723], device='cuda:0')), ('power', tensor([-21.6086], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.1244986355304718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5761, 5.6570, 6.3060],
        [4.5761, 4.9735, 4.9931],
        [4.5761, 4.5889, 4.5779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.12437473982572556 
model_pd.l_d.mean(): -14.266332626342773 
model_pd.lagr.mean(): -14.141958236694336 
model_pd.lambdas: dict_items([('pout', tensor([1.2791], device='cuda:0')), ('power', tensor([0.7166], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9714], device='cuda:0')), ('power', tensor([-21.6072], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.12437473982572556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5793, 4.9766, 4.9962],
        [4.5793, 4.5922, 4.5811],
        [4.5793, 5.6606, 6.3095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.12424882501363754 
model_pd.l_d.mean(): -14.24211311340332 
model_pd.lagr.mean(): -14.117864608764648 
model_pd.lambdas: dict_items([('pout', tensor([1.2800], device='cuda:0')), ('power', tensor([0.7156], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9706], device='cuda:0')), ('power', tensor([-21.6057], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.12424882501363754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5827, 4.5955, 4.5845],
        [4.5827, 5.6642, 6.3131],
        [4.5827, 4.9798, 4.9992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.1241215318441391 
model_pd.l_d.mean(): -14.217897415161133 
model_pd.lagr.mean(): -14.093775749206543 
model_pd.lambdas: dict_items([('pout', tensor([1.2810], device='cuda:0')), ('power', tensor([0.7145], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9697], device='cuda:0')), ('power', tensor([-21.6042], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.1241215318441391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5860, 4.5988, 4.5878],
        [4.5860, 4.9831, 5.0023],
        [4.5860, 5.6679, 6.3168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.12399376928806305 
model_pd.l_d.mean(): -14.193682670593262 
model_pd.lagr.mean(): -14.06968879699707 
model_pd.lambdas: dict_items([('pout', tensor([1.2820], device='cuda:0')), ('power', tensor([0.7134], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9688], device='cuda:0')), ('power', tensor([-21.6027], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.12399376928806305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5894, 4.6021, 4.5912],
        [4.5894, 5.6715, 6.3204],
        [4.5894, 4.9863, 5.0054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.12386640161275864 
model_pd.l_d.mean(): -14.169476509094238 
model_pd.lagr.mean(): -14.045610427856445 
model_pd.lambdas: dict_items([('pout', tensor([1.2830], device='cuda:0')), ('power', tensor([0.7123], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9679], device='cuda:0')), ('power', tensor([-21.6012], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.12386640161275864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5927, 4.9896, 5.0085],
        [4.5927, 4.6055, 4.5945],
        [4.5927, 5.6752, 6.3241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.12373992800712585 
model_pd.l_d.mean(): -14.145275115966797 
model_pd.lagr.mean(): -14.02153491973877 
model_pd.lambdas: dict_items([('pout', tensor([1.2839], device='cuda:0')), ('power', tensor([0.7112], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9670], device='cuda:0')), ('power', tensor([-21.5997], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.12373992800712585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5960, 4.6088, 4.5978],
        [4.5960, 4.9928, 5.0116],
        [4.5960, 5.6788, 6.3276]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.12361475080251694 
model_pd.l_d.mean(): -14.121076583862305 
model_pd.lagr.mean(): -13.997462272644043 
model_pd.lambdas: dict_items([('pout', tensor([1.2849], device='cuda:0')), ('power', tensor([0.7102], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9661], device='cuda:0')), ('power', tensor([-21.5982], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.12361475080251694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5993, 4.9960, 5.0147],
        [4.5993, 4.6120, 4.6011],
        [4.5993, 5.6824, 6.3312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.12349094450473785 
model_pd.l_d.mean(): -14.096883773803711 
model_pd.lagr.mean(): -13.973392486572266 
model_pd.lambdas: dict_items([('pout', tensor([1.2859], device='cuda:0')), ('power', tensor([0.7091], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9653], device='cuda:0')), ('power', tensor([-21.5967], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.12349094450473785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6026, 5.6860, 6.3348],
        [4.6026, 4.6153, 4.6044],
        [4.6026, 4.9992, 5.0177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.12336818873882294 
model_pd.l_d.mean(): -14.072697639465332 
model_pd.lagr.mean(): -13.949329376220703 
model_pd.lambdas: dict_items([('pout', tensor([1.2868], device='cuda:0')), ('power', tensor([0.7080], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9644], device='cuda:0')), ('power', tensor([-21.5952], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.12336818873882294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6059, 4.6186, 4.6077],
        [4.6059, 5.0023, 5.0207],
        [4.6059, 5.6896, 6.3383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.12324600666761398 
model_pd.l_d.mean(): -14.048516273498535 
model_pd.lagr.mean(): -13.925270080566406 
model_pd.lambdas: dict_items([('pout', tensor([1.2878], device='cuda:0')), ('power', tensor([0.7069], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9635], device='cuda:0')), ('power', tensor([-21.5938], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.12324600666761398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6092, 5.0055, 5.0237],
        [4.6092, 4.6218, 4.6109],
        [4.6092, 5.6932, 6.3418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.12312445044517517 
model_pd.l_d.mean(): -14.024335861206055 
model_pd.lagr.mean(): -13.901211738586426 
model_pd.lambdas: dict_items([('pout', tensor([1.2887], device='cuda:0')), ('power', tensor([0.7058], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9627], device='cuda:0')), ('power', tensor([-21.5923], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.12312445044517517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6124, 5.0086, 5.0267],
        [4.6124, 5.6967, 6.3453],
        [4.6124, 4.6250, 4.6142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.12300559133291245 
model_pd.l_d.mean(): -14.000164031982422 
model_pd.lagr.mean(): -13.877158164978027 
model_pd.lambdas: dict_items([('pout', tensor([1.2897], device='cuda:0')), ('power', tensor([0.7048], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9618], device='cuda:0')), ('power', tensor([-21.5908], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.12300559133291245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6155, 4.6282, 4.6173],
        [4.6155, 5.7001, 6.3486],
        [4.6155, 5.0117, 5.0296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.12288981676101685 
model_pd.l_d.mean(): -13.975994110107422 
model_pd.lagr.mean(): -13.853104591369629 
model_pd.lambdas: dict_items([('pout', tensor([1.2907], device='cuda:0')), ('power', tensor([0.7037], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9610], device='cuda:0')), ('power', tensor([-21.5894], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.12288981676101685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6186, 5.0146, 5.0325],
        [4.6186, 5.7035, 6.3519],
        [4.6186, 4.6312, 4.6204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.12277688831090927 
model_pd.l_d.mean(): -13.951831817626953 
model_pd.lagr.mean(): -13.829054832458496 
model_pd.lambdas: dict_items([('pout', tensor([1.2916], device='cuda:0')), ('power', tensor([0.7026], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9602], device='cuda:0')), ('power', tensor([-21.5880], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.12277688831090927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6217, 4.6343, 4.6234],
        [4.6217, 5.7068, 6.3551],
        [4.6217, 5.0176, 5.0353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.12266594171524048 
model_pd.l_d.mean(): -13.927674293518066 
model_pd.lagr.mean(): -13.805007934570312 
model_pd.lambdas: dict_items([('pout', tensor([1.2926], device='cuda:0')), ('power', tensor([0.7015], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9594], device='cuda:0')), ('power', tensor([-21.5866], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.12266594171524048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6247, 4.6373, 4.6265],
        [4.6247, 5.7100, 6.3584],
        [4.6247, 5.0205, 5.0381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.12255597114562988 
model_pd.l_d.mean(): -13.903519630432129 
model_pd.lagr.mean(): -13.780963897705078 
model_pd.lambdas: dict_items([('pout', tensor([1.2935], device='cuda:0')), ('power', tensor([0.7004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9586], device='cuda:0')), ('power', tensor([-21.5853], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.12255597114562988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6277, 5.7133, 6.3616],
        [4.6277, 5.0235, 5.0409],
        [4.6277, 4.6403, 4.6295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.12244560569524765 
model_pd.l_d.mean(): -13.879369735717773 
model_pd.lagr.mean(): -13.75692367553711 
model_pd.lambdas: dict_items([('pout', tensor([1.2945], device='cuda:0')), ('power', tensor([0.6994], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9578], device='cuda:0')), ('power', tensor([-21.5839], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.12244560569524765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6308, 4.6434, 4.6326],
        [4.6308, 5.0265, 5.0438],
        [4.6308, 5.7167, 6.3649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.12233351916074753 
model_pd.l_d.mean(): -13.855217933654785 
model_pd.lagr.mean(): -13.732884407043457 
model_pd.lambdas: dict_items([('pout', tensor([1.2955], device='cuda:0')), ('power', tensor([0.6983], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9570], device='cuda:0')), ('power', tensor([-21.5824], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.12233351916074753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6340, 5.0296, 5.0468],
        [4.6340, 4.6465, 4.6357],
        [4.6340, 5.7202, 6.3684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.12221872806549072 
model_pd.l_d.mean(): -13.831066131591797 
model_pd.lagr.mean(): -13.708847045898438 
model_pd.lambdas: dict_items([('pout', tensor([1.2964], device='cuda:0')), ('power', tensor([0.6972], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9561], device='cuda:0')), ('power', tensor([-21.5810], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.12221872806549072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6372, 5.0328, 5.0498],
        [4.6372, 5.7238, 6.3719],
        [4.6372, 4.6498, 4.6390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.1221008226275444 
model_pd.l_d.mean(): -13.806921005249023 
model_pd.lagr.mean(): -13.684820175170898 
model_pd.lambdas: dict_items([('pout', tensor([1.2974], device='cuda:0')), ('power', tensor([0.6961], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9553], device='cuda:0')), ('power', tensor([-21.5795], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.1221008226275444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6406, 5.7275, 6.3757],
        [4.6406, 5.0360, 5.0530],
        [4.6406, 4.6531, 4.6423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.12197980284690857 
model_pd.l_d.mean(): -13.7827730178833 
model_pd.lagr.mean(): -13.66079330444336 
model_pd.lambdas: dict_items([('pout', tensor([1.2983], device='cuda:0')), ('power', tensor([0.6951], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9544], device='cuda:0')), ('power', tensor([-21.5779], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.12197980284690857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6440, 4.6565, 4.6458],
        [4.6440, 5.7313, 6.3795],
        [4.6440, 5.0394, 5.0563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.12185608595609665 
model_pd.l_d.mean(): -13.758625984191895 
model_pd.lagr.mean(): -13.636770248413086 
model_pd.lambdas: dict_items([('pout', tensor([1.2993], device='cuda:0')), ('power', tensor([0.6940], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9535], device='cuda:0')), ('power', tensor([-21.5762], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.12185608595609665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6475, 5.0429, 5.0596],
        [4.6475, 4.6600, 4.6492],
        [4.6475, 5.7353, 6.3835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.12173037230968475 
model_pd.l_d.mean(): -13.734485626220703 
model_pd.lagr.mean(): -13.612754821777344 
model_pd.lambdas: dict_items([('pout', tensor([1.3002], device='cuda:0')), ('power', tensor([0.6929], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9526], device='cuda:0')), ('power', tensor([-21.5746], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.12173037230968475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6510, 5.0464, 5.0630],
        [4.6510, 5.7392, 6.3875],
        [4.6510, 4.6635, 4.6528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.12160345166921616 
model_pd.l_d.mean(): -13.710346221923828 
model_pd.lagr.mean(): -13.588743209838867 
model_pd.lambdas: dict_items([('pout', tensor([1.3012], device='cuda:0')), ('power', tensor([0.6918], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9516], device='cuda:0')), ('power', tensor([-21.5729], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.12160345166921616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6546, 5.7432, 6.3915],
        [4.6546, 5.0499, 5.0664],
        [4.6546, 4.6670, 4.6563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.12147627770900726 
model_pd.l_d.mean(): -13.686210632324219 
model_pd.lagr.mean(): -13.56473445892334 
model_pd.lambdas: dict_items([('pout', tensor([1.3021], device='cuda:0')), ('power', tensor([0.6907], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9507], device='cuda:0')), ('power', tensor([-21.5712], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.12147627770900726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6581, 5.7472, 6.3956],
        [4.6581, 5.0534, 5.0698],
        [4.6581, 4.6705, 4.6598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.12134954333305359 
model_pd.l_d.mean(): -13.662083625793457 
model_pd.lagr.mean(): -13.54073429107666 
model_pd.lambdas: dict_items([('pout', tensor([1.3031], device='cuda:0')), ('power', tensor([0.6897], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9498], device='cuda:0')), ('power', tensor([-21.5695], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.12134954333305359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6616, 5.0569, 5.0732],
        [4.6616, 4.6741, 4.6634],
        [4.6616, 5.7512, 6.3996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.12122365832328796 
model_pd.l_d.mean(): -13.637960433959961 
model_pd.lagr.mean(): -13.51673698425293 
model_pd.lambdas: dict_items([('pout', tensor([1.3040], device='cuda:0')), ('power', tensor([0.6886], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9488], device='cuda:0')), ('power', tensor([-21.5678], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.12122365832328796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6652, 5.0604, 5.0765],
        [4.6652, 5.7551, 6.4036],
        [4.6652, 4.6776, 4.6669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.12109886109828949 
model_pd.l_d.mean(): -13.61384391784668 
model_pd.lagr.mean(): -13.492745399475098 
model_pd.lambdas: dict_items([('pout', tensor([1.3050], device='cuda:0')), ('power', tensor([0.6875], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9479], device='cuda:0')), ('power', tensor([-21.5661], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.12109886109828949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6687, 4.6811, 4.6704],
        [4.6687, 5.7591, 6.4076],
        [4.6687, 5.0639, 5.0799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.12097480148077011 
model_pd.l_d.mean(): -13.589733123779297 
model_pd.lagr.mean(): -13.468758583068848 
model_pd.lambdas: dict_items([('pout', tensor([1.3059], device='cuda:0')), ('power', tensor([0.6864], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9470], device='cuda:0')), ('power', tensor([-21.5644], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.12097480148077011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6722, 5.7630, 6.4116],
        [4.6722, 5.0673, 5.0832],
        [4.6722, 4.6846, 4.6739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.12085121124982834 
model_pd.l_d.mean(): -13.565629959106445 
model_pd.lagr.mean(): -13.444778442382812 
model_pd.lambdas: dict_items([('pout', tensor([1.3069], device='cuda:0')), ('power', tensor([0.6853], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9460], device='cuda:0')), ('power', tensor([-21.5626], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.12085121124982834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6757, 5.7670, 6.4156],
        [4.6757, 5.0708, 5.0866],
        [4.6757, 4.6881, 4.6774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.12072746455669403 
model_pd.l_d.mean(): -13.541525840759277 
model_pd.lagr.mean(): -13.420798301696777 
model_pd.lambdas: dict_items([('pout', tensor([1.3078], device='cuda:0')), ('power', tensor([0.6843], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9451], device='cuda:0')), ('power', tensor([-21.5609], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.12072746455669403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6793, 4.6916, 4.6810],
        [4.6793, 5.7710, 6.4196],
        [4.6793, 5.0744, 5.0900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.12060307711362839 
model_pd.l_d.mean(): -13.517431259155273 
model_pd.lagr.mean(): -13.396828651428223 
model_pd.lambdas: dict_items([('pout', tensor([1.3088], device='cuda:0')), ('power', tensor([0.6832], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9442], device='cuda:0')), ('power', tensor([-21.5592], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.12060307711362839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6829, 5.0779, 5.0934],
        [4.6829, 5.7751, 6.4238],
        [4.6829, 4.6952, 4.6846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.1204775795340538 
model_pd.l_d.mean(): -13.493337631225586 
model_pd.lagr.mean(): -13.372859954833984 
model_pd.lambdas: dict_items([('pout', tensor([1.3097], device='cuda:0')), ('power', tensor([0.6821], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9432], device='cuda:0')), ('power', tensor([-21.5574], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.1204775795340538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6865, 5.0815, 5.0969],
        [4.6865, 5.7792, 6.4280],
        [4.6865, 4.6989, 4.6882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.12035094201564789 
model_pd.l_d.mean(): -13.46924877166748 
model_pd.lagr.mean(): -13.348897933959961 
model_pd.lambdas: dict_items([('pout', tensor([1.3106], device='cuda:0')), ('power', tensor([0.6810], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9422], device='cuda:0')), ('power', tensor([-21.5556], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.12035094201564789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6902, 5.0852, 5.1005],
        [4.6902, 4.7025, 4.6919],
        [4.6902, 5.7833, 6.4322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.12022309750318527 
model_pd.l_d.mean(): -13.445164680480957 
model_pd.lagr.mean(): -13.324941635131836 
model_pd.lambdas: dict_items([('pout', tensor([1.3116], device='cuda:0')), ('power', tensor([0.6800], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9413], device='cuda:0')), ('power', tensor([-21.5538], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.12022309750318527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6939, 4.7062, 4.6957],
        [4.6939, 5.7876, 6.4365],
        [4.6939, 5.0889, 5.1041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.12009435892105103 
model_pd.l_d.mean(): -13.421085357666016 
model_pd.lagr.mean(): -13.30099105834961 
model_pd.lambdas: dict_items([('pout', tensor([1.3125], device='cuda:0')), ('power', tensor([0.6789], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9403], device='cuda:0')), ('power', tensor([-21.5520], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.12009435892105103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6977, 5.7918, 6.4409],
        [4.6977, 5.0927, 5.1077],
        [4.6977, 4.7100, 4.6994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.119965098798275 
model_pd.l_d.mean(): -13.397008895874023 
model_pd.lagr.mean(): -13.277043342590332 
model_pd.lambdas: dict_items([('pout', tensor([1.3135], device='cuda:0')), ('power', tensor([0.6778], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9393], device='cuda:0')), ('power', tensor([-21.5501], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.119965098798275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7015, 4.7137, 4.7032],
        [4.7015, 5.7961, 6.4453],
        [4.7015, 5.0964, 5.1113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.11983581632375717 
model_pd.l_d.mean(): -13.37293815612793 
model_pd.lagr.mean(): -13.25310230255127 
model_pd.lambdas: dict_items([('pout', tensor([1.3144], device='cuda:0')), ('power', tensor([0.6767], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9383], device='cuda:0')), ('power', tensor([-21.5482], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.11983581632375717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7052, 5.8004, 6.4497],
        [4.7052, 5.1002, 5.1150],
        [4.7052, 4.7175, 4.7069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.11970671266317368 
model_pd.l_d.mean(): -13.34887409210205 
model_pd.lagr.mean(): -13.229166984558105 
model_pd.lambdas: dict_items([('pout', tensor([1.3153], device='cuda:0')), ('power', tensor([0.6756], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9373], device='cuda:0')), ('power', tensor([-21.5463], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.11970671266317368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7090, 5.1039, 5.1186],
        [4.7090, 5.8047, 6.4541],
        [4.7090, 4.7213, 4.7107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.11957802623510361 
model_pd.l_d.mean(): -13.32481575012207 
model_pd.lagr.mean(): -13.20523738861084 
model_pd.lambdas: dict_items([('pout', tensor([1.3163], device='cuda:0')), ('power', tensor([0.6746], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9363], device='cuda:0')), ('power', tensor([-21.5444], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.11957802623510361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7128, 5.1077, 5.1222],
        [4.7128, 5.8090, 6.4585],
        [4.7128, 4.7250, 4.7145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.11944979429244995 
model_pd.l_d.mean(): -13.300762176513672 
model_pd.lagr.mean(): -13.181312561035156 
model_pd.lambdas: dict_items([('pout', tensor([1.3172], device='cuda:0')), ('power', tensor([0.6735], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9353], device='cuda:0')), ('power', tensor([-21.5425], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.11944979429244995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7166, 5.8133, 6.4629],
        [4.7166, 4.7288, 4.7183],
        [4.7166, 5.1115, 5.1259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.1193217858672142 
model_pd.l_d.mean(): -13.276716232299805 
model_pd.lagr.mean(): -13.157394409179688 
model_pd.lambdas: dict_items([('pout', tensor([1.3182], device='cuda:0')), ('power', tensor([0.6724], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9343], device='cuda:0')), ('power', tensor([-21.5406], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.1193217858672142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7204, 4.7326, 4.7221],
        [4.7204, 5.1153, 5.1296],
        [4.7204, 5.8176, 6.4673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.11919384449720383 
model_pd.l_d.mean(): -13.252674102783203 
model_pd.lagr.mean(): -13.133480072021484 
model_pd.lambdas: dict_items([('pout', tensor([1.3191], device='cuda:0')), ('power', tensor([0.6713], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9333], device='cuda:0')), ('power', tensor([-21.5387], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.11919384449720383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7242, 5.8219, 6.4717],
        [4.7242, 5.1191, 5.1332],
        [4.7242, 4.7364, 4.7259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.11906574666500092 
model_pd.l_d.mean(): -13.228636741638184 
model_pd.lagr.mean(): -13.10957145690918 
model_pd.lambdas: dict_items([('pout', tensor([1.3200], device='cuda:0')), ('power', tensor([0.6703], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9323], device='cuda:0')), ('power', tensor([-21.5368], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.11906574666500092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7281, 5.8263, 6.4762],
        [4.7281, 4.7403, 4.7298],
        [4.7281, 5.1229, 5.1370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.11893715709447861 
model_pd.l_d.mean(): -13.204607963562012 
model_pd.lagr.mean(): -13.085670471191406 
model_pd.lambdas: dict_items([('pout', tensor([1.3209], device='cuda:0')), ('power', tensor([0.6692], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9313], device='cuda:0')), ('power', tensor([-21.5348], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.11893715709447861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7320, 4.7441, 4.7336],
        [4.7320, 5.8307, 6.4808],
        [4.7320, 5.1268, 5.1407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.11880815029144287 
model_pd.l_d.mean(): -13.180581092834473 
model_pd.lagr.mean(): -13.061773300170898 
model_pd.lambdas: dict_items([('pout', tensor([1.3219], device='cuda:0')), ('power', tensor([0.6681], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9303], device='cuda:0')), ('power', tensor([-21.5329], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.11880815029144287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7359, 4.7480, 4.7375],
        [4.7359, 5.8352, 6.4854],
        [4.7359, 5.1307, 5.1445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.11867868900299072 
model_pd.l_d.mean(): -13.156558990478516 
model_pd.lagr.mean(): -13.037879943847656 
model_pd.lambdas: dict_items([('pout', tensor([1.3228], device='cuda:0')), ('power', tensor([0.6670], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9293], device='cuda:0')), ('power', tensor([-21.5309], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.11867868900299072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7398, 5.8397, 6.4900],
        [4.7398, 4.7519, 4.7415],
        [4.7398, 5.1346, 5.1483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.11854883283376694 
model_pd.l_d.mean(): -13.132545471191406 
model_pd.lagr.mean(): -13.013997077941895 
model_pd.lambdas: dict_items([('pout', tensor([1.3237], device='cuda:0')), ('power', tensor([0.6660], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9282], device='cuda:0')), ('power', tensor([-21.5289], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.11854883283376694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7438, 5.1386, 5.1521],
        [4.7438, 4.7559, 4.7454],
        [4.7438, 5.8442, 6.4946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.11841883510351181 
model_pd.l_d.mean(): -13.10853385925293 
model_pd.lagr.mean(): -12.99011516571045 
model_pd.lambdas: dict_items([('pout', tensor([1.3247], device='cuda:0')), ('power', tensor([0.6649], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9272], device='cuda:0')), ('power', tensor([-21.5269], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.11841883510351181
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7477, 5.1425, 5.1560],
        [4.7477, 5.8487, 6.4993],
        [4.7477, 4.7598, 4.7494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.11828895658254623 
model_pd.l_d.mean(): -13.0845308303833 
model_pd.lagr.mean(): -12.966241836547852 
model_pd.lambdas: dict_items([('pout', tensor([1.3256], device='cuda:0')), ('power', tensor([0.6638], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9261], device='cuda:0')), ('power', tensor([-21.5248], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.11828895658254623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7517, 5.1465, 5.1598],
        [4.7517, 5.8533, 6.5040],
        [4.7517, 4.7638, 4.7533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.11815925687551498 
model_pd.l_d.mean(): -13.060531616210938 
model_pd.lagr.mean(): -12.94237232208252 
model_pd.lambdas: dict_items([('pout', tensor([1.3265], device='cuda:0')), ('power', tensor([0.6627], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9251], device='cuda:0')), ('power', tensor([-21.5228], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.11815925687551498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7557, 5.8578, 6.5086],
        [4.7557, 4.7677, 4.7573],
        [4.7557, 5.1505, 5.1637]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.11802986264228821 
model_pd.l_d.mean(): -13.036538124084473 
model_pd.lagr.mean(): -12.918508529663086 
model_pd.lambdas: dict_items([('pout', tensor([1.3274], device='cuda:0')), ('power', tensor([0.6616], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9241], device='cuda:0')), ('power', tensor([-21.5207], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.11802986264228821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7596, 4.7717, 4.7613],
        [4.7596, 5.1545, 5.1675],
        [4.7596, 5.8624, 6.5133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.11790087819099426 
model_pd.l_d.mean(): -13.012552261352539 
model_pd.lagr.mean(): -12.894651412963867 
model_pd.lambdas: dict_items([('pout', tensor([1.3284], device='cuda:0')), ('power', tensor([0.6606], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9230], device='cuda:0')), ('power', tensor([-21.5187], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.11790087819099426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7636, 5.1585, 5.1714],
        [4.7636, 4.7757, 4.7653],
        [4.7636, 5.8669, 6.5180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.11777211725711823 
model_pd.l_d.mean(): -12.988571166992188 
model_pd.lagr.mean(): -12.87079906463623 
model_pd.lambdas: dict_items([('pout', tensor([1.3293], device='cuda:0')), ('power', tensor([0.6595], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9220], device='cuda:0')), ('power', tensor([-21.5166], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.11777211725711823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7676, 5.1625, 5.1753],
        [4.7676, 5.8715, 6.5228],
        [4.7676, 4.7797, 4.7693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.1176435798406601 
model_pd.l_d.mean(): -12.964598655700684 
model_pd.lagr.mean(): -12.846955299377441 
model_pd.lambdas: dict_items([('pout', tensor([1.3302], device='cuda:0')), ('power', tensor([0.6584], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9209], device='cuda:0')), ('power', tensor([-21.5146], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.1176435798406601
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7716, 5.8761, 6.5275],
        [4.7716, 4.7837, 4.7733],
        [4.7716, 5.1665, 5.1792]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.11751502752304077 
model_pd.l_d.mean(): -12.940629959106445 
model_pd.lagr.mean(): -12.823115348815918 
model_pd.lambdas: dict_items([('pout', tensor([1.3311], device='cuda:0')), ('power', tensor([0.6573], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9199], device='cuda:0')), ('power', tensor([-21.5125], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.11751502752304077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7757, 5.8807, 6.5323],
        [4.7757, 5.1705, 5.1831],
        [4.7757, 4.7877, 4.7773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.11738642305135727 
model_pd.l_d.mean(): -12.916666030883789 
model_pd.lagr.mean(): -12.79927921295166 
model_pd.lambdas: dict_items([('pout', tensor([1.3320], device='cuda:0')), ('power', tensor([0.6563], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9188], device='cuda:0')), ('power', tensor([-21.5104], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.11738642305135727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7797, 4.7917, 4.7814],
        [4.7797, 5.1746, 5.1870],
        [4.7797, 5.8854, 6.5371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.11725760996341705 
model_pd.l_d.mean(): -12.892708778381348 
model_pd.lagr.mean(): -12.775450706481934 
model_pd.lambdas: dict_items([('pout', tensor([1.3330], device='cuda:0')), ('power', tensor([0.6552], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9178], device='cuda:0')), ('power', tensor([-21.5083], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.11725760996341705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7838, 5.1787, 5.1910],
        [4.7838, 4.7958, 4.7854],
        [4.7838, 5.8900, 6.5419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.11712858080863953 
model_pd.l_d.mean(): -12.868759155273438 
model_pd.lagr.mean(): -12.751630783081055 
model_pd.lambdas: dict_items([('pout', tensor([1.3339], device='cuda:0')), ('power', tensor([0.6541], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9167], device='cuda:0')), ('power', tensor([-21.5061], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.11712858080863953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7879, 5.8947, 6.5468],
        [4.7879, 4.7999, 4.7895],
        [4.7879, 5.1828, 5.1950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.11699944734573364 
model_pd.l_d.mean(): -12.844812393188477 
model_pd.lagr.mean(): -12.727812767028809 
model_pd.lambdas: dict_items([('pout', tensor([1.3348], device='cuda:0')), ('power', tensor([0.6530], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9156], device='cuda:0')), ('power', tensor([-21.5040], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.11699944734573364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7920, 4.8040, 4.7936],
        [4.7920, 5.8995, 6.5517],
        [4.7920, 5.1869, 5.1990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.11687026917934418 
model_pd.l_d.mean(): -12.82087230682373 
model_pd.lagr.mean(): -12.704002380371094 
model_pd.lambdas: dict_items([('pout', tensor([1.3357], device='cuda:0')), ('power', tensor([0.6520], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9146], device='cuda:0')), ('power', tensor([-21.5018], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.11687026917934418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7961, 5.1911, 5.2030],
        [4.7961, 5.9042, 6.5566],
        [4.7961, 4.8081, 4.7978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.1167411059141159 
model_pd.l_d.mean(): -12.7969388961792 
model_pd.lagr.mean(): -12.680197715759277 
model_pd.lambdas: dict_items([('pout', tensor([1.3366], device='cuda:0')), ('power', tensor([0.6509], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9135], device='cuda:0')), ('power', tensor([-21.4997], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.1167411059141159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8003, 4.8122, 4.8019],
        [4.8003, 5.9090, 6.5615],
        [4.8003, 5.1952, 5.2070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.11661207675933838 
model_pd.l_d.mean(): -12.7730131149292 
model_pd.lagr.mean(): -12.656400680541992 
model_pd.lambdas: dict_items([('pout', tensor([1.3375], device='cuda:0')), ('power', tensor([0.6498], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9124], device='cuda:0')), ('power', tensor([-21.4975], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.11661207675933838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8044, 4.8163, 4.8061],
        [4.8044, 5.9137, 6.5664],
        [4.8044, 5.1994, 5.2111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.116483174264431 
model_pd.l_d.mean(): -12.749092102050781 
model_pd.lagr.mean(): -12.632609367370605 
model_pd.lambdas: dict_items([('pout', tensor([1.3384], device='cuda:0')), ('power', tensor([0.6487], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113], device='cuda:0')), ('power', tensor([-21.4953], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.116483174264431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8086, 5.2036, 5.2151],
        [4.8086, 5.9185, 6.5714],
        [4.8086, 4.8205, 4.8102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.11635448783636093 
model_pd.l_d.mean(): -12.725175857543945 
model_pd.lagr.mean(): -12.608820915222168 
model_pd.lambdas: dict_items([('pout', tensor([1.3394], device='cuda:0')), ('power', tensor([0.6477], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9103], device='cuda:0')), ('power', tensor([-21.4931], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.11635448783636093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8128, 5.2078, 5.2192],
        [4.8128, 5.9233, 6.5764],
        [4.8128, 4.8247, 4.8144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.1162259578704834 
model_pd.l_d.mean(): -12.70126724243164 
model_pd.lagr.mean(): -12.585041046142578 
model_pd.lambdas: dict_items([('pout', tensor([1.3403], device='cuda:0')), ('power', tensor([0.6466], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9092], device='cuda:0')), ('power', tensor([-21.4909], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.1162259578704834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8169, 4.8288, 4.8186],
        [4.8169, 5.9282, 6.5814],
        [4.8169, 5.2120, 5.2233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.11609750241041183 
model_pd.l_d.mean(): -12.677364349365234 
model_pd.lagr.mean(): -12.561266899108887 
model_pd.lambdas: dict_items([('pout', tensor([1.3412], device='cuda:0')), ('power', tensor([0.6455], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9081], device='cuda:0')), ('power', tensor([-21.4886], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.11609750241041183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8212, 4.8330, 4.8228],
        [4.8212, 5.9330, 6.5864],
        [4.8212, 5.2162, 5.2274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.1159689649939537 
model_pd.l_d.mean(): -12.653467178344727 
model_pd.lagr.mean(): -12.537498474121094 
model_pd.lambdas: dict_items([('pout', tensor([1.3421], device='cuda:0')), ('power', tensor([0.6444], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9070], device='cuda:0')), ('power', tensor([-21.4864], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.1159689649939537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8254, 4.8372, 4.8270],
        [4.8254, 5.9379, 6.5915],
        [4.8254, 5.2205, 5.2315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.11584027111530304 
model_pd.l_d.mean(): -12.62957763671875 
model_pd.lagr.mean(): -12.513737678527832 
model_pd.lambdas: dict_items([('pout', tensor([1.3430], device='cuda:0')), ('power', tensor([0.6434], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9059], device='cuda:0')), ('power', tensor([-21.4841], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.11584027111530304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8296, 5.9428, 6.5966],
        [4.8296, 5.2248, 5.2357],
        [4.8296, 4.8415, 4.8312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.11571149528026581 
model_pd.l_d.mean(): -12.605693817138672 
model_pd.lagr.mean(): -12.489982604980469 
model_pd.lambdas: dict_items([('pout', tensor([1.3439], device='cuda:0')), ('power', tensor([0.6423], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9048], device='cuda:0')), ('power', tensor([-21.4819], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.11571149528026581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8339, 5.9477, 6.6017],
        [4.8339, 5.2291, 5.2399],
        [4.8339, 4.8457, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.11558254808187485 
model_pd.l_d.mean(): -12.581816673278809 
model_pd.lagr.mean(): -12.46623420715332 
model_pd.lambdas: dict_items([('pout', tensor([1.3448], device='cuda:0')), ('power', tensor([0.6412], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9037], device='cuda:0')), ('power', tensor([-21.4796], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.11558254808187485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8382, 4.8500, 4.8398],
        [4.8382, 5.2334, 5.2441],
        [4.8382, 5.9527, 6.6068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.1154535710811615 
model_pd.l_d.mean(): -12.557944297790527 
model_pd.lagr.mean(): -12.442490577697754 
model_pd.lambdas: dict_items([('pout', tensor([1.3457], device='cuda:0')), ('power', tensor([0.6402], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9026], device='cuda:0')), ('power', tensor([-21.4773], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.1154535710811615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8425, 5.9576, 6.6120],
        [4.8425, 5.2377, 5.2483],
        [4.8425, 4.8543, 4.8441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.11532455682754517 
model_pd.l_d.mean(): -12.534079551696777 
model_pd.lagr.mean(): -12.418754577636719 
model_pd.lambdas: dict_items([('pout', tensor([1.3466], device='cuda:0')), ('power', tensor([0.6391], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9015], device='cuda:0')), ('power', tensor([-21.4750], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.11532455682754517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8468, 5.2421, 5.2525],
        [4.8468, 5.9626, 6.6172],
        [4.8468, 4.8586, 4.8484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.1151956170797348 
model_pd.l_d.mean(): -12.510221481323242 
model_pd.lagr.mean(): -12.395026206970215 
model_pd.lambdas: dict_items([('pout', tensor([1.3475], device='cuda:0')), ('power', tensor([0.6380], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9004], device='cuda:0')), ('power', tensor([-21.4726], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.1151956170797348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8511, 4.8629, 4.8527],
        [4.8511, 5.9677, 6.6225],
        [4.8511, 5.2465, 5.2568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.11506675183773041 
model_pd.l_d.mean(): -12.486369132995605 
model_pd.lagr.mean(): -12.371302604675293 
model_pd.lambdas: dict_items([('pout', tensor([1.3484], device='cuda:0')), ('power', tensor([0.6369], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8992], device='cuda:0')), ('power', tensor([-21.4703], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.11506675183773041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8555, 4.8672, 4.8571],
        [4.8555, 5.9727, 6.6277],
        [4.8555, 5.2509, 5.2610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.11493806540966034 
model_pd.l_d.mean(): -12.462520599365234 
model_pd.lagr.mean(): -12.347582817077637 
model_pd.lambdas: dict_items([('pout', tensor([1.3493], device='cuda:0')), ('power', tensor([0.6359], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8981], device='cuda:0')), ('power', tensor([-21.4679], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.11493806540966034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8598, 5.9778, 6.6330],
        [4.8598, 5.2553, 5.2653],
        [4.8598, 4.8716, 4.8614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.1148095354437828 
model_pd.l_d.mean(): -12.438681602478027 
model_pd.lagr.mean(): -12.323871612548828 
model_pd.lambdas: dict_items([('pout', tensor([1.3502], device='cuda:0')), ('power', tensor([0.6348], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8970], device='cuda:0')), ('power', tensor([-21.4655], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.1148095354437828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8642, 4.8759, 4.8658],
        [4.8642, 5.2597, 5.2696],
        [4.8642, 5.9828, 6.6383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.11468100547790527 
model_pd.l_d.mean(): -12.414848327636719 
model_pd.lagr.mean(): -12.300167083740234 
model_pd.lambdas: dict_items([('pout', tensor([1.3511], device='cuda:0')), ('power', tensor([0.6337], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8959], device='cuda:0')), ('power', tensor([-21.4632], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.11468100547790527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8686, 5.9879, 6.6436],
        [4.8686, 5.2641, 5.2739],
        [4.8686, 4.8803, 4.8702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.11455252021551132 
model_pd.l_d.mean(): -12.391020774841309 
model_pd.lagr.mean(): -12.276468276977539 
model_pd.lambdas: dict_items([('pout', tensor([1.3520], device='cuda:0')), ('power', tensor([0.6326], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8947], device='cuda:0')), ('power', tensor([-21.4608], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.11455252021551132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8730, 4.8847, 4.8746],
        [4.8730, 5.2686, 5.2783],
        [4.8730, 5.9930, 6.6489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.1144239753484726 
model_pd.l_d.mean(): -12.367199897766113 
model_pd.lagr.mean(): -12.252776145935059 
model_pd.lambdas: dict_items([('pout', tensor([1.3529], device='cuda:0')), ('power', tensor([0.6316], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8936], device='cuda:0')), ('power', tensor([-21.4583], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.1144239753484726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8774, 5.2731, 5.2826],
        [4.8774, 5.9982, 6.6543],
        [4.8774, 4.8891, 4.8790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.1142953485250473 
model_pd.l_d.mean(): -12.343387603759766 
model_pd.lagr.mean(): -12.229092597961426 
model_pd.lambdas: dict_items([('pout', tensor([1.3538], device='cuda:0')), ('power', tensor([0.6305], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8925], device='cuda:0')), ('power', tensor([-21.4559], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.1142953485250473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8819, 6.0034, 6.6597],
        [4.8819, 5.2776, 5.2870],
        [4.8819, 4.8936, 4.8835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.11416660994291306 
model_pd.l_d.mean(): -12.319580078125 
model_pd.lagr.mean(): -12.205413818359375 
model_pd.lambdas: dict_items([('pout', tensor([1.3547], device='cuda:0')), ('power', tensor([0.6294], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8913], device='cuda:0')), ('power', tensor([-21.4535], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.11416660994291306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8864, 6.0086, 6.6651],
        [4.8864, 5.2821, 5.2914],
        [4.8864, 4.8980, 4.8879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.11403782665729523 
model_pd.l_d.mean(): -12.295780181884766 
model_pd.lagr.mean(): -12.181742668151855 
model_pd.lambdas: dict_items([('pout', tensor([1.3555], device='cuda:0')), ('power', tensor([0.6283], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8902], device='cuda:0')), ('power', tensor([-21.4510], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.11403782665729523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8909, 6.0138, 6.6706],
        [4.8909, 4.9025, 4.8924],
        [4.8909, 5.2867, 5.2958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.11390894651412964 
model_pd.l_d.mean(): -12.27198600769043 
model_pd.lagr.mean(): -12.158077239990234 
model_pd.lambdas: dict_items([('pout', tensor([1.3564], device='cuda:0')), ('power', tensor([0.6273], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8890], device='cuda:0')), ('power', tensor([-21.4485], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.11390894651412964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8954, 5.2913, 5.3003],
        [4.8954, 6.0191, 6.6761],
        [4.8954, 4.9070, 4.8969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.11378017067909241 
model_pd.l_d.mean(): -12.248201370239258 
model_pd.lagr.mean(): -12.134421348571777 
model_pd.lambdas: dict_items([('pout', tensor([1.3573], device='cuda:0')), ('power', tensor([0.6262], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8878], device='cuda:0')), ('power', tensor([-21.4460], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.11378017067909241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8999, 4.9115, 4.9015],
        [4.8999, 6.0243, 6.6816],
        [4.8999, 5.2958, 5.3048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.11365139484405518 
model_pd.l_d.mean(): -12.224420547485352 
model_pd.lagr.mean(): -12.110769271850586 
model_pd.lambdas: dict_items([('pout', tensor([1.3582], device='cuda:0')), ('power', tensor([0.6251], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8867], device='cuda:0')), ('power', tensor([-21.4435], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.11365139484405518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9044, 5.3005, 5.3092],
        [4.9044, 4.9161, 4.9060],
        [4.9044, 6.0296, 6.6872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.1135227233171463 
model_pd.l_d.mean(): -12.200648307800293 
model_pd.lagr.mean(): -12.087125778198242 
model_pd.lambdas: dict_items([('pout', tensor([1.3591], device='cuda:0')), ('power', tensor([0.6241], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8855], device='cuda:0')), ('power', tensor([-21.4410], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.1135227233171463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9090, 5.3051, 5.3138],
        [4.9090, 4.9206, 4.9105],
        [4.9090, 6.0350, 6.6927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.1133941113948822 
model_pd.l_d.mean(): -12.17688274383545 
model_pd.lagr.mean(): -12.063488960266113 
model_pd.lambdas: dict_items([('pout', tensor([1.3600], device='cuda:0')), ('power', tensor([0.6230], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8843], device='cuda:0')), ('power', tensor([-21.4384], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.1133941113948822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9136, 6.0403, 6.6983],
        [4.9136, 4.9252, 4.9151],
        [4.9136, 5.3097, 5.3183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.11326553672552109 
model_pd.l_d.mean(): -12.15312385559082 
model_pd.lagr.mean(): -12.039857864379883 
model_pd.lambdas: dict_items([('pout', tensor([1.3609], device='cuda:0')), ('power', tensor([0.6219], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8832], device='cuda:0')), ('power', tensor([-21.4359], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.11326553672552109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9181, 5.3144, 5.3228],
        [4.9181, 4.9297, 4.9197],
        [4.9181, 6.0457, 6.7040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.11313697695732117 
model_pd.l_d.mean(): -12.12937068939209 
model_pd.lagr.mean(): -12.016233444213867 
model_pd.lambdas: dict_items([('pout', tensor([1.3617], device='cuda:0')), ('power', tensor([0.6208], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8820], device='cuda:0')), ('power', tensor([-21.4333], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.11313697695732117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9228, 5.3191, 5.3274],
        [4.9228, 4.9343, 4.9243],
        [4.9228, 6.0511, 6.7096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.11300843954086304 
model_pd.l_d.mean(): -12.10562515258789 
model_pd.lagr.mean(): -11.992616653442383 
model_pd.lambdas: dict_items([('pout', tensor([1.3626], device='cuda:0')), ('power', tensor([0.6198], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8808], device='cuda:0')), ('power', tensor([-21.4307], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.11300843954086304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9274, 5.3238, 5.3320],
        [4.9274, 6.0565, 6.7153],
        [4.9274, 4.9390, 4.9289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.11287981271743774 
model_pd.l_d.mean(): -12.081888198852539 
model_pd.lagr.mean(): -11.969008445739746 
model_pd.lambdas: dict_items([('pout', tensor([1.3635], device='cuda:0')), ('power', tensor([0.6187], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8796], device='cuda:0')), ('power', tensor([-21.4281], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.11287981271743774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9320, 6.0619, 6.7210],
        [4.9320, 4.9436, 4.9336],
        [4.9320, 5.3285, 5.3366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.11275117099285126 
model_pd.l_d.mean(): -12.05815601348877 
model_pd.lagr.mean(): -11.945405006408691 
model_pd.lambdas: dict_items([('pout', tensor([1.3644], device='cuda:0')), ('power', tensor([0.6176], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8784], device='cuda:0')), ('power', tensor([-21.4255], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.11275117099285126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9367, 4.9483, 4.9383],
        [4.9367, 5.3333, 5.3412],
        [4.9367, 6.0674, 6.7268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.11262246966362 
model_pd.l_d.mean(): -12.034433364868164 
model_pd.lagr.mean(): -11.9218111038208 
model_pd.lambdas: dict_items([('pout', tensor([1.3653], device='cuda:0')), ('power', tensor([0.6166], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8773], device='cuda:0')), ('power', tensor([-21.4229], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.11262246966362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9414, 4.9529, 4.9430],
        [4.9414, 6.0729, 6.7326],
        [4.9414, 5.3381, 5.3459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.11249373853206635 
model_pd.l_d.mean(): -12.01071548461914 
model_pd.lagr.mean(): -11.898221969604492 
model_pd.lambdas: dict_items([('pout', tensor([1.3661], device='cuda:0')), ('power', tensor([0.6155], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8761], device='cuda:0')), ('power', tensor([-21.4202], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.11249373853206635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9461, 6.0784, 6.7384],
        [4.9461, 5.3429, 5.3506],
        [4.9461, 4.9576, 4.9477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.1123650074005127 
model_pd.l_d.mean(): -11.987004280090332 
model_pd.lagr.mean(): -11.874639511108398 
model_pd.lambdas: dict_items([('pout', tensor([1.3670], device='cuda:0')), ('power', tensor([0.6144], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8748], device='cuda:0')), ('power', tensor([-21.4175], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.1123650074005127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9508, 5.3477, 5.3553],
        [4.9508, 6.0840, 6.7442],
        [4.9508, 4.9624, 4.9524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.11223630607128143 
model_pd.l_d.mean(): -11.963301658630371 
model_pd.lagr.mean(): -11.851065635681152 
model_pd.lambdas: dict_items([('pout', tensor([1.3679], device='cuda:0')), ('power', tensor([0.6133], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8736], device='cuda:0')), ('power', tensor([-21.4149], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.11223630607128143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9556, 4.9671, 4.9571],
        [4.9556, 6.0896, 6.7501],
        [4.9556, 5.3526, 5.3600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.11210767924785614 
model_pd.l_d.mean(): -11.939604759216309 
model_pd.lagr.mean(): -11.827497482299805 
model_pd.lambdas: dict_items([('pout', tensor([1.3688], device='cuda:0')), ('power', tensor([0.6123], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8724], device='cuda:0')), ('power', tensor([-21.4122], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.11210767924785614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9604, 4.9719, 4.9619],
        [4.9604, 6.0952, 6.7560],
        [4.9604, 5.3574, 5.3647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.11197908222675323 
model_pd.l_d.mean(): -11.91591739654541 
model_pd.lagr.mean(): -11.803937911987305 
model_pd.lambdas: dict_items([('pout', tensor([1.3696], device='cuda:0')), ('power', tensor([0.6112], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8712], device='cuda:0')), ('power', tensor([-21.4094], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.11197908222675323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9652, 4.9766, 4.9667],
        [4.9652, 5.3623, 5.3695],
        [4.9652, 6.1008, 6.7619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.11185049265623093 
model_pd.l_d.mean(): -11.892236709594727 
model_pd.lagr.mean(): -11.780385971069336 
model_pd.lambdas: dict_items([('pout', tensor([1.3705], device='cuda:0')), ('power', tensor([0.6101], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8700], device='cuda:0')), ('power', tensor([-21.4067], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.11185049265623093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9700, 4.9814, 4.9715],
        [4.9700, 5.3672, 5.3743],
        [4.9700, 6.1064, 6.7679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.1117219552397728 
model_pd.l_d.mean(): -11.868562698364258 
model_pd.lagr.mean(): -11.756840705871582 
model_pd.lambdas: dict_items([('pout', tensor([1.3714], device='cuda:0')), ('power', tensor([0.6091], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8688], device='cuda:0')), ('power', tensor([-21.4039], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.1117219552397728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9748, 4.9862, 4.9763],
        [4.9748, 6.1121, 6.7739],
        [4.9748, 5.3722, 5.3791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.11159342527389526 
model_pd.l_d.mean(): -11.84489631652832 
model_pd.lagr.mean(): -11.73330307006836 
model_pd.lambdas: dict_items([('pout', tensor([1.3722], device='cuda:0')), ('power', tensor([0.6080], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8675], device='cuda:0')), ('power', tensor([-21.4012], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.11159342527389526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9796, 4.9911, 4.9812],
        [4.9796, 5.3771, 5.3839],
        [4.9796, 6.1178, 6.7799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.1114649772644043 
model_pd.l_d.mean(): -11.821237564086914 
model_pd.lagr.mean(): -11.709772109985352 
model_pd.lambdas: dict_items([('pout', tensor([1.3731], device='cuda:0')), ('power', tensor([0.6069], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8663], device='cuda:0')), ('power', tensor([-21.3984], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.1114649772644043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9845, 5.3821, 5.3888],
        [4.9845, 6.1236, 6.7859],
        [4.9845, 4.9959, 4.9860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.1113366112112999 
model_pd.l_d.mean(): -11.797587394714355 
model_pd.lagr.mean(): -11.686250686645508 
model_pd.lambdas: dict_items([('pout', tensor([1.3740], device='cuda:0')), ('power', tensor([0.6059], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8651], device='cuda:0')), ('power', tensor([-21.3956], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.1113366112112999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9894, 5.3871, 5.3937],
        [4.9894, 6.1293, 6.7920],
        [4.9894, 5.0008, 4.9909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.11120828241109848 
model_pd.l_d.mean(): -11.773943901062012 
model_pd.lagr.mean(): -11.662735939025879 
model_pd.lambdas: dict_items([('pout', tensor([1.3748], device='cuda:0')), ('power', tensor([0.6048], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8638], device='cuda:0')), ('power', tensor([-21.3928], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.11120828241109848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9943, 5.0057, 4.9958],
        [4.9943, 5.3921, 5.3986],
        [4.9943, 6.1351, 6.7981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.11107999086380005 
model_pd.l_d.mean(): -11.750307083129883 
model_pd.lagr.mean(): -11.639226913452148 
model_pd.lambdas: dict_items([('pout', tensor([1.3757], device='cuda:0')), ('power', tensor([0.6037], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8626], device='cuda:0')), ('power', tensor([-21.3899], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.11107999086380005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9992, 5.3971, 5.4035],
        [4.9992, 5.0106, 5.0007],
        [4.9992, 6.1409, 6.8043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.1109517514705658 
model_pd.l_d.mean(): -11.726677894592285 
model_pd.lagr.mean(): -11.615726470947266 
model_pd.lambdas: dict_items([('pout', tensor([1.3766], device='cuda:0')), ('power', tensor([0.6026], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8614], device='cuda:0')), ('power', tensor([-21.3871], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.1109517514705658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0041, 6.1467, 6.8104],
        [5.0041, 5.0155, 5.0056],
        [5.0041, 5.4022, 5.4084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.11082357168197632 
model_pd.l_d.mean(): -11.703056335449219 
model_pd.lagr.mean(): -11.592232704162598 
model_pd.lambdas: dict_items([('pout', tensor([1.3774], device='cuda:0')), ('power', tensor([0.6016], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8601], device='cuda:0')), ('power', tensor([-21.3842], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.11082357168197632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0091, 5.0205, 5.0106],
        [5.0091, 5.4073, 5.4134],
        [5.0091, 6.1526, 6.8166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.11069535464048386 
model_pd.l_d.mean(): -11.679444313049316 
model_pd.lagr.mean(): -11.56874942779541 
model_pd.lambdas: dict_items([('pout', tensor([1.3783], device='cuda:0')), ('power', tensor([0.6005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8589], device='cuda:0')), ('power', tensor([-21.3813], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.11069535464048386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0140, 5.4124, 5.4184],
        [5.0140, 6.1585, 6.8229],
        [5.0140, 5.0254, 5.0156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.1105671152472496 
model_pd.l_d.mean(): -11.655838012695312 
model_pd.lagr.mean(): -11.545270919799805 
model_pd.lambdas: dict_items([('pout', tensor([1.3791], device='cuda:0')), ('power', tensor([0.5994], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8576], device='cuda:0')), ('power', tensor([-21.3784], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.1105671152472496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0190, 6.1644, 6.8292],
        [5.0190, 5.4175, 5.4234],
        [5.0190, 5.0304, 5.0206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.11043882369995117 
model_pd.l_d.mean(): -11.632240295410156 
model_pd.lagr.mean(): -11.521800994873047 
model_pd.lambdas: dict_items([('pout', tensor([1.3800], device='cuda:0')), ('power', tensor([0.5984], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8563], device='cuda:0')), ('power', tensor([-21.3755], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.11043882369995117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0241, 5.4227, 5.4284],
        [5.0241, 6.1704, 6.8355],
        [5.0241, 5.0354, 5.0256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.11031051725149155 
model_pd.l_d.mean(): -11.608651161193848 
model_pd.lagr.mean(): -11.498340606689453 
model_pd.lambdas: dict_items([('pout', tensor([1.3808], device='cuda:0')), ('power', tensor([0.5973], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8551], device='cuda:0')), ('power', tensor([-21.3725], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.11031051725149155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0291, 6.1763, 6.8418],
        [5.0291, 5.0405, 5.0306],
        [5.0291, 5.4279, 5.4335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.1101822555065155 
model_pd.l_d.mean(): -11.585067749023438 
model_pd.lagr.mean(): -11.474885940551758 
model_pd.lambdas: dict_items([('pout', tensor([1.3817], device='cuda:0')), ('power', tensor([0.5962], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8538], device='cuda:0')), ('power', tensor([-21.3696], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.1101822555065155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0342, 5.0455, 5.0357],
        [5.0342, 5.4331, 5.4386],
        [5.0342, 6.1824, 6.8482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.11005398631095886 
model_pd.l_d.mean(): -11.561492919921875 
model_pd.lagr.mean(): -11.451438903808594 
model_pd.lambdas: dict_items([('pout', tensor([1.3826], device='cuda:0')), ('power', tensor([0.5952], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8525], device='cuda:0')), ('power', tensor([-21.3666], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.11005398631095886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0393, 5.0506, 5.0408],
        [5.0393, 6.1884, 6.8546],
        [5.0393, 5.4383, 5.4437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.10992573946714401 
model_pd.l_d.mean(): -11.537925720214844 
model_pd.lagr.mean(): -11.428000450134277 
model_pd.lambdas: dict_items([('pout', tensor([1.3834], device='cuda:0')), ('power', tensor([0.5941], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8512], device='cuda:0')), ('power', tensor([-21.3636], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.10992573946714401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0444, 5.4436, 5.4488],
        [5.0444, 5.0557, 5.0459],
        [5.0444, 6.1945, 6.8610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.10979755967855453 
model_pd.l_d.mean(): -11.514368057250977 
model_pd.lagr.mean(): -11.404570579528809 
model_pd.lambdas: dict_items([('pout', tensor([1.3843], device='cuda:0')), ('power', tensor([0.5930], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8499], device='cuda:0')), ('power', tensor([-21.3606], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.10979755967855453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0495, 5.4488, 5.4540],
        [5.0495, 5.0608, 5.0510],
        [5.0495, 6.2006, 6.8675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.10966935008764267 
model_pd.l_d.mean(): -11.490817070007324 
model_pd.lagr.mean(): -11.381147384643555 
model_pd.lambdas: dict_items([('pout', tensor([1.3851], device='cuda:0')), ('power', tensor([0.5920], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8486], device='cuda:0')), ('power', tensor([-21.3575], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.10966935008764267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0546, 5.4541, 5.4592],
        [5.0546, 6.2067, 6.8740],
        [5.0546, 5.0660, 5.0561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.1095411628484726 
model_pd.l_d.mean(): -11.467273712158203 
model_pd.lagr.mean(): -11.357732772827148 
model_pd.lambdas: dict_items([('pout', tensor([1.3859], device='cuda:0')), ('power', tensor([0.5909], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8474], device='cuda:0')), ('power', tensor([-21.3545], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.1095411628484726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0598, 5.0711, 5.0613],
        [5.0598, 6.2128, 6.8806],
        [5.0598, 5.4595, 5.4644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.1094130128622055 
model_pd.l_d.mean(): -11.443737983703613 
model_pd.lagr.mean(): -11.334324836730957 
model_pd.lambdas: dict_items([('pout', tensor([1.3868], device='cuda:0')), ('power', tensor([0.5898], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8461], device='cuda:0')), ('power', tensor([-21.3514], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.1094130128622055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0650, 5.0763, 5.0665],
        [5.0650, 5.4648, 5.4696],
        [5.0650, 6.2190, 6.8871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.109284907579422 
model_pd.l_d.mean(): -11.420212745666504 
model_pd.lagr.mean(): -11.310927391052246 
model_pd.lambdas: dict_items([('pout', tensor([1.3876], device='cuda:0')), ('power', tensor([0.5888], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8447], device='cuda:0')), ('power', tensor([-21.3483], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.109284907579422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0702, 5.4702, 5.4749],
        [5.0702, 5.0815, 5.0717],
        [5.0702, 6.2252, 6.8938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.10915675759315491 
model_pd.l_d.mean(): -11.39669418334961 
model_pd.lagr.mean(): -11.287537574768066 
model_pd.lambdas: dict_items([('pout', tensor([1.3885], device='cuda:0')), ('power', tensor([0.5877], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8434], device='cuda:0')), ('power', tensor([-21.3452], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.10915675759315491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0755, 6.2315, 6.9004],
        [5.0755, 5.0867, 5.0770],
        [5.0755, 5.4756, 5.4802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.109028659760952 
model_pd.l_d.mean(): -11.373183250427246 
model_pd.lagr.mean(): -11.264154434204102 
model_pd.lambdas: dict_items([('pout', tensor([1.3893], device='cuda:0')), ('power', tensor([0.5866], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8421], device='cuda:0')), ('power', tensor([-21.3421], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.109028659760952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0807, 6.2378, 6.9071],
        [5.0807, 5.0920, 5.0822],
        [5.0807, 5.4810, 5.4855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.10890048742294312 
model_pd.l_d.mean(): -11.349681854248047 
model_pd.lagr.mean(): -11.240781784057617 
model_pd.lambdas: dict_items([('pout', tensor([1.3902], device='cuda:0')), ('power', tensor([0.5856], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8408], device='cuda:0')), ('power', tensor([-21.3389], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.10890048742294312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0860, 6.2441, 6.9138],
        [5.0860, 5.4865, 5.4909],
        [5.0860, 5.0973, 5.0875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.1087723970413208 
model_pd.l_d.mean(): -11.326188087463379 
model_pd.lagr.mean(): -11.217415809631348 
model_pd.lambdas: dict_items([('pout', tensor([1.3910], device='cuda:0')), ('power', tensor([0.5845], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8395], device='cuda:0')), ('power', tensor([-21.3357], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.1087723970413208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0913, 6.2504, 6.9206],
        [5.0913, 5.1026, 5.0928],
        [5.0913, 5.4920, 5.4962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.10864429175853729 
model_pd.l_d.mean(): -11.302703857421875 
model_pd.lagr.mean(): -11.194059371948242 
model_pd.lambdas: dict_items([('pout', tensor([1.3918], device='cuda:0')), ('power', tensor([0.5834], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8382], device='cuda:0')), ('power', tensor([-21.3325], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.10864429175853729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0966, 5.1079, 5.0981],
        [5.0966, 6.2568, 6.9274],
        [5.0966, 5.4975, 5.5016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.1085161566734314 
model_pd.l_d.mean(): -11.279226303100586 
model_pd.lagr.mean(): -11.170710563659668 
model_pd.lambdas: dict_items([('pout', tensor([1.3927], device='cuda:0')), ('power', tensor([0.5824], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8368], device='cuda:0')), ('power', tensor([-21.3293], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.1085161566734314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1020, 5.5030, 5.5071],
        [5.1020, 5.1132, 5.1035],
        [5.1020, 6.2632, 6.9342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.10838809609413147 
model_pd.l_d.mean(): -11.255758285522461 
model_pd.lagr.mean(): -11.147370338439941 
model_pd.lambdas: dict_items([('pout', tensor([1.3935], device='cuda:0')), ('power', tensor([0.5813], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8355], device='cuda:0')), ('power', tensor([-21.3261], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.10838809609413147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1074, 5.1186, 5.1088],
        [5.1074, 5.5086, 5.5125],
        [5.1074, 6.2696, 6.9411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.10826005786657333 
model_pd.l_d.mean(): -11.232297897338867 
model_pd.lagr.mean(): -11.124037742614746 
model_pd.lambdas: dict_items([('pout', tensor([1.3944], device='cuda:0')), ('power', tensor([0.5802], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8341], device='cuda:0')), ('power', tensor([-21.3228], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.10826005786657333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1128, 6.2761, 6.9480],
        [5.1128, 5.1240, 5.1142],
        [5.1128, 5.5142, 5.5180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.10813208669424057 
model_pd.l_d.mean(): -11.208846092224121 
model_pd.lagr.mean(): -11.100713729858398 
model_pd.lambdas: dict_items([('pout', tensor([1.3952], device='cuda:0')), ('power', tensor([0.5792], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8328], device='cuda:0')), ('power', tensor([-21.3195], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.10813208669424057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1182, 5.1294, 5.1197],
        [5.1182, 6.2826, 6.9549],
        [5.1182, 5.5198, 5.5235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.1080041378736496 
model_pd.l_d.mean(): -11.185404777526855 
model_pd.lagr.mean(): -11.077400207519531 
model_pd.lambdas: dict_items([('pout', tensor([1.3960], device='cuda:0')), ('power', tensor([0.5781], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8314], device='cuda:0')), ('power', tensor([-21.3162], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.1080041378736496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1236, 5.1348, 5.1251],
        [5.1236, 5.5255, 5.5290],
        [5.1236, 6.2891, 6.9619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.10787615180015564 
model_pd.l_d.mean(): -11.161970138549805 
model_pd.lagr.mean(): -11.054094314575195 
model_pd.lambdas: dict_items([('pout', tensor([1.3968], device='cuda:0')), ('power', tensor([0.5770], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8301], device='cuda:0')), ('power', tensor([-21.3129], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.10787615180015564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1291, 5.1403, 5.1306],
        [5.1291, 6.2956, 6.9690],
        [5.1291, 5.5311, 5.5346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.10774821043014526 
model_pd.l_d.mean(): -11.138545036315918 
model_pd.lagr.mean(): -11.030797004699707 
model_pd.lambdas: dict_items([('pout', tensor([1.3977], device='cuda:0')), ('power', tensor([0.5760], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8287], device='cuda:0')), ('power', tensor([-21.3096], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.10774821043014526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1346, 6.3022, 6.9760],
        [5.1346, 5.1458, 5.1360],
        [5.1346, 5.5368, 5.5402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.10762028396129608 
model_pd.l_d.mean(): -11.115127563476562 
model_pd.lagr.mean(): -11.00750732421875 
model_pd.lambdas: dict_items([('pout', tensor([1.3985], device='cuda:0')), ('power', tensor([0.5749], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8274], device='cuda:0')), ('power', tensor([-21.3062], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.10762028396129608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1401, 6.3089, 6.9831],
        [5.1401, 5.5426, 5.5458],
        [5.1401, 5.1513, 5.1416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.10749238729476929 
model_pd.l_d.mean(): -11.091719627380371 
model_pd.lagr.mean(): -10.984227180480957 
model_pd.lambdas: dict_items([('pout', tensor([1.3993], device='cuda:0')), ('power', tensor([0.5738], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8260], device='cuda:0')), ('power', tensor([-21.3028], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.10749238729476929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1456, 5.1568, 5.1471],
        [5.1456, 5.5483, 5.5514],
        [5.1456, 6.3155, 6.9903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.10736452043056488 
model_pd.l_d.mean(): -11.068321228027344 
model_pd.lagr.mean(): -10.960956573486328 
model_pd.lambdas: dict_items([('pout', tensor([1.4002], device='cuda:0')), ('power', tensor([0.5728], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8246], device='cuda:0')), ('power', tensor([-21.2994], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.10736452043056488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1512, 5.5541, 5.5571],
        [5.1512, 6.3222, 6.9974],
        [5.1512, 5.1624, 5.1527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.10723665356636047 
model_pd.l_d.mean(): -11.044930458068848 
model_pd.lagr.mean(): -10.93769359588623 
model_pd.lambdas: dict_items([('pout', tensor([1.4010], device='cuda:0')), ('power', tensor([0.5717], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8232], device='cuda:0')), ('power', tensor([-21.2960], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.10723665356636047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1568, 6.3289, 7.0046],
        [5.1568, 5.1679, 5.1582],
        [5.1568, 5.5599, 5.5628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.10710879415273666 
model_pd.l_d.mean(): -11.021550178527832 
model_pd.lagr.mean(): -10.914441108703613 
model_pd.lambdas: dict_items([('pout', tensor([1.4018], device='cuda:0')), ('power', tensor([0.5706], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8218], device='cuda:0')), ('power', tensor([-21.2926], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.10710879415273666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 6.3357, 7.0119],
        [5.1624, 5.1735, 5.1638],
        [5.1624, 5.5657, 5.5686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.10698091983795166 
model_pd.l_d.mean(): -10.998176574707031 
model_pd.lagr.mean(): -10.891195297241211 
model_pd.lambdas: dict_items([('pout', tensor([1.4026], device='cuda:0')), ('power', tensor([0.5696], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8204], device='cuda:0')), ('power', tensor([-21.2891], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.10698091983795166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1680, 5.1792, 5.1695],
        [5.1680, 6.3425, 7.0192],
        [5.1680, 5.5716, 5.5743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.10685311257839203 
model_pd.l_d.mean(): -10.974813461303711 
model_pd.lagr.mean(): -10.867959976196289 
model_pd.lambdas: dict_items([('pout', tensor([1.4034], device='cuda:0')), ('power', tensor([0.5685], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8190], device='cuda:0')), ('power', tensor([-21.2856], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.10685311257839203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1737, 5.5775, 5.5801],
        [5.1737, 5.1848, 5.1751],
        [5.1737, 6.3493, 7.0265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.10672533512115479 
model_pd.l_d.mean(): -10.951458930969238 
model_pd.lagr.mean(): -10.844733238220215 
model_pd.lambdas: dict_items([('pout', tensor([1.4043], device='cuda:0')), ('power', tensor([0.5674], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8176], device='cuda:0')), ('power', tensor([-21.2821], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.10672533512115479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1794, 5.5834, 5.5859],
        [5.1794, 5.1905, 5.1808],
        [5.1794, 6.3562, 7.0339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.10659760236740112 
model_pd.l_d.mean(): -10.928112030029297 
model_pd.lagr.mean(): -10.821514129638672 
model_pd.lambdas: dict_items([('pout', tensor([1.4051], device='cuda:0')), ('power', tensor([0.5664], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8162], device='cuda:0')), ('power', tensor([-21.2786], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.10659760236740112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1851, 5.5894, 5.5918],
        [5.1851, 5.1962, 5.1865],
        [5.1851, 6.3631, 7.0413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.10646992921829224 
model_pd.l_d.mean(): -10.904777526855469 
model_pd.lagr.mean(): -10.798307418823242 
model_pd.lambdas: dict_items([('pout', tensor([1.4059], device='cuda:0')), ('power', tensor([0.5653], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8148], device='cuda:0')), ('power', tensor([-21.2750], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.10646992921829224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1908, 5.5953, 5.5976],
        [5.1908, 6.3700, 7.0488],
        [5.1908, 5.2019, 5.1922]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.10634227842092514 
model_pd.l_d.mean(): -10.881449699401855 
model_pd.lagr.mean(): -10.775107383728027 
model_pd.lambdas: dict_items([('pout', tensor([1.4067], device='cuda:0')), ('power', tensor([0.5642], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8134], device='cuda:0')), ('power', tensor([-21.2714], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.10634227842092514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1965, 6.3769, 7.0563],
        [5.1965, 5.6013, 5.6035],
        [5.1965, 5.2077, 5.1980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.10621469467878342 
model_pd.l_d.mean(): -10.858131408691406 
model_pd.lagr.mean(): -10.751916885375977 
model_pd.lambdas: dict_items([('pout', tensor([1.4075], device='cuda:0')), ('power', tensor([0.5632], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8120], device='cuda:0')), ('power', tensor([-21.2678], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.10621469467878342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2023, 6.3839, 7.0638],
        [5.2023, 5.6074, 5.6095],
        [5.2023, 5.2134, 5.2038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.1060871109366417 
model_pd.l_d.mean(): -10.834824562072754 
model_pd.lagr.mean(): -10.728737831115723 
model_pd.lambdas: dict_items([('pout', tensor([1.4083], device='cuda:0')), ('power', tensor([0.5621], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8106], device='cuda:0')), ('power', tensor([-21.2642], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.1060871109366417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2081, 6.3910, 7.0714],
        [5.2081, 5.2193, 5.2096],
        [5.2081, 5.6134, 5.6154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10595950484275818 
model_pd.l_d.mean(): -10.811525344848633 
model_pd.lagr.mean(): -10.705565452575684 
model_pd.lambdas: dict_items([('pout', tensor([1.4091], device='cuda:0')), ('power', tensor([0.5611], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8091], device='cuda:0')), ('power', tensor([-21.2606], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.10595950484275818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2140, 5.6195, 5.6214],
        [5.2140, 5.2251, 5.2154],
        [5.2140, 6.3980, 7.0790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.10583178699016571 
model_pd.l_d.mean(): -10.788235664367676 
model_pd.lagr.mean(): -10.682403564453125 
model_pd.lambdas: dict_items([('pout', tensor([1.4099], device='cuda:0')), ('power', tensor([0.5600], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8077], device='cuda:0')), ('power', tensor([-21.2569], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.10583178699016571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2198, 6.4052, 7.0867],
        [5.2198, 5.2310, 5.2213],
        [5.2198, 5.6257, 5.6275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.10570387542247772 
model_pd.l_d.mean(): -10.76495361328125 
model_pd.lagr.mean(): -10.659249305725098 
model_pd.lambdas: dict_items([('pout', tensor([1.4107], device='cuda:0')), ('power', tensor([0.5589], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8062], device='cuda:0')), ('power', tensor([-21.2532], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.10570387542247772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2257, 5.2369, 5.2272],
        [5.2257, 5.6319, 5.6335],
        [5.2257, 6.4123, 7.0944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.10557585209608078 
model_pd.l_d.mean(): -10.74168586730957 
model_pd.lagr.mean(): -10.636110305786133 
model_pd.lambdas: dict_items([('pout', tensor([1.4115], device='cuda:0')), ('power', tensor([0.5579], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8048], device='cuda:0')), ('power', tensor([-21.2495], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.10557585209608078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2317, 5.6381, 5.6397],
        [5.2317, 5.2428, 5.2331],
        [5.2317, 6.4195, 7.1022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.10544773936271667 
model_pd.l_d.mean(): -10.718423843383789 
model_pd.lagr.mean(): -10.61297607421875 
model_pd.lambdas: dict_items([('pout', tensor([1.4124], device='cuda:0')), ('power', tensor([0.5568], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8033], device='cuda:0')), ('power', tensor([-21.2457], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.10544773936271667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2376, 5.2487, 5.2391],
        [5.2376, 6.4268, 7.1101],
        [5.2376, 5.6443, 5.6458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.10531967878341675 
model_pd.l_d.mean(): -10.695172309875488 
model_pd.lagr.mean(): -10.589852333068848 
model_pd.lambdas: dict_items([('pout', tensor([1.4132], device='cuda:0')), ('power', tensor([0.5557], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8019], device='cuda:0')), ('power', tensor([-21.2419], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.10531967878341675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2436, 5.2547, 5.2451],
        [5.2436, 6.4341, 7.1179],
        [5.2436, 5.6506, 5.6520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.10519175231456757 
model_pd.l_d.mean(): -10.671929359436035 
model_pd.lagr.mean(): -10.566737174987793 
model_pd.lambdas: dict_items([('pout', tensor([1.4140], device='cuda:0')), ('power', tensor([0.5547], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.8004], device='cuda:0')), ('power', tensor([-21.2381], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.10519175231456757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2496, 6.4414, 7.1258],
        [5.2496, 5.6569, 5.6582],
        [5.2496, 5.2607, 5.2511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.10506397485733032 
model_pd.l_d.mean(): -10.648697853088379 
model_pd.lagr.mean(): -10.543633460998535 
model_pd.lambdas: dict_items([('pout', tensor([1.4148], device='cuda:0')), ('power', tensor([0.5536], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7989], device='cuda:0')), ('power', tensor([-21.2343], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.10506397485733032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2557, 5.2668, 5.2571],
        [5.2557, 6.4487, 7.1338],
        [5.2557, 5.6632, 5.6644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.1049363762140274 
model_pd.l_d.mean(): -10.625473976135254 
model_pd.lagr.mean(): -10.520537376403809 
model_pd.lambdas: dict_items([('pout', tensor([1.4156], device='cuda:0')), ('power', tensor([0.5526], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7975], device='cuda:0')), ('power', tensor([-21.2304], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.1049363762140274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2617, 6.4561, 7.1418],
        [5.2617, 5.6696, 5.6706],
        [5.2617, 5.2728, 5.2631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.10480895638465881 
model_pd.l_d.mean(): -10.602264404296875 
model_pd.lagr.mean(): -10.497455596923828 
model_pd.lambdas: dict_items([('pout', tensor([1.4163], device='cuda:0')), ('power', tensor([0.5515], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7960], device='cuda:0')), ('power', tensor([-21.2266], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.10480895638465881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2678, 5.6759, 5.6769],
        [5.2678, 5.2789, 5.2692],
        [5.2678, 6.4635, 7.1498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.1046815738081932 
model_pd.l_d.mean(): -10.579061508178711 
model_pd.lagr.mean(): -10.474379539489746 
model_pd.lambdas: dict_items([('pout', tensor([1.4171], device='cuda:0')), ('power', tensor([0.5504], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7945], device='cuda:0')), ('power', tensor([-21.2227], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.1046815738081932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2739, 6.4709, 7.1578],
        [5.2739, 5.6824, 5.6832],
        [5.2739, 5.2850, 5.2753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.10455422848463058 
model_pd.l_d.mean(): -10.555870056152344 
model_pd.lagr.mean(): -10.451315879821777 
model_pd.lambdas: dict_items([('pout', tensor([1.4179], device='cuda:0')), ('power', tensor([0.5494], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7930], device='cuda:0')), ('power', tensor([-21.2188], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.10455422848463058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2800, 5.2911, 5.2814],
        [5.2800, 6.4784, 7.1660],
        [5.2800, 5.6888, 5.6896]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.10442682355642319 
model_pd.l_d.mean(): -10.532687187194824 
model_pd.lagr.mean(): -10.428260803222656 
model_pd.lambdas: dict_items([('pout', tensor([1.4187], device='cuda:0')), ('power', tensor([0.5483], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7915], device='cuda:0')), ('power', tensor([-21.2148], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.10442682355642319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2862, 5.6953, 5.6960],
        [5.2862, 6.4859, 7.1741],
        [5.2862, 5.2973, 5.2876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.10429932922124863 
model_pd.l_d.mean(): -10.509516716003418 
model_pd.lagr.mean(): -10.405217170715332 
model_pd.lambdas: dict_items([('pout', tensor([1.4195], device='cuda:0')), ('power', tensor([0.5473], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7900], device='cuda:0')), ('power', tensor([-21.2109], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.10429932922124863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2924, 5.7018, 5.7024],
        [5.2924, 5.3034, 5.2938],
        [5.2924, 6.4935, 7.1824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.10417170822620392 
model_pd.l_d.mean(): -10.486352920532227 
model_pd.lagr.mean(): -10.382181167602539 
model_pd.lambdas: dict_items([('pout', tensor([1.4203], device='cuda:0')), ('power', tensor([0.5462], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7885], device='cuda:0')), ('power', tensor([-21.2069], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.10417170822620392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2986, 5.3097, 5.3000],
        [5.2986, 6.5011, 7.1906],
        [5.2986, 5.7084, 5.7089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.10404407978057861 
model_pd.l_d.mean(): -10.463201522827148 
model_pd.lagr.mean(): -10.35915756225586 
model_pd.lambdas: dict_items([('pout', tensor([1.4211], device='cuda:0')), ('power', tensor([0.5451], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7870], device='cuda:0')), ('power', tensor([-21.2028], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.10404407978057861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3048, 5.3159, 5.3063],
        [5.3048, 6.5087, 7.1990],
        [5.3048, 5.7150, 5.7154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.1039164662361145 
model_pd.l_d.mean(): -10.440059661865234 
model_pd.lagr.mean(): -10.336143493652344 
model_pd.lambdas: dict_items([('pout', tensor([1.4219], device='cuda:0')), ('power', tensor([0.5441], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7854], device='cuda:0')), ('power', tensor([-21.1988], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.1039164662361145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3111, 5.7216, 5.7219],
        [5.3111, 6.5164, 7.2073],
        [5.3111, 5.3222, 5.3126]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.10378896445035934 
model_pd.l_d.mean(): -10.416926383972168 
model_pd.lagr.mean(): -10.31313705444336 
model_pd.lambdas: dict_items([('pout', tensor([1.4227], device='cuda:0')), ('power', tensor([0.5430], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7839], device='cuda:0')), ('power', tensor([-21.1947], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.10378896445035934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3174, 5.3285, 5.3189],
        [5.3174, 5.7282, 5.7285],
        [5.3174, 6.5242, 7.2157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.10366155952215195 
model_pd.l_d.mean(): -10.393807411193848 
model_pd.lagr.mean(): -10.290145874023438 
model_pd.lambdas: dict_items([('pout', tensor([1.4234], device='cuda:0')), ('power', tensor([0.5420], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7824], device='cuda:0')), ('power', tensor([-21.1906], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.10366155952215195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3238, 5.3348, 5.3252],
        [5.3238, 5.7349, 5.7351],
        [5.3238, 6.5319, 7.2242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.10353429615497589 
model_pd.l_d.mean(): -10.370697021484375 
model_pd.lagr.mean(): -10.267162322998047 
model_pd.lambdas: dict_items([('pout', tensor([1.4242], device='cuda:0')), ('power', tensor([0.5409], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7808], device='cuda:0')), ('power', tensor([-21.1865], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.10353429615497589
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3301, 5.7416, 5.7417],
        [5.3301, 6.5397, 7.2327],
        [5.3301, 5.3412, 5.3316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.1034071296453476 
model_pd.l_d.mean(): -10.347597122192383 
model_pd.lagr.mean(): -10.244190216064453 
model_pd.lambdas: dict_items([('pout', tensor([1.4250], device='cuda:0')), ('power', tensor([0.5398], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7793], device='cuda:0')), ('power', tensor([-21.1823], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.1034071296453476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3365, 5.3476, 5.3379],
        [5.3365, 6.5476, 7.2412],
        [5.3365, 5.7484, 5.7483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.10327993333339691 
model_pd.l_d.mean(): -10.324506759643555 
model_pd.lagr.mean(): -10.221226692199707 
model_pd.lambdas: dict_items([('pout', tensor([1.4258], device='cuda:0')), ('power', tensor([0.5388], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7778], device='cuda:0')), ('power', tensor([-21.1782], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.10327993333339691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3429, 6.5555, 7.2498],
        [5.3429, 5.3540, 5.3444],
        [5.3429, 5.7552, 5.7550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.10315261781215668 
model_pd.l_d.mean(): -10.30142593383789 
model_pd.lagr.mean(): -10.198273658752441 
model_pd.lambdas: dict_items([('pout', tensor([1.4266], device='cuda:0')), ('power', tensor([0.5377], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7762], device='cuda:0')), ('power', tensor([-21.1740], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.10315261781215668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3494, 5.3605, 5.3508],
        [5.3494, 5.7620, 5.7618],
        [5.3494, 6.5634, 7.2585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.10302522033452988 
model_pd.l_d.mean(): -10.278358459472656 
model_pd.lagr.mean(): -10.175333023071289 
model_pd.lambdas: dict_items([('pout', tensor([1.4273], device='cuda:0')), ('power', tensor([0.5367], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7746], device='cuda:0')), ('power', tensor([-21.1697], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.10302522033452988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3559, 5.7689, 5.7686],
        [5.3559, 6.5714, 7.2672],
        [5.3559, 5.3670, 5.3573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.10289768874645233 
model_pd.l_d.mean(): -10.255302429199219 
model_pd.lagr.mean(): -10.15240478515625 
model_pd.lambdas: dict_items([('pout', tensor([1.4281], device='cuda:0')), ('power', tensor([0.5356], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7731], device='cuda:0')), ('power', tensor([-21.1655], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.10289768874645233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3624, 6.5794, 7.2760],
        [5.3624, 5.7758, 5.7754],
        [5.3624, 5.3735, 5.3639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.102770134806633 
model_pd.l_d.mean(): -10.232253074645996 
model_pd.lagr.mean(): -10.129483222961426 
model_pd.lambdas: dict_items([('pout', tensor([1.4289], device='cuda:0')), ('power', tensor([0.5345], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7715], device='cuda:0')), ('power', tensor([-21.1611], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.102770134806633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3690, 6.5875, 7.2848],
        [5.3690, 5.3801, 5.3704],
        [5.3690, 5.7827, 5.7823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.10264256596565247 
model_pd.l_d.mean(): -10.20921802520752 
model_pd.lagr.mean(): -10.106575012207031 
model_pd.lambdas: dict_items([('pout', tensor([1.4296], device='cuda:0')), ('power', tensor([0.5335], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7699], device='cuda:0')), ('power', tensor([-21.1568], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.10264256596565247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3756, 5.3867, 5.3770],
        [5.3756, 6.5956, 7.2937],
        [5.3756, 5.7897, 5.7892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.1025150716304779 
model_pd.l_d.mean(): -10.18619155883789 
model_pd.lagr.mean(): -10.0836763381958 
model_pd.lambdas: dict_items([('pout', tensor([1.4304], device='cuda:0')), ('power', tensor([0.5324], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7683], device='cuda:0')), ('power', tensor([-21.1525], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.1025150716304779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3822, 5.3933, 5.3836],
        [5.3822, 5.7967, 5.7961],
        [5.3822, 6.6038, 7.3026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.10238772630691528 
model_pd.l_d.mean(): -10.163178443908691 
model_pd.lagr.mean(): -10.060791015625 
model_pd.lambdas: dict_items([('pout', tensor([1.4312], device='cuda:0')), ('power', tensor([0.5314], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7667], device='cuda:0')), ('power', tensor([-21.1481], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.10238772630691528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3888, 6.6120, 7.3116],
        [5.3888, 5.3999, 5.3903],
        [5.3888, 5.8038, 5.8031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.1022605448961258 
model_pd.l_d.mean(): -10.140174865722656 
model_pd.lagr.mean(): -10.037914276123047 
model_pd.lambdas: dict_items([('pout', tensor([1.4319], device='cuda:0')), ('power', tensor([0.5303], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7651], device='cuda:0')), ('power', tensor([-21.1437], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.1022605448961258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3955, 5.8109, 5.8101],
        [5.3955, 6.6202, 7.3206],
        [5.3955, 5.4066, 5.3969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.10213356465101242 
model_pd.l_d.mean(): -10.117182731628418 
model_pd.lagr.mean(): -10.01504898071289 
model_pd.lambdas: dict_items([('pout', tensor([1.4327], device='cuda:0')), ('power', tensor([0.5293], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7635], device='cuda:0')), ('power', tensor([-21.1392], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.10213356465101242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4022, 5.8180, 5.8171],
        [5.4022, 6.6285, 7.3297],
        [5.4022, 5.4133, 5.4036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10200671851634979 
model_pd.l_d.mean(): -10.094202995300293 
model_pd.lagr.mean(): -9.992196083068848 
model_pd.lambdas: dict_items([('pout', tensor([1.4335], device='cuda:0')), ('power', tensor([0.5282], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7619], device='cuda:0')), ('power', tensor([-21.1348], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10200671851634979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4089, 6.6368, 7.3388],
        [5.4089, 5.8251, 5.8241],
        [5.4089, 5.4200, 5.4103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.10187994688749313 
model_pd.l_d.mean(): -10.071233749389648 
model_pd.lagr.mean(): -9.969353675842285 
model_pd.lambdas: dict_items([('pout', tensor([1.4342], device='cuda:0')), ('power', tensor([0.5271], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7603], device='cuda:0')), ('power', tensor([-21.1303], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.10187994688749313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4156, 5.4267, 5.4171],
        [5.4156, 5.8323, 5.8312],
        [5.4156, 6.6451, 7.3479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.10175321996212006 
model_pd.l_d.mean(): -10.0482759475708 
model_pd.lagr.mean(): -9.94652271270752 
model_pd.lambdas: dict_items([('pout', tensor([1.4350], device='cuda:0')), ('power', tensor([0.5261], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7587], device='cuda:0')), ('power', tensor([-21.1258], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.10175321996212006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4224, 5.4335, 5.4239],
        [5.4224, 5.8395, 5.8384],
        [5.4224, 6.6535, 7.3571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.1016264483332634 
model_pd.l_d.mean(): -10.02532958984375 
model_pd.lagr.mean(): -9.92370319366455 
model_pd.lambdas: dict_items([('pout', tensor([1.4357], device='cuda:0')), ('power', tensor([0.5250], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7570], device='cuda:0')), ('power', tensor([-21.1213], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.1016264483332634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4292, 5.8467, 5.8455],
        [5.4292, 6.6620, 7.3664],
        [5.4292, 5.4403, 5.4307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.10149966925382614 
model_pd.l_d.mean(): -10.002392768859863 
model_pd.lagr.mean(): -9.900893211364746 
model_pd.lambdas: dict_items([('pout', tensor([1.4365], device='cuda:0')), ('power', tensor([0.5240], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7554], device='cuda:0')), ('power', tensor([-21.1167], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.10149966925382614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4361, 5.8540, 5.8528],
        [5.4361, 5.4472, 5.4375],
        [5.4361, 6.6705, 7.3757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.10137291252613068 
model_pd.l_d.mean(): -9.979467391967773 
model_pd.lagr.mean(): -9.878094673156738 
model_pd.lambdas: dict_items([('pout', tensor([1.4373], device='cuda:0')), ('power', tensor([0.5229], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7538], device='cuda:0')), ('power', tensor([-21.1121], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.10137291252613068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4430, 5.4541, 5.4444],
        [5.4430, 6.6790, 7.3851],
        [5.4430, 5.8614, 5.8600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.10124628245830536 
model_pd.l_d.mean(): -9.956554412841797 
model_pd.lagr.mean(): -9.855308532714844 
model_pd.lambdas: dict_items([('pout', tensor([1.4380], device='cuda:0')), ('power', tensor([0.5219], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7521], device='cuda:0')), ('power', tensor([-21.1074], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.10124628245830536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4499, 5.4610, 5.4513],
        [5.4499, 5.8687, 5.8673],
        [5.4499, 6.6876, 7.3945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.1011197417974472 
model_pd.l_d.mean(): -9.933656692504883 
model_pd.lagr.mean(): -9.832536697387695 
model_pd.lambdas: dict_items([('pout', tensor([1.4388], device='cuda:0')), ('power', tensor([0.5208], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7505], device='cuda:0')), ('power', tensor([-21.1028], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.1011197417974472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4568, 5.8761, 5.8746],
        [5.4568, 5.4679, 5.4582],
        [5.4568, 6.6962, 7.4040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.10099336504936218 
model_pd.l_d.mean(): -9.910768508911133 
model_pd.lagr.mean(): -9.809775352478027 
model_pd.lambdas: dict_items([('pout', tensor([1.4395], device='cuda:0')), ('power', tensor([0.5198], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7488], device='cuda:0')), ('power', tensor([-21.0981], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:0.10099336504936218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4638, 5.8835, 5.8820],
        [5.4638, 5.4749, 5.4652],
        [5.4638, 6.7048, 7.4135]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.10086711496114731 
model_pd.l_d.mean(): -9.887890815734863 
model_pd.lagr.mean(): -9.787023544311523 
model_pd.lambdas: dict_items([('pout', tensor([1.4403], device='cuda:0')), ('power', tensor([0.5187], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7472], device='cuda:0')), ('power', tensor([-21.0934], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.10086711496114731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4708, 6.7135, 7.4231],
        [5.4708, 5.4819, 5.4722],
        [5.4708, 5.8910, 5.8894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.10074090957641602 
model_pd.l_d.mean(): -9.865026473999023 
model_pd.lagr.mean(): -9.764286041259766 
model_pd.lambdas: dict_items([('pout', tensor([1.4410], device='cuda:0')), ('power', tensor([0.5176], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7455], device='cuda:0')), ('power', tensor([-21.0887], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.10074090957641602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4778, 5.4889, 5.4792],
        [5.4778, 5.8985, 5.8968],
        [5.4778, 6.7223, 7.4327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.10061480104923248 
model_pd.l_d.mean(): -9.842171669006348 
model_pd.lagr.mean(): -9.741557121276855 
model_pd.lambdas: dict_items([('pout', tensor([1.4417], device='cuda:0')), ('power', tensor([0.5166], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7438], device='cuda:0')), ('power', tensor([-21.0839], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.10061480104923248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4848, 5.4960, 5.4863],
        [5.4848, 6.7310, 7.4424],
        [5.4848, 5.9061, 5.9043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.10048870742321014 
model_pd.l_d.mean(): -9.819331169128418 
model_pd.lagr.mean(): -9.718842506408691 
model_pd.lambdas: dict_items([('pout', tensor([1.4425], device='cuda:0')), ('power', tensor([0.5155], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7422], device='cuda:0')), ('power', tensor([-21.0791], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.10048870742321014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4919, 5.9136, 5.9118],
        [5.4919, 6.7399, 7.4521],
        [5.4919, 5.5031, 5.4934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.1003626137971878 
model_pd.l_d.mean(): -9.796501159667969 
model_pd.lagr.mean(): -9.696138381958008 
model_pd.lambdas: dict_items([('pout', tensor([1.4432], device='cuda:0')), ('power', tensor([0.5145], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7405], device='cuda:0')), ('power', tensor([-21.0742], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.1003626137971878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4991, 5.9213, 5.9194],
        [5.4991, 5.5102, 5.5005],
        [5.4991, 6.7488, 7.4619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.10023657977581024 
model_pd.l_d.mean(): -9.77368450164795 
model_pd.lagr.mean(): -9.673447608947754 
model_pd.lambdas: dict_items([('pout', tensor([1.4440], device='cuda:0')), ('power', tensor([0.5134], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7388], device='cuda:0')), ('power', tensor([-21.0694], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.10023657977581024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5062, 5.5174, 5.5077],
        [5.5062, 6.7577, 7.4718],
        [5.5062, 5.9289, 5.9270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.10011057555675507 
model_pd.l_d.mean(): -9.750879287719727 
model_pd.lagr.mean(): -9.650768280029297 
model_pd.lambdas: dict_items([('pout', tensor([1.4447], device='cuda:0')), ('power', tensor([0.5124], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7371], device='cuda:0')), ('power', tensor([-21.0645], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.10011057555675507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5134, 5.5246, 5.5148],
        [5.5134, 6.7667, 7.4817],
        [5.5134, 5.9366, 5.9346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.09998463839292526 
model_pd.l_d.mean(): -9.7280855178833 
model_pd.lagr.mean(): -9.628101348876953 
model_pd.lambdas: dict_items([('pout', tensor([1.4454], device='cuda:0')), ('power', tensor([0.5113], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7354], device='cuda:0')), ('power', tensor([-21.0596], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.09998463839292526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5206, 6.7757, 7.4917],
        [5.5206, 5.5318, 5.5221],
        [5.5206, 5.9444, 5.9423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.0998588427901268 
model_pd.l_d.mean(): -9.705306053161621 
model_pd.lagr.mean(): -9.605446815490723 
model_pd.lambdas: dict_items([('pout', tensor([1.4462], device='cuda:0')), ('power', tensor([0.5103], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7337], device='cuda:0')), ('power', tensor([-21.0546], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.0998588427901268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5279, 5.5391, 5.5293],
        [5.5279, 5.9522, 5.9500],
        [5.5279, 6.7848, 7.5017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.0997331365942955 
model_pd.l_d.mean(): -9.682536125183105 
model_pd.lagr.mean(): -9.582802772521973 
model_pd.lambdas: dict_items([('pout', tensor([1.4469], device='cuda:0')), ('power', tensor([0.5092], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7320], device='cuda:0')), ('power', tensor([-21.0496], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.0997331365942955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5352, 5.5464, 5.5366],
        [5.5352, 6.7939, 7.5118],
        [5.5352, 5.9600, 5.9578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.09960754215717316 
model_pd.l_d.mean(): -9.65977954864502 
model_pd.lagr.mean(): -9.560172080993652 
model_pd.lambdas: dict_items([('pout', tensor([1.4476], device='cuda:0')), ('power', tensor([0.5082], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7302], device='cuda:0')), ('power', tensor([-21.0446], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.09960754215717316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5425, 5.5537, 5.5439],
        [5.5425, 6.8030, 7.5219],
        [5.5425, 5.9678, 5.9655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.09948207437992096 
model_pd.l_d.mean(): -9.63703727722168 
model_pd.lagr.mean(): -9.537554740905762 
model_pd.lambdas: dict_items([('pout', tensor([1.4484], device='cuda:0')), ('power', tensor([0.5071], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7285], device='cuda:0')), ('power', tensor([-21.0395], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.09948207437992096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5498, 5.9757, 5.9734],
        [5.5498, 6.8122, 7.5321],
        [5.5498, 5.5610, 5.5513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.09935671091079712 
model_pd.l_d.mean(): -9.61430549621582 
model_pd.lagr.mean(): -9.514948844909668 
model_pd.lambdas: dict_items([('pout', tensor([1.4491], device='cuda:0')), ('power', tensor([0.5061], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7268], device='cuda:0')), ('power', tensor([-21.0345], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.09935671091079712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5572, 5.9836, 5.9812],
        [5.5572, 5.5684, 5.5587],
        [5.5572, 6.8215, 7.5423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.09923143684864044 
model_pd.l_d.mean(): -9.591588020324707 
model_pd.lagr.mean(): -9.492356300354004 
model_pd.lambdas: dict_items([('pout', tensor([1.4498], device='cuda:0')), ('power', tensor([0.5050], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7250], device='cuda:0')), ('power', tensor([-21.0294], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.09923143684864044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5646, 5.5758, 5.5661],
        [5.5646, 6.8308, 7.5526],
        [5.5646, 5.9916, 5.9892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.09910616278648376 
model_pd.l_d.mean(): -9.568880081176758 
model_pd.lagr.mean(): -9.46977424621582 
model_pd.lambdas: dict_items([('pout', tensor([1.4505], device='cuda:0')), ('power', tensor([0.5040], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7233], device='cuda:0')), ('power', tensor([-21.0242], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.09910616278648376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5721, 6.8401, 7.5630],
        [5.5721, 5.5833, 5.5735],
        [5.5721, 5.9996, 5.9971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.09898095577955246 
model_pd.l_d.mean(): -9.546188354492188 
model_pd.lagr.mean(): -9.4472074508667 
model_pd.lambdas: dict_items([('pout', tensor([1.4513], device='cuda:0')), ('power', tensor([0.5029], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7215], device='cuda:0')), ('power', tensor([-21.0190], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.09898095577955246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5796, 5.5908, 5.5810],
        [5.5796, 6.0077, 6.0051],
        [5.5796, 6.8495, 7.5734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.09885581582784653 
model_pd.l_d.mean(): -9.523508071899414 
model_pd.lagr.mean(): -9.424652099609375 
model_pd.lambdas: dict_items([('pout', tensor([1.4520], device='cuda:0')), ('power', tensor([0.5019], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7198], device='cuda:0')), ('power', tensor([-21.0138], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.09885581582784653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5871, 6.8589, 7.5839],
        [5.5871, 5.5983, 5.5885],
        [5.5871, 6.0158, 6.0132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09873074293136597 
model_pd.l_d.mean(): -9.50084114074707 
model_pd.lagr.mean(): -9.40211009979248 
model_pd.lambdas: dict_items([('pout', tensor([1.4527], device='cuda:0')), ('power', tensor([0.5008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7180], device='cuda:0')), ('power', tensor([-21.0086], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.09873074293136597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5946, 6.0239, 6.0213],
        [5.5946, 6.8684, 7.5944],
        [5.5946, 5.6059, 5.5961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.09860576689243317 
model_pd.l_d.mean(): -9.478187561035156 
model_pd.lagr.mean(): -9.379581451416016 
model_pd.lambdas: dict_items([('pout', tensor([1.4534], device='cuda:0')), ('power', tensor([0.4998], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7163], device='cuda:0')), ('power', tensor([-21.0033], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.09860576689243317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6022, 6.8780, 7.6050],
        [5.6022, 5.6135, 5.6037],
        [5.6022, 6.0321, 6.0294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.09848088026046753 
model_pd.l_d.mean(): -9.455544471740723 
model_pd.lagr.mean(): -9.357063293457031 
model_pd.lambdas: dict_items([('pout', tensor([1.4541], device='cuda:0')), ('power', tensor([0.4987], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7145], device='cuda:0')), ('power', tensor([-20.9980], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.09848088026046753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6098, 5.6211, 5.6113],
        [5.6098, 6.0403, 6.0376],
        [5.6098, 6.8876, 7.6156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.09835612028837204 
model_pd.l_d.mean(): -9.432918548583984 
model_pd.lagr.mean(): -9.334562301635742 
model_pd.lambdas: dict_items([('pout', tensor([1.4548], device='cuda:0')), ('power', tensor([0.4977], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7127], device='cuda:0')), ('power', tensor([-20.9927], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.09835612028837204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6175, 6.0486, 6.0458],
        [5.6175, 5.6288, 5.6189],
        [5.6175, 6.8972, 7.6263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.0982314795255661 
model_pd.l_d.mean(): -9.41030216217041 
model_pd.lagr.mean(): -9.312070846557617 
model_pd.lambdas: dict_items([('pout', tensor([1.4556], device='cuda:0')), ('power', tensor([0.4966], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7109], device='cuda:0')), ('power', tensor([-20.9873], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.0982314795255661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6252, 5.6365, 5.6266],
        [5.6252, 6.0569, 6.0540],
        [5.6252, 6.9069, 7.6371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.09810693562030792 
model_pd.l_d.mean(): -9.387701034545898 
model_pd.lagr.mean(): -9.289593696594238 
model_pd.lambdas: dict_items([('pout', tensor([1.4563], device='cuda:0')), ('power', tensor([0.4956], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7091], device='cuda:0')), ('power', tensor([-20.9819], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.09810693562030792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6329, 5.6442, 5.6343],
        [5.6329, 6.9166, 7.6479],
        [5.6329, 6.0652, 6.0623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.0979825109243393 
model_pd.l_d.mean(): -9.3651123046875 
model_pd.lagr.mean(): -9.267129898071289 
model_pd.lambdas: dict_items([('pout', tensor([1.4570], device='cuda:0')), ('power', tensor([0.4945], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7073], device='cuda:0')), ('power', tensor([-20.9765], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.0979825109243393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6407, 6.0736, 6.0707],
        [5.6407, 6.9264, 7.6588],
        [5.6407, 5.6520, 5.6421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09785815328359604 
model_pd.l_d.mean(): -9.342536926269531 
model_pd.lagr.mean(): -9.244678497314453 
model_pd.lambdas: dict_items([('pout', tensor([1.4577], device='cuda:0')), ('power', tensor([0.4935], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7055], device='cuda:0')), ('power', tensor([-20.9710], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.09785815328359604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6485, 6.9362, 7.6698],
        [5.6485, 5.6598, 5.6499],
        [5.6485, 6.0820, 6.0790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.09773385524749756 
model_pd.l_d.mean(): -9.319974899291992 
model_pd.lagr.mean(): -9.222241401672363 
model_pd.lambdas: dict_items([('pout', tensor([1.4584], device='cuda:0')), ('power', tensor([0.4924], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7037], device='cuda:0')), ('power', tensor([-20.9655], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.09773385524749756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6563, 6.0905, 6.0875],
        [5.6563, 6.9461, 7.6808],
        [5.6563, 5.6676, 5.6577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.09760965406894684 
model_pd.l_d.mean(): -9.2974271774292 
model_pd.lagr.mean(): -9.199817657470703 
model_pd.lambdas: dict_items([('pout', tensor([1.4591], device='cuda:0')), ('power', tensor([0.4914], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7019], device='cuda:0')), ('power', tensor([-20.9600], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.09760965406894684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6641, 6.0990, 6.0959],
        [5.6641, 6.9560, 7.6919],
        [5.6641, 5.6755, 5.6656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.0974855050444603 
model_pd.l_d.mean(): -9.274892807006836 
model_pd.lagr.mean(): -9.177407264709473 
model_pd.lambdas: dict_items([('pout', tensor([1.4598], device='cuda:0')), ('power', tensor([0.4903], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.7001], device='cuda:0')), ('power', tensor([-20.9544], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.0974855050444603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6721, 6.9660, 7.7030],
        [5.6721, 6.1075, 6.1045],
        [5.6721, 5.6834, 5.6735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.09736144542694092 
model_pd.l_d.mean(): -9.252371788024902 
model_pd.lagr.mean(): -9.155010223388672 
model_pd.lambdas: dict_items([('pout', tensor([1.4605], device='cuda:0')), ('power', tensor([0.4893], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6982], device='cuda:0')), ('power', tensor([-20.9488], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.09736144542694092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6800, 6.9761, 7.7142],
        [5.6800, 6.1161, 6.1130],
        [5.6800, 5.6914, 5.6814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.0972374975681305 
model_pd.l_d.mean(): -9.229865074157715 
model_pd.lagr.mean(): -9.132627487182617 
model_pd.lambdas: dict_items([('pout', tensor([1.4612], device='cuda:0')), ('power', tensor([0.4882], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6964], device='cuda:0')), ('power', tensor([-20.9431], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.0972374975681305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6880, 6.9862, 7.7255],
        [5.6880, 6.1248, 6.1216],
        [5.6880, 5.6994, 5.6894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.0971137136220932 
model_pd.l_d.mean(): -9.207372665405273 
model_pd.lagr.mean(): -9.110259056091309 
model_pd.lambdas: dict_items([('pout', tensor([1.4619], device='cuda:0')), ('power', tensor([0.4872], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6946], device='cuda:0')), ('power', tensor([-20.9375], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.0971137136220932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6960, 5.7074, 5.6974],
        [5.6960, 6.9963, 7.7368],
        [5.6960, 6.1335, 6.1303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.09699003398418427 
model_pd.l_d.mean(): -9.184893608093262 
model_pd.lagr.mean(): -9.08790397644043 
model_pd.lambdas: dict_items([('pout', tensor([1.4626], device='cuda:0')), ('power', tensor([0.4861], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6927], device='cuda:0')), ('power', tensor([-20.9318], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.09699003398418427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7040, 6.1422, 6.1390],
        [5.7040, 7.0065, 7.7482],
        [5.7040, 5.7154, 5.7055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.09686645865440369 
model_pd.l_d.mean(): -9.16242790222168 
model_pd.lagr.mean(): -9.065561294555664 
model_pd.lambdas: dict_items([('pout', tensor([1.4633], device='cuda:0')), ('power', tensor([0.4851], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6908], device='cuda:0')), ('power', tensor([-20.9260], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.09686645865440369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7121, 5.7235, 5.7135],
        [5.7121, 7.0167, 7.7596],
        [5.7121, 6.1510, 6.1477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.09674295783042908 
model_pd.l_d.mean(): -9.13997745513916 
model_pd.lagr.mean(): -9.043234825134277 
model_pd.lambdas: dict_items([('pout', tensor([1.4639], device='cuda:0')), ('power', tensor([0.4840], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6890], device='cuda:0')), ('power', tensor([-20.9202], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.09674295783042908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7202, 6.1598, 6.1565],
        [5.7202, 7.0270, 7.7711],
        [5.7202, 5.7317, 5.7217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.09661953896284103 
model_pd.l_d.mean(): -9.117542266845703 
model_pd.lagr.mean(): -9.020922660827637 
model_pd.lambdas: dict_items([('pout', tensor([1.4646], device='cuda:0')), ('power', tensor([0.4830], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6871], device='cuda:0')), ('power', tensor([-20.9144], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.09661953896284103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7284, 6.1686, 6.1653],
        [5.7284, 7.0374, 7.7827],
        [5.7284, 5.7398, 5.7298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.09649622440338135 
model_pd.l_d.mean(): -9.09511947631836 
model_pd.lagr.mean(): -8.99862289428711 
model_pd.lambdas: dict_items([('pout', tensor([1.4653], device='cuda:0')), ('power', tensor([0.4819], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6852], device='cuda:0')), ('power', tensor([-20.9086], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.09649622440338135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7365, 5.7480, 5.7380],
        [5.7365, 6.1775, 6.1742],
        [5.7365, 7.0478, 7.7944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.09637302160263062 
model_pd.l_d.mean(): -9.072710037231445 
model_pd.lagr.mean(): -8.976337432861328 
model_pd.lambdas: dict_items([('pout', tensor([1.4660], device='cuda:0')), ('power', tensor([0.4809], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6834], device='cuda:0')), ('power', tensor([-20.9027], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.09637302160263062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7448, 6.1865, 6.1831],
        [5.7448, 5.7563, 5.7462],
        [5.7448, 7.0583, 7.8061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.09624990820884705 
model_pd.l_d.mean(): -9.050317764282227 
model_pd.lagr.mean(): -8.954068183898926 
model_pd.lambdas: dict_items([('pout', tensor([1.4667], device='cuda:0')), ('power', tensor([0.4799], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6815], device='cuda:0')), ('power', tensor([-20.8968], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.09624990820884705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7530, 6.1955, 6.1921],
        [5.7530, 5.7646, 5.7545],
        [5.7530, 7.0688, 7.8179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.09612690657377243 
model_pd.l_d.mean(): -9.027937889099121 
model_pd.lagr.mean(): -8.931811332702637 
model_pd.lambdas: dict_items([('pout', tensor([1.4674], device='cuda:0')), ('power', tensor([0.4788], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6796], device='cuda:0')), ('power', tensor([-20.8908], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.09612690657377243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7613, 7.0793, 7.8297],
        [5.7613, 5.7729, 5.7628],
        [5.7613, 6.2045, 6.2011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.09600399434566498 
model_pd.l_d.mean(): -9.005574226379395 
model_pd.lagr.mean(): -8.909570693969727 
model_pd.lambdas: dict_items([('pout', tensor([1.4680], device='cuda:0')), ('power', tensor([0.4778], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6777], device='cuda:0')), ('power', tensor([-20.8848], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.09600399434566498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7697, 6.2136, 6.2102],
        [5.7697, 5.7812, 5.7712],
        [5.7697, 7.0900, 7.8416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.09588124603033066 
model_pd.l_d.mean(): -8.983222961425781 
model_pd.lagr.mean(): -8.887341499328613 
model_pd.lambdas: dict_items([('pout', tensor([1.4687], device='cuda:0')), ('power', tensor([0.4767], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6758], device='cuda:0')), ('power', tensor([-20.8788], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.09588124603033066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7781, 7.1006, 7.8536],
        [5.7781, 6.2228, 6.2193],
        [5.7781, 5.7896, 5.7795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.0957585871219635 
model_pd.l_d.mean(): -8.960887908935547 
model_pd.lagr.mean(): -8.865129470825195 
model_pd.lambdas: dict_items([('pout', tensor([1.4694], device='cuda:0')), ('power', tensor([0.4757], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6739], device='cuda:0')), ('power', tensor([-20.8727], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.0957585871219635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7865, 6.2319, 6.2285],
        [5.7865, 7.1114, 7.8656],
        [5.7865, 5.7981, 5.7880]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.0956360325217247 
model_pd.l_d.mean(): -8.938567161560059 
model_pd.lagr.mean(): -8.842930793762207 
model_pd.lambdas: dict_items([('pout', tensor([1.4701], device='cuda:0')), ('power', tensor([0.4746], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6719], device='cuda:0')), ('power', tensor([-20.8666], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.0956360325217247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7949, 6.2412, 6.2377],
        [5.7949, 5.8065, 5.7964],
        [5.7949, 7.1222, 7.8777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.09551358222961426 
model_pd.l_d.mean(): -8.91626262664795 
model_pd.lagr.mean(): -8.820749282836914 
model_pd.lambdas: dict_items([('pout', tensor([1.4707], device='cuda:0')), ('power', tensor([0.4736], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6700], device='cuda:0')), ('power', tensor([-20.8605], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.09551358222961426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8034, 7.1330, 7.8899],
        [5.8034, 5.8151, 5.8049],
        [5.8034, 6.2504, 6.2469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.09539126604795456 
model_pd.l_d.mean(): -8.893973350524902 
model_pd.lagr.mean(): -8.798582077026367 
model_pd.lambdas: dict_items([('pout', tensor([1.4714], device='cuda:0')), ('power', tensor([0.4726], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6681], device='cuda:0')), ('power', tensor([-20.8543], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.09539126604795456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8120, 7.1439, 7.9021],
        [5.8120, 5.8236, 5.8134],
        [5.8120, 6.2597, 6.2563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.09526902437210083 
model_pd.l_d.mean(): -8.871696472167969 
model_pd.lagr.mean(): -8.776427268981934 
model_pd.lambdas: dict_items([('pout', tensor([1.4721], device='cuda:0')), ('power', tensor([0.4715], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6661], device='cuda:0')), ('power', tensor([-20.8481], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.09526902437210083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8205, 7.1548, 7.9145],
        [5.8205, 5.8322, 5.8220],
        [5.8205, 6.2691, 6.2656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.09514691680669785 
model_pd.l_d.mean(): -8.849435806274414 
model_pd.lagr.mean(): -8.754288673400879 
model_pd.lambdas: dict_items([('pout', tensor([1.4727], device='cuda:0')), ('power', tensor([0.4705], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6642], device='cuda:0')), ('power', tensor([-20.8418], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.09514691680669785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8291, 5.8408, 5.8306],
        [5.8291, 6.2785, 6.2750],
        [5.8291, 7.1658, 7.9268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.09502492845058441 
model_pd.l_d.mean(): -8.827190399169922 
model_pd.lagr.mean(): -8.732165336608887 
model_pd.lambdas: dict_items([('pout', tensor([1.4734], device='cuda:0')), ('power', tensor([0.4694], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6622], device='cuda:0')), ('power', tensor([-20.8355], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.09502492845058441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8378, 6.2880, 6.2845],
        [5.8378, 5.8495, 5.8393],
        [5.8378, 7.1769, 7.9393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.09490303695201874 
model_pd.l_d.mean(): -8.804962158203125 
model_pd.lagr.mean(): -8.71005916595459 
model_pd.lambdas: dict_items([('pout', tensor([1.4740], device='cuda:0')), ('power', tensor([0.4684], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6603], device='cuda:0')), ('power', tensor([-20.8292], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.09490303695201874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8465, 6.2975, 6.2940],
        [5.8465, 5.8582, 5.8480],
        [5.8465, 7.1880, 7.9518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.09478124231100082 
model_pd.l_d.mean(): -8.782747268676758 
model_pd.lagr.mean(): -8.687966346740723 
model_pd.lambdas: dict_items([('pout', tensor([1.4747], device='cuda:0')), ('power', tensor([0.4673], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6583], device='cuda:0')), ('power', tensor([-20.8228], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.09478124231100082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8552, 6.3071, 6.3035],
        [5.8552, 7.1992, 7.9644],
        [5.8552, 5.8670, 5.8567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.09465959668159485 
model_pd.l_d.mean(): -8.760546684265137 
model_pd.lagr.mean(): -8.665886878967285 
model_pd.lambdas: dict_items([('pout', tensor([1.4754], device='cuda:0')), ('power', tensor([0.4663], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6563], device='cuda:0')), ('power', tensor([-20.8164], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.09465959668159485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8640, 7.2105, 7.9771],
        [5.8640, 5.8758, 5.8655],
        [5.8640, 6.3167, 6.3131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.09453807771205902 
model_pd.l_d.mean(): -8.738365173339844 
model_pd.lagr.mean(): -8.643827438354492 
model_pd.lambdas: dict_items([('pout', tensor([1.4760], device='cuda:0')), ('power', tensor([0.4653], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6544], device='cuda:0')), ('power', tensor([-20.8099], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.09453807771205902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8728, 7.2218, 7.9898],
        [5.8728, 6.3263, 6.3228],
        [5.8728, 5.8846, 5.8743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.09441664069890976 
model_pd.l_d.mean(): -8.71619701385498 
model_pd.lagr.mean(): -8.621780395507812 
model_pd.lambdas: dict_items([('pout', tensor([1.4767], device='cuda:0')), ('power', tensor([0.4642], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6524], device='cuda:0')), ('power', tensor([-20.8034], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.09441664069890976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8816, 7.2331, 8.0026],
        [5.8816, 6.3360, 6.3325],
        [5.8816, 5.8935, 5.8831]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.09429534524679184 
model_pd.l_d.mean(): -8.694046020507812 
model_pd.lagr.mean(): -8.599750518798828 
model_pd.lambdas: dict_items([('pout', tensor([1.4773], device='cuda:0')), ('power', tensor([0.4632], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6504], device='cuda:0')), ('power', tensor([-20.7969], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.09429534524679184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8905, 7.2445, 8.0155],
        [5.8905, 5.9024, 5.8920],
        [5.8905, 6.3458, 6.3422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.09417420625686646 
model_pd.l_d.mean(): -8.671908378601074 
model_pd.lagr.mean(): -8.577733993530273 
model_pd.lambdas: dict_items([('pout', tensor([1.4780], device='cuda:0')), ('power', tensor([0.4621], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6484], device='cuda:0')), ('power', tensor([-20.7903], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.09417420625686646
