
bounds:tensor([-2.])	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.]), 'power': tensor([1.])},
vars:{'pout': tensor([0.]), 'power': tensor([0.])}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[2.3753, 2.7571, 2.9162],
        [2.3753, 2.6743, 2.7534],
        [2.3753, 2.3910, 2.3785],
        [2.3753, 2.5079, 2.4824]], grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.30922529101371765 
model_pd.l_d.mean(): -21.68641471862793 
model_pd.lagr.mean(): -21.37718963623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0016])), ('power', tensor([0.9988]))]) 
model_pd.vars: dict_items([('pout', tensor([1.6211])), ('power', tensor([-23.3075]))])
epoch：0	 i:0 	 global-step:0	 l-p:0.30922529101371765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[2.4887, 2.4890, 2.4887],
        [2.4887, 2.4974, 2.4899],
        [2.4887, 3.1458, 3.6241],
        [2.4887, 3.1025, 3.5211]], grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.07866354286670685 
model_pd.l_d.mean(): -21.72688102722168 
model_pd.lagr.mean(): -21.648218154907227 
model_pd.lambdas: dict_items([('pout', tensor([1.0032])), ('power', tensor([0.9977]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5597])), ('power', tensor([-23.3162]))])
epoch：1	 i:0 	 global-step:20	 l-p:0.07866354286670685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[2.6033, 3.1887, 3.5454],
        [2.6033, 3.3010, 3.8072],
        [2.6033, 2.6582, 2.6268],
        [2.6033, 2.6828, 2.6466]], grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.22531892359256744 
model_pd.l_d.mean(): -21.754812240600586 
model_pd.lagr.mean(): -21.52949333190918 
model_pd.lambdas: dict_items([('pout', tensor([1.0047])), ('power', tensor([0.9965]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5004])), ('power', tensor([-23.3143]))])
epoch：2	 i:0 	 global-step:40	 l-p:0.22531892359256744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[2.7192, 2.7192, 2.7192],
        [2.7192, 2.8459, 2.8087],
        [2.7192, 3.1695, 3.3521],
        [2.7192, 3.1356, 3.2843]], grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.18739114701747894 
model_pd.l_d.mean(): -21.77145767211914 
model_pd.lagr.mean(): -21.58406639099121 
model_pd.lambdas: dict_items([('pout', tensor([1.0061])), ('power', tensor([0.9953]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4431])), ('power', tensor([-23.3028]))])
epoch：3	 i:0 	 global-step:60	 l-p:0.18739114701747894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[2.8363, 3.0859, 3.0912],
        [2.8363, 2.8366, 2.8363],
        [2.8363, 2.8534, 2.8395],
        [2.8363, 2.8363, 2.8363]], grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.1666826456785202 
model_pd.l_d.mean(): -21.777862548828125 
model_pd.lagr.mean(): -21.61117935180664 
model_pd.lambdas: dict_items([('pout', tensor([1.0075])), ('power', tensor([0.9942]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3877])), ('power', tensor([-23.2826]))])
epoch：4	 i:0 	 global-step:80	 l-p:0.1666826456785202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[2.9547, 2.9889, 2.9644],
        [2.9547, 2.9586, 2.9550],
        [2.9547, 2.9884, 2.9642],
        [2.9547, 2.9547, 2.9547]], grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.1511005163192749 
model_pd.l_d.mean(): -21.77494239807129 
model_pd.lagr.mean(): -21.623842239379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0088])), ('power', tensor([0.9930]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3339])), ('power', tensor([-23.2544]))])
epoch：5	 i:0 	 global-step:100	 l-p:0.1511005163192749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.0745, 3.1102, 3.0846],
        [3.0745, 3.0745, 3.0745],
        [3.0745, 3.0745, 3.0745],
        [3.0745, 3.2546, 3.2183]], grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.13723525404930115 
model_pd.l_d.mean(): -21.763477325439453 
model_pd.lagr.mean(): -21.62624168395996 
model_pd.lambdas: dict_items([('pout', tensor([1.0101])), ('power', tensor([0.9919]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2817])), ('power', tensor([-23.2188]))])
epoch：6	 i:0 	 global-step:120	 l-p:0.13723525404930115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.1955, 3.5981, 3.6833],
        [3.1955, 3.5012, 3.5169],
        [3.1955, 3.1986, 3.1957],
        [3.1955, 3.4646, 3.4595]], grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.12133602797985077 
model_pd.l_d.mean(): -21.744136810302734 
model_pd.lagr.mean(): -21.622800827026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0114])), ('power', tensor([0.9907]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2310])), ('power', tensor([-23.1765]))])
epoch：7	 i:0 	 global-step:140	 l-p:0.12133602797985077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[3.3175, 4.2035, 4.7970],
        [3.3175, 3.5986, 3.5929],
        [3.3175, 3.4565, 3.4070],
        [3.3175, 4.2679, 4.9478]], grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.08130720257759094 
model_pd.l_d.mean(): -21.717533111572266 
model_pd.lagr.mean(): -21.636226654052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0125])), ('power', tensor([0.9895]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1818])), ('power', tensor([-23.1281]))])
epoch：8	 i:0 	 global-step:160	 l-p:0.08130720257759094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.4411, 3.5859, 3.5342],
        [3.4411, 3.4814, 3.4525],
        [3.4411, 3.4811, 3.4523],
        [3.4411, 3.4415, 3.4411]], grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.1903744637966156 
model_pd.l_d.mean(): -21.684207916259766 
model_pd.lagr.mean(): -21.493833541870117 
model_pd.lambdas: dict_items([('pout', tensor([1.0137])), ('power', tensor([0.9884]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1338])), ('power', tensor([-23.0737]))])
epoch：9	 i:0 	 global-step:180	 l-p:0.1903744637966156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.5663, 4.1444, 4.3454],
        [3.5663, 3.9358, 3.9666],
        [3.5663, 3.6593, 3.6102],
        [3.5663, 3.5667, 3.5663]], grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.14026063680648804 
model_pd.l_d.mean(): -21.644590377807617 
model_pd.lagr.mean(): -21.504329681396484 
model_pd.lambdas: dict_items([('pout', tensor([1.0148])), ('power', tensor([0.9872]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0869])), ('power', tensor([-23.0138]))])
epoch：10	 i:0 	 global-step:200	 l-p:0.14026063680648804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[3.6825, 3.8006, 3.7460],
        [3.6825, 4.0213, 4.0258],
        [3.6825, 3.7054, 3.6868],
        [3.6825, 3.6890, 3.6831]], grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.12811602652072906 
model_pd.l_d.mean(): -21.600778579711914 
model_pd.lagr.mean(): -21.4726619720459 
model_pd.lambdas: dict_items([('pout', tensor([1.0158])), ('power', tensor([0.9861]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0448])), ('power', tensor([-22.9541]))])
epoch：11	 i:0 	 global-step:220	 l-p:0.12811602652072906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[3.7795, 4.6695, 5.1758],
        [3.7795, 3.7861, 3.7801],
        [3.7795, 4.1284, 4.1328],
        [3.7795, 3.7798, 3.7795]], grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12182565033435822 
model_pd.l_d.mean(): -21.55634307861328 
model_pd.lagr.mean(): -21.43451690673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0168])), ('power', tensor([0.9849]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0107])), ('power', tensor([-22.9018]))])
epoch：12	 i:0 	 global-step:240	 l-p:0.12182565033435822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.8529, 3.8529, 3.8529],
        [3.8529, 4.1551, 4.1318],
        [3.8529, 4.0418, 3.9848],
        [3.8529, 4.2323, 4.2499]], grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.11805557459592819 
model_pd.l_d.mean(): -21.514251708984375 
model_pd.lagr.mean(): -21.396196365356445 
model_pd.lambdas: dict_items([('pout', tensor([1.0178])), ('power', tensor([0.9838]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9855])), ('power', tensor([-22.8607]))])
epoch：13	 i:0 	 global-step:260	 l-p:0.11805557459592819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.9021, 3.9480, 3.9150],
        [3.9021, 4.5978, 4.8716],
        [3.9021, 4.0279, 3.9696],
        [3.9021, 3.9486, 3.9153]], grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.11583395302295685 
model_pd.l_d.mean(): -21.476341247558594 
model_pd.lagr.mean(): -21.36050796508789 
model_pd.lambdas: dict_items([('pout', tensor([1.0188])), ('power', tensor([0.9827]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9689])), ('power', tensor([-22.8325]))])
epoch：14	 i:0 	 global-step:280	 l-p:0.11583395302295685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[3.9284, 3.9285, 3.9284],
        [3.9284, 4.6343, 4.9149],
        [3.9284, 4.2371, 4.2132],
        [3.9284, 3.9750, 3.9416]], grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.11472374200820923 
model_pd.l_d.mean(): -21.443313598632812 
model_pd.lagr.mean(): -21.328590393066406 
model_pd.lambdas: dict_items([('pout', tensor([1.0197])), ('power', tensor([0.9815]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9601])), ('power', tensor([-22.8172]))])
epoch：15	 i:0 	 global-step:300	 l-p:0.11472374200820923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.9344, 4.1275, 4.0692],
        [3.9344, 3.9397, 3.9348],
        [3.9344, 3.9346, 3.9344],
        [3.9344, 3.9344, 3.9344]], grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.11448006331920624 
model_pd.l_d.mean(): -21.415008544921875 
model_pd.lagr.mean(): -21.300527572631836 
model_pd.lambdas: dict_items([('pout', tensor([1.0207])), ('power', tensor([0.9804]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9581])), ('power', tensor([-22.8138]))])
epoch：16	 i:0 	 global-step:320	 l-p:0.11448006331920624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.9231, 3.9269, 3.9233],
        [3.9231, 5.0072, 5.7277],
        [3.9231, 3.9231, 3.9231],
        [3.9231, 4.0263, 3.9717]], grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.11495279520750046 
model_pd.l_d.mean(): -21.39077377319336 
model_pd.lagr.mean(): -21.275821685791016 
model_pd.lambdas: dict_items([('pout', tensor([1.0217])), ('power', tensor([0.9792]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9619])), ('power', tensor([-22.8205]))])
epoch：17	 i:0 	 global-step:340	 l-p:0.11495279520750046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[3.8974, 4.2812, 4.2988],
        [3.8974, 4.4043, 4.5084],
        [3.8974, 4.4403, 4.5739],
        [3.8974, 4.0229, 3.9647]], grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.11605066061019897 
model_pd.l_d.mean(): -21.369688034057617 
model_pd.lagr.mean(): -21.253637313842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0226])), ('power', tensor([0.9781]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9705])), ('power', tensor([-22.8355]))])
epoch：18	 i:0 	 global-step:360	 l-p:0.11605066061019897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[3.8604, 4.0069, 3.9477],
        [3.8604, 4.5512, 4.8260],
        [3.8604, 4.0248, 3.9657],
        [3.8604, 4.2171, 4.2212]], grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.11772628128528595 
model_pd.l_d.mean(): -21.350753784179688 
model_pd.lagr.mean(): -21.233028411865234 
model_pd.lambdas: dict_items([('pout', tensor([1.0236])), ('power', tensor([0.9769]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9831])), ('power', tensor([-22.8569]))])
epoch：19	 i:0 	 global-step:380	 l-p:0.11772628128528595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.8146, 3.9298, 3.8741],
        [3.8146, 4.4964, 4.7682],
        [3.8146, 4.4910, 4.7574],
        [3.8146, 3.8147, 3.8147]], grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.11997178196907043 
model_pd.l_d.mean(): -21.332996368408203 
model_pd.lagr.mean(): -21.213024139404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0246])), ('power', tensor([0.9758]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9987])), ('power', tensor([-22.8828]))])
epoch：20	 i:0 	 global-step:400	 l-p:0.11997178196907043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[3.7628, 3.7628, 3.7628],
        [3.7628, 3.8833, 3.8275],
        [3.7628, 3.8067, 3.7751],
        [3.7628, 4.1197, 4.1301]], grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.12282309681177139 
model_pd.l_d.mean(): -21.31553840637207 
model_pd.lagr.mean(): -21.19271469116211 
model_pd.lambdas: dict_items([('pout', tensor([1.0256])), ('power', tensor([0.9747]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0167])), ('power', tensor([-22.9117]))])
epoch：21	 i:0 	 global-step:420	 l-p:0.12282309681177139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[3.7073, 3.7887, 3.7417],
        [3.7073, 4.3662, 4.6292],
        [3.7073, 3.7073, 3.7073],
        [3.7073, 3.7109, 3.7076]], grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.12637487053871155 
model_pd.l_d.mean(): -21.297618865966797 
model_pd.lagr.mean(): -21.17124366760254 
model_pd.lambdas: dict_items([('pout', tensor([1.0267])), ('power', tensor([0.9735]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0362])), ('power', tensor([-22.9418]))])
epoch：22	 i:0 	 global-step:440	 l-p:0.12637487053871155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.6508, 3.6511, 3.6508],
        [3.6508, 4.5025, 4.9873],
        [3.6508, 4.0061, 4.0229],
        [3.6508, 3.6508, 3.6508]], grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.13081136345863342 
model_pd.l_d.mean(): -21.278642654418945 
model_pd.lagr.mean(): -21.147830963134766 
model_pd.lambdas: dict_items([('pout', tensor([1.0277])), ('power', tensor([0.9724]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0564])), ('power', tensor([-22.9717]))])
epoch：23	 i:0 	 global-step:460	 l-p:0.13081136345863342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.5961, 3.5961, 3.5961],
        [3.5961, 3.5962, 3.5961],
        [3.5961, 3.9675, 3.9981],
        [3.5961, 3.7700, 3.7177]], grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.1364503651857376 
model_pd.l_d.mean(): -21.258150100708008 
model_pd.lagr.mean(): -21.121700286865234 
model_pd.lambdas: dict_items([('pout', tensor([1.0288])), ('power', tensor([0.9712]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0762])), ('power', tensor([-22.9999]))])
epoch：24	 i:0 	 global-step:480	 l-p:0.1364503651857376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[3.5470, 4.5053, 5.1445],
        [3.5470, 4.1715, 4.4215],
        [3.5470, 3.6595, 3.6075],
        [3.5470, 3.8491, 3.8422]], grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.1437033861875534 
model_pd.l_d.mean(): -21.235855102539062 
model_pd.lagr.mean(): -21.092151641845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0299])), ('power', tensor([0.9701]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0943])), ('power', tensor([-23.0245]))])
epoch：25	 i:0 	 global-step:500	 l-p:0.1437033861875534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.5091, 3.6562, 3.6036],
        [3.5091, 3.5103, 3.5092],
        [3.5091, 3.5091, 3.5091],
        [3.5091, 4.0736, 4.2696]], grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.1523255854845047 
model_pd.l_d.mean(): -21.211610794067383 
model_pd.lagr.mean(): -21.05928611755371 
model_pd.lambdas: dict_items([('pout', tensor([1.0310])), ('power', tensor([0.9689]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1084])), ('power', tensor([-23.0431]))])
epoch：26	 i:0 	 global-step:520	 l-p:0.1523255854845047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.4907, 3.4907, 3.4907],
        [3.4907, 3.6889, 3.6441],
        [3.4907, 3.7868, 3.7801],
        [3.4907, 3.4909, 3.4907]], grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.15848837792873383 
model_pd.l_d.mean(): -21.185340881347656 
model_pd.lagr.mean(): -21.026851654052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0321])), ('power', tensor([0.9678]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1154])), ('power', tensor([-23.0520]))])
epoch：27	 i:0 	 global-step:540	 l-p:0.15848837792873383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.4900, 3.5025, 3.4917],
        [3.4900, 3.4900, 3.4900],
        [3.4900, 3.4912, 3.4900],
        [3.4900, 3.4902, 3.4900]], grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.15880762040615082 
model_pd.l_d.mean(): -21.157665252685547 
model_pd.lagr.mean(): -20.998857498168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0332])), ('power', tensor([0.9666]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1156])), ('power', tensor([-23.0524]))])
epoch：28	 i:0 	 global-step:560	 l-p:0.15880762040615082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.5049, 4.0088, 4.1500],
        [3.5049, 3.5096, 3.5053],
        [3.5049, 3.7038, 3.6589],
        [3.5049, 3.5049, 3.5049]], grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.15372252464294434 
model_pd.l_d.mean(): -21.128829956054688 
model_pd.lagr.mean(): -20.975107192993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0343])), ('power', tensor([0.9654]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1101])), ('power', tensor([-23.0454]))])
epoch：29	 i:0 	 global-step:580	 l-p:0.15372252464294434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[3.5336, 3.6815, 3.6285],
        [3.5336, 3.6248, 3.5766],
        [3.5336, 3.8052, 3.7846],
        [3.5336, 3.7532, 3.7130]], grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.14649085700511932 
model_pd.l_d.mean(): -21.09878158569336 
model_pd.lagr.mean(): -20.95229148864746 
model_pd.lambdas: dict_items([('pout', tensor([1.0354])), ('power', tensor([0.9643]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0994])), ('power', tensor([-23.0317]))])
epoch：30	 i:0 	 global-step:600	 l-p:0.14649085700511932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[3.5684, 3.5687, 3.5684],
        [3.5684, 4.0827, 4.2265],
        [3.5684, 4.1960, 4.4469],
        [3.5684, 3.6813, 3.6291]], grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.14031875133514404 
model_pd.l_d.mean(): -21.067893981933594 
model_pd.lagr.mean(): -20.927576065063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0365])), ('power', tensor([0.9631]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0865])), ('power', tensor([-23.0146]))])
epoch：31	 i:0 	 global-step:620	 l-p:0.14031875133514404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[3.6040, 3.6074, 3.6042],
        [3.6040, 4.2388, 4.4923],
        [3.6040, 3.6456, 3.6157],
        [3.6040, 3.8284, 3.7872]], grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.13565489649772644 
model_pd.l_d.mean(): -21.03656768798828 
model_pd.lagr.mean(): -20.90091323852539 
model_pd.lambdas: dict_items([('pout', tensor([1.0376])), ('power', tensor([0.9620]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0735])), ('power', tensor([-22.9968]))])
epoch：32	 i:0 	 global-step:640	 l-p:0.13565489649772644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[3.6368, 3.6370, 3.6368],
        [3.6368, 3.6380, 3.6368],
        [3.6368, 3.6368, 3.6368],
        [3.6368, 3.6403, 3.6370]], grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.13222220540046692 
model_pd.l_d.mean(): -21.005226135253906 
model_pd.lagr.mean(): -20.873003005981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0387])), ('power', tensor([0.9608]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0617])), ('power', tensor([-22.9801]))])
epoch：33	 i:0 	 global-step:660	 l-p:0.13222220540046692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.6647, 3.6647, 3.6647],
        [3.6647, 3.7074, 3.6768],
        [3.6647, 3.8932, 3.8511],
        [3.6647, 4.0198, 4.0362]], grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.1297397017478943 
model_pd.l_d.mean(): -20.97423553466797 
model_pd.lagr.mean(): -20.84449577331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0397])), ('power', tensor([0.9597]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0517])), ('power', tensor([-22.9657]))])
epoch：34	 i:0 	 global-step:680	 l-p:0.1297397017478943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.6865, 3.6877, 3.6865],
        [3.6865, 3.7119, 3.6916],
        [3.6865, 4.0007, 3.9930],
        [3.6865, 3.7297, 3.6987]], grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.12801195681095123 
model_pd.l_d.mean(): -20.943918228149414 
model_pd.lagr.mean(): -20.815906524658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0407])), ('power', tensor([0.9586]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0439])), ('power', tensor([-22.9543]))])
epoch：35	 i:0 	 global-step:700	 l-p:0.12801195681095123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[3.7015, 3.9324, 3.8898],
        [3.7015, 4.5473, 5.0178],
        [3.7015, 4.5637, 5.0534],
        [3.7015, 4.2373, 4.3863]], grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.1269090324640274 
model_pd.l_d.mean(): -20.914472579956055 
model_pd.lagr.mean(): -20.78756332397461 
model_pd.lambdas: dict_items([('pout', tensor([1.0418])), ('power', tensor([0.9574]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0386])), ('power', tensor([-22.9465]))])
epoch：36	 i:0 	 global-step:720	 l-p:0.1269090324640274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.7096, 4.3098, 4.5168],
        [3.7096, 3.7132, 3.7099],
        [3.7096, 3.7099, 3.7096],
        [3.7096, 4.2465, 4.3958]], grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.12634259462356567 
model_pd.l_d.mean(): -20.886001586914062 
model_pd.lagr.mean(): -20.759658813476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0428])), ('power', tensor([0.9563]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0357])), ('power', tensor([-22.9422]))])
epoch：37	 i:0 	 global-step:740	 l-p:0.12634259462356567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.7111, 4.0596, 4.0694],
        [3.7111, 4.1860, 4.2835],
        [3.7111, 3.7918, 3.7451],
        [3.7111, 3.7111, 3.7111]], grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.12624973058700562 
model_pd.l_d.mean(): -20.858497619628906 
model_pd.lagr.mean(): -20.732248306274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0439])), ('power', tensor([0.9551]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0352])), ('power', tensor([-22.9416]))])
epoch：38	 i:0 	 global-step:760	 l-p:0.12624973058700562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[3.7065, 3.7129, 3.7071],
        [3.7065, 4.3556, 4.6109],
        [3.7065, 3.7067, 3.7065],
        [3.7065, 4.1805, 4.2777]], grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.12658338248729706 
model_pd.l_d.mean(): -20.831859588623047 
model_pd.lagr.mean(): -20.705276489257812 
model_pd.lambdas: dict_items([('pout', tensor([1.0449])), ('power', tensor([0.9540]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0369])), ('power', tensor([-22.9442]))])
epoch：39	 i:0 	 global-step:780	 l-p:0.12658338248729706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.6967, 3.6967, 3.6967],
        [3.6967, 4.2935, 4.4992],
        [3.6967, 3.7002, 3.6969],
        [3.6967, 3.7392, 3.7086]], grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.1273057758808136 
model_pd.l_d.mean(): -20.805931091308594 
model_pd.lagr.mean(): -20.678625106811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0459])), ('power', tensor([0.9528]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0404])), ('power', tensor([-22.9496]))])
epoch：40	 i:0 	 global-step:800	 l-p:0.1273057758808136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.6826, 4.0169, 4.0206],
        [3.6826, 3.7050, 3.6868],
        [3.6826, 3.8198, 3.7643],
        [3.6826, 3.6826, 3.6826]], grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.1283830851316452 
model_pd.l_d.mean(): -20.780498504638672 
model_pd.lagr.mean(): -20.652114868164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0470])), ('power', tensor([0.9517]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0455])), ('power', tensor([-22.9572]))])
epoch：41	 i:0 	 global-step:820	 l-p:0.1283830851316452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.6655, 3.6655, 3.6655],
        [3.6655, 3.9978, 4.0015],
        [3.6655, 3.6655, 3.6655],
        [3.6655, 4.0190, 4.0351]], grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.1297793984413147 
model_pd.l_d.mean(): -20.75533676147461 
model_pd.lagr.mean(): -20.62555694580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0480])), ('power', tensor([0.9505]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0516])), ('power', tensor([-22.9663]))])
epoch：42	 i:0 	 global-step:840	 l-p:0.1297793984413147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.6467, 3.8726, 3.8308],
        [3.6467, 3.6479, 3.6467],
        [3.6467, 4.0205, 4.0506],
        [3.6467, 3.6467, 3.6467]], grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.13144758343696594 
model_pd.l_d.mean(): -20.730228424072266 
model_pd.lagr.mean(): -20.59878158569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0491])), ('power', tensor([0.9494]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0584])), ('power', tensor([-22.9763]))])
epoch：43	 i:0 	 global-step:860	 l-p:0.13144758343696594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[3.6276, 3.7348, 3.6829],
        [3.6276, 4.0884, 4.1830],
        [3.6276, 3.7621, 3.7077],
        [3.6276, 3.6682, 3.6388]], grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.1333151012659073 
model_pd.l_d.mean(): -20.704973220825195 
model_pd.lagr.mean(): -20.571657180786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0502])), ('power', tensor([0.9482]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0653])), ('power', tensor([-22.9864]))])
epoch：44	 i:0 	 global-step:880	 l-p:0.1333151012659073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[3.6097, 3.6109, 3.6097],
        [3.6097, 3.9786, 4.0083],
        [3.6097, 4.4272, 4.8817],
        [3.6097, 3.6158, 3.6102]], grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.135264053940773 
model_pd.l_d.mean(): -20.679412841796875 
model_pd.lagr.mean(): -20.54414939880371 
model_pd.lambdas: dict_items([('pout', tensor([1.0512])), ('power', tensor([0.9471]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0718])), ('power', tensor([-22.9957]))])
epoch：45	 i:0 	 global-step:900	 l-p:0.135264053940773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.5945, 4.0498, 4.1433],
        [3.5945, 3.5979, 3.5947],
        [3.5945, 3.5949, 3.5945],
        [3.5945, 4.2224, 4.4720]], grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.1371108591556549 
model_pd.l_d.mean(): -20.65342903137207 
model_pd.lagr.mean(): -20.51631736755371 
model_pd.lambdas: dict_items([('pout', tensor([1.0523])), ('power', tensor([0.9459]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0774])), ('power', tensor([-23.0036]))])
epoch：46	 i:0 	 global-step:920	 l-p:0.1371108591556549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[3.5833, 3.5833, 3.5833],
        [3.5833, 3.7540, 3.7024],
        [3.5833, 4.4451, 4.9579],
        [3.5833, 3.8041, 3.7633]], grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.138606458902359 
model_pd.l_d.mean(): -20.626956939697266 
model_pd.lagr.mean(): -20.48834991455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0534])), ('power', tensor([0.9448]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0815])), ('power', tensor([-23.0093]))])
epoch：47	 i:0 	 global-step:940	 l-p:0.138606458902359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.5773, 3.5989, 3.5813],
        [3.5773, 3.6686, 3.6203],
        [3.5773, 3.7873, 3.7437],
        [3.5773, 3.7475, 3.6960]], grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.13948959112167358 
model_pd.l_d.mean(): -20.599971771240234 
model_pd.lagr.mean(): -20.460481643676758 
model_pd.lambdas: dict_items([('pout', tensor([1.0545])), ('power', tensor([0.9436]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0838])), ('power', tensor([-23.0125]))])
epoch：48	 i:0 	 global-step:960	 l-p:0.13948959112167358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.5768, 3.6166, 3.5878],
        [3.5768, 3.9082, 3.9175],
        [3.5768, 3.5815, 3.5771],
        [3.5768, 3.5984, 3.5808]], grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.13959640264511108 
model_pd.l_d.mean(): -20.57248306274414 
model_pd.lagr.mean(): -20.432886123657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0556])), ('power', tensor([0.9425]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0840])), ('power', tensor([-23.0130]))])
epoch：49	 i:0 	 global-step:980	 l-p:0.13959640264511108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[3.5815, 3.6214, 3.5925],
        [3.5815, 3.5827, 3.5816],
        [3.5815, 3.5817, 3.5815],
        [3.5815, 4.4415, 4.9529]], grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.13895271718502045 
model_pd.l_d.mean(): -20.54454803466797 
model_pd.lagr.mean(): -20.405595779418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0566])), ('power', tensor([0.9413]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0823])), ('power', tensor([-23.0108]))])
epoch：50	 i:0 	 global-step:1000	 l-p:0.13895271718502045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.5906, 3.7920, 3.7460],
        [3.5906, 3.8921, 3.8844],
        [3.5906, 3.7610, 3.7094],
        [3.5906, 3.5908, 3.5906]], grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.13775761425495148 
model_pd.l_d.mean(): -20.516239166259766 
model_pd.lagr.mean(): -20.378480911254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0577])), ('power', tensor([0.9402]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0790])), ('power', tensor([-23.0064]))])
epoch：51	 i:0 	 global-step:1020	 l-p:0.13775761425495148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[3.6026, 3.6797, 3.6350],
        [3.6026, 3.6440, 3.6143],
        [3.6026, 3.9051, 3.8972],
        [3.6026, 3.6026, 3.6026]], grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.13627667725086212 
model_pd.l_d.mean(): -20.48765754699707 
model_pd.lagr.mean(): -20.351381301879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0588])), ('power', tensor([0.9390]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0747])), ('power', tensor([-23.0005]))])
epoch：52	 i:0 	 global-step:1040	 l-p:0.13627667725086212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[3.6161, 3.6378, 3.6201],
        [3.6161, 3.8278, 3.7837],
        [3.6161, 3.9406, 3.9439],
        [3.6161, 4.4320, 4.8849]], grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.1347421109676361 
model_pd.l_d.mean(): -20.458911895751953 
model_pd.lagr.mean(): -20.324169158935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0599])), ('power', tensor([0.9379]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0698])), ('power', tensor([-22.9938]))])
epoch：53	 i:0 	 global-step:1060	 l-p:0.1347421109676361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[3.6297, 4.6764, 5.4187],
        [3.6297, 4.5019, 5.0199],
        [3.6297, 3.8332, 3.7866],
        [3.6297, 3.9054, 3.8836]], grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1333136409521103 
model_pd.l_d.mean(): -20.430124282836914 
model_pd.lagr.mean(): -20.296810150146484 
model_pd.lambdas: dict_items([('pout', tensor([1.0609])), ('power', tensor([0.9367]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0649])), ('power', tensor([-22.9870]))])
epoch：54	 i:0 	 global-step:1080	 l-p:0.1333136409521103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.6423, 3.7762, 3.7220],
        [3.6423, 4.1020, 4.1957],
        [3.6423, 3.6428, 3.6424],
        [3.6423, 3.6424, 3.6424]], grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.13208425045013428 
model_pd.l_d.mean(): -20.401416778564453 
model_pd.lagr.mean(): -20.269332885742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0620])), ('power', tensor([0.9356]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0604])), ('power', tensor([-22.9806]))])
epoch：55	 i:0 	 global-step:1100	 l-p:0.13208425045013428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[3.6531, 3.6778, 3.6581],
        [3.6531, 4.2362, 4.4362],
        [3.6531, 4.0018, 4.0170],
        [3.6531, 3.7668, 3.7139]], grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.13109976053237915 
model_pd.l_d.mean(): -20.372882843017578 
model_pd.lagr.mean(): -20.241783142089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0630])), ('power', tensor([0.9344]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0565])), ('power', tensor([-22.9752]))])
epoch：56	 i:0 	 global-step:1120	 l-p:0.13109976053237915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[3.6615, 4.2948, 4.5428],
        [3.6615, 3.6615, 3.6615],
        [3.6615, 4.5041, 4.9812],
        [3.6615, 4.1234, 4.2173]], grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.13037645816802979 
model_pd.l_d.mean(): -20.344608306884766 
model_pd.lagr.mean(): -20.214231491088867 
model_pd.lambdas: dict_items([('pout', tensor([1.0641])), ('power', tensor([0.9333]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0536])), ('power', tensor([-22.9710]))])
epoch：57	 i:0 	 global-step:1140	 l-p:0.13037645816802979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[3.6671, 4.3056, 4.5581],
        [3.6671, 3.7453, 3.7000],
        [3.6671, 4.0393, 4.0685],
        [3.6671, 4.3063, 4.5595]], grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.1299130767583847 
model_pd.l_d.mean(): -20.316638946533203 
model_pd.lagr.mean(): -20.186725616455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0651])), ('power', tensor([0.9321]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0516])), ('power', tensor([-22.9682]))])
epoch：58	 i:0 	 global-step:1160	 l-p:0.1299130767583847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[3.6699, 3.7628, 3.7136],
        [3.6699, 3.6699, 3.6699],
        [3.6699, 3.6946, 3.6749],
        [3.6699, 3.7770, 3.7250]], grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.12969720363616943 
model_pd.l_d.mean(): -20.288986206054688 
model_pd.lagr.mean(): -20.15928840637207 
model_pd.lambdas: dict_items([('pout', tensor([1.0662])), ('power', tensor([0.9310]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0506])), ('power', tensor([-22.9669]))])
epoch：59	 i:0 	 global-step:1180	 l-p:0.12969720363616943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.6700, 3.6700, 3.6700],
        [3.6700, 4.1927, 4.3368],
        [3.6700, 3.8433, 3.7906],
        [3.6700, 3.8840, 3.8392]], grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.12970854341983795 
model_pd.l_d.mean(): -20.261629104614258 
model_pd.lagr.mean(): -20.131919860839844 
model_pd.lambdas: dict_items([('pout', tensor([1.0672])), ('power', tensor([0.9298]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0507])), ('power', tensor([-22.9671]))])
epoch：60	 i:0 	 global-step:1200	 l-p:0.12970854341983795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[3.6677, 4.1897, 4.3334],
        [3.6677, 4.0165, 4.0315],
        [3.6677, 4.1621, 4.2824],
        [3.6677, 3.8407, 3.7881]], grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.12992019951343536 
model_pd.l_d.mean(): -20.234527587890625 
model_pd.lagr.mean(): -20.10460662841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0683])), ('power', tensor([0.9287]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0515])), ('power', tensor([-22.9685]))])
epoch：61	 i:0 	 global-step:1220	 l-p:0.12992019951343536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.6635, 4.0009, 4.0095],
        [3.6635, 4.6454, 5.2959],
        [3.6635, 3.6853, 3.6676],
        [3.6635, 3.9694, 3.9610]], grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.13029852509498596 
model_pd.l_d.mean(): -20.207622528076172 
model_pd.lagr.mean(): -20.07732391357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0694])), ('power', tensor([0.9275]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0530])), ('power', tensor([-22.9709]))])
epoch：62	 i:0 	 global-step:1240	 l-p:0.13029852509498596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[3.6580, 3.7710, 3.7184],
        [3.6580, 4.1776, 4.3206],
        [3.6580, 3.6581, 3.6580],
        [3.6580, 3.7915, 3.7373]], grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.13080254197120667 
model_pd.l_d.mean(): -20.180830001831055 
model_pd.lagr.mean(): -20.05002784729004 
model_pd.lambdas: dict_items([('pout', tensor([1.0704])), ('power', tensor([0.9264]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0551])), ('power', tensor([-22.9739]))])
epoch：63	 i:0 	 global-step:1260	 l-p:0.13080254197120667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[3.6519, 3.6519, 3.6519],
        [3.6519, 4.2315, 4.4298],
        [3.6519, 3.6934, 3.6636],
        [3.6519, 3.7850, 3.7309]], grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.13138329982757568 
model_pd.l_d.mean(): -20.15407371520996 
model_pd.lagr.mean(): -20.022689819335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0715])), ('power', tensor([0.9252]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0573])), ('power', tensor([-22.9773]))])
epoch：64	 i:0 	 global-step:1280	 l-p:0.13138329982757568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[3.6457, 3.6700, 3.6506],
        [3.6457, 3.6457, 3.6457],
        [3.6457, 3.6469, 3.6458],
        [3.6457, 3.6457, 3.6457]], grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.13198432326316833 
model_pd.l_d.mean(): -20.127294540405273 
model_pd.lagr.mean(): -19.995309829711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0725])), ('power', tensor([0.9241]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0596])), ('power', tensor([-22.9808]))])
epoch：65	 i:0 	 global-step:1300	 l-p:0.13198432326316833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.6402, 4.6124, 5.2562],
        [3.6402, 3.8510, 3.8067],
        [3.6402, 3.6405, 3.6402],
        [3.6402, 3.6406, 3.6402]], grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.13254447281360626 
model_pd.l_d.mean(): -20.100425720214844 
model_pd.lagr.mean(): -19.96788215637207 
model_pd.lambdas: dict_items([('pout', tensor([1.0736])), ('power', tensor([0.9229]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0616])), ('power', tensor([-22.9838]))])
epoch：66	 i:0 	 global-step:1320	 l-p:0.13254447281360626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.6358, 3.6574, 3.6399],
        [3.6358, 4.2647, 4.5132],
        [3.6358, 4.1229, 4.2412],
        [3.6358, 3.6358, 3.6358]], grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.13300305604934692 
model_pd.l_d.mean(): -20.073421478271484 
model_pd.lagr.mean(): -19.940418243408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0747])), ('power', tensor([0.9218]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0632])), ('power', tensor([-22.9863]))])
epoch：67	 i:0 	 global-step:1340	 l-p:0.13300305604934692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[3.6331, 3.6331, 3.6331],
        [3.6331, 3.9982, 4.0264],
        [3.6331, 4.0869, 4.1785],
        [3.6331, 4.6739, 5.4100]], grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.13330791890621185 
model_pd.l_d.mean(): -20.046260833740234 
model_pd.lagr.mean(): -19.912952423095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0757])), ('power', tensor([0.9206]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0642])), ('power', tensor([-22.9879]))])
epoch：68	 i:0 	 global-step:1360	 l-p:0.13330791890621185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.6322, 4.1180, 4.2357],
        [3.6322, 3.6356, 3.6325],
        [3.6322, 3.9748, 3.9892],
        [3.6322, 3.6727, 3.6436]], grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.13342486321926117 
model_pd.l_d.mean(): -20.0189266204834 
model_pd.lagr.mean(): -19.885501861572266 
model_pd.lambdas: dict_items([('pout', tensor([1.0768])), ('power', tensor([0.9195]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0646])), ('power', tensor([-22.9886]))])
epoch：69	 i:0 	 global-step:1380	 l-p:0.13342486321926117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.6333, 3.6366, 3.6335],
        [3.6333, 3.6333, 3.6333],
        [3.6333, 3.6333, 3.6333],
        [3.6333, 3.6337, 3.6333]], grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.13334451615810394 
model_pd.l_d.mean(): -19.99142837524414 
model_pd.lagr.mean(): -19.858083724975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0778])), ('power', tensor([0.9183]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0643])), ('power', tensor([-22.9882]))])
epoch：70	 i:0 	 global-step:1400	 l-p:0.13334451615810394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.6361, 4.6762, 5.4114],
        [3.6361, 4.2098, 4.4054],
        [3.6361, 3.6575, 3.6401],
        [3.6361, 3.6365, 3.6361]], grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.13308337330818176 
model_pd.l_d.mean(): -19.96377944946289 
model_pd.lagr.mean(): -19.83069610595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0789])), ('power', tensor([0.9172]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0633])), ('power', tensor([-22.9870]))])
epoch：71	 i:0 	 global-step:1420	 l-p:0.13308337330818176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[3.6404, 4.6815, 5.4170],
        [3.6404, 3.6814, 3.6519],
        [3.6404, 3.6407, 3.6404],
        [3.6404, 3.6529, 3.6421]], grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.1326790452003479 
model_pd.l_d.mean(): -19.936006546020508 
model_pd.lagr.mean(): -19.803327560424805 
model_pd.lambdas: dict_items([('pout', tensor([1.0800])), ('power', tensor([0.9160]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0618])), ('power', tensor([-22.9850]))])
epoch：72	 i:0 	 global-step:1440	 l-p:0.1326790452003479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[3.6457, 3.9677, 3.9700],
        [3.6457, 3.9468, 3.9381],
        [3.6457, 3.7221, 3.6778],
        [3.6457, 3.6457, 3.6457]], grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.1321810632944107 
model_pd.l_d.mean(): -19.908153533935547 
model_pd.lagr.mean(): -19.775972366333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0810])), ('power', tensor([0.9149]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0599])), ('power', tensor([-22.9824]))])
epoch：73	 i:0 	 global-step:1460	 l-p:0.1321810632944107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[3.6517, 3.7833, 3.7297],
        [3.6517, 3.7630, 3.7111],
        [3.6517, 4.2755, 4.5183],
        [3.6517, 4.2805, 4.5283]], grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.1316414773464203 
model_pd.l_d.mean(): -19.8802490234375 
model_pd.lagr.mean(): -19.748607635498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0821])), ('power', tensor([0.9137]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0578])), ('power', tensor([-22.9796]))])
epoch：74	 i:0 	 global-step:1480	 l-p:0.1316414773464203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[3.6577, 3.6987, 3.6692],
        [3.6577, 4.4901, 4.9594],
        [3.6577, 3.8588, 3.8122],
        [3.6577, 3.6985, 3.6692]], grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.13110743463039398 
model_pd.l_d.mean(): -19.85234260559082 
model_pd.lagr.mean(): -19.721235275268555 
model_pd.lambdas: dict_items([('pout', tensor([1.0831])), ('power', tensor([0.9126]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0557])), ('power', tensor([-22.9766]))])
epoch：75	 i:0 	 global-step:1500	 l-p:0.13110743463039398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[3.6634, 3.7043, 3.6749],
        [3.6634, 3.6668, 3.6636],
        [3.6634, 4.2887, 4.5317],
        [3.6634, 4.5343, 5.0487]], grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.13061706721782684 
model_pd.l_d.mean(): -19.824474334716797 
model_pd.lagr.mean(): -19.693857192993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0842])), ('power', tensor([0.9114]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0537])), ('power', tensor([-22.9739]))])
epoch：76	 i:0 	 global-step:1520	 l-p:0.13061706721782684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[3.6684, 3.6686, 3.6684],
        [3.6684, 4.2993, 4.5475],
        [3.6684, 3.6696, 3.6685],
        [3.6684, 3.7093, 3.6799]], grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.13019780814647675 
model_pd.l_d.mean(): -19.796669006347656 
model_pd.lagr.mean(): -19.666471481323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0852])), ('power', tensor([0.9103]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0520])), ('power', tensor([-22.9714]))])
epoch：77	 i:0 	 global-step:1540	 l-p:0.13019780814647675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[3.6725, 4.4914, 4.9428],
        [3.6725, 3.7491, 3.7046],
        [3.6725, 3.9957, 3.9976],
        [3.6725, 3.8829, 3.8383]], grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.129866823554039 
model_pd.l_d.mean(): -19.768951416015625 
model_pd.lagr.mean(): -19.639083862304688 
model_pd.lambdas: dict_items([('pout', tensor([1.0863])), ('power', tensor([0.9091]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0505])), ('power', tensor([-22.9695]))])
epoch：78	 i:0 	 global-step:1560	 l-p:0.129866823554039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.6756, 3.6802, 3.6759],
        [3.6756, 4.2530, 4.4490],
        [3.6756, 3.6756, 3.6756],
        [3.6756, 3.6756, 3.6756]], grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.12963201105594635 
model_pd.l_d.mean(): -19.74134635925293 
model_pd.lagr.mean(): -19.611713409423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0873])), ('power', tensor([0.9080]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0495])), ('power', tensor([-22.9681]))])
epoch：79	 i:0 	 global-step:1580	 l-p:0.12963201105594635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[3.6775, 4.3037, 4.5466],
        [3.6775, 3.7823, 3.7312],
        [3.6775, 3.6775, 3.6775],
        [3.6775, 3.8979, 3.8561]], grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.12949304282665253 
model_pd.l_d.mean(): -19.713842391967773 
model_pd.lagr.mean(): -19.584348678588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0884])), ('power', tensor([0.9069]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0489])), ('power', tensor([-22.9673]))])
epoch：80	 i:0 	 global-step:1600	 l-p:0.12949304282665253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.6784, 3.6844, 3.6789],
        [3.6784, 3.6784, 3.6784],
        [3.6784, 3.6784, 3.6784],
        [3.6784, 4.1664, 4.2835]], grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.12944278120994568 
model_pd.l_d.mean(): -19.686447143554688 
model_pd.lagr.mean(): -19.557004928588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0894])), ('power', tensor([0.9057]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0486])), ('power', tensor([-22.9671]))])
epoch：81	 i:0 	 global-step:1620	 l-p:0.12944278120994568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[3.6783, 4.0216, 4.0351],
        [3.6783, 3.7187, 3.6896],
        [3.6783, 4.0109, 4.0183],
        [3.6783, 3.6783, 3.6783]], grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.12946833670139313 
model_pd.l_d.mean(): -19.659133911132812 
model_pd.lagr.mean(): -19.529664993286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0905])), ('power', tensor([0.9046]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0487])), ('power', tensor([-22.9674]))])
epoch：82	 i:0 	 global-step:1640	 l-p:0.12946833670139313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[3.6776, 4.5486, 5.0619],
        [3.6776, 3.7183, 3.6890],
        [3.6776, 3.8781, 3.8314],
        [3.6776, 3.6836, 3.6781]], grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.12955214083194733 
model_pd.l_d.mean(): -19.631879806518555 
model_pd.lagr.mean(): -19.50232696533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0915])), ('power', tensor([0.9034]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0490])), ('power', tensor([-22.9680]))])
epoch：83	 i:0 	 global-step:1660	 l-p:0.12955214083194733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[3.6764, 3.6889, 3.6781],
        [3.6764, 4.5465, 5.0592],
        [3.6764, 3.6810, 3.6768],
        [3.6764, 4.4930, 4.9423]], grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.1296733021736145 
model_pd.l_d.mean(): -19.60466194152832 
model_pd.lagr.mean(): -19.47498893737793 
model_pd.lambdas: dict_items([('pout', tensor([1.0926])), ('power', tensor([0.9023]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0494])), ('power', tensor([-22.9688]))])
epoch：84	 i:0 	 global-step:1680	 l-p:0.1296733021736145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[3.6751, 3.6751, 3.6751],
        [3.6751, 3.6990, 3.6799],
        [3.6751, 3.8060, 3.7525],
        [3.6751, 3.6752, 3.6751]], grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.12980909645557404 
model_pd.l_d.mean(): -19.577455520629883 
model_pd.lagr.mean(): -19.447647094726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0936])), ('power', tensor([0.9011]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0500])), ('power', tensor([-22.9698]))])
epoch：85	 i:0 	 global-step:1700	 l-p:0.12980909645557404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[3.6739, 3.6741, 3.6739],
        [3.6739, 3.6739, 3.6739],
        [3.6739, 4.0375, 4.0643],
        [3.6739, 3.6751, 3.6740]], grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.12993645668029785 
model_pd.l_d.mean(): -19.55023765563965 
model_pd.lagr.mean(): -19.42030143737793 
model_pd.lambdas: dict_items([('pout', tensor([1.0947])), ('power', tensor([0.9000]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0504])), ('power', tensor([-22.9706]))])
epoch：86	 i:0 	 global-step:1720	 l-p:0.12993645668029785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.6731, 3.6731, 3.6731],
        [3.6731, 3.6764, 3.6733],
        [3.6731, 3.8724, 3.8258],
        [3.6731, 3.6735, 3.6731]], grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.13003426790237427 
model_pd.l_d.mean(): -19.52298355102539 
model_pd.lagr.mean(): -19.3929500579834 
model_pd.lambdas: dict_items([('pout', tensor([1.0957])), ('power', tensor([0.8988]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0508])), ('power', tensor([-22.9713]))])
epoch：87	 i:0 	 global-step:1740	 l-p:0.13003426790237427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[3.6728, 3.7133, 3.6842],
        [3.6728, 4.6428, 5.2813],
        [3.6728, 3.6730, 3.6728],
        [3.6728, 3.7829, 3.7313]], grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.13008545339107513 
model_pd.l_d.mean(): -19.495683670043945 
model_pd.lagr.mean(): -19.365598678588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0968])), ('power', tensor([0.8977]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0509])), ('power', tensor([-22.9717]))])
epoch：88	 i:0 	 global-step:1760	 l-p:0.13008545339107513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[3.6732, 4.6426, 5.2806],
        [3.6732, 3.8032, 3.7500],
        [3.6732, 3.6736, 3.6732],
        [3.6732, 3.6855, 3.6748]], grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.1300782561302185 
model_pd.l_d.mean(): -19.46832847595215 
model_pd.lagr.mean(): -19.3382511138916 
model_pd.lambdas: dict_items([('pout', tensor([1.0978])), ('power', tensor([0.8965]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0509])), ('power', tensor([-22.9718]))])
epoch：89	 i:0 	 global-step:1780	 l-p:0.1300782561302185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[3.6743, 3.6745, 3.6743],
        [3.6743, 3.6745, 3.6743],
        [3.6743, 3.8919, 3.8502],
        [3.6743, 3.8202, 3.7669]], grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.13000807166099548 
model_pd.l_d.mean(): -19.440914154052734 
model_pd.lagr.mean(): -19.31090545654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0989])), ('power', tensor([0.8954]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0505])), ('power', tensor([-22.9714]))])
epoch：90	 i:0 	 global-step:1800	 l-p:0.13000807166099548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[3.6761, 4.1864, 4.3243],
        [3.6761, 3.7514, 3.7075],
        [3.6761, 3.8059, 3.7527],
        [3.6761, 3.6884, 3.6777]], grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.1298770308494568 
model_pd.l_d.mean(): -19.413450241088867 
model_pd.lagr.mean(): -19.283573150634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0999])), ('power', tensor([0.8942]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0499])), ('power', tensor([-22.9707]))])
epoch：91	 i:0 	 global-step:1820	 l-p:0.1298770308494568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.6785, 3.7883, 3.7368],
        [3.6785, 3.7817, 3.7313],
        [3.6785, 4.4908, 4.9366],
        [3.6785, 3.6785, 3.6785]], grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.1296934187412262 
model_pd.l_d.mean(): -19.38593292236328 
model_pd.lagr.mean(): -19.25623893737793 
model_pd.lambdas: dict_items([('pout', tensor([1.1010])), ('power', tensor([0.8931]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0491])), ('power', tensor([-22.9697]))])
epoch：92	 i:0 	 global-step:1840	 l-p:0.1296934187412262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[3.6815, 3.7566, 3.7129],
        [3.6815, 3.7218, 3.6928],
        [3.6815, 3.7846, 3.7342],
        [3.6815, 4.7238, 5.4561]], grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.12946999073028564 
model_pd.l_d.mean(): -19.358386993408203 
model_pd.lagr.mean(): -19.22891616821289 
model_pd.lambdas: dict_items([('pout', tensor([1.1020])), ('power', tensor([0.8919]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0481])), ('power', tensor([-22.9684]))])
epoch：93	 i:0 	 global-step:1860	 l-p:0.12946999073028564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[3.6847, 3.6847, 3.6847],
        [3.6847, 3.6906, 3.6852],
        [3.6847, 3.7236, 3.6954],
        [3.6847, 4.0464, 4.0725]], grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.12922221422195435 
model_pd.l_d.mean(): -19.33081817626953 
model_pd.lagr.mean(): -19.201595306396484 
model_pd.lambdas: dict_items([('pout', tensor([1.1031])), ('power', tensor([0.8908]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0470])), ('power', tensor([-22.9669]))])
epoch：94	 i:0 	 global-step:1880	 l-p:0.12922221422195435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.6881, 3.7912, 3.7408],
        [3.6881, 3.6881, 3.6881],
        [3.6881, 3.7977, 3.7463],
        [3.6881, 3.8952, 3.8507]], grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.12896610796451569 
model_pd.l_d.mean(): -19.30324363708496 
model_pd.lagr.mean(): -19.174278259277344 
model_pd.lambdas: dict_items([('pout', tensor([1.1041])), ('power', tensor([0.8896]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0458])), ('power', tensor([-22.9654]))])
epoch：95	 i:0 	 global-step:1900	 l-p:0.12896610796451569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.6915, 4.6623, 5.2997],
        [3.6915, 4.0310, 4.0434],
        [3.6915, 4.5584, 5.0672],
        [3.6915, 3.6919, 3.6915]], grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.12871642410755157 
model_pd.l_d.mean(): -19.275676727294922 
model_pd.lagr.mean(): -19.146961212158203 
model_pd.lambdas: dict_items([('pout', tensor([1.1052])), ('power', tensor([0.8885]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0447])), ('power', tensor([-22.9638]))])
epoch：96	 i:0 	 global-step:1920	 l-p:0.12871642410755157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[3.6947, 4.7389, 5.4716],
        [3.6947, 3.8401, 3.7868],
        [3.6947, 4.3203, 4.5640],
        [3.6947, 3.6947, 3.6947]], grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.1284855753183365 
model_pd.l_d.mean(): -19.24813461303711 
model_pd.lagr.mean(): -19.11964988708496 
model_pd.lambdas: dict_items([('pout', tensor([1.1062])), ('power', tensor([0.8873]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0436])), ('power', tensor([-22.9624]))])
epoch：97	 i:0 	 global-step:1940	 l-p:0.1284855753183365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[3.6975, 3.8429, 3.7896],
        [3.6975, 3.8005, 3.7500],
        [3.6975, 4.2079, 4.3450],
        [3.6975, 3.6976, 3.6975]], grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.12828271090984344 
model_pd.l_d.mean(): -19.220623016357422 
model_pd.lagr.mean(): -19.09234046936035 
model_pd.lambdas: dict_items([('pout', tensor([1.1073])), ('power', tensor([0.8862]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0427])), ('power', tensor([-22.9611]))])
epoch：98	 i:0 	 global-step:1960	 l-p:0.12828271090984344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[3.7000, 4.6714, 5.3087],
        [3.7000, 4.3205, 4.5589],
        [3.7000, 3.9067, 3.8621],
        [3.7000, 4.3255, 4.5689]], grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.12811347842216492 
model_pd.l_d.mean(): -19.193147659301758 
model_pd.lagr.mean(): -19.065034866333008 
model_pd.lambdas: dict_items([('pout', tensor([1.1083])), ('power', tensor([0.8850]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0419])), ('power', tensor([-22.9601]))])
epoch：99	 i:0 	 global-step:1980	 l-p:0.12811347842216492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[3.7020, 3.7255, 3.7067],
        [3.7020, 4.5316, 4.9953],
        [3.7020, 3.8113, 3.7599],
        [3.7020, 3.7407, 3.7126]], grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.1279798150062561 
model_pd.l_d.mean(): -19.16571807861328 
model_pd.lagr.mean(): -19.037738800048828 
model_pd.lambdas: dict_items([('pout', tensor([1.1094])), ('power', tensor([0.8839]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0412])), ('power', tensor([-22.9593]))])
epoch：100	 i:0 	 global-step:2000	 l-p:0.1279798150062561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[3.7036, 3.7036, 3.7036],
        [3.7036, 3.7036, 3.7036],
        [3.7036, 4.7481, 5.4800],
        [3.7036, 3.7068, 3.7038]], grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.1278814971446991 
model_pd.l_d.mean(): -19.138320922851562 
model_pd.lagr.mean(): -19.010438919067383 
model_pd.lambdas: dict_items([('pout', tensor([1.1104])), ('power', tensor([0.8827]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0407])), ('power', tensor([-22.9587]))])
epoch：101	 i:0 	 global-step:2020	 l-p:0.1278814971446991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[3.7047, 3.7936, 3.7461],
        [3.7047, 3.7048, 3.7047],
        [3.7047, 4.1869, 4.3004],
        [3.7047, 4.3245, 4.5621]], grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.12781545519828796 
model_pd.l_d.mean(): -19.110960006713867 
model_pd.lagr.mean(): -18.983144760131836 
model_pd.lambdas: dict_items([('pout', tensor([1.1114])), ('power', tensor([0.8816]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0403])), ('power', tensor([-22.9583]))])
epoch：102	 i:0 	 global-step:2040	 l-p:0.12781545519828796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.7056, 3.8080, 3.7577],
        [3.7056, 3.8720, 3.8203],
        [3.7056, 4.0231, 4.0233],
        [3.7056, 3.7056, 3.7056]], grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.1277751326560974 
model_pd.l_d.mean(): -19.083627700805664 
model_pd.lagr.mean(): -18.955852508544922 
model_pd.lambdas: dict_items([('pout', tensor([1.1125])), ('power', tensor([0.8804]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0401])), ('power', tensor([-22.9582]))])
epoch：103	 i:0 	 global-step:2060	 l-p:0.1277751326560974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[3.7062, 3.7063, 3.7062],
        [3.7062, 3.7064, 3.7062],
        [3.7062, 3.7455, 3.7171],
        [3.7062, 3.7062, 3.7062]], grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.12775194644927979 
model_pd.l_d.mean(): -19.056310653686523 
model_pd.lagr.mean(): -18.928558349609375 
model_pd.lambdas: dict_items([('pout', tensor([1.1135])), ('power', tensor([0.8793]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0399])), ('power', tensor([-22.9581]))])
epoch：104	 i:0 	 global-step:2080	 l-p:0.12775194644927979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.7067, 3.7300, 3.7113],
        [3.7067, 3.7069, 3.7067],
        [3.7067, 4.0337, 4.0392],
        [3.7067, 3.7067, 3.7067]], grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.12773661315441132 
model_pd.l_d.mean(): -19.029003143310547 
model_pd.lagr.mean(): -18.90126609802246 
model_pd.lambdas: dict_items([('pout', tensor([1.1146])), ('power', tensor([0.8781]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0398])), ('power', tensor([-22.9581]))])
epoch：105	 i:0 	 global-step:2100	 l-p:0.12773661315441132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[3.7072, 4.3302, 4.5716],
        [3.7072, 3.7074, 3.7072],
        [3.7072, 3.7072, 3.7072],
        [3.7072, 3.7305, 3.7118]], grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.1277194619178772 
model_pd.l_d.mean(): -19.001691818237305 
model_pd.lagr.mean(): -18.873971939086914 
model_pd.lambdas: dict_items([('pout', tensor([1.1156])), ('power', tensor([0.8770]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0397])), ('power', tensor([-22.9581]))])
epoch：106	 i:0 	 global-step:2120	 l-p:0.1277194619178772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.7078, 4.0668, 4.0916],
        [3.7078, 3.7111, 3.7080],
        [3.7078, 3.7078, 3.7078],
        [3.7078, 3.7078, 3.7078]], grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.1276918649673462 
model_pd.l_d.mean(): -18.97437286376953 
model_pd.lagr.mean(): -18.846681594848633 
model_pd.lambdas: dict_items([('pout', tensor([1.1166])), ('power', tensor([0.8758]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0395])), ('power', tensor([-22.9580]))])
epoch：107	 i:0 	 global-step:2140	 l-p:0.1276918649673462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.7087, 3.7208, 3.7103],
        [3.7087, 3.7087, 3.7087],
        [3.7087, 3.7145, 3.7092],
        [3.7087, 4.0040, 3.9932]], grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.12764672935009003 
model_pd.l_d.mean(): -18.947036743164062 
model_pd.lagr.mean(): -18.81938934326172 
model_pd.lambdas: dict_items([('pout', tensor([1.1177])), ('power', tensor([0.8747]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0393])), ('power', tensor([-22.9578]))])
epoch：108	 i:0 	 global-step:2160	 l-p:0.12764672935009003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.7099, 3.8180, 3.7670],
        [3.7099, 3.7099, 3.7099],
        [3.7099, 3.7305, 3.7137],
        [3.7099, 3.8752, 3.8237]], grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.1275794357061386 
model_pd.l_d.mean(): -18.919677734375 
model_pd.lagr.mean(): -18.792098999023438 
model_pd.lambdas: dict_items([('pout', tensor([1.1187])), ('power', tensor([0.8736]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0389])), ('power', tensor([-22.9574]))])
epoch：109	 i:0 	 global-step:2180	 l-p:0.1275794357061386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[3.7114, 3.7114, 3.7114],
        [3.7114, 3.9778, 3.9539],
        [3.7114, 4.7524, 5.4798],
        [3.7114, 3.7235, 3.7130]], grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.12748798727989197 
model_pd.l_d.mean(): -18.892301559448242 
model_pd.lagr.mean(): -18.764814376831055 
model_pd.lambdas: dict_items([('pout', tensor([1.1197])), ('power', tensor([0.8724]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0384])), ('power', tensor([-22.9568]))])
epoch：110	 i:0 	 global-step:2200	 l-p:0.12748798727989197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[3.7132, 3.7522, 3.7240],
        [3.7132, 3.7134, 3.7132],
        [3.7132, 3.7134, 3.7132],
        [3.7132, 3.9794, 3.9555]], grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.12737306952476501 
model_pd.l_d.mean(): -18.864904403686523 
model_pd.lagr.mean(): -18.737531661987305 
model_pd.lambdas: dict_items([('pout', tensor([1.1208])), ('power', tensor([0.8713]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0378])), ('power', tensor([-22.9561]))])
epoch：111	 i:0 	 global-step:2220	 l-p:0.12737306952476501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.7153, 3.9814, 3.9574],
        [3.7153, 3.7546, 3.7262],
        [3.7153, 3.7153, 3.7153],
        [3.7153, 3.7153, 3.7153]], grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.12723779678344727 
model_pd.l_d.mean(): -18.837486267089844 
model_pd.lagr.mean(): -18.710248947143555 
model_pd.lambdas: dict_items([('pout', tensor([1.1218])), ('power', tensor([0.8701]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0372])), ('power', tensor([-22.9553]))])
epoch：112	 i:0 	 global-step:2240	 l-p:0.12723779678344727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[3.7176, 3.7176, 3.7176],
        [3.7176, 4.0427, 4.0476],
        [3.7176, 3.7233, 3.7181],
        [3.7176, 3.7208, 3.7178]], grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.12708723545074463 
model_pd.l_d.mean(): -18.8100528717041 
model_pd.lagr.mean(): -18.682966232299805 
model_pd.lambdas: dict_items([('pout', tensor([1.1229])), ('power', tensor([0.8690]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0364])), ('power', tensor([-22.9543]))])
epoch：113	 i:0 	 global-step:2260	 l-p:0.12708723545074463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[3.7200, 3.7594, 3.7310],
        [3.7200, 3.7204, 3.7200],
        [3.7200, 3.7200, 3.7200],
        [3.7200, 4.7613, 5.4880]], grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.1269274353981018 
model_pd.l_d.mean(): -18.782621383666992 
model_pd.lagr.mean(): -18.65569305419922 
model_pd.lambdas: dict_items([('pout', tensor([1.1239])), ('power', tensor([0.8678]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0356])), ('power', tensor([-22.9532]))])
epoch：114	 i:0 	 global-step:2280	 l-p:0.1269274353981018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.7225, 4.0474, 4.0521],
        [3.7225, 3.8102, 3.7631],
        [3.7225, 4.5859, 5.0890],
        [3.7225, 3.7225, 3.7225]], grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.12676489353179932 
model_pd.l_d.mean(): -18.75518226623535 
model_pd.lagr.mean(): -18.62841796875 
model_pd.lambdas: dict_items([('pout', tensor([1.1249])), ('power', tensor([0.8667]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0348])), ('power', tensor([-22.9521]))])
epoch：115	 i:0 	 global-step:2300	 l-p:0.12676489353179932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[3.7250, 3.7629, 3.7353],
        [3.7250, 4.2921, 4.4801],
        [3.7250, 3.9388, 3.8966],
        [3.7250, 3.8522, 3.7995]], grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.126605823636055 
model_pd.l_d.mean(): -18.727750778198242 
model_pd.lagr.mean(): -18.601144790649414 
model_pd.lambdas: dict_items([('pout', tensor([1.1260])), ('power', tensor([0.8655]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0340])), ('power', tensor([-22.9510]))])
epoch：116	 i:0 	 global-step:2320	 l-p:0.126605823636055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.7273, 4.5908, 5.0936],
        [3.7273, 3.7661, 3.7380],
        [3.7273, 3.7393, 3.7289],
        [3.7273, 3.7273, 3.7273]], grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.12645535171031952 
model_pd.l_d.mean(): -18.700332641601562 
model_pd.lagr.mean(): -18.573877334594727 
model_pd.lambdas: dict_items([('pout', tensor([1.1270])), ('power', tensor([0.8644]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0332])), ('power', tensor([-22.9500]))])
epoch：117	 i:0 	 global-step:2340	 l-p:0.12645535171031952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[3.7295, 3.7339, 3.7298],
        [3.7295, 3.8566, 3.8040],
        [3.7295, 3.9949, 3.9706],
        [3.7295, 3.8368, 3.7861]], grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.1263173520565033 
model_pd.l_d.mean(): -18.672924041748047 
model_pd.lagr.mean(): -18.546606063842773 
model_pd.lambdas: dict_items([('pout', tensor([1.1280])), ('power', tensor([0.8632]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0325])), ('power', tensor([-22.9491]))])
epoch：118	 i:0 	 global-step:2360	 l-p:0.1263173520565033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.7315, 3.7372, 3.7320],
        [3.7315, 4.3471, 4.5809],
        [3.7315, 3.7315, 3.7315],
        [3.7315, 4.2368, 4.3701]], grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.12619388103485107 
model_pd.l_d.mean(): -18.645538330078125 
model_pd.lagr.mean(): -18.519344329833984 
model_pd.lambdas: dict_items([('pout', tensor([1.1291])), ('power', tensor([0.8621]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0318])), ('power', tensor([-22.9482]))])
epoch：119	 i:0 	 global-step:2380	 l-p:0.12619388103485107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[3.7333, 4.2998, 4.4872],
        [3.7333, 3.7537, 3.7371],
        [3.7333, 3.7333, 3.7333],
        [3.7333, 3.7335, 3.7333]], grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.12608543038368225 
model_pd.l_d.mean(): -18.61817169189453 
model_pd.lagr.mean(): -18.49208641052246 
model_pd.lambdas: dict_items([('pout', tensor([1.1301])), ('power', tensor([0.8609]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0312])), ('power', tensor([-22.9475]))])
epoch：120	 i:0 	 global-step:2400	 l-p:0.12608543038368225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[3.7349, 3.9380, 3.8932],
        [3.7349, 3.7406, 3.7354],
        [3.7349, 3.8082, 3.7653],
        [3.7349, 3.8616, 3.8091]], grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.12599070370197296 
model_pd.l_d.mean(): -18.590816497802734 
model_pd.lagr.mean(): -18.464826583862305 
model_pd.lambdas: dict_items([('pout', tensor([1.1311])), ('power', tensor([0.8598]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0307])), ('power', tensor([-22.9469]))])
epoch：121	 i:0 	 global-step:2420	 l-p:0.12599070370197296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.7364, 3.7593, 3.7409],
        [3.7364, 3.7366, 3.7364],
        [3.7364, 3.8630, 3.8105],
        [3.7364, 3.7755, 3.7473]], grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.12590719759464264 
model_pd.l_d.mean(): -18.5634708404541 
model_pd.lagr.mean(): -18.437562942504883 
model_pd.lambdas: dict_items([('pout', tensor([1.1322])), ('power', tensor([0.8586]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0303])), ('power', tensor([-22.9463]))])
epoch：122	 i:0 	 global-step:2440	 l-p:0.12590719759464264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.7377, 3.7409, 3.7379],
        [3.7377, 4.2421, 4.3747],
        [3.7377, 4.6004, 5.1017],
        [3.7377, 3.7768, 3.7486]], grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.12583132088184357 
model_pd.l_d.mean(): -18.536144256591797 
model_pd.lagr.mean(): -18.41031265258789 
model_pd.lambdas: dict_items([('pout', tensor([1.1332])), ('power', tensor([0.8575]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0298])), ('power', tensor([-22.9458]))])
epoch：123	 i:0 	 global-step:2460	 l-p:0.12583132088184357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[3.7390, 3.7775, 3.7497],
        [3.7390, 4.0728, 4.0828],
        [3.7390, 4.0316, 4.0198],
        [3.7390, 3.8811, 3.8284]], grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.12575899064540863 
model_pd.l_d.mean(): -18.508817672729492 
model_pd.lagr.mean(): -18.383058547973633 
model_pd.lambdas: dict_items([('pout', tensor([1.1342])), ('power', tensor([0.8563]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0294])), ('power', tensor([-22.9454]))])
epoch：124	 i:0 	 global-step:2480	 l-p:0.12575899064540863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.7404, 3.8470, 3.7965],
        [3.7404, 4.0533, 4.0519],
        [3.7404, 3.7405, 3.7404],
        [3.7404, 4.2169, 4.3266]], grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.12568598985671997 
model_pd.l_d.mean(): -18.48149299621582 
model_pd.lagr.mean(): -18.355806350708008 
model_pd.lambdas: dict_items([('pout', tensor([1.1352])), ('power', tensor([0.8552]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0290])), ('power', tensor([-22.9449]))])
epoch：125	 i:0 	 global-step:2500	 l-p:0.12568598985671997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[3.7417, 3.7793, 3.7520],
        [3.7417, 4.0750, 4.0848],
        [3.7417, 4.3556, 4.5878],
        [3.7417, 4.0971, 4.1199]], grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.1256086379289627 
model_pd.l_d.mean(): -18.454160690307617 
model_pd.lagr.mean(): -18.32855224609375 
model_pd.lambdas: dict_items([('pout', tensor([1.1363])), ('power', tensor([0.8540]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0286])), ('power', tensor([-22.9444]))])
epoch：126	 i:0 	 global-step:2520	 l-p:0.1256086379289627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.7432, 3.7432, 3.7432],
        [3.7432, 4.7840, 5.5073],
        [3.7432, 3.7434, 3.7432],
        [3.7432, 3.8299, 3.7833]], grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.12552392482757568 
model_pd.l_d.mean(): -18.426834106445312 
model_pd.lagr.mean(): -18.30130958557129 
model_pd.lambdas: dict_items([('pout', tensor([1.1373])), ('power', tensor([0.8529]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0281])), ('power', tensor([-22.9438]))])
epoch：127	 i:0 	 global-step:2540	 l-p:0.12552392482757568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[3.7449, 4.1882, 4.2718],
        [3.7449, 3.9078, 3.8564],
        [3.7449, 3.8512, 3.8008],
        [3.7449, 4.3634, 4.6000]], grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.12542998790740967 
model_pd.l_d.mean(): -18.39948844909668 
model_pd.lagr.mean(): -18.274059295654297 
model_pd.lambdas: dict_items([('pout', tensor([1.1383])), ('power', tensor([0.8518]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0276])), ('power', tensor([-22.9432]))])
epoch：128	 i:0 	 global-step:2560	 l-p:0.12542998790740967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[3.7466, 4.5702, 5.0254],
        [3.7466, 4.2496, 4.3811],
        [3.7466, 3.7498, 3.7468],
        [3.7466, 3.8724, 3.8201]], grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.1253259927034378 
model_pd.l_d.mean(): -18.372146606445312 
model_pd.lagr.mean(): -18.2468204498291 
model_pd.lambdas: dict_items([('pout', tensor([1.1394])), ('power', tensor([0.8506]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0270])), ('power', tensor([-22.9424]))])
epoch：129	 i:0 	 global-step:2580	 l-p:0.1253259927034378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.7486, 3.9113, 3.8599],
        [3.7486, 4.5558, 4.9922],
        [3.7486, 3.7604, 3.7501],
        [3.7486, 3.8350, 3.7885]], grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.1252126693725586 
model_pd.l_d.mean(): -18.344785690307617 
model_pd.lagr.mean(): -18.219573974609375 
model_pd.lambdas: dict_items([('pout', tensor([1.1404])), ('power', tensor([0.8495]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0264])), ('power', tensor([-22.9416]))])
epoch：130	 i:0 	 global-step:2600	 l-p:0.1252126693725586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[3.7506, 4.2260, 4.3347],
        [3.7506, 3.9621, 3.9197],
        [3.7506, 3.7506, 3.7506],
        [3.7506, 4.1934, 4.2767]], grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.12509143352508545 
model_pd.l_d.mean(): -18.317426681518555 
model_pd.lagr.mean(): -18.19233512878418 
model_pd.lambdas: dict_items([('pout', tensor([1.1414])), ('power', tensor([0.8483]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0257])), ('power', tensor([-22.9407]))])
epoch：131	 i:0 	 global-step:2620	 l-p:0.12509143352508545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[3.7528, 3.7528, 3.7528],
        [3.7528, 4.1070, 4.1293],
        [3.7528, 3.7754, 3.7572],
        [3.7528, 4.3657, 4.5966]], grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.124964639544487 
model_pd.l_d.mean(): -18.290061950683594 
model_pd.lagr.mean(): -18.165098190307617 
model_pd.lambdas: dict_items([('pout', tensor([1.1424])), ('power', tensor([0.8472]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0250])), ('power', tensor([-22.9398]))])
epoch：132	 i:0 	 global-step:2640	 l-p:0.124964639544487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.7549, 3.7922, 3.7651],
        [3.7549, 3.8803, 3.8281],
        [3.7549, 3.9475, 3.9007],
        [3.7549, 3.7551, 3.7549]], grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.12483510375022888 
model_pd.l_d.mean(): -18.26270294189453 
model_pd.lagr.mean(): -18.137866973876953 
model_pd.lambdas: dict_items([('pout', tensor([1.1435])), ('power', tensor([0.8460]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0243])), ('power', tensor([-22.9388]))])
epoch：133	 i:0 	 global-step:2660	 l-p:0.12483510375022888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[3.7571, 3.7574, 3.7571],
        [3.7571, 4.1111, 4.1332],
        [3.7571, 4.0786, 4.0819],
        [3.7571, 3.7572, 3.7571]], grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.12470569461584091 
model_pd.l_d.mean(): -18.235342025756836 
model_pd.lagr.mean(): -18.11063575744629 
model_pd.lambdas: dict_items([('pout', tensor([1.1445])), ('power', tensor([0.8449]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0236])), ('power', tensor([-22.9379]))])
epoch：134	 i:0 	 global-step:2680	 l-p:0.12470569461584091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.7593, 3.7649, 3.7598],
        [3.7593, 3.9001, 3.8476],
        [3.7593, 4.5827, 5.0368],
        [3.7593, 4.2342, 4.3423]], grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.12457893043756485 
model_pd.l_d.mean(): -18.207984924316406 
model_pd.lagr.mean(): -18.083406448364258 
model_pd.lambdas: dict_items([('pout', tensor([1.1455])), ('power', tensor([0.8437]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0229])), ('power', tensor([-22.9369]))])
epoch：135	 i:0 	 global-step:2700	 l-p:0.12457893043756485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.7614, 3.9624, 3.9174],
        [3.7614, 4.6229, 5.1212],
        [3.7614, 3.7670, 3.7619],
        [3.7614, 3.7614, 3.7614]], grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.12445687502622604 
model_pd.l_d.mean(): -18.18063735961914 
model_pd.lagr.mean(): -18.056180953979492 
model_pd.lambdas: dict_items([('pout', tensor([1.1465])), ('power', tensor([0.8426]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0222])), ('power', tensor([-22.9360]))])
epoch：136	 i:0 	 global-step:2720	 l-p:0.12445687502622604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.7634, 3.7690, 3.7639],
        [3.7634, 3.9742, 3.9317],
        [3.7634, 3.8494, 3.8030],
        [3.7634, 4.2380, 4.3459]], grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.12434069812297821 
model_pd.l_d.mean(): -18.153295516967773 
model_pd.lagr.mean(): -18.028955459594727 
model_pd.lambdas: dict_items([('pout', tensor([1.1475])), ('power', tensor([0.8414]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0215])), ('power', tensor([-22.9351]))])
epoch：137	 i:0 	 global-step:2740	 l-p:0.12434069812297821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.7654, 3.8025, 3.7754],
        [3.7654, 4.5724, 5.0074],
        [3.7654, 4.3287, 4.5126],
        [3.7654, 3.8033, 3.7758]], grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.12423071265220642 
model_pd.l_d.mean(): -18.125967025756836 
model_pd.lagr.mean(): -18.00173568725586 
model_pd.lambdas: dict_items([('pout', tensor([1.1486])), ('power', tensor([0.8403]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0209])), ('power', tensor([-22.9342]))])
epoch：138	 i:0 	 global-step:2760	 l-p:0.12423071265220642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[3.7672, 3.7675, 3.7673],
        [3.7672, 4.0985, 4.1072],
        [3.7672, 3.7728, 3.7677],
        [3.7672, 4.7342, 5.3603]], grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.12412652373313904 
model_pd.l_d.mean(): -18.098648071289062 
model_pd.lagr.mean(): -17.97452163696289 
model_pd.lambdas: dict_items([('pout', tensor([1.1496])), ('power', tensor([0.8391]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0203])), ('power', tensor([-22.9334]))])
epoch：139	 i:0 	 global-step:2780	 l-p:0.12412652373313904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[3.7690, 3.9794, 3.9368],
        [3.7690, 4.3811, 4.6106],
        [3.7690, 3.7721, 3.7692],
        [3.7690, 4.0308, 4.0055]], grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.12402708828449249 
model_pd.l_d.mean(): -18.071338653564453 
model_pd.lagr.mean(): -17.947311401367188 
model_pd.lambdas: dict_items([('pout', tensor([1.1506])), ('power', tensor([0.8380]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0197])), ('power', tensor([-22.9327]))])
epoch：140	 i:0 	 global-step:2800	 l-p:0.12402708828449249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.7708, 3.7824, 3.7723],
        [3.7708, 3.8091, 3.7814],
        [3.7708, 3.9623, 3.9154],
        [3.7708, 4.3870, 4.6206]], grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.12393084168434143 
model_pd.l_d.mean(): -18.044031143188477 
model_pd.lagr.mean(): -17.920101165771484 
model_pd.lambdas: dict_items([('pout', tensor([1.1516])), ('power', tensor([0.8368]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0192])), ('power', tensor([-22.9320]))])
epoch：141	 i:0 	 global-step:2820	 l-p:0.12393084168434143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[3.7725, 4.1032, 4.1116],
        [3.7725, 3.7924, 3.7762],
        [3.7725, 3.7727, 3.7725],
        [3.7725, 3.7768, 3.7728]], grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.12383594363927841 
model_pd.l_d.mean(): -18.0167293548584 
model_pd.lagr.mean(): -17.892892837524414 
model_pd.lambdas: dict_items([('pout', tensor([1.1526])), ('power', tensor([0.8357]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0186])), ('power', tensor([-22.9312]))])
epoch：142	 i:0 	 global-step:2840	 l-p:0.12383594363927841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[3.7743, 3.7798, 3.7748],
        [3.7743, 3.7967, 3.7787],
        [3.7743, 4.3902, 4.6233],
        [3.7743, 3.7774, 3.7745]], grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.12374047935009003 
model_pd.l_d.mean(): -17.989431381225586 
model_pd.lagr.mean(): -17.865690231323242 
model_pd.lambdas: dict_items([('pout', tensor([1.1537])), ('power', tensor([0.8345]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0181])), ('power', tensor([-22.9305]))])
epoch：143	 i:0 	 global-step:2860	 l-p:0.12374047935009003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[3.7761, 3.7765, 3.7761],
        [3.7761, 3.7761, 3.7761],
        [3.7761, 3.7761, 3.7761],
        [3.7761, 3.7772, 3.7761]], grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.12364271283149719 
model_pd.l_d.mean(): -17.962135314941406 
model_pd.lagr.mean(): -17.83849334716797 
model_pd.lambdas: dict_items([('pout', tensor([1.1547])), ('power', tensor([0.8334]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0175])), ('power', tensor([-22.9297]))])
epoch：144	 i:0 	 global-step:2880	 l-p:0.12364271283149719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[3.7779, 3.7779, 3.7779],
        [3.7779, 3.8148, 3.7879],
        [3.7779, 4.7447, 5.3693],
        [3.7779, 3.8764, 3.8275]], grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.12354138493537903 
model_pd.l_d.mean(): -17.934839248657227 
model_pd.lagr.mean(): -17.811298370361328 
model_pd.lambdas: dict_items([('pout', tensor([1.1557])), ('power', tensor([0.8323]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0169])), ('power', tensor([-22.9290]))])
epoch：145	 i:0 	 global-step:2900	 l-p:0.12354138493537903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.7799, 3.7854, 3.7804],
        [3.7799, 3.7915, 3.7814],
        [3.7799, 3.7997, 3.7835],
        [3.7799, 4.3417, 4.5242]], grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.12343577295541763 
model_pd.l_d.mean(): -17.907543182373047 
model_pd.lagr.mean(): -17.784107208251953 
model_pd.lambdas: dict_items([('pout', tensor([1.1567])), ('power', tensor([0.8311]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0163])), ('power', tensor([-22.9281]))])
epoch：146	 i:0 	 global-step:2920	 l-p:0.12343577295541763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.7819, 4.5880, 5.0210],
        [3.7819, 3.8198, 3.7923],
        [3.7819, 4.2820, 4.4106],
        [3.7819, 3.7821, 3.7819]], grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.12332579493522644 
model_pd.l_d.mean(): -17.8802433013916 
model_pd.lagr.mean(): -17.75691795349121 
model_pd.lambdas: dict_items([('pout', tensor([1.1577])), ('power', tensor([0.8300]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0156])), ('power', tensor([-22.9272]))])
epoch：147	 i:0 	 global-step:2940	 l-p:0.12332579493522644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[3.7840, 4.0443, 4.0186],
        [3.7840, 4.7507, 5.3748],
        [3.7840, 3.8553, 3.8133],
        [3.7840, 3.8885, 3.8386]], grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12321185320615768 
model_pd.l_d.mean(): -17.852943420410156 
model_pd.lagr.mean(): -17.729732513427734 
model_pd.lambdas: dict_items([('pout', tensor([1.1587])), ('power', tensor([0.8288]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0150])), ('power', tensor([-22.9263]))])
epoch：148	 i:0 	 global-step:2960	 l-p:0.12321185320615768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[3.7861, 3.7862, 3.7861],
        [3.7861, 3.7863, 3.7861],
        [3.7861, 3.7872, 3.7862],
        [3.7861, 3.8228, 3.7960]], grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.12309490144252777 
model_pd.l_d.mean(): -17.825645446777344 
model_pd.lagr.mean(): -17.702550888061523 
model_pd.lambdas: dict_items([('pout', tensor([1.1598])), ('power', tensor([0.8277]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0143])), ('power', tensor([-22.9254]))])
epoch：149	 i:0 	 global-step:2980	 l-p:0.12309490144252777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.7883, 4.1175, 4.1252],
        [3.7883, 4.2879, 4.4161],
        [3.7883, 3.7883, 3.7883],
        [3.7883, 4.4032, 4.6350]], grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.12297620624303818 
model_pd.l_d.mean(): -17.798349380493164 
model_pd.lagr.mean(): -17.675373077392578 
model_pd.lambdas: dict_items([('pout', tensor([1.1608])), ('power', tensor([0.8265]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0136])), ('power', tensor([-22.9244]))])
epoch：150	 i:0 	 global-step:3000	 l-p:0.12297620624303818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.7905, 3.7907, 3.7905],
        [3.7905, 3.7909, 3.7905],
        [3.7905, 3.8102, 3.7941],
        [3.7905, 3.8753, 3.8294]], grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.12285706400871277 
model_pd.l_d.mean(): -17.77105712890625 
model_pd.lagr.mean(): -17.64820098876953 
model_pd.lambdas: dict_items([('pout', tensor([1.1618])), ('power', tensor([0.8254]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0129])), ('power', tensor([-22.9234]))])
epoch：151	 i:0 	 global-step:3020	 l-p:0.12285706400871277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[3.7927, 4.8347, 5.5534],
        [3.7927, 4.1216, 4.1291],
        [3.7927, 4.1012, 4.0977],
        [3.7927, 3.9527, 3.9014]], grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.12273884564638138 
model_pd.l_d.mean(): -17.743764877319336 
model_pd.lagr.mean(): -17.62102508544922 
model_pd.lambdas: dict_items([('pout', tensor([1.1628])), ('power', tensor([0.8242]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0121])), ('power', tensor([-22.9224]))])
epoch：152	 i:0 	 global-step:3040	 l-p:0.12273884564638138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.7949, 3.7949, 3.7949],
        [3.7949, 3.7949, 3.7949],
        [3.7949, 4.2667, 4.3721],
        [3.7949, 3.8322, 3.8051]], grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.12262257933616638 
model_pd.l_d.mean(): -17.716482162475586 
model_pd.lagr.mean(): -17.59385871887207 
model_pd.lambdas: dict_items([('pout', tensor([1.1638])), ('power', tensor([0.8231]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0114])), ('power', tensor([-22.9214]))])
epoch：153	 i:0 	 global-step:3060	 l-p:0.12262257933616638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[3.7971, 4.6030, 5.0346],
        [3.7971, 3.8947, 3.8460],
        [3.7971, 3.7971, 3.7971],
        [3.7971, 4.7642, 5.3873]], grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.12250906974077225 
model_pd.l_d.mean(): -17.6892032623291 
model_pd.lagr.mean(): -17.566694259643555 
model_pd.lambdas: dict_items([('pout', tensor([1.1648])), ('power', tensor([0.8219]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0108])), ('power', tensor([-22.9205]))])
epoch：154	 i:0 	 global-step:3080	 l-p:0.12250906974077225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.7991, 4.4141, 4.6456],
        [3.7991, 3.8836, 3.8378],
        [3.7991, 4.1495, 4.1694],
        [3.7991, 3.8369, 3.8095]], grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.12240017205476761 
model_pd.l_d.mean(): -17.66193389892578 
model_pd.lagr.mean(): -17.539533615112305 
model_pd.lambdas: dict_items([('pout', tensor([1.1658])), ('power', tensor([0.8208]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0101])), ('power', tensor([-22.9196]))])
epoch：155	 i:0 	 global-step:3100	 l-p:0.12240017205476761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.8011, 4.1189, 4.1203],
        [3.8011, 4.2398, 4.3195],
        [3.8011, 3.8986, 3.8500],
        [3.8011, 3.8011, 3.8011]], grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.12229695171117783 
model_pd.l_d.mean(): -17.634666442871094 
model_pd.lagr.mean(): -17.51236915588379 
model_pd.lambdas: dict_items([('pout', tensor([1.1668])), ('power', tensor([0.8196]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0095])), ('power', tensor([-22.9187]))])
epoch：156	 i:0 	 global-step:3120	 l-p:0.12229695171117783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[3.8030, 3.8030, 3.8030],
        [3.8030, 4.4126, 4.6388],
        [3.8030, 4.6087, 5.0397],
        [3.8030, 3.9004, 3.8518]], grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.12219926714897156 
model_pd.l_d.mean(): -17.607412338256836 
model_pd.lagr.mean(): -17.485212326049805 
model_pd.lambdas: dict_items([('pout', tensor([1.1678])), ('power', tensor([0.8185]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0088])), ('power', tensor([-22.9179]))])
epoch：157	 i:0 	 global-step:3140	 l-p:0.12219926714897156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[3.8048, 3.8049, 3.8048],
        [3.8048, 3.8755, 3.8338],
        [3.8048, 3.8891, 3.8434],
        [3.8048, 3.8425, 3.8152]], grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.12210635840892792 
model_pd.l_d.mean(): -17.58016586303711 
model_pd.lagr.mean(): -17.458059310913086 
model_pd.lambdas: dict_items([('pout', tensor([1.1689])), ('power', tensor([0.8174]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0083])), ('power', tensor([-22.9171]))])
epoch：158	 i:0 	 global-step:3160	 l-p:0.12210635840892792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.8066, 3.8069, 3.8066],
        [3.8066, 4.0652, 4.0389],
        [3.8066, 4.2447, 4.3239],
        [3.8066, 3.8066, 3.8066]], grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.1220192164182663 
model_pd.l_d.mean(): -17.552921295166016 
model_pd.lagr.mean(): -17.4309024810791 
model_pd.lambdas: dict_items([('pout', tensor([1.1699])), ('power', tensor([0.8162]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0077])), ('power', tensor([-22.9163]))])
epoch：159	 i:0 	 global-step:3180	 l-p:0.1220192164182663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[3.8082, 4.1253, 4.1263],
        [3.8082, 3.8093, 3.8083],
        [3.8082, 3.9116, 3.8620],
        [3.8082, 4.6300, 5.0788]], grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.12193679064512253 
model_pd.l_d.mean(): -17.525686264038086 
model_pd.lagr.mean(): -17.403749465942383 
model_pd.lambdas: dict_items([('pout', tensor([1.1709])), ('power', tensor([0.8151]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0072])), ('power', tensor([-22.9156]))])
epoch：160	 i:0 	 global-step:3200	 l-p:0.12193679064512253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[3.8098, 4.7767, 5.3982],
        [3.8098, 3.8938, 3.8482],
        [3.8098, 3.8098, 3.8098],
        [3.8098, 3.8101, 3.8098]], grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.12185685336589813 
model_pd.l_d.mean(): -17.498449325561523 
model_pd.lagr.mean(): -17.3765926361084 
model_pd.lambdas: dict_items([('pout', tensor([1.1719])), ('power', tensor([0.8139]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0067])), ('power', tensor([-22.9150]))])
epoch：161	 i:0 	 global-step:3220	 l-p:0.12185685336589813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.8114, 4.1386, 4.1452],
        [3.8114, 3.9338, 3.8822],
        [3.8114, 3.8115, 3.8114],
        [3.8114, 3.8309, 3.8150]], grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.12177655845880508 
model_pd.l_d.mean(): -17.471220016479492 
model_pd.lagr.mean(): -17.349443435668945 
model_pd.lambdas: dict_items([('pout', tensor([1.1729])), ('power', tensor([0.8128]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0062])), ('power', tensor([-22.9143]))])
epoch：162	 i:0 	 global-step:3240	 l-p:0.12177655845880508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.8131, 3.8142, 3.8132],
        [3.8131, 3.8161, 3.8133],
        [3.8131, 4.0102, 3.9648],
        [3.8131, 3.8325, 3.8167]], grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.12169286608695984 
model_pd.l_d.mean(): -17.443988800048828 
model_pd.lagr.mean(): -17.322296142578125 
model_pd.lambdas: dict_items([('pout', tensor([1.1739])), ('power', tensor([0.8116]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0057])), ('power', tensor([-22.9135]))])
epoch：163	 i:0 	 global-step:3260	 l-p:0.12169286608695984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.8149, 3.8510, 3.8246],
        [3.8149, 4.0217, 3.9786],
        [3.8149, 3.9735, 3.9222],
        [3.8149, 4.1008, 4.0863]], grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.1216031163930893 
model_pd.l_d.mean(): -17.416751861572266 
model_pd.lagr.mean(): -17.295148849487305 
model_pd.lambdas: dict_items([('pout', tensor([1.1749])), ('power', tensor([0.8105]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0051])), ('power', tensor([-22.9128]))])
epoch：164	 i:0 	 global-step:3280	 l-p:0.1216031163930893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[3.8168, 4.7836, 5.4043],
        [3.8168, 4.4302, 4.6597],
        [3.8168, 3.8171, 3.8168],
        [3.8168, 4.8591, 5.5751]], grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.12150554358959198 
model_pd.l_d.mean(): -17.389511108398438 
model_pd.lagr.mean(): -17.26800537109375 
model_pd.lambdas: dict_items([('pout', tensor([1.1759])), ('power', tensor([0.8093]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0045])), ('power', tensor([-22.9119]))])
epoch：165	 i:0 	 global-step:3300	 l-p:0.12150554358959198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.8189, 4.4322, 4.6615],
        [3.8189, 3.8192, 3.8189],
        [3.8189, 3.8189, 3.8189],
        [3.8189, 3.8189, 3.8189]], grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.12139943242073059 
model_pd.l_d.mean(): -17.362260818481445 
model_pd.lagr.mean(): -17.240861892700195 
model_pd.lambdas: dict_items([('pout', tensor([1.1769])), ('power', tensor([0.8082]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0038])), ('power', tensor([-22.9109]))])
epoch：166	 i:0 	 global-step:3320	 l-p:0.12139943242073059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.8212, 3.8266, 3.8216],
        [3.8212, 3.8429, 3.8254],
        [3.8212, 4.6260, 5.0548],
        [3.8212, 3.8212, 3.8212]], grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.12128517031669617 
model_pd.l_d.mean(): -17.335010528564453 
model_pd.lagr.mean(): -17.213726043701172 
model_pd.lambdas: dict_items([('pout', tensor([1.1779])), ('power', tensor([0.8070]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0031])), ('power', tensor([-22.9099]))])
epoch：167	 i:0 	 global-step:3340	 l-p:0.12128517031669617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[3.8235, 4.0201, 3.9746],
        [3.8235, 3.8453, 3.8278],
        [3.8235, 3.8246, 3.8236],
        [3.8235, 4.1295, 4.1247]], grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.12116417288780212 
model_pd.l_d.mean(): -17.307756423950195 
model_pd.lagr.mean(): -17.18659210205078 
model_pd.lambdas: dict_items([('pout', tensor([1.1789])), ('power', tensor([0.8059]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0023])), ('power', tensor([-22.9087]))])
epoch：168	 i:0 	 global-step:3360	 l-p:0.12116417288780212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[3.8260, 3.8630, 3.8361],
        [3.8260, 4.2954, 4.3983],
        [3.8260, 3.8260, 3.8260],
        [3.8260, 3.8619, 3.8356]], grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.12103843688964844 
model_pd.l_d.mean(): -17.280500411987305 
model_pd.lagr.mean(): -17.159461975097656 
model_pd.lambdas: dict_items([('pout', tensor([1.1799])), ('power', tensor([0.8048]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0015])), ('power', tensor([-22.9076]))])
epoch：169	 i:0 	 global-step:3380	 l-p:0.12103843688964844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.8285, 3.9655, 3.9134],
        [3.8285, 3.8478, 3.8320],
        [3.8285, 3.8657, 3.8387],
        [3.8285, 4.3872, 4.5654]], grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.12091058492660522 
model_pd.l_d.mean(): -17.253244400024414 
model_pd.lagr.mean(): -17.132333755493164 
model_pd.lambdas: dict_items([('pout', tensor([1.1809])), ('power', tensor([0.8036]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0007])), ('power', tensor([-22.9063]))])
epoch：170	 i:0 	 global-step:3400	 l-p:0.12091058492660522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[3.8310, 4.0371, 3.9939],
        [3.8310, 3.9527, 3.9011],
        [3.8310, 4.0273, 3.9818],
        [3.8310, 3.8351, 3.8313]], grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.12078294903039932 
model_pd.l_d.mean(): -17.225996017456055 
model_pd.lagr.mean(): -17.105213165283203 
model_pd.lambdas: dict_items([('pout', tensor([1.1819])), ('power', tensor([0.8025]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9999])), ('power', tensor([-22.9051]))])
epoch：171	 i:0 	 global-step:3420	 l-p:0.12078294903039932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.8335, 3.8704, 3.8436],
        [3.8335, 3.8335, 3.8335],
        [3.8335, 4.0211, 3.9738],
        [3.8335, 4.3302, 4.4548]], grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.12065774202346802 
model_pd.l_d.mean(): -17.198749542236328 
model_pd.lagr.mean(): -17.078092575073242 
model_pd.lambdas: dict_items([('pout', tensor([1.1829])), ('power', tensor([0.8013]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9991])), ('power', tensor([-22.9039]))])
epoch：172	 i:0 	 global-step:3440	 l-p:0.12065774202346802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[3.8359, 3.8362, 3.8359],
        [3.8359, 3.8730, 3.8461],
        [3.8359, 3.8400, 3.8362],
        [3.8359, 3.8413, 3.8364]], grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.12053626775741577 
model_pd.l_d.mean(): -17.17151641845703 
model_pd.lagr.mean(): -17.050979614257812 
model_pd.lambdas: dict_items([('pout', tensor([1.1839])), ('power', tensor([0.8002]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9983])), ('power', tensor([-22.9028]))])
epoch：173	 i:0 	 global-step:3460	 l-p:0.12053626775741577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[3.8383, 3.8393, 3.8383],
        [3.8383, 4.0442, 4.0008],
        [3.8383, 4.8068, 5.4268],
        [3.8383, 4.8827, 5.5984]], grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.12041926383972168 
model_pd.l_d.mean(): -17.144287109375 
model_pd.lagr.mean(): -17.023868560791016 
model_pd.lambdas: dict_items([('pout', tensor([1.1849])), ('power', tensor([0.7990]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9975])), ('power', tensor([-22.9016]))])
epoch：174	 i:0 	 global-step:3480	 l-p:0.12041926383972168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.8406, 4.1459, 4.1406],
        [3.8406, 3.8406, 3.8406],
        [3.8406, 3.8776, 3.8507],
        [3.8406, 4.0365, 3.9909]], grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.12030644714832306 
model_pd.l_d.mean(): -17.117069244384766 
model_pd.lagr.mean(): -16.996763229370117 
model_pd.lambdas: dict_items([('pout', tensor([1.1859])), ('power', tensor([0.7979]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9968])), ('power', tensor([-22.9005]))])
epoch：175	 i:0 	 global-step:3500	 l-p:0.12030644714832306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[3.8428, 3.8439, 3.8429],
        [3.8428, 4.3118, 4.4138],
        [3.8428, 4.4554, 4.6826],
        [3.8428, 3.8429, 3.8428]], grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.12019703537225723 
model_pd.l_d.mean(): -17.089855194091797 
model_pd.lagr.mean(): -16.96965789794922 
model_pd.lambdas: dict_items([('pout', tensor([1.1869])), ('power', tensor([0.7967]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9961])), ('power', tensor([-22.8995]))])
epoch：176	 i:0 	 global-step:3520	 l-p:0.12019703537225723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[3.8450, 4.0025, 3.9511],
        [3.8450, 4.4035, 4.5807],
        [3.8450, 4.1601, 4.1598],
        [3.8450, 3.8450, 3.8450]], grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.12008953094482422 
model_pd.l_d.mean(): -17.062650680541992 
model_pd.lagr.mean(): -16.942562103271484 
model_pd.lambdas: dict_items([('pout', tensor([1.1879])), ('power', tensor([0.7956]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9954])), ('power', tensor([-22.8984]))])
epoch：177	 i:0 	 global-step:3540	 l-p:0.12008953094482422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.8473, 3.8840, 3.8573],
        [3.8473, 4.1946, 4.2124],
        [3.8473, 3.8514, 3.8475],
        [3.8473, 4.1727, 4.1779]], grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.11998238414525986 
model_pd.l_d.mean(): -17.035444259643555 
model_pd.lagr.mean(): -16.915462493896484 
model_pd.lambdas: dict_items([('pout', tensor([1.1889])), ('power', tensor([0.7944]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9946])), ('power', tensor([-22.8973]))])
epoch：178	 i:0 	 global-step:3560	 l-p:0.11998238414525986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[3.8495, 4.1968, 4.2144],
        [3.8495, 4.8949, 5.6102],
        [3.8495, 3.8495, 3.8495],
        [3.8495, 3.8495, 3.8495]], grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.11987395584583282 
model_pd.l_d.mean(): -17.00824546813965 
model_pd.lagr.mean(): -16.88837242126465 
model_pd.lambdas: dict_items([('pout', tensor([1.1899])), ('power', tensor([0.7933]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9939])), ('power', tensor([-22.8963]))])
epoch：179	 i:0 	 global-step:3580	 l-p:0.11987395584583282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.8518, 3.9880, 3.9359],
        [3.8518, 4.0090, 3.9576],
        [3.8518, 3.8571, 3.8523],
        [3.8518, 3.8518, 3.8518]], grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.11976295709609985 
model_pd.l_d.mean(): -16.98104476928711 
model_pd.lagr.mean(): -16.861282348632812 
model_pd.lambdas: dict_items([('pout', tensor([1.1909])), ('power', tensor([0.7922]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9932])), ('power', tensor([-22.8952]))])
epoch：180	 i:0 	 global-step:3600	 l-p:0.11976295709609985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.8542, 3.9751, 3.9236],
        [3.8542, 4.1103, 4.0829],
        [3.8542, 3.8733, 3.8577],
        [3.8542, 4.3227, 4.4241]], grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.1196485310792923 
model_pd.l_d.mean(): -16.953842163085938 
model_pd.lagr.mean(): -16.83419418334961 
model_pd.lambdas: dict_items([('pout', tensor([1.1919])), ('power', tensor([0.7910]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9924])), ('power', tensor([-22.8940]))])
epoch：181	 i:0 	 global-step:3620	 l-p:0.1196485310792923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[3.8567, 3.8757, 3.8601],
        [3.8567, 3.9394, 3.8942],
        [3.8567, 3.9260, 3.8848],
        [3.8567, 4.1127, 4.0852]], grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.11953046917915344 
model_pd.l_d.mean(): -16.92664337158203 
model_pd.lagr.mean(): -16.807113647460938 
model_pd.lambdas: dict_items([('pout', tensor([1.1929])), ('power', tensor([0.7899]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9916])), ('power', tensor([-22.8928]))])
epoch：182	 i:0 	 global-step:3640	 l-p:0.11953046917915344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[3.8592, 3.8645, 3.8596],
        [3.8592, 3.8592, 3.8592],
        [3.8592, 3.8592, 3.8592],
        [3.8592, 3.9548, 3.9067]], grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.1194089725613594 
model_pd.l_d.mean(): -16.899444580078125 
model_pd.lagr.mean(): -16.7800350189209 
model_pd.lambdas: dict_items([('pout', tensor([1.1938])), ('power', tensor([0.7887]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9908])), ('power', tensor([-22.8915]))])
epoch：183	 i:0 	 global-step:3660	 l-p:0.1194089725613594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[3.8618, 4.1176, 4.0901],
        [3.8618, 4.1867, 4.1914],
        [3.8618, 3.8984, 3.8718],
        [3.8618, 3.8659, 3.8621]], grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.11928476393222809 
model_pd.l_d.mean(): -16.87224769592285 
model_pd.lagr.mean(): -16.752962112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.1948])), ('power', tensor([0.7876]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9900])), ('power', tensor([-22.8902]))])
epoch：184	 i:0 	 global-step:3680	 l-p:0.11928476393222809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[3.8644, 3.8647, 3.8644],
        [3.8644, 4.6710, 5.0978],
        [3.8644, 4.0003, 3.9482],
        [3.8644, 4.0693, 4.0256]], grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.11915908008813858 
model_pd.l_d.mean(): -16.84505271911621 
model_pd.lagr.mean(): -16.725893020629883 
model_pd.lambdas: dict_items([('pout', tensor([1.1958])), ('power', tensor([0.7864]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9891])), ('power', tensor([-22.8889]))])
epoch：185	 i:0 	 global-step:3700	 l-p:0.11915908008813858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.8671, 4.3630, 4.4858],
        [3.8671, 3.9038, 3.8771],
        [3.8671, 4.0535, 4.0059],
        [3.8671, 4.0620, 4.0161]], grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.11903415620326996 
model_pd.l_d.mean(): -16.817861557006836 
model_pd.lagr.mean(): -16.698827743530273 
model_pd.lambdas: dict_items([('pout', tensor([1.1968])), ('power', tensor([0.7853]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9882])), ('power', tensor([-22.8876]))])
epoch：186	 i:0 	 global-step:3720	 l-p:0.11903415620326996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[3.8696, 3.8697, 3.8696],
        [3.8696, 4.4777, 4.6992],
        [3.8696, 4.1252, 4.0975],
        [3.8696, 4.1738, 4.1676]], grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.11891648173332214 
model_pd.l_d.mean(): -16.790678024291992 
model_pd.lagr.mean(): -16.671762466430664 
model_pd.lambdas: dict_items([('pout', tensor([1.1978])), ('power', tensor([0.7841]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9874])), ('power', tensor([-22.8864]))])
epoch：187	 i:0 	 global-step:3740	 l-p:0.11891648173332214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[3.8719, 3.8719, 3.8719],
        [3.8719, 4.3402, 4.4408],
        [3.8719, 3.9086, 3.8819],
        [3.8719, 4.2185, 4.2353]], grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.11880878359079361 
model_pd.l_d.mean(): -16.763511657714844 
model_pd.lagr.mean(): -16.644702911376953 
model_pd.lambdas: dict_items([('pout', tensor([1.1988])), ('power', tensor([0.7830]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9867])), ('power', tensor([-22.8852]))])
epoch：188	 i:0 	 global-step:3760	 l-p:0.11880878359079361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.8741, 4.8455, 5.4648],
        [3.8741, 3.8742, 3.8741],
        [3.8741, 3.9107, 3.8841],
        [3.8741, 3.8930, 3.8775]], grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.11871073395013809 
model_pd.l_d.mean(): -16.736351013183594 
model_pd.lagr.mean(): -16.617639541625977 
model_pd.lambdas: dict_items([('pout', tensor([1.1998])), ('power', tensor([0.7819]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9860])), ('power', tensor([-22.8842]))])
epoch：189	 i:0 	 global-step:3780	 l-p:0.11871073395013809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.8761, 4.3113, 4.3867],
        [3.8761, 3.8974, 3.8802],
        [3.8761, 4.4341, 4.6096],
        [3.8761, 4.4885, 4.7138]], grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.11861980706453323 
model_pd.l_d.mean(): -16.70920753479004 
model_pd.lagr.mean(): -16.590587615966797 
model_pd.lambdas: dict_items([('pout', tensor([1.2008])), ('power', tensor([0.7807]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9854])), ('power', tensor([-22.8832]))])
epoch：190	 i:0 	 global-step:3800	 l-p:0.11861980706453323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[3.8780, 4.0823, 4.0385],
        [3.8780, 3.8780, 3.8780],
        [3.8780, 3.8821, 3.8783],
        [3.8780, 4.1918, 4.1904]], grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.11853185296058655 
model_pd.l_d.mean(): -16.68206787109375 
model_pd.lagr.mean(): -16.563535690307617 
model_pd.lambdas: dict_items([('pout', tensor([1.2017])), ('power', tensor([0.7796]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9848])), ('power', tensor([-22.8823]))])
epoch：191	 i:0 	 global-step:3820	 l-p:0.11853185296058655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[3.8800, 4.0154, 3.9633],
        [3.8800, 3.8800, 3.8800],
        [3.8800, 3.8802, 3.8800],
        [3.8800, 3.9164, 3.8899]], grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.11844219267368317 
model_pd.l_d.mean(): -16.654932022094727 
model_pd.lagr.mean(): -16.536489486694336 
model_pd.lambdas: dict_items([('pout', tensor([1.2027])), ('power', tensor([0.7784]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9841])), ('power', tensor([-22.8813]))])
epoch：192	 i:0 	 global-step:3840	 l-p:0.11844219267368317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[3.8821, 3.8823, 3.8821],
        [3.8821, 3.9181, 3.8919],
        [3.8821, 4.0022, 3.9508],
        [3.8821, 3.8821, 3.8821]], grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.1183464452624321 
model_pd.l_d.mean(): -16.627790451049805 
model_pd.lagr.mean(): -16.509443283081055 
model_pd.lambdas: dict_items([('pout', tensor([1.2037])), ('power', tensor([0.7773]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9834])), ('power', tensor([-22.8803]))])
epoch：193	 i:0 	 global-step:3860	 l-p:0.1183464452624321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[3.8844, 4.1674, 4.1510],
        [3.8844, 3.8848, 3.8844],
        [3.8844, 3.9665, 3.9216],
        [3.8844, 4.2304, 4.2467]], grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.1182415708899498 
model_pd.l_d.mean(): -16.600645065307617 
model_pd.lagr.mean(): -16.482402801513672 
model_pd.lambdas: dict_items([('pout', tensor([1.2047])), ('power', tensor([0.7761]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9827])), ('power', tensor([-22.8791]))])
epoch：194	 i:0 	 global-step:3880	 l-p:0.1182415708899498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[3.8870, 4.6943, 5.1201],
        [3.8870, 3.9557, 3.9148],
        [3.8870, 3.8870, 3.8870],
        [3.8870, 3.8872, 3.8870]], grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.11812610924243927 
model_pd.l_d.mean(): -16.573495864868164 
model_pd.lagr.mean(): -16.45536994934082 
model_pd.lambdas: dict_items([('pout', tensor([1.2057])), ('power', tensor([0.7750]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9819])), ('power', tensor([-22.8779]))])
epoch：195	 i:0 	 global-step:3900	 l-p:0.11812610924243927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.8897, 3.9257, 3.8994],
        [3.8897, 3.8898, 3.8897],
        [3.8897, 4.7534, 5.2425],
        [3.8897, 4.1726, 4.1561]], grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.11800041049718857 
model_pd.l_d.mean(): -16.546337127685547 
model_pd.lagr.mean(): -16.42833709716797 
model_pd.lambdas: dict_items([('pout', tensor([1.2067])), ('power', tensor([0.7739]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9810])), ('power', tensor([-22.8764]))])
epoch：196	 i:0 	 global-step:3920	 l-p:0.11800041049718857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.8927, 3.9289, 3.9025],
        [3.8927, 4.8658, 5.4847],
        [3.8927, 4.7566, 5.2458],
        [3.8927, 4.5052, 4.7298]], grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.11786644160747528 
model_pd.l_d.mean(): -16.519176483154297 
model_pd.lagr.mean(): -16.401309967041016 
model_pd.lambdas: dict_items([('pout', tensor([1.2076])), ('power', tensor([0.7727]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9801])), ('power', tensor([-22.8749]))])
epoch：197	 i:0 	 global-step:3940	 l-p:0.11786644160747528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[3.8957, 4.8693, 5.4884],
        [3.8957, 4.0996, 4.0555],
        [3.8957, 4.7039, 5.1297],
        [3.8957, 4.5039, 4.7242]], grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.117727130651474 
model_pd.l_d.mean(): -16.492015838623047 
model_pd.lagr.mean(): -16.37428855895996 
model_pd.lambdas: dict_items([('pout', tensor([1.2086])), ('power', tensor([0.7716]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9791])), ('power', tensor([-22.8733]))])
epoch：198	 i:0 	 global-step:3960	 l-p:0.117727130651474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.8988, 4.5124, 4.7375],
        [3.8988, 3.9352, 3.9087],
        [3.8988, 3.8990, 3.8988],
        [3.8988, 4.5117, 4.7362]], grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.11758603155612946 
model_pd.l_d.mean(): -16.464855194091797 
model_pd.lagr.mean(): -16.34726905822754 
model_pd.lambdas: dict_items([('pout', tensor([1.2096])), ('power', tensor([0.7704]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9781])), ('power', tensor([-22.8717]))])
epoch：199	 i:0 	 global-step:3980	 l-p:0.11758603155612946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[3.9019, 4.9538, 5.6697],
        [3.9019, 4.5105, 4.7307],
        [3.9019, 4.5150, 4.7394],
        [3.9019, 3.9838, 3.9389]], grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.1174464002251625 
model_pd.l_d.mean(): -16.437702178955078 
model_pd.lagr.mean(): -16.320255279541016 
model_pd.lambdas: dict_items([('pout', tensor([1.2106])), ('power', tensor([0.7693]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9770])), ('power', tensor([-22.8700]))])
epoch：200	 i:0 	 global-step:4000	 l-p:0.1174464002251625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[3.9050, 3.9408, 3.9146],
        [3.9050, 4.0058, 3.9568],
        [3.9050, 3.9050, 3.9050],
        [3.9050, 4.0904, 4.0425]], grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.11731099337339401 
model_pd.l_d.mean(): -16.410558700561523 
model_pd.lagr.mean(): -16.29324722290039 
model_pd.lambdas: dict_items([('pout', tensor([1.2115])), ('power', tensor([0.7681]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9761])), ('power', tensor([-22.8685]))])
epoch：201	 i:0 	 global-step:4020	 l-p:0.11731099337339401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[3.9079, 4.2539, 4.2695],
        [3.9079, 3.9079, 3.9079],
        [3.9079, 3.9131, 3.9083],
        [3.9079, 3.9291, 3.9120]], grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.11718139052391052 
model_pd.l_d.mean(): -16.3834228515625 
model_pd.lagr.mean(): -16.2662410736084 
model_pd.lambdas: dict_items([('pout', tensor([1.2125])), ('power', tensor([0.7670]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9751])), ('power', tensor([-22.8669]))])
epoch：202	 i:0 	 global-step:4040	 l-p:0.11718139052391052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.9107, 3.9457, 3.9200],
        [3.9107, 3.9110, 3.9107],
        [3.9107, 3.9295, 3.9141],
        [3.9107, 4.4695, 4.6437]], grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.11705794185400009 
model_pd.l_d.mean(): -16.356300354003906 
model_pd.lagr.mean(): -16.239242553710938 
model_pd.lambdas: dict_items([('pout', tensor([1.2135])), ('power', tensor([0.7658]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9742])), ('power', tensor([-22.8655]))])
epoch：203	 i:0 	 global-step:4060	 l-p:0.11705794185400009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.9134, 4.5277, 4.7525],
        [3.9134, 4.1073, 4.0609],
        [3.9134, 3.9186, 3.9138],
        [3.9134, 3.9134, 3.9134]], grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.1169399842619896 
model_pd.l_d.mean(): -16.32918357849121 
model_pd.lagr.mean(): -16.212244033813477 
model_pd.lambdas: dict_items([('pout', tensor([1.2145])), ('power', tensor([0.7647]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9734])), ('power', tensor([-22.8641]))])
epoch：204	 i:0 	 global-step:4080	 l-p:0.1169399842619896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[3.9160, 4.3843, 4.4831],
        [3.9160, 4.0167, 3.9677],
        [3.9160, 4.5297, 4.7538],
        [3.9160, 4.1013, 4.0532]], grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.11682584136724472 
model_pd.l_d.mean(): -16.302078247070312 
model_pd.lagr.mean(): -16.185253143310547 
model_pd.lambdas: dict_items([('pout', tensor([1.2154])), ('power', tensor([0.7636]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9725])), ('power', tensor([-22.8627]))])
epoch：205	 i:0 	 global-step:4100	 l-p:0.11682584136724472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.9186, 3.9186, 3.9186],
        [3.9186, 3.9187, 3.9186],
        [3.9186, 4.2219, 4.2144],
        [3.9186, 4.0742, 4.0226]], grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.11671341210603714 
model_pd.l_d.mean(): -16.274978637695312 
model_pd.lagr.mean(): -16.158266067504883 
model_pd.lambdas: dict_items([('pout', tensor([1.2164])), ('power', tensor([0.7624]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9717])), ('power', tensor([-22.8613]))])
epoch：206	 i:0 	 global-step:4120	 l-p:0.11671341210603714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[3.9212, 3.9212, 3.9212],
        [3.9212, 4.5305, 4.7502],
        [3.9212, 4.7486, 5.1933],
        [3.9212, 4.2345, 4.2319]], grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.11660286784172058 
model_pd.l_d.mean(): -16.247879028320312 
model_pd.lagr.mean(): -16.131277084350586 
model_pd.lambdas: dict_items([('pout', tensor([1.2174])), ('power', tensor([0.7613]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9709])), ('power', tensor([-22.8600]))])
epoch：207	 i:0 	 global-step:4140	 l-p:0.11660286784172058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.9237, 3.9266, 3.9238],
        [3.9237, 4.0792, 4.0276],
        [3.9237, 3.9247, 3.9237],
        [3.9237, 4.4199, 4.5405]], grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.11649517714977264 
model_pd.l_d.mean(): -16.22078514099121 
model_pd.lagr.mean(): -16.104290008544922 
model_pd.lambdas: dict_items([('pout', tensor([1.2184])), ('power', tensor([0.7601]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9701])), ('power', tensor([-22.8587]))])
epoch：208	 i:0 	 global-step:4160	 l-p:0.11649517714977264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[3.9261, 3.9263, 3.9261],
        [3.9261, 4.0457, 3.9942],
        [3.9261, 4.3615, 4.4351],
        [3.9261, 3.9262, 3.9261]], grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.11638940870761871 
model_pd.l_d.mean(): -16.193693161010742 
model_pd.lagr.mean(): -16.077302932739258 
model_pd.lambdas: dict_items([('pout', tensor([1.2193])), ('power', tensor([0.7590]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9693])), ('power', tensor([-22.8574]))])
epoch：209	 i:0 	 global-step:4180	 l-p:0.11638940870761871
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[3.9286, 3.9395, 3.9300],
        [3.9286, 3.9286, 3.9286],
        [3.9286, 3.9338, 3.9291],
        [3.9286, 4.7568, 5.2014]], grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.11628368496894836 
model_pd.l_d.mean(): -16.166603088378906 
model_pd.lagr.mean(): -16.05031967163086 
model_pd.lambdas: dict_items([('pout', tensor([1.2203])), ('power', tensor([0.7578]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9685])), ('power', tensor([-22.8561]))])
epoch：210	 i:0 	 global-step:4200	 l-p:0.11628368496894836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.9311, 4.2138, 4.1965],
        [3.9311, 3.9321, 3.9312],
        [3.9311, 3.9311, 3.9311],
        [3.9311, 3.9311, 3.9311]], grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.11617603898048401 
model_pd.l_d.mean(): -16.139514923095703 
model_pd.lagr.mean(): -16.023338317871094 
model_pd.lambdas: dict_items([('pout', tensor([1.2213])), ('power', tensor([0.7567]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9677])), ('power', tensor([-22.8547]))])
epoch：211	 i:0 	 global-step:4220	 l-p:0.11617603898048401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.9337, 4.7625, 5.2072],
        [3.9337, 3.9525, 3.9371],
        [3.9337, 3.9337, 3.9337],
        [3.9337, 4.0154, 3.9705]], grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.1160648763179779 
model_pd.l_d.mean(): -16.1124267578125 
model_pd.lagr.mean(): -15.99636173248291 
model_pd.lambdas: dict_items([('pout', tensor([1.2222])), ('power', tensor([0.7556]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9668])), ('power', tensor([-22.8533]))])
epoch：212	 i:0 	 global-step:4240	 l-p:0.1160648763179779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.9365, 3.9721, 3.9461],
        [3.9365, 4.5467, 4.7661],
        [3.9365, 3.9405, 3.9367],
        [3.9365, 3.9365, 3.9365]], grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.115949347615242 
model_pd.l_d.mean(): -16.085342407226562 
model_pd.lagr.mean(): -15.969392776489258 
model_pd.lambdas: dict_items([('pout', tensor([1.2232])), ('power', tensor([0.7544]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9659])), ('power', tensor([-22.8518]))])
epoch：213	 i:0 	 global-step:4260	 l-p:0.115949347615242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[3.9393, 4.1939, 4.1648],
        [3.9393, 3.9750, 3.9489],
        [3.9393, 4.7688, 5.2136],
        [3.9393, 4.2528, 4.2499]], grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.11582943797111511 
model_pd.l_d.mean(): -16.05825424194336 
model_pd.lagr.mean(): -15.942424774169922 
model_pd.lambdas: dict_items([('pout', tensor([1.2242])), ('power', tensor([0.7533]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9650])), ('power', tensor([-22.8503]))])
epoch：214	 i:0 	 global-step:4280	 l-p:0.11582943797111511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[3.9422, 4.3781, 4.4514],
        [3.9422, 3.9422, 3.9422],
        [3.9422, 4.5575, 4.7811],
        [3.9422, 3.9782, 3.9519]], grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.11570589989423752 
model_pd.l_d.mean(): -16.031171798706055 
model_pd.lagr.mean(): -15.91546630859375 
model_pd.lambdas: dict_items([('pout', tensor([1.2251])), ('power', tensor([0.7521]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9641])), ('power', tensor([-22.8487]))])
epoch：215	 i:0 	 global-step:4300	 l-p:0.11570589989423752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[3.9452, 3.9492, 3.9455],
        [3.9452, 3.9455, 3.9452],
        [3.9452, 3.9452, 3.9452],
        [3.9452, 4.1999, 4.1707]], grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.11558007448911667 
model_pd.l_d.mean(): -16.004091262817383 
model_pd.lagr.mean(): -15.888511657714844 
model_pd.lambdas: dict_items([('pout', tensor([1.2261])), ('power', tensor([0.7510]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9631])), ('power', tensor([-22.8471]))])
epoch：216	 i:0 	 global-step:4320	 l-p:0.11558007448911667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[3.9482, 3.9484, 3.9482],
        [3.9482, 4.5594, 4.7788],
        [3.9482, 3.9693, 3.9523],
        [3.9482, 4.4456, 4.5657]], grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.11545361578464508 
model_pd.l_d.mean(): -15.977012634277344 
model_pd.lagr.mean(): -15.86155891418457 
model_pd.lambdas: dict_items([('pout', tensor([1.2270])), ('power', tensor([0.7498]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9621])), ('power', tensor([-22.8454]))])
epoch：217	 i:0 	 global-step:4340	 l-p:0.11545361578464508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.9512, 4.3875, 4.4607],
        [3.9512, 3.9512, 3.9512],
        [3.9512, 3.9621, 3.9526],
        [3.9512, 4.4208, 4.5188]], grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.11532800644636154 
model_pd.l_d.mean(): -15.94994068145752 
model_pd.lagr.mean(): -15.834612846374512 
model_pd.lambdas: dict_items([('pout', tensor([1.2280])), ('power', tensor([0.7487]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9612])), ('power', tensor([-22.8438]))])
epoch：218	 i:0 	 global-step:4360	 l-p:0.11532800644636154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[3.9542, 3.9543, 3.9542],
        [3.9542, 4.2680, 4.2649],
        [3.9542, 3.9542, 3.9542],
        [3.9542, 4.4239, 4.5219]], grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.11520443111658096 
model_pd.l_d.mean(): -15.922880172729492 
model_pd.lagr.mean(): -15.8076753616333 
model_pd.lambdas: dict_items([('pout', tensor([1.2290])), ('power', tensor([0.7476]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9602])), ('power', tensor([-22.8421]))])
epoch：219	 i:0 	 global-step:4380	 l-p:0.11520443111658096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[3.9571, 4.2610, 4.2527],
        [3.9571, 4.2120, 4.1826],
        [3.9571, 4.1424, 4.0939],
        [3.9571, 3.9571, 3.9571]], grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.11508359760046005 
model_pd.l_d.mean(): -15.89582633972168 
model_pd.lagr.mean(): -15.780742645263672 
model_pd.lambdas: dict_items([('pout', tensor([1.2299])), ('power', tensor([0.7464]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9593])), ('power', tensor([-22.8405]))])
epoch：220	 i:0 	 global-step:4400	 l-p:0.11508359760046005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[3.9600, 4.7924, 5.2380],
        [3.9600, 4.1453, 4.0967],
        [3.9600, 3.9961, 3.9698],
        [3.9600, 4.0415, 3.9966]], grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.11496570706367493 
model_pd.l_d.mean(): -15.868776321411133 
model_pd.lagr.mean(): -15.75381088256836 
model_pd.lambdas: dict_items([('pout', tensor([1.2309])), ('power', tensor([0.7453]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9583])), ('power', tensor([-22.8389]))])
epoch：221	 i:0 	 global-step:4420	 l-p:0.11496570706367493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[3.9628, 4.4330, 4.5308],
        [3.9628, 4.0975, 4.0450],
        [3.9628, 4.5753, 4.7945],
        [3.9628, 4.5805, 4.8047]], grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.11485027521848679 
model_pd.l_d.mean(): -15.841737747192383 
model_pd.lagr.mean(): -15.726887702941895 
model_pd.lambdas: dict_items([('pout', tensor([1.2318])), ('power', tensor([0.7441]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9574])), ('power', tensor([-22.8374]))])
epoch：222	 i:0 	 global-step:4440	 l-p:0.11485027521848679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[3.9656, 4.0339, 3.9930],
        [3.9656, 3.9656, 3.9656],
        [3.9656, 3.9708, 3.9661],
        [3.9656, 4.8389, 5.3299]], grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.11473646759986877 
model_pd.l_d.mean(): -15.814706802368164 
model_pd.lagr.mean(): -15.699970245361328 
model_pd.lambdas: dict_items([('pout', tensor([1.2328])), ('power', tensor([0.7430]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9565])), ('power', tensor([-22.8359]))])
epoch：223	 i:0 	 global-step:4460	 l-p:0.11473646759986877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[3.9684, 4.0040, 3.9780],
        [3.9684, 3.9871, 3.9718],
        [3.9684, 4.0044, 3.9781],
        [3.9684, 4.9536, 5.5762]], grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.11462312936782837 
model_pd.l_d.mean(): -15.787679672241211 
model_pd.lagr.mean(): -15.673056602478027 
model_pd.lambdas: dict_items([('pout', tensor([1.2338])), ('power', tensor([0.7419]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9556])), ('power', tensor([-22.8343]))])
epoch：224	 i:0 	 global-step:4480	 l-p:0.11462312936782837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[3.9713, 3.9923, 3.9753],
        [3.9713, 4.2263, 4.1967],
        [3.9713, 4.5844, 4.8036],
        [3.9713, 4.1750, 4.1300]], grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.11450900882482529 
model_pd.l_d.mean(): -15.76065731048584 
model_pd.lagr.mean(): -15.646148681640625 
model_pd.lambdas: dict_items([('pout', tensor([1.2347])), ('power', tensor([0.7407]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9547])), ('power', tensor([-22.8327]))])
epoch：225	 i:0 	 global-step:4500	 l-p:0.11450900882482529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[3.9741, 4.0685, 4.0205],
        [3.9741, 4.0556, 4.0107],
        [3.9741, 3.9952, 3.9782],
        [3.9741, 4.1297, 4.0776]], grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.11439309269189835 
model_pd.l_d.mean(): -15.733638763427734 
model_pd.lagr.mean(): -15.619245529174805 
model_pd.lambdas: dict_items([('pout', tensor([1.2357])), ('power', tensor([0.7396]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9538])), ('power', tensor([-22.8311]))])
epoch：226	 i:0 	 global-step:4520	 l-p:0.11439309269189835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.9771, 3.9823, 3.9775],
        [3.9771, 4.5907, 4.8099],
        [3.9771, 3.9880, 3.9785],
        [3.9771, 4.3020, 4.3039]], grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.11427456140518188 
model_pd.l_d.mean(): -15.706624031066895 
model_pd.lagr.mean(): -15.5923490524292 
model_pd.lambdas: dict_items([('pout', tensor([1.2366])), ('power', tensor([0.7384]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9529])), ('power', tensor([-22.8295]))])
epoch：227	 i:0 	 global-step:4540	 l-p:0.11427456140518188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[3.9802, 4.0012, 3.9842],
        [3.9802, 3.9910, 3.9816],
        [3.9802, 4.9674, 5.5909],
        [3.9802, 3.9988, 3.9835]], grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.11415304243564606 
model_pd.l_d.mean(): -15.679609298706055 
model_pd.lagr.mean(): -15.56545639038086 
model_pd.lambdas: dict_items([('pout', tensor([1.2376])), ('power', tensor([0.7373]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9519])), ('power', tensor([-22.8278]))])
epoch：228	 i:0 	 global-step:4560	 l-p:0.11415304243564606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[3.9833, 4.0192, 3.9930],
        [3.9833, 3.9833, 3.9833],
        [3.9833, 4.0189, 3.9928],
        [3.9833, 4.6020, 4.8255]], grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.11402866244316101 
model_pd.l_d.mean(): -15.652597427368164 
model_pd.lagr.mean(): -15.538568496704102 
model_pd.lambdas: dict_items([('pout', tensor([1.2385])), ('power', tensor([0.7361]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9509])), ('power', tensor([-22.8261]))])
epoch：229	 i:0 	 global-step:4580	 l-p:0.11402866244316101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[3.9865, 4.6055, 4.8290],
        [3.9865, 4.4579, 4.5554],
        [3.9865, 4.2418, 4.2119],
        [3.9865, 3.9875, 3.9865]], grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.11390222609043121 
model_pd.l_d.mean(): -15.625594139099121 
model_pd.lagr.mean(): -15.51169204711914 
model_pd.lambdas: dict_items([('pout', tensor([1.2395])), ('power', tensor([0.7350]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9498])), ('power', tensor([-22.8243]))])
epoch：230	 i:0 	 global-step:4600	 l-p:0.11390222609043121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[3.9897, 4.0579, 4.0171],
        [3.9897, 4.6098, 4.8340],
        [3.9897, 4.3150, 4.3167],
        [3.9897, 4.0253, 3.9992]], grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.11377454549074173 
model_pd.l_d.mean(): -15.598587036132812 
model_pd.lagr.mean(): -15.48481273651123 
model_pd.lambdas: dict_items([('pout', tensor([1.2404])), ('power', tensor([0.7339]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9488])), ('power', tensor([-22.8224]))])
epoch：231	 i:0 	 global-step:4620	 l-p:0.11377454549074173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[3.9930, 3.9930, 3.9930],
        [3.9930, 3.9930, 3.9930],
        [3.9930, 4.1487, 4.0964],
        [3.9930, 4.5569, 4.7301]], grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.11364656686782837 
model_pd.l_d.mean(): -15.571592330932617 
model_pd.lagr.mean(): -15.457945823669434 
model_pd.lambdas: dict_items([('pout', tensor([1.2414])), ('power', tensor([0.7327]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9478])), ('power', tensor([-22.8206]))])
epoch：232	 i:0 	 global-step:4640	 l-p:0.11364656686782837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[3.9962, 4.6116, 4.8309],
        [3.9962, 4.0906, 4.0425],
        [3.9962, 4.1157, 4.0639],
        [3.9962, 3.9962, 3.9962]], grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.1135191023349762 
model_pd.l_d.mean(): -15.544602394104004 
model_pd.lagr.mean(): -15.431083679199219 
model_pd.lambdas: dict_items([('pout', tensor([1.2423])), ('power', tensor([0.7316]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9467])), ('power', tensor([-22.8188]))])
epoch：233	 i:0 	 global-step:4660	 l-p:0.1135191023349762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[3.9995, 4.1000, 4.0507],
        [3.9995, 4.0343, 4.0087],
        [3.9995, 4.0677, 4.0268],
        [3.9995, 3.9997, 3.9995]], grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.11339272558689117 
model_pd.l_d.mean(): -15.517617225646973 
model_pd.lagr.mean(): -15.404224395751953 
model_pd.lambdas: dict_items([('pout', tensor([1.2433])), ('power', tensor([0.7304]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9457])), ('power', tensor([-22.8169]))])
epoch：234	 i:0 	 global-step:4680	 l-p:0.11339272558689117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.0027, 4.1883, 4.1393],
        [4.0027, 4.1033, 4.0539],
        [4.0027, 4.5034, 4.6229],
        [4.0027, 4.0028, 4.0027]], grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.11326780170202255 
model_pd.l_d.mean(): -15.490642547607422 
model_pd.lagr.mean(): -15.377374649047852 
model_pd.lambdas: dict_items([('pout', tensor([1.2442])), ('power', tensor([0.7293]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9446])), ('power', tensor([-22.8151]))])
epoch：235	 i:0 	 global-step:4700	 l-p:0.11326780170202255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.0059, 4.1065, 4.0571],
        [4.0059, 4.0061, 4.0059],
        [4.0059, 4.0246, 4.0093],
        [4.0059, 4.0741, 4.0333]], grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.11314432322978973 
model_pd.l_d.mean(): -15.463672637939453 
model_pd.lagr.mean(): -15.350528717041016 
model_pd.lambdas: dict_items([('pout', tensor([1.2451])), ('power', tensor([0.7282]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9436])), ('power', tensor([-22.8133]))])
epoch：236	 i:0 	 global-step:4720	 l-p:0.11314432322978973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.0091, 4.0907, 4.0456],
        [4.0091, 4.4820, 4.5792],
        [4.0091, 5.0021, 5.6280],
        [4.0091, 4.0200, 4.0105]], grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.11302205175161362 
model_pd.l_d.mean(): -15.436710357666016 
model_pd.lagr.mean(): -15.323688507080078 
model_pd.lambdas: dict_items([('pout', tensor([1.2461])), ('power', tensor([0.7270]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9426])), ('power', tensor([-22.8115]))])
epoch：237	 i:0 	 global-step:4740	 l-p:0.11302205175161362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[4.0123, 5.0059, 5.6321],
        [4.0123, 4.0123, 4.0123],
        [4.0123, 4.0479, 4.0218],
        [4.0123, 4.0163, 4.0126]], grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.11290057003498077 
model_pd.l_d.mean(): -15.409754753112793 
model_pd.lagr.mean(): -15.296854019165039 
model_pd.lambdas: dict_items([('pout', tensor([1.2470])), ('power', tensor([0.7259]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9416])), ('power', tensor([-22.8096]))])
epoch：238	 i:0 	 global-step:4760	 l-p:0.11290057003498077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.0155, 4.4887, 4.5860],
        [4.0155, 4.2999, 4.2811],
        [4.0155, 4.0165, 4.0156],
        [4.0155, 4.0503, 4.0247]], grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.11277925223112106 
model_pd.l_d.mean(): -15.382804870605469 
model_pd.lagr.mean(): -15.270025253295898 
model_pd.lambdas: dict_items([('pout', tensor([1.2480])), ('power', tensor([0.7247]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9406])), ('power', tensor([-22.8078]))])
epoch：239	 i:0 	 global-step:4780	 l-p:0.11277925223112106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[4.0188, 4.0535, 4.0279],
        [4.0188, 4.0188, 4.0188],
        [4.0188, 4.0188, 4.0188],
        [4.0188, 4.1746, 4.1221]], grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.1126575767993927 
model_pd.l_d.mean(): -15.355863571166992 
model_pd.lagr.mean(): -15.243206024169922 
model_pd.lambdas: dict_items([('pout', tensor([1.2489])), ('power', tensor([0.7236]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9395])), ('power', tensor([-22.8060]))])
epoch：240	 i:0 	 global-step:4800	 l-p:0.1126575767993927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.0220, 4.9043, 5.3983],
        [4.0220, 4.1779, 4.1254],
        [4.0220, 4.5883, 4.7615],
        [4.0220, 4.0220, 4.0220]], grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.1125350371003151 
model_pd.l_d.mean(): -15.328923225402832 
model_pd.lagr.mean(): -15.216387748718262 
model_pd.lambdas: dict_items([('pout', tensor([1.2499])), ('power', tensor([0.7225]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9385])), ('power', tensor([-22.8041]))])
epoch：241	 i:0 	 global-step:4820	 l-p:0.1125350371003151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[4.0253, 4.5919, 4.7651],
        [4.0253, 4.2198, 4.1721],
        [4.0253, 4.4991, 4.5963],
        [4.0253, 4.5275, 4.6469]], grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.11241135746240616 
model_pd.l_d.mean(): -15.30199146270752 
model_pd.lagr.mean(): -15.189579963684082 
model_pd.lambdas: dict_items([('pout', tensor([1.2508])), ('power', tensor([0.7213]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9374])), ('power', tensor([-22.8022]))])
epoch：242	 i:0 	 global-step:4840	 l-p:0.11241135746240616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.0287, 4.4688, 4.5409],
        [4.0287, 4.6525, 4.8770],
        [4.0287, 4.0496, 4.0327],
        [4.0287, 4.1483, 4.0963]], grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.11228646337985992 
model_pd.l_d.mean(): -15.275063514709473 
model_pd.lagr.mean(): -15.162776947021484 
model_pd.lambdas: dict_items([('pout', tensor([1.2517])), ('power', tensor([0.7202]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9364])), ('power', tensor([-22.8003]))])
epoch：243	 i:0 	 global-step:4860	 l-p:0.11228646337985992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.0321, 4.6555, 4.8794],
        [4.0321, 4.1517, 4.0996],
        [4.0321, 4.0429, 4.0335],
        [4.0321, 4.6509, 4.8705]], grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.11216043680906296 
model_pd.l_d.mean(): -15.248140335083008 
model_pd.lagr.mean(): -15.135979652404785 
model_pd.lambdas: dict_items([('pout', tensor([1.2527])), ('power', tensor([0.7190]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9353])), ('power', tensor([-22.7984]))])
epoch：244	 i:0 	 global-step:4880	 l-p:0.11216043680906296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.0355, 4.0355, 4.0355],
        [4.0355, 4.8792, 5.3282],
        [4.0355, 4.0463, 4.0369],
        [4.0355, 4.0355, 4.0355]], grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.11203351616859436 
model_pd.l_d.mean(): -15.221221923828125 
model_pd.lagr.mean(): -15.109188079833984 
model_pd.lambdas: dict_items([('pout', tensor([1.2536])), ('power', tensor([0.7179]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9342])), ('power', tensor([-22.7964]))])
epoch：245	 i:0 	 global-step:4900	 l-p:0.11203351616859436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.0390, 4.1205, 4.0754],
        [4.0390, 4.0390, 4.0390],
        [4.0390, 4.6631, 4.8871],
        [4.0390, 4.3449, 4.3353]], grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.11190611124038696 
model_pd.l_d.mean(): -15.194308280944824 
model_pd.lagr.mean(): -15.082402229309082 
model_pd.lambdas: dict_items([('pout', tensor([1.2545])), ('power', tensor([0.7168]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9331])), ('power', tensor([-22.7944]))])
epoch：246	 i:0 	 global-step:4920	 l-p:0.11190611124038696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[4.0425, 4.4834, 4.5554],
        [4.0425, 4.1774, 4.1244],
        [4.0425, 5.0421, 5.6711],
        [4.0425, 4.6677, 4.8924]], grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.11177852749824524 
model_pd.l_d.mean(): -15.167402267456055 
model_pd.lagr.mean(): -15.055624008178711 
model_pd.lambdas: dict_items([('pout', tensor([1.2555])), ('power', tensor([0.7156]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9320])), ('power', tensor([-22.7924]))])
epoch：247	 i:0 	 global-step:4940	 l-p:0.11177852749824524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.0459, 4.4871, 4.5590],
        [4.0459, 4.0488, 4.0461],
        [4.0459, 5.1269, 5.8557],
        [4.0459, 4.1142, 4.0732]], grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.11165118217468262 
model_pd.l_d.mean(): -15.140501976013184 
model_pd.lagr.mean(): -15.028850555419922 
model_pd.lambdas: dict_items([('pout', tensor([1.2564])), ('power', tensor([0.7145]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9309])), ('power', tensor([-22.7904]))])
epoch：248	 i:0 	 global-step:4960	 l-p:0.11165118217468262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.0494, 4.0498, 4.0494],
        [4.0494, 4.0534, 4.0497],
        [4.0494, 4.0496, 4.0494],
        [4.0494, 4.1501, 4.1005]], grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.11152436584234238 
model_pd.l_d.mean(): -15.11361026763916 
model_pd.lagr.mean(): -15.00208568572998 
model_pd.lambdas: dict_items([('pout', tensor([1.2573])), ('power', tensor([0.7133]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9298])), ('power', tensor([-22.7883]))])
epoch：249	 i:0 	 global-step:4980	 l-p:0.11152436584234238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.0529, 4.0539, 4.0530],
        [4.0529, 4.6220, 4.7952],
        [4.0529, 4.0529, 4.0529],
        [4.0529, 5.1355, 5.8652]], grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.11139821261167526 
model_pd.l_d.mean(): -15.086725234985352 
model_pd.lagr.mean(): -14.975327491760254 
model_pd.lambdas: dict_items([('pout', tensor([1.2582])), ('power', tensor([0.7122]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9287])), ('power', tensor([-22.7863]))])
epoch：250	 i:0 	 global-step:5000	 l-p:0.11139821261167526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.0564, 4.0567, 4.0564],
        [4.0564, 4.0574, 4.0565],
        [4.0564, 4.9035, 5.3537],
        [4.0564, 4.0564, 4.0564]], grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.11127272248268127 
model_pd.l_d.mean(): -15.059846878051758 
model_pd.lagr.mean(): -14.94857406616211 
model_pd.lambdas: dict_items([('pout', tensor([1.2592])), ('power', tensor([0.7111]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9276])), ('power', tensor([-22.7843]))])
epoch：251	 i:0 	 global-step:5020	 l-p:0.11127272248268127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.0599, 4.1950, 4.1419],
        [4.0599, 4.0947, 4.0691],
        [4.0599, 4.0958, 4.0695],
        [4.0599, 4.5360, 4.6329]], grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.11114780604839325 
model_pd.l_d.mean(): -15.032975196838379 
model_pd.lagr.mean(): -14.92182731628418 
model_pd.lambdas: dict_items([('pout', tensor([1.2601])), ('power', tensor([0.7099]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9265])), ('power', tensor([-22.7823]))])
epoch：252	 i:0 	 global-step:5040	 l-p:0.11114780604839325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[4.0634, 5.0674, 5.6984],
        [4.0634, 4.0634, 4.0634],
        [4.0634, 4.0663, 4.0636],
        [4.0634, 4.2197, 4.1667]], grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.11102326214313507 
model_pd.l_d.mean(): -15.006113052368164 
model_pd.lagr.mean(): -14.895090103149414 
model_pd.lambdas: dict_items([('pout', tensor([1.2610])), ('power', tensor([0.7088]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9254])), ('power', tensor([-22.7802]))])
epoch：253	 i:0 	 global-step:5060	 l-p:0.11102326214313507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.0669, 4.0778, 4.0683],
        [4.0669, 4.1677, 4.1180],
        [4.0669, 4.4175, 4.4303],
        [4.0669, 4.0669, 4.0669]], grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.11089885979890823 
model_pd.l_d.mean(): -14.979254722595215 
model_pd.lagr.mean(): -14.868355751037598 
model_pd.lambdas: dict_items([('pout', tensor([1.2620])), ('power', tensor([0.7076]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9243])), ('power', tensor([-22.7782]))])
epoch：254	 i:0 	 global-step:5080	 l-p:0.11089885979890823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[4.0705, 5.0759, 5.7076],
        [4.0705, 4.1065, 4.0802],
        [4.0705, 4.3775, 4.3673],
        [4.0705, 4.3985, 4.3989]], grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.11077434569597244 
model_pd.l_d.mean(): -14.95240306854248 
model_pd.lagr.mean(): -14.841629028320312 
model_pd.lambdas: dict_items([('pout', tensor([1.2629])), ('power', tensor([0.7065]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9232])), ('power', tensor([-22.7761]))])
epoch：255	 i:0 	 global-step:5100	 l-p:0.11077434569597244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.0740, 4.6971, 4.9172],
        [4.0740, 4.0740, 4.0740],
        [4.0740, 4.0780, 4.0743],
        [4.0740, 4.0740, 4.0740]], grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.11064949631690979 
model_pd.l_d.mean(): -14.925559043884277 
model_pd.lagr.mean(): -14.814909934997559 
model_pd.lambdas: dict_items([('pout', tensor([1.2638])), ('power', tensor([0.7054]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9220])), ('power', tensor([-22.7740]))])
epoch：256	 i:0 	 global-step:5120	 l-p:0.11064949631690979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.0776, 4.2728, 4.2244],
        [4.0776, 4.7011, 4.9213],
        [4.0776, 4.7058, 4.9303],
        [4.0776, 4.0962, 4.0809]], grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.11052411794662476 
model_pd.l_d.mean(): -14.898717880249023 
model_pd.lagr.mean(): -14.788193702697754 
model_pd.lambdas: dict_items([('pout', tensor([1.2647])), ('power', tensor([0.7042]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9209])), ('power', tensor([-22.7719]))])
epoch：257	 i:0 	 global-step:5140	 l-p:0.11052411794662476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.0812, 4.0841, 4.0814],
        [4.0812, 4.0864, 4.0816],
        [4.0812, 5.0889, 5.7217],
        [4.0812, 4.0812, 4.0812]], grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.11039818078279495 
model_pd.l_d.mean(): -14.87188720703125 
model_pd.lagr.mean(): -14.761488914489746 
model_pd.lambdas: dict_items([('pout', tensor([1.2656])), ('power', tensor([0.7031]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9198])), ('power', tensor([-22.7698]))])
epoch：258	 i:0 	 global-step:5160	 l-p:0.11039818078279495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[4.0849, 4.1196, 4.0940],
        [4.0849, 4.7138, 4.9384],
        [4.0849, 4.0849, 4.0849],
        [4.0849, 4.1058, 4.0889]], grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11027166992425919 
model_pd.l_d.mean(): -14.845060348510742 
model_pd.lagr.mean(): -14.73478889465332 
model_pd.lambdas: dict_items([('pout', tensor([1.2666])), ('power', tensor([0.7020]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9186])), ('power', tensor([-22.7677]))])
epoch：259	 i:0 	 global-step:5180	 l-p:0.11027166992425919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.0885, 4.0895, 4.0886],
        [4.0885, 4.0885, 4.0885],
        [4.0885, 4.3750, 4.3550],
        [4.0885, 4.1832, 4.1346]], grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.11014468967914581 
model_pd.l_d.mean(): -14.818241119384766 
model_pd.lagr.mean(): -14.708096504211426 
model_pd.lambdas: dict_items([('pout', tensor([1.2675])), ('power', tensor([0.7008]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9175])), ('power', tensor([-22.7655]))])
epoch：260	 i:0 	 global-step:5200	 l-p:0.11014468967914581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.0922, 4.1283, 4.1019],
        [4.0922, 4.7227, 4.9482],
        [4.0922, 4.0922, 4.0922],
        [4.0922, 4.1270, 4.1014]], grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.11001741141080856 
model_pd.l_d.mean(): -14.791425704956055 
model_pd.lagr.mean(): -14.681407928466797 
model_pd.lambdas: dict_items([('pout', tensor([1.2684])), ('power', tensor([0.6997]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9163])), ('power', tensor([-22.7633]))])
epoch：261	 i:0 	 global-step:5220	 l-p:0.11001741141080856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[4.0960, 4.1968, 4.1470],
        [4.0960, 4.1906, 4.1421],
        [4.0960, 4.6035, 4.7226],
        [4.0960, 4.2314, 4.1779]], grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.1098899245262146 
model_pd.l_d.mean(): -14.764619827270508 
model_pd.lagr.mean(): -14.654729843139648 
model_pd.lambdas: dict_items([('pout', tensor([1.2693])), ('power', tensor([0.6985]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9151])), ('power', tensor([-22.7611]))])
epoch：262	 i:0 	 global-step:5240	 l-p:0.1098899245262146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.0997, 4.1007, 4.0997],
        [4.0997, 4.2564, 4.2031],
        [4.0997, 4.0997, 4.0997],
        [4.0997, 4.1353, 4.1092]], grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.10976246744394302 
model_pd.l_d.mean(): -14.737820625305176 
model_pd.lagr.mean(): -14.628058433532715 
model_pd.lambdas: dict_items([('pout', tensor([1.2702])), ('power', tensor([0.6974]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9140])), ('power', tensor([-22.7589]))])
epoch：263	 i:0 	 global-step:5260	 l-p:0.10976246744394302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.1034, 4.9583, 5.4114],
        [4.1034, 4.1393, 4.1130],
        [4.1034, 4.7297, 4.9503],
        [4.1034, 4.1035, 4.1034]], grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.1096351370215416 
model_pd.l_d.mean(): -14.711027145385742 
model_pd.lagr.mean(): -14.601391792297363 
model_pd.lambdas: dict_items([('pout', tensor([1.2711])), ('power', tensor([0.6963]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9128])), ('power', tensor([-22.7567]))])
epoch：264	 i:0 	 global-step:5280	 l-p:0.1096351370215416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.1072, 4.3130, 4.2662],
        [4.1072, 4.5867, 4.6833],
        [4.1072, 4.1258, 4.1105],
        [4.1072, 4.1756, 4.1344]], grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.10950804501771927 
model_pd.l_d.mean(): -14.68424129486084 
model_pd.lagr.mean(): -14.574732780456543 
model_pd.lambdas: dict_items([('pout', tensor([1.2720])), ('power', tensor([0.6951]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9116])), ('power', tensor([-22.7544]))])
epoch：265	 i:0 	 global-step:5300	 l-p:0.10950804501771927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.1110, 4.1458, 4.1201],
        [4.1110, 4.6197, 4.7388],
        [4.1110, 4.4194, 4.4087],
        [4.1110, 4.1469, 4.1206]], grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.10938116163015366 
model_pd.l_d.mean(): -14.65746021270752 
model_pd.lagr.mean(): -14.548079490661621 
model_pd.lambdas: dict_items([('pout', tensor([1.2730])), ('power', tensor([0.6940]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9104])), ('power', tensor([-22.7522]))])
epoch：266	 i:0 	 global-step:5320	 l-p:0.10938116163015366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.1148, 4.1149, 4.1148],
        [4.1148, 4.4021, 4.3817],
        [4.1148, 4.1148, 4.1148],
        [4.1148, 4.1831, 4.1420]], grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.10925449430942535 
model_pd.l_d.mean(): -14.63068962097168 
model_pd.lagr.mean(): -14.521434783935547 
model_pd.lambdas: dict_items([('pout', tensor([1.2739])), ('power', tensor([0.6928]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9093])), ('power', tensor([-22.7499]))])
epoch：267	 i:0 	 global-step:5340	 l-p:0.10925449430942535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.1185, 4.1185, 4.1185],
        [4.1185, 4.7519, 4.9778],
        [4.1185, 5.2165, 5.9544],
        [4.1185, 4.1187, 4.1186]], grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.10912803560495377 
model_pd.l_d.mean(): -14.603926658630371 
model_pd.lagr.mean(): -14.49479866027832 
model_pd.lambdas: dict_items([('pout', tensor([1.2748])), ('power', tensor([0.6917]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9081])), ('power', tensor([-22.7477]))])
epoch：268	 i:0 	 global-step:5360	 l-p:0.10912803560495377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[4.1224, 5.0224, 5.5234],
        [4.1224, 4.1908, 4.1495],
        [4.1224, 4.6029, 4.6995],
        [4.1224, 4.3184, 4.2694]], grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.10900165885686874 
model_pd.l_d.mean(): -14.577168464660645 
model_pd.lagr.mean(): -14.46816635131836 
model_pd.lambdas: dict_items([('pout', tensor([1.2757])), ('power', tensor([0.6906]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9069])), ('power', tensor([-22.7454]))])
epoch：269	 i:0 	 global-step:5380	 l-p:0.10900165885686874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[4.1262, 4.2464, 4.1937],
        [4.1262, 4.2619, 4.2081],
        [4.1262, 5.2260, 5.9649],
        [4.1262, 4.1262, 4.1262]], grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.10887517780065536 
model_pd.l_d.mean(): -14.550420761108398 
model_pd.lagr.mean(): -14.441545486450195 
model_pd.lambdas: dict_items([('pout', tensor([1.2766])), ('power', tensor([0.6894]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9057])), ('power', tensor([-22.7431]))])
epoch：270	 i:0 	 global-step:5400	 l-p:0.10887517780065536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.1300, 4.1510, 4.1340],
        [4.1300, 4.1300, 4.1300],
        [4.1300, 4.1656, 4.1395],
        [4.1300, 4.1300, 4.1300]], grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.10874858498573303 
model_pd.l_d.mean(): -14.523675918579102 
model_pd.lagr.mean(): -14.41492748260498 
model_pd.lambdas: dict_items([('pout', tensor([1.2775])), ('power', tensor([0.6883]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9045])), ('power', tensor([-22.7408]))])
epoch：271	 i:0 	 global-step:5420	 l-p:0.10874858498573303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[4.1339, 4.6445, 4.7636],
        [4.1339, 4.7635, 4.9846],
        [4.1339, 4.1339, 4.1339],
        [4.1339, 5.0360, 5.5380]], grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.108621746301651 
model_pd.l_d.mean(): -14.496940612792969 
model_pd.lagr.mean(): -14.38831901550293 
model_pd.lambdas: dict_items([('pout', tensor([1.2784])), ('power', tensor([0.6872]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9033])), ('power', tensor([-22.7385]))])
epoch：272	 i:0 	 global-step:5440	 l-p:0.108621746301651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.1378, 4.4916, 4.5033],
        [4.1378, 4.1378, 4.1378],
        [4.1378, 4.1737, 4.1474],
        [4.1378, 4.2062, 4.1650]], grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.10849462449550629 
model_pd.l_d.mean(): -14.47021198272705 
model_pd.lagr.mean(): -14.361717224121094 
model_pd.lambdas: dict_items([('pout', tensor([1.2793])), ('power', tensor([0.6860]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9021])), ('power', tensor([-22.7362]))])
epoch：273	 i:0 	 global-step:5460	 l-p:0.10849462449550629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[4.1417, 4.5890, 4.6601],
        [4.1417, 4.1776, 4.1513],
        [4.1417, 4.1421, 4.1417],
        [4.1417, 4.4618, 4.4556]], grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.10836721211671829 
model_pd.l_d.mean(): -14.44349193572998 
model_pd.lagr.mean(): -14.335124969482422 
model_pd.lambdas: dict_items([('pout', tensor([1.2802])), ('power', tensor([0.6849]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9009])), ('power', tensor([-22.7338]))])
epoch：274	 i:0 	 global-step:5480	 l-p:0.10836721211671829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.1456, 4.1458, 4.1456],
        [4.1456, 4.4555, 4.4442],
        [4.1456, 4.2661, 4.2132],
        [4.1456, 4.6280, 4.7245]], grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.10823948681354523 
model_pd.l_d.mean(): -14.416775703430176 
model_pd.lagr.mean(): -14.308536529541016 
model_pd.lambdas: dict_items([('pout', tensor([1.2811])), ('power', tensor([0.6838]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8997])), ('power', tensor([-22.7314]))])
epoch：275	 i:0 	 global-step:5500	 l-p:0.10823948681354523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[4.1496, 4.1498, 4.1496],
        [4.1496, 4.1706, 4.1536],
        [4.1496, 5.0547, 5.5579],
        [4.1496, 4.1536, 4.1499]], grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.10811149328947067 
model_pd.l_d.mean(): -14.390069961547852 
model_pd.lagr.mean(): -14.28195858001709 
model_pd.lambdas: dict_items([('pout', tensor([1.2820])), ('power', tensor([0.6826]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8984])), ('power', tensor([-22.7290]))])
epoch：276	 i:0 	 global-step:5520	 l-p:0.10811149328947067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.1536, 4.1536, 4.1536],
        [4.1536, 4.1644, 4.1550],
        [4.1536, 4.7855, 5.0070],
        [4.1536, 4.4423, 4.4214]], grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.10798332840204239 
model_pd.l_d.mean(): -14.363369941711426 
model_pd.lagr.mean(): -14.255386352539062 
model_pd.lambdas: dict_items([('pout', tensor([1.2829])), ('power', tensor([0.6815]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8972])), ('power', tensor([-22.7266]))])
epoch：277	 i:0 	 global-step:5540	 l-p:0.10798332840204239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[4.1576, 4.2396, 4.1939],
        [4.1576, 4.1576, 4.1576],
        [4.1576, 4.4679, 4.4565],
        [4.1576, 5.0642, 5.5681]], grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.10785504430532455 
model_pd.l_d.mean(): -14.336677551269531 
model_pd.lagr.mean(): -14.228822708129883 
model_pd.lambdas: dict_items([('pout', tensor([1.2838])), ('power', tensor([0.6803]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8959])), ('power', tensor([-22.7242]))])
epoch：278	 i:0 	 global-step:5560	 l-p:0.10785504430532455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[4.1616, 4.4214, 4.3889],
        [4.1616, 4.3496, 4.2987],
        [4.1616, 4.8000, 5.0267],
        [4.1616, 4.2437, 4.1980]], grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.10772668570280075 
model_pd.l_d.mean(): -14.309990882873535 
model_pd.lagr.mean(): -14.202263832092285 
model_pd.lambdas: dict_items([('pout', tensor([1.2847])), ('power', tensor([0.6792]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8947])), ('power', tensor([-22.7217]))])
epoch：279	 i:0 	 global-step:5580	 l-p:0.10772668570280075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[4.1657, 5.0738, 5.5783],
        [4.1657, 4.1660, 4.1657],
        [4.1657, 5.0133, 5.4509],
        [4.1657, 5.0314, 5.4887]], grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.10759834945201874 
model_pd.l_d.mean(): -14.28331184387207 
model_pd.lagr.mean(): -14.175713539123535 
model_pd.lambdas: dict_items([('pout', tensor([1.2856])), ('power', tensor([0.6781]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8934])), ('power', tensor([-22.7193]))])
epoch：280	 i:0 	 global-step:5600	 l-p:0.10759834945201874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.1697, 4.2046, 4.1788],
        [4.1697, 4.1806, 4.1711],
        [4.1697, 4.8090, 5.0360],
        [4.1697, 4.6191, 4.6900]], grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.10747012495994568 
model_pd.l_d.mean(): -14.256643295288086 
model_pd.lagr.mean(): -14.14917278289795 
model_pd.lambdas: dict_items([('pout', tensor([1.2865])), ('power', tensor([0.6769]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8922])), ('power', tensor([-22.7168]))])
epoch：281	 i:0 	 global-step:5620	 l-p:0.10747012495994568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.1738, 4.1738, 4.1738],
        [4.1738, 4.3709, 4.3213],
        [4.1738, 4.8128, 5.0392],
        [4.1738, 4.7545, 4.9288]], grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.10734193027019501 
model_pd.l_d.mean(): -14.22998046875 
model_pd.lagr.mean(): -14.122638702392578 
model_pd.lambdas: dict_items([('pout', tensor([1.2874])), ('power', tensor([0.6758]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8909])), ('power', tensor([-22.7143]))])
epoch：282	 i:0 	 global-step:5640	 l-p:0.10734193027019501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.1779, 4.6278, 4.6987],
        [4.1779, 4.8182, 5.0453],
        [4.1779, 4.1830, 4.1783],
        [4.1779, 4.2136, 4.1873]], grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.1072138249874115 
model_pd.l_d.mean(): -14.203325271606445 
model_pd.lagr.mean(): -14.096111297607422 
model_pd.lambdas: dict_items([('pout', tensor([1.2882])), ('power', tensor([0.6747]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8897])), ('power', tensor([-22.7118]))])
epoch：283	 i:0 	 global-step:5660	 l-p:0.1072138249874115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.1820, 4.2835, 4.2331],
        [4.1820, 4.3704, 4.3192],
        [4.1820, 5.0933, 5.5991],
        [4.1820, 4.1820, 4.1820]], grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.10708573460578918 
model_pd.l_d.mean(): -14.176677703857422 
model_pd.lagr.mean(): -14.069591522216797 
model_pd.lambdas: dict_items([('pout', tensor([1.2891])), ('power', tensor([0.6735]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8884])), ('power', tensor([-22.7093]))])
epoch：284	 i:0 	 global-step:5680	 l-p:0.10708573460578918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.1861, 4.3746, 4.3234],
        [4.1861, 4.7012, 4.8204],
        [4.1861, 4.2223, 4.1958],
        [4.1861, 4.8218, 5.0440]], grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.10695763677358627 
model_pd.l_d.mean(): -14.15003776550293 
model_pd.lagr.mean(): -14.04308032989502 
model_pd.lambdas: dict_items([('pout', tensor([1.2900])), ('power', tensor([0.6724]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8871])), ('power', tensor([-22.7068]))])
epoch：285	 i:0 	 global-step:5700	 l-p:0.10695763677358627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[4.1903, 5.3059, 6.0536],
        [4.1903, 5.0604, 5.5195],
        [4.1903, 4.5020, 4.4901],
        [4.1903, 4.1954, 4.1907]], grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.10682947933673859 
model_pd.l_d.mean(): -14.123403549194336 
model_pd.lagr.mean(): -14.016573905944824 
model_pd.lambdas: dict_items([('pout', tensor([1.2909])), ('power', tensor([0.6713]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8859])), ('power', tensor([-22.7042]))])
epoch：286	 i:0 	 global-step:5720	 l-p:0.10682947933673859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.1944, 4.2960, 4.2455],
        [4.1944, 4.5279, 4.5265],
        [4.1944, 4.1944, 4.1944],
        [4.1944, 4.2306, 4.2041]], grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.10670120269060135 
model_pd.l_d.mean(): -14.096779823303223 
model_pd.lagr.mean(): -13.990078926086426 
model_pd.lambdas: dict_items([('pout', tensor([1.2918])), ('power', tensor([0.6701]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8846])), ('power', tensor([-22.7017]))])
epoch：287	 i:0 	 global-step:5740	 l-p:0.10670120269060135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.1986, 4.2809, 4.2350],
        [4.1986, 4.1988, 4.1986],
        [4.1986, 4.2026, 4.1989],
        [4.1986, 5.2321, 5.8779]], grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.10657273977994919 
model_pd.l_d.mean(): -14.070161819458008 
model_pd.lagr.mean(): -13.96358871459961 
model_pd.lambdas: dict_items([('pout', tensor([1.2927])), ('power', tensor([0.6690]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8833])), ('power', tensor([-22.6991]))])
epoch：288	 i:0 	 global-step:5760	 l-p:0.10657273977994919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[4.2028, 4.5367, 4.5352],
        [4.2028, 4.2038, 4.2028],
        [4.2028, 4.3395, 4.2850],
        [4.2028, 4.5256, 4.5186]], grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.10644412040710449 
model_pd.l_d.mean(): -14.04355239868164 
model_pd.lagr.mean(): -13.937108039855957 
model_pd.lambdas: dict_items([('pout', tensor([1.2936])), ('power', tensor([0.6679]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8820])), ('power', tensor([-22.6965]))])
epoch：289	 i:0 	 global-step:5780	 l-p:0.10644412040710449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.2070, 4.2122, 4.2075],
        [4.2070, 5.2424, 5.8892],
        [4.2070, 4.4049, 4.3548],
        [4.2070, 4.6944, 4.7908]], grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.10631530731916428 
model_pd.l_d.mean(): -14.016950607299805 
model_pd.lagr.mean(): -13.910634994506836 
model_pd.lambdas: dict_items([('pout', tensor([1.2944])), ('power', tensor([0.6667]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8807])), ('power', tensor([-22.6939]))])
epoch：290	 i:0 	 global-step:5800	 l-p:0.10631530731916428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.2113, 5.3322, 6.0830],
        [4.2113, 4.3324, 4.2790],
        [4.2113, 4.3696, 4.3151],
        [4.2113, 4.2221, 4.2127]], grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.10618625581264496 
model_pd.l_d.mean(): -13.990354537963867 
model_pd.lagr.mean(): -13.88416862487793 
model_pd.lambdas: dict_items([('pout', tensor([1.2953])), ('power', tensor([0.6656]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8794])), ('power', tensor([-22.6913]))])
epoch：291	 i:0 	 global-step:5820	 l-p:0.10618625581264496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.2156, 4.2342, 4.2189],
        [4.2156, 4.8548, 5.0777],
        [4.2156, 4.5067, 4.4849],
        [4.2156, 4.2165, 4.2156]], grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.1060570701956749 
model_pd.l_d.mean(): -13.96377182006836 
model_pd.lagr.mean(): -13.857714653015137 
model_pd.lambdas: dict_items([('pout', tensor([1.2962])), ('power', tensor([0.6644]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8781])), ('power', tensor([-22.6886]))])
epoch：292	 i:0 	 global-step:5840	 l-p:0.1060570701956749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.2199, 4.2556, 4.2293],
        [4.2199, 4.2200, 4.2199],
        [4.2199, 4.2548, 4.2290],
        [4.2199, 4.7083, 4.8047]], grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.1059277281165123 
model_pd.l_d.mean(): -13.937191009521484 
model_pd.lagr.mean(): -13.831263542175293 
model_pd.lambdas: dict_items([('pout', tensor([1.2971])), ('power', tensor([0.6633]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8768])), ('power', tensor([-22.6860]))])
epoch：293	 i:0 	 global-step:5860	 l-p:0.1059277281165123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.2242, 4.4861, 4.4527],
        [4.2242, 4.4327, 4.3843],
        [4.2242, 4.2242, 4.2242],
        [4.2242, 4.7427, 4.8621]], grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.10579825192689896 
model_pd.l_d.mean(): -13.910622596740723 
model_pd.lagr.mean(): -13.804823875427246 
model_pd.lambdas: dict_items([('pout', tensor([1.2979])), ('power', tensor([0.6622]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8755])), ('power', tensor([-22.6833]))])
epoch：294	 i:0 	 global-step:5880	 l-p:0.10579825192689896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.2285, 4.2643, 4.2379],
        [4.2285, 4.2295, 4.2285],
        [4.2285, 4.2285, 4.2285],
        [4.2285, 4.3304, 4.2797]], grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.10566869378089905 
model_pd.l_d.mean(): -13.88405990600586 
model_pd.lagr.mean(): -13.778390884399414 
model_pd.lambdas: dict_items([('pout', tensor([1.2988])), ('power', tensor([0.6610]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8742])), ('power', tensor([-22.6806]))])
epoch：295	 i:0 	 global-step:5900	 l-p:0.10566869378089905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.2329, 4.4224, 4.3705],
        [4.2329, 4.8791, 5.1069],
        [4.2329, 4.2691, 4.2425],
        [4.2329, 4.2330, 4.2329]], grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.10553907603025436 
model_pd.l_d.mean(): -13.857505798339844 
model_pd.lagr.mean(): -13.75196647644043 
model_pd.lambdas: dict_items([('pout', tensor([1.2997])), ('power', tensor([0.6599]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8728])), ('power', tensor([-22.6779]))])
epoch：296	 i:0 	 global-step:5920	 l-p:0.10553907603025436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.2372, 4.8248, 4.9999],
        [4.2372, 4.2372, 4.2372],
        [4.2372, 5.1161, 5.5787],
        [4.2372, 4.8840, 5.1119]], grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.10540942847728729 
model_pd.l_d.mean(): -13.83095932006836 
model_pd.lagr.mean(): -13.725549697875977 
model_pd.lambdas: dict_items([('pout', tensor([1.3006])), ('power', tensor([0.6588]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8715])), ('power', tensor([-22.6751]))])
epoch：297	 i:0 	 global-step:5940	 l-p:0.10540942847728729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[4.2416, 4.2416, 4.2416],
        [4.2416, 4.2416, 4.2416],
        [4.2416, 4.3374, 4.2878],
        [4.2416, 4.2416, 4.2416]], grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.10527972131967545 
model_pd.l_d.mean(): -13.804420471191406 
model_pd.lagr.mean(): -13.699140548706055 
model_pd.lambdas: dict_items([('pout', tensor([1.3014])), ('power', tensor([0.6576]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8702])), ('power', tensor([-22.6724]))])
epoch：298	 i:0 	 global-step:5960	 l-p:0.10527972131967545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.2460, 4.2462, 4.2460],
        [4.2460, 4.2461, 4.2460],
        [4.2460, 4.2460, 4.2460],
        [4.2460, 4.2569, 4.2474]], grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.10514996200799942 
model_pd.l_d.mean(): -13.7778902053833 
model_pd.lagr.mean(): -13.67273998260498 
model_pd.lambdas: dict_items([('pout', tensor([1.3023])), ('power', tensor([0.6565]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8688])), ('power', tensor([-22.6696]))])
epoch：299	 i:0 	 global-step:5980	 l-p:0.10514996200799942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.2505, 5.1752, 5.6871],
        [4.2505, 4.4095, 4.3545],
        [4.2505, 4.2613, 4.2519],
        [4.2505, 4.3878, 4.3328]], grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.10502011328935623 
model_pd.l_d.mean(): -13.75136947631836 
model_pd.lagr.mean(): -13.64634895324707 
model_pd.lambdas: dict_items([('pout', tensor([1.3032])), ('power', tensor([0.6554]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8675])), ('power', tensor([-22.6669]))])
epoch：300	 i:0 	 global-step:6000	 l-p:0.10502011328935623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[4.2549, 4.3508, 4.3011],
        [4.2549, 4.9039, 5.1323],
        [4.2549, 4.4540, 4.4032],
        [4.2549, 4.3571, 4.3061]], grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.10489019751548767 
model_pd.l_d.mean(): -13.72485637664795 
model_pd.lagr.mean(): -13.619966506958008 
model_pd.lambdas: dict_items([('pout', tensor([1.3040])), ('power', tensor([0.6542]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8661])), ('power', tensor([-22.6641]))])
epoch：301	 i:0 	 global-step:6020	 l-p:0.10489019751548767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[4.2594, 4.3811, 4.3272],
        [4.2594, 4.2957, 4.2690],
        [4.2594, 4.2633, 4.2597],
        [4.2594, 4.3421, 4.2958]], grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.10476016998291016 
model_pd.l_d.mean(): -13.69835090637207 
model_pd.lagr.mean(): -13.59359073638916 
model_pd.lambdas: dict_items([('pout', tensor([1.3049])), ('power', tensor([0.6531]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8648])), ('power', tensor([-22.6612]))])
epoch：302	 i:0 	 global-step:6040	 l-p:0.10476016998291016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.2639, 4.7204, 4.7910],
        [4.2639, 4.5897, 4.5819],
        [4.2639, 4.2747, 4.2653],
        [4.2639, 4.2997, 4.2733]], grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.1046299859881401 
model_pd.l_d.mean(): -13.671854972839355 
model_pd.lagr.mean(): -13.567225456237793 
model_pd.lambdas: dict_items([('pout', tensor([1.3058])), ('power', tensor([0.6520]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8634])), ('power', tensor([-22.6584]))])
epoch：303	 i:0 	 global-step:6060	 l-p:0.1046299859881401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.2684, 4.3707, 4.3197],
        [4.2684, 4.5944, 4.5866],
        [4.2684, 4.2684, 4.2684],
        [4.2684, 4.2684, 4.2684]], grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.10449960082769394 
model_pd.l_d.mean(): -13.645364761352539 
model_pd.lagr.mean(): -13.540864944458008 
model_pd.lambdas: dict_items([('pout', tensor([1.3066])), ('power', tensor([0.6508]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8620])), ('power', tensor([-22.6556]))])
epoch：304	 i:0 	 global-step:6080	 l-p:0.10449960082769394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.2729, 4.3091, 4.2825],
        [4.2729, 4.5885, 4.5755],
        [4.2729, 4.2729, 4.2729],
        [4.2729, 5.4098, 6.1697]], grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.10436899960041046 
model_pd.l_d.mean(): -13.61888599395752 
model_pd.lagr.mean(): -13.514516830444336 
model_pd.lambdas: dict_items([('pout', tensor([1.3075])), ('power', tensor([0.6497]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8607])), ('power', tensor([-22.6527]))])
epoch：305	 i:0 	 global-step:6100	 l-p:0.10436899960041046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.2775, 4.3467, 4.3047],
        [4.2775, 4.2775, 4.2775],
        [4.2775, 4.2776, 4.2775],
        [4.2775, 4.4153, 4.3600]], grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.10423821210861206 
model_pd.l_d.mean(): -13.592415809631348 
model_pd.lagr.mean(): -13.488177299499512 
model_pd.lambdas: dict_items([('pout', tensor([1.3084])), ('power', tensor([0.6486]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8593])), ('power', tensor([-22.6498]))])
epoch：306	 i:0 	 global-step:6120	 l-p:0.10423821210861206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.2821, 4.4042, 4.3500],
        [4.2821, 4.4819, 4.4308],
        [4.2821, 4.2823, 4.2821],
        [4.2821, 4.3185, 4.2917]], grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.10410714894533157 
model_pd.l_d.mean(): -13.56595230102539 
model_pd.lagr.mean(): -13.461845397949219 
model_pd.lambdas: dict_items([('pout', tensor([1.3092])), ('power', tensor([0.6474]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8579])), ('power', tensor([-22.6469]))])
epoch：307	 i:0 	 global-step:6140	 l-p:0.10410714894533157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.2867, 4.7451, 4.8156],
        [4.2867, 4.2867, 4.2867],
        [4.2867, 4.2867, 4.2867],
        [4.2867, 4.2867, 4.2867]], grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.10397592186927795 
model_pd.l_d.mean(): -13.539496421813965 
model_pd.lagr.mean(): -13.43552017211914 
model_pd.lambdas: dict_items([('pout', tensor([1.3101])), ('power', tensor([0.6463]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8565])), ('power', tensor([-22.6440]))])
epoch：308	 i:0 	 global-step:6160	 l-p:0.10397592186927795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.2913, 4.9401, 5.1650],
        [4.2913, 4.2913, 4.2913],
        [4.2913, 4.2914, 4.2913],
        [4.2913, 4.4823, 4.4297]], grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.10384450107812881 
model_pd.l_d.mean(): -13.51305103302002 
model_pd.lagr.mean(): -13.40920639038086 
model_pd.lambdas: dict_items([('pout', tensor([1.3109])), ('power', tensor([0.6452]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8551])), ('power', tensor([-22.6410]))])
epoch：309	 i:0 	 global-step:6180	 l-p:0.10384450107812881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.2960, 4.3986, 4.3473],
        [4.2960, 4.2961, 4.2960],
        [4.2960, 5.1861, 5.6535],
        [4.2960, 4.8215, 4.9412]], grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.10371292382478714 
model_pd.l_d.mean(): -13.486613273620605 
model_pd.lagr.mean(): -13.38290023803711 
model_pd.lambdas: dict_items([('pout', tensor([1.3118])), ('power', tensor([0.6441]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8537])), ('power', tensor([-22.6381]))])
epoch：310	 i:0 	 global-step:6200	 l-p:0.10371292382478714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.3006, 5.1729, 5.6203],
        [4.3006, 4.3046, 4.3009],
        [4.3006, 4.4230, 4.3687],
        [4.3006, 4.4388, 4.3833]], grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.10358127951622009 
model_pd.l_d.mean(): -13.460185050964355 
model_pd.lagr.mean(): -13.356603622436523 
model_pd.lambdas: dict_items([('pout', tensor([1.3126])), ('power', tensor([0.6429]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8523])), ('power', tensor([-22.6351]))])
epoch：311	 i:0 	 global-step:6220	 l-p:0.10358127951622009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.3053, 4.3413, 4.3148],
        [4.3053, 4.6447, 4.6419],
        [4.3053, 4.3055, 4.3053],
        [4.3053, 4.3053, 4.3053]], grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.1034495159983635 
model_pd.l_d.mean(): -13.433764457702637 
model_pd.lagr.mean(): -13.330314636230469 
model_pd.lambdas: dict_items([('pout', tensor([1.3135])), ('power', tensor([0.6418]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8509])), ('power', tensor([-22.6321]))])
epoch：312	 i:0 	 global-step:6240	 l-p:0.1034495159983635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.3100, 5.3698, 6.0295],
        [4.3100, 4.3104, 4.3101],
        [4.3100, 4.3209, 4.3114],
        [4.3100, 4.3100, 4.3100]], grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.10331764817237854 
model_pd.l_d.mean(): -13.407350540161133 
model_pd.lagr.mean(): -13.304033279418945 
model_pd.lambdas: dict_items([('pout', tensor([1.3143])), ('power', tensor([0.6407]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8495])), ('power', tensor([-22.6291]))])
epoch：313	 i:0 	 global-step:6260	 l-p:0.10331764817237854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[4.3148, 4.3842, 4.3421],
        [4.3148, 4.4374, 4.3829],
        [4.3148, 4.3511, 4.3244],
        [4.3148, 4.3148, 4.3148]], grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.1031857281923294 
model_pd.l_d.mean(): -13.380949020385742 
model_pd.lagr.mean(): -13.277763366699219 
model_pd.lambdas: dict_items([('pout', tensor([1.3152])), ('power', tensor([0.6395]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8481])), ('power', tensor([-22.6260]))])
epoch：314	 i:0 	 global-step:6280	 l-p:0.1031857281923294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[4.3195, 4.9166, 5.0932],
        [4.3195, 4.6154, 4.5921],
        [4.3195, 4.4422, 4.3877],
        [4.3195, 5.2585, 5.7769]], grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.10305365920066833 
model_pd.l_d.mean(): -13.3545560836792 
model_pd.lagr.mean(): -13.25150203704834 
model_pd.lambdas: dict_items([('pout', tensor([1.3160])), ('power', tensor([0.6384]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8466])), ('power', tensor([-22.6230]))])
epoch：315	 i:0 	 global-step:6300	 l-p:0.10305365920066833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.3243, 4.4470, 4.3925],
        [4.3243, 4.4629, 4.4072],
        [4.3243, 5.2200, 5.6898],
        [4.3243, 4.3352, 4.3257]], grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.10292147845029831 
model_pd.l_d.mean(): -13.328168869018555 
model_pd.lagr.mean(): -13.225247383117676 
model_pd.lambdas: dict_items([('pout', tensor([1.3169])), ('power', tensor([0.6373]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8452])), ('power', tensor([-22.6199]))])
epoch：316	 i:0 	 global-step:6320	 l-p:0.10292147845029831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.3291, 4.5212, 4.4680],
        [4.3291, 4.4519, 4.3973],
        [4.3291, 4.3301, 4.3291],
        [4.3291, 4.3292, 4.3291]], grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.10278914123773575 
model_pd.l_d.mean(): -13.301794052124023 
model_pd.lagr.mean(): -13.199005126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.3177])), ('power', tensor([0.6361]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8438])), ('power', tensor([-22.6168]))])
epoch：317	 i:0 	 global-step:6340	 l-p:0.10278914123773575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.3339, 4.6002, 4.5653],
        [4.3339, 4.3705, 4.3436],
        [4.3339, 4.3379, 4.3342],
        [4.3339, 4.9884, 5.2145]], grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.10265661031007767 
model_pd.l_d.mean(): -13.275426864624023 
model_pd.lagr.mean(): -13.172770500183105 
model_pd.lambdas: dict_items([('pout', tensor([1.3186])), ('power', tensor([0.6350]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8423])), ('power', tensor([-22.6137]))])
epoch：318	 i:0 	 global-step:6360	 l-p:0.10265661031007767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.3388, 4.7041, 4.7136],
        [4.3388, 4.4222, 4.3754],
        [4.3388, 4.3428, 4.3391],
        [4.3388, 4.3388, 4.3388]], grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.10252384096384048 
model_pd.l_d.mean(): -13.249069213867188 
model_pd.lagr.mean(): -13.14654541015625 
model_pd.lambdas: dict_items([('pout', tensor([1.3194])), ('power', tensor([0.6339]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8409])), ('power', tensor([-22.6106]))])
epoch：319	 i:0 	 global-step:6380	 l-p:0.10252384096384048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[4.3437, 5.0052, 5.2370],
        [4.3437, 4.3446, 4.3437],
        [4.3437, 4.6852, 4.6821],
        [4.3437, 4.4271, 4.3803]], grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.1023908257484436 
model_pd.l_d.mean(): -13.222721099853516 
model_pd.lagr.mean(): -13.120329856872559 
model_pd.lambdas: dict_items([('pout', tensor([1.3202])), ('power', tensor([0.6327]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8394])), ('power', tensor([-22.6074]))])
epoch：320	 i:0 	 global-step:6400	 l-p:0.1023908257484436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.3485, 5.0100, 5.2412],
        [4.3485, 4.5608, 4.5107],
        [4.3485, 4.3485, 4.3485],
        [4.3485, 4.3486, 4.3485]], grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.10225757956504822 
model_pd.l_d.mean(): -13.196379661560059 
model_pd.lagr.mean(): -13.094121932983398 
model_pd.lambdas: dict_items([('pout', tensor([1.3211])), ('power', tensor([0.6316]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8380])), ('power', tensor([-22.6043]))])
epoch：321	 i:0 	 global-step:6420	 l-p:0.10225757956504822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.3535, 4.3535, 4.3535],
        [4.3535, 5.0106, 5.2373],
        [4.3535, 4.3535, 4.3535],
        [4.3535, 4.4504, 4.4000]], grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.10212406516075134 
model_pd.l_d.mean(): -13.170047760009766 
model_pd.lagr.mean(): -13.067923545837402 
model_pd.lambdas: dict_items([('pout', tensor([1.3219])), ('power', tensor([0.6305]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8365])), ('power', tensor([-22.6011]))])
epoch：322	 i:0 	 global-step:6440	 l-p:0.10212406516075134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[4.3584, 4.3584, 4.3584],
        [4.3584, 4.3636, 4.3588],
        [4.3584, 4.3797, 4.3625],
        [4.3584, 4.5605, 4.5083]], grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.10199031978845596 
model_pd.l_d.mean(): -13.14372730255127 
model_pd.lagr.mean(): -13.041736602783203 
model_pd.lambdas: dict_items([('pout', tensor([1.3227])), ('power', tensor([0.6294]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8350])), ('power', tensor([-22.5979]))])
epoch：323	 i:0 	 global-step:6460	 l-p:0.10199031978845596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.3634, 4.6614, 4.6376],
        [4.3634, 4.3996, 4.3729],
        [4.3634, 4.7303, 4.7396],
        [4.3634, 4.3822, 4.3667]], grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.10185635834932327 
model_pd.l_d.mean(): -13.117414474487305 
model_pd.lagr.mean(): -13.015558242797852 
model_pd.lambdas: dict_items([('pout', tensor([1.3236])), ('power', tensor([0.6282]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8336])), ('power', tensor([-22.5946]))])
epoch：324	 i:0 	 global-step:6480	 l-p:0.10185635834932327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.3684, 5.3178, 5.8410],
        [4.3684, 4.3694, 4.3684],
        [4.3684, 4.4383, 4.3958],
        [4.3684, 4.3686, 4.3684]], grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.10172215104103088 
model_pd.l_d.mean(): -13.091111183166504 
model_pd.lagr.mean(): -12.989389419555664 
model_pd.lambdas: dict_items([('pout', tensor([1.3244])), ('power', tensor([0.6271]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8321])), ('power', tensor([-22.5914]))])
epoch：325	 i:0 	 global-step:6500	 l-p:0.10172215104103088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[4.3734, 4.6720, 4.6480],
        [4.3734, 5.0383, 5.2703],
        [4.3734, 4.3734, 4.3734],
        [4.3734, 4.3735, 4.3734]], grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.10158773511648178 
model_pd.l_d.mean(): -13.06481647491455 
model_pd.lagr.mean(): -12.963229179382324 
model_pd.lambdas: dict_items([('pout', tensor([1.3252])), ('power', tensor([0.6260]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8306])), ('power', tensor([-22.5881]))])
epoch：326	 i:0 	 global-step:6520	 l-p:0.10158773511648178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.3784, 4.3784, 4.3784],
        [4.3784, 4.8816, 4.9785],
        [4.3784, 4.3794, 4.3785],
        [4.3784, 4.4757, 4.4251]], grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.10145313292741776 
model_pd.l_d.mean(): -13.038532257080078 
model_pd.lagr.mean(): -12.937079429626465 
model_pd.lambdas: dict_items([('pout', tensor([1.3261])), ('power', tensor([0.6248]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8291])), ('power', tensor([-22.5848]))])
epoch：327	 i:0 	 global-step:6540	 l-p:0.10145313292741776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.3835, 4.3945, 4.3849],
        [4.3835, 4.3837, 4.3835],
        [4.3835, 4.7160, 4.7068],
        [4.3835, 4.4024, 4.3868]], grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.10131828486919403 
model_pd.l_d.mean(): -13.012256622314453 
model_pd.lagr.mean(): -12.910938262939453 
model_pd.lambdas: dict_items([('pout', tensor([1.3269])), ('power', tensor([0.6237]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8276])), ('power', tensor([-22.5815]))])
epoch：328	 i:0 	 global-step:6560	 l-p:0.10131828486919403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.3886, 4.5916, 4.5390],
        [4.3886, 4.5124, 4.4572],
        [4.3886, 4.4925, 4.4403],
        [4.3886, 4.3915, 4.3888]], grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.10118324309587479 
model_pd.l_d.mean(): -12.985990524291992 
model_pd.lagr.mean(): -12.884807586669922 
model_pd.lambdas: dict_items([('pout', tensor([1.3277])), ('power', tensor([0.6226]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8261])), ('power', tensor([-22.5781]))])
epoch：329	 i:0 	 global-step:6580	 l-p:0.10118324309587479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.3937, 4.5337, 4.4771],
        [4.3937, 5.0623, 5.2957],
        [4.3937, 4.4305, 4.4034],
        [4.3937, 4.5877, 4.5336]], grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.10104795545339584 
model_pd.l_d.mean(): -12.959733963012695 
model_pd.lagr.mean(): -12.858686447143555 
model_pd.lambdas: dict_items([('pout', tensor([1.3285])), ('power', tensor([0.6214]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8246])), ('power', tensor([-22.5748]))])
epoch：330	 i:0 	 global-step:6600	 l-p:0.10104795545339584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.3989, 4.8671, 4.9377],
        [4.3989, 4.3991, 4.3989],
        [4.3989, 4.4178, 4.4022],
        [4.3989, 4.5390, 4.4823]], grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.10091245919466019 
model_pd.l_d.mean(): -12.933485984802246 
model_pd.lagr.mean(): -12.832573890686035 
model_pd.lambdas: dict_items([('pout', tensor([1.3294])), ('power', tensor([0.6203]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8231])), ('power', tensor([-22.5714]))])
epoch：331	 i:0 	 global-step:6620	 l-p:0.10091245919466019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.4040, 4.4042, 4.4040],
        [4.4040, 4.4742, 4.4315],
        [4.4040, 4.4396, 4.4132],
        [4.4040, 5.0733, 5.3064]], grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.10077669471502304 
model_pd.l_d.mean(): -12.907248497009277 
model_pd.lagr.mean(): -12.806471824645996 
model_pd.lambdas: dict_items([('pout', tensor([1.3302])), ('power', tensor([0.6192]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8215])), ('power', tensor([-22.5680]))])
epoch：332	 i:0 	 global-step:6640	 l-p:0.10077669471502304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.4092, 4.8784, 4.9490],
        [4.4092, 4.6036, 4.5494],
        [4.4092, 4.4093, 4.4092],
        [4.4092, 4.4459, 4.4189]], grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.10064062476158142 
model_pd.l_d.mean(): -12.881021499633789 
model_pd.lagr.mean(): -12.780381202697754 
model_pd.lambdas: dict_items([('pout', tensor([1.3310])), ('power', tensor([0.6181]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8200])), ('power', tensor([-22.5646]))])
epoch：333	 i:0 	 global-step:6660	 l-p:0.10064062476158142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.4144, 4.4146, 4.4144],
        [4.4144, 4.4500, 4.4236],
        [4.4144, 5.3739, 5.9019],
        [4.4144, 5.0232, 5.2019]], grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.1005043238401413 
model_pd.l_d.mean(): -12.854804039001465 
model_pd.lagr.mean(): -12.754300117492676 
model_pd.lambdas: dict_items([('pout', tensor([1.3318])), ('power', tensor([0.6169]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8185])), ('power', tensor([-22.5611]))])
epoch：334	 i:0 	 global-step:6680	 l-p:0.1005043238401413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[4.4197, 4.7544, 4.7447],
        [4.4197, 4.6237, 4.5707],
        [4.4197, 4.4900, 4.4472],
        [4.4197, 4.5826, 4.5255]], grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.10036768019199371 
model_pd.l_d.mean(): -12.828593254089355 
model_pd.lagr.mean(): -12.728225708007812 
model_pd.lambdas: dict_items([('pout', tensor([1.3326])), ('power', tensor([0.6158]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8169])), ('power', tensor([-22.5577]))])
epoch：335	 i:0 	 global-step:6700	 l-p:0.10036768019199371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.4249, 4.4249, 4.4249],
        [4.4249, 4.7600, 4.7503],
        [4.4249, 4.4359, 4.4263],
        [4.4249, 4.4253, 4.4249]], grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.10023075342178345 
model_pd.l_d.mean(): -12.80239200592041 
model_pd.lagr.mean(): -12.702160835266113 
model_pd.lambdas: dict_items([('pout', tensor([1.3335])), ('power', tensor([0.6147]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8154])), ('power', tensor([-22.5542]))])
epoch：336	 i:0 	 global-step:6720	 l-p:0.10023075342178345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.4302, 5.0982, 5.3275],
        [4.4302, 4.4302, 4.4302],
        [4.4302, 4.4302, 4.4302],
        [4.4302, 4.4306, 4.4302]], grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.10009351372718811 
model_pd.l_d.mean(): -12.776205062866211 
model_pd.lagr.mean(): -12.676111221313477 
model_pd.lambdas: dict_items([('pout', tensor([1.3343])), ('power', tensor([0.6136]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8138])), ('power', tensor([-22.5507]))])
epoch：337	 i:0 	 global-step:6740	 l-p:0.10009351372718811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.4355, 4.7065, 4.6701],
        [4.4355, 5.1094, 5.3436],
        [4.4355, 4.7374, 4.7126],
        [4.4355, 4.4355, 4.4355]], grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.0999564677476883 
model_pd.l_d.mean(): -12.750024795532227 
model_pd.lagr.mean(): -12.650068283081055 
model_pd.lambdas: dict_items([('pout', tensor([1.3351])), ('power', tensor([0.6124]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8123])), ('power', tensor([-22.5471]))])
epoch：338	 i:0 	 global-step:6760	 l-p:0.0999564677476883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.4408, 4.4408, 4.4408],
        [4.4408, 4.4412, 4.4408],
        [4.4408, 4.9503, 5.0478],
        [4.4408, 4.4408, 4.4408]], grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.09981990605592728 
model_pd.l_d.mean(): -12.723855972290039 
model_pd.lagr.mean(): -12.624035835266113 
model_pd.lambdas: dict_items([('pout', tensor([1.3359])), ('power', tensor([0.6113]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8107])), ('power', tensor([-22.5436]))])
epoch：339	 i:0 	 global-step:6780	 l-p:0.09981990605592728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.4461, 4.4461, 4.4461],
        [4.4461, 5.6304, 6.4186],
        [4.4461, 4.5308, 4.4831],
        [4.4461, 4.4571, 4.4475]], grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.0996839627623558 
model_pd.l_d.mean(): -12.697696685791016 
model_pd.lagr.mean(): -12.598012924194336 
model_pd.lambdas: dict_items([('pout', tensor([1.3367])), ('power', tensor([0.6102]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8092])), ('power', tensor([-22.5401]))])
epoch：340	 i:0 	 global-step:6800	 l-p:0.0996839627623558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.4514, 5.3733, 5.8547],
        [4.4514, 4.7770, 4.7619],
        [4.4514, 4.4518, 4.4514],
        [4.4514, 4.4514, 4.4514]], grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.09954841434955597 
model_pd.l_d.mean(): -12.671547889709473 
model_pd.lagr.mean(): -12.571999549865723 
model_pd.lambdas: dict_items([('pout', tensor([1.3375])), ('power', tensor([0.6090]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8076])), ('power', tensor([-22.5366]))])
epoch：341	 i:0 	 global-step:6820	 l-p:0.09954841434955597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.4567, 4.9305, 5.0012],
        [4.4567, 4.4757, 4.4600],
        [4.4567, 4.4570, 4.4567],
        [4.4567, 4.4577, 4.4567]], grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.0994129404425621 
model_pd.l_d.mean(): -12.645411491394043 
model_pd.lagr.mean(): -12.545998573303223 
model_pd.lambdas: dict_items([('pout', tensor([1.3383])), ('power', tensor([0.6079]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8061])), ('power', tensor([-22.5330]))])
epoch：342	 i:0 	 global-step:6840	 l-p:0.0994129404425621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[4.4620, 4.6582, 4.6032],
        [4.4620, 4.5873, 4.5312],
        [4.4620, 5.1398, 5.3750],
        [4.4620, 5.3666, 5.8278]], grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.09927716106176376 
model_pd.l_d.mean(): -12.619281768798828 
model_pd.lagr.mean(): -12.520004272460938 
model_pd.lambdas: dict_items([('pout', tensor([1.3391])), ('power', tensor([0.6068]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8046])), ('power', tensor([-22.5295]))])
epoch：343	 i:0 	 global-step:6860	 l-p:0.09927716106176376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.4674, 5.5670, 6.2485],
        [4.4674, 5.3927, 5.8757],
        [4.4674, 4.5044, 4.4771],
        [4.4674, 4.7940, 4.7787]], grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.09914056956768036 
model_pd.l_d.mean(): -12.593164443969727 
model_pd.lagr.mean(): -12.494024276733398 
model_pd.lambdas: dict_items([('pout', tensor([1.3399])), ('power', tensor([0.6057]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8030])), ('power', tensor([-22.5259]))])
epoch：344	 i:0 	 global-step:6880	 l-p:0.09914056956768036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.4728, 4.9857, 5.0835],
        [4.4728, 4.4728, 4.4728],
        [4.4728, 4.5780, 4.5250],
        [4.4728, 4.4728, 4.4728]], grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.09900284558534622 
model_pd.l_d.mean(): -12.567054748535156 
model_pd.lagr.mean(): -12.46805191040039 
model_pd.lambdas: dict_items([('pout', tensor([1.3407])), ('power', tensor([0.6045]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8014])), ('power', tensor([-22.5222]))])
epoch：345	 i:0 	 global-step:6900	 l-p:0.09900284558534622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.4783, 5.0235, 5.1454],
        [4.4783, 4.5634, 4.5154],
        [4.4783, 4.5150, 4.4879],
        [4.4783, 5.5807, 6.2639]], grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.09886374324560165 
model_pd.l_d.mean(): -12.540956497192383 
model_pd.lagr.mean(): -12.442092895507812 
model_pd.lambdas: dict_items([('pout', tensor([1.3415])), ('power', tensor([0.6034]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7998])), ('power', tensor([-22.5185]))])
epoch：346	 i:0 	 global-step:6920	 l-p:0.09886374324560165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.4838, 4.6096, 4.5532],
        [4.4838, 4.7010, 4.6490],
        [4.4838, 4.9603, 5.0312],
        [4.4838, 4.5198, 4.4931]], grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.0987231433391571 
model_pd.l_d.mean(): -12.51486587524414 
model_pd.lagr.mean(): -12.416142463684082 
model_pd.lambdas: dict_items([('pout', tensor([1.3423])), ('power', tensor([0.6023]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7982])), ('power', tensor([-22.5148]))])
epoch：347	 i:0 	 global-step:6940	 l-p:0.0987231433391571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[4.4895, 4.4895, 4.4895],
        [4.4895, 5.1662, 5.3979],
        [4.4895, 4.4895, 4.4895],
        [4.4895, 4.8406, 4.8360]], grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.09858117997646332 
model_pd.l_d.mean(): -12.488784790039062 
model_pd.lagr.mean(): -12.390203475952148 
model_pd.lambdas: dict_items([('pout', tensor([1.3431])), ('power', tensor([0.6012]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7966])), ('power', tensor([-22.5110]))])
epoch：348	 i:0 	 global-step:6960	 l-p:0.09858117997646332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.4951, 4.5325, 4.5050],
        [4.4951, 4.4962, 4.4952],
        [4.4951, 4.5168, 4.4992],
        [4.4951, 4.9728, 5.0438]], grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.09843799471855164 
model_pd.l_d.mean(): -12.462713241577148 
model_pd.lagr.mean(): -12.364274978637695 
model_pd.lambdas: dict_items([('pout', tensor([1.3439])), ('power', tensor([0.6000]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7949])), ('power', tensor([-22.5071]))])
epoch：349	 i:0 	 global-step:6980	 l-p:0.09843799471855164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[4.5009, 4.5010, 4.5009],
        [4.5009, 4.5011, 4.5009],
        [4.5009, 4.5009, 4.5009],
        [4.5009, 4.7753, 4.7381]], grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.09829387813806534 
model_pd.l_d.mean(): -12.436650276184082 
model_pd.lagr.mean(): -12.338356018066406 
model_pd.lambdas: dict_items([('pout', tensor([1.3447])), ('power', tensor([0.5989]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7933])), ('power', tensor([-22.5032]))])
epoch：350	 i:0 	 global-step:7000	 l-p:0.09829387813806534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.5066, 4.8590, 4.8543],
        [4.5066, 4.7140, 4.6597],
        [4.5066, 4.5441, 4.5165],
        [4.5066, 4.5070, 4.5067]], grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.09814915806055069 
model_pd.l_d.mean(): -12.410600662231445 
model_pd.lagr.mean(): -12.312451362609863 
model_pd.lambdas: dict_items([('pout', tensor([1.3455])), ('power', tensor([0.5978]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7916])), ('power', tensor([-22.4993]))])
epoch：351	 i:0 	 global-step:7020	 l-p:0.09814915806055069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[4.5124, 4.5236, 4.5139],
        [4.5124, 4.5341, 4.5165],
        [4.5124, 4.5124, 4.5124],
        [4.5124, 4.8534, 4.8428]], grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.09800411015748978 
model_pd.l_d.mean(): -12.38455867767334 
model_pd.lagr.mean(): -12.286554336547852 
model_pd.lambdas: dict_items([('pout', tensor([1.3463])), ('power', tensor([0.5967]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7899])), ('power', tensor([-22.4954]))])
epoch：352	 i:0 	 global-step:7040	 l-p:0.09800411015748978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[4.5183, 4.5183, 4.5183],
        [4.5183, 4.5183, 4.5183],
        [4.5183, 5.6315, 6.3207],
        [4.5183, 4.5399, 4.5224]], grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.09785891324281693 
model_pd.l_d.mean(): -12.35853099822998 
model_pd.lagr.mean(): -12.260671615600586 
model_pd.lambdas: dict_items([('pout', tensor([1.3471])), ('power', tensor([0.5955]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7883])), ('power', tensor([-22.4914]))])
epoch：353	 i:0 	 global-step:7060	 l-p:0.09785891324281693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[4.5241, 4.5242, 4.5241],
        [4.5241, 4.5352, 4.5255],
        [4.5241, 4.6507, 4.5939],
        [4.5241, 4.5241, 4.5241]], grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.09771385043859482 
model_pd.l_d.mean(): -12.332513809204102 
model_pd.lagr.mean(): -12.234800338745117 
model_pd.lambdas: dict_items([('pout', tensor([1.3479])), ('power', tensor([0.5944]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7866])), ('power', tensor([-22.4875]))])
epoch：354	 i:0 	 global-step:7080	 l-p:0.09771385043859482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[4.5299, 5.0493, 5.1478],
        [4.5299, 4.7490, 4.6964],
        [4.5299, 4.6362, 4.5826],
        [4.5299, 4.7382, 4.6836]], grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.0975690558552742 
model_pd.l_d.mean(): -12.306507110595703 
model_pd.lagr.mean(): -12.208937644958496 
model_pd.lambdas: dict_items([('pout', tensor([1.3487])), ('power', tensor([0.5933]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7849])), ('power', tensor([-22.4835]))])
epoch：355	 i:0 	 global-step:7100	 l-p:0.0975690558552742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[4.5357, 4.6792, 4.6208],
        [4.5357, 4.7442, 4.6895],
        [4.5357, 4.6217, 4.5732],
        [4.5357, 4.8437, 4.8178]], grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.0974244475364685 
model_pd.l_d.mean(): -12.280511856079102 
model_pd.lagr.mean(): -12.183087348937988 
model_pd.lambdas: dict_items([('pout', tensor([1.3494])), ('power', tensor([0.5922]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7832])), ('power', tensor([-22.4795]))])
epoch：356	 i:0 	 global-step:7120	 l-p:0.0974244475364685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.5416, 4.5418, 4.5416],
        [4.5416, 4.5417, 4.5416],
        [4.5416, 5.2317, 5.4703],
        [4.5416, 4.5445, 4.5417]], grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.09727984666824341 
model_pd.l_d.mean(): -12.254528999328613 
model_pd.lagr.mean(): -12.157249450683594 
model_pd.lambdas: dict_items([('pout', tensor([1.3502])), ('power', tensor([0.5910]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7816])), ('power', tensor([-22.4755]))])
epoch：357	 i:0 	 global-step:7140	 l-p:0.09727984666824341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[4.5474, 5.5375, 6.0805],
        [4.5474, 4.7141, 4.6553],
        [4.5474, 4.7673, 4.7144],
        [4.5474, 4.7564, 4.7015]], grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.0971350446343422 
model_pd.l_d.mean(): -12.228554725646973 
model_pd.lagr.mean(): -12.131420135498047 
model_pd.lambdas: dict_items([('pout', tensor([1.3510])), ('power', tensor([0.5899]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7799])), ('power', tensor([-22.4715]))])
epoch：358	 i:0 	 global-step:7160	 l-p:0.0971350446343422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.5533, 4.5910, 4.5632],
        [4.5533, 4.7735, 4.7204],
        [4.5533, 4.5533, 4.5533],
        [4.5533, 4.6252, 4.5813]], grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.0969897210597992 
model_pd.l_d.mean(): -12.202593803405762 
model_pd.lagr.mean(): -12.10560417175293 
model_pd.lambdas: dict_items([('pout', tensor([1.3518])), ('power', tensor([0.5888]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7782])), ('power', tensor([-22.4675]))])
epoch：359	 i:0 	 global-step:7180	 l-p:0.0969897210597992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.5592, 5.0820, 5.1808],
        [4.5592, 5.4848, 5.9556],
        [4.5592, 4.8369, 4.7989],
        [4.5592, 4.5705, 4.5607]], grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.09684358537197113 
model_pd.l_d.mean(): -12.176642417907715 
model_pd.lagr.mean(): -12.079798698425293 
model_pd.lambdas: dict_items([('pout', tensor([1.3526])), ('power', tensor([0.5877]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7765])), ('power', tensor([-22.4634]))])
epoch：360	 i:0 	 global-step:7200	 l-p:0.09684358537197113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.5652, 5.1211, 5.2445],
        [4.5652, 5.5123, 6.0054],
        [4.5652, 4.5652, 4.5652],
        [4.5652, 5.2591, 5.4988]], grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.0966964140534401 
model_pd.l_d.mean(): -12.150700569152832 
model_pd.lagr.mean(): -12.054003715515137 
model_pd.lambdas: dict_items([('pout', tensor([1.3533])), ('power', tensor([0.5865]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7748])), ('power', tensor([-22.4593]))])
epoch：361	 i:0 	 global-step:7220	 l-p:0.0966964140534401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.5712, 4.7387, 4.6795],
        [4.5712, 4.6434, 4.5993],
        [4.5712, 4.5716, 4.5713],
        [4.5712, 5.4995, 5.9715]], grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.09654808044433594 
model_pd.l_d.mean(): -12.124771118164062 
model_pd.lagr.mean(): -12.028223037719727 
model_pd.lambdas: dict_items([('pout', tensor([1.3541])), ('power', tensor([0.5854]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7731])), ('power', tensor([-22.4551]))])
epoch：362	 i:0 	 global-step:7240	 l-p:0.09654808044433594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.5773, 4.5774, 4.5773],
        [4.5773, 5.5746, 6.1211],
        [4.5773, 4.6150, 4.5872],
        [4.5773, 5.0636, 5.1352]], grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.09639839828014374 
model_pd.l_d.mean(): -12.098852157592773 
model_pd.lagr.mean(): -12.002453804016113 
model_pd.lambdas: dict_items([('pout', tensor([1.3549])), ('power', tensor([0.5843]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7713])), ('power', tensor([-22.4509]))])
epoch：363	 i:0 	 global-step:7260	 l-p:0.09639839828014374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.5835, 5.2750, 5.5107],
        [4.5835, 4.6212, 4.5934],
        [4.5835, 4.5876, 4.5838],
        [4.5835, 5.8082, 6.6212]], grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.09624753892421722 
model_pd.l_d.mean(): -12.072940826416016 
model_pd.lagr.mean(): -11.976693153381348 
model_pd.lambdas: dict_items([('pout', tensor([1.3556])), ('power', tensor([0.5832]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7696])), ('power', tensor([-22.4467]))])
epoch：364	 i:0 	 global-step:7280	 l-p:0.09624753892421722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.5897, 5.2884, 5.5300],
        [4.5897, 4.5897, 4.5897],
        [4.5897, 4.7346, 4.6755],
        [4.5897, 4.5897, 4.5897]], grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.09609553962945938 
model_pd.l_d.mean(): -12.047042846679688 
model_pd.lagr.mean(): -11.950947761535645 
model_pd.lambdas: dict_items([('pout', tensor([1.3564])), ('power', tensor([0.5821]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7678])), ('power', tensor([-22.4424]))])
epoch：365	 i:0 	 global-step:7300	 l-p:0.09609553962945938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[4.5959, 4.6000, 4.5962],
        [4.5959, 4.6013, 4.5964],
        [4.5959, 4.7642, 4.7047],
        [4.5959, 5.2956, 5.5376]], grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.09594260156154633 
model_pd.l_d.mean(): -12.021154403686523 
model_pd.lagr.mean(): -11.925211906433105 
model_pd.lambdas: dict_items([('pout', tensor([1.3572])), ('power', tensor([0.5809]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7660])), ('power', tensor([-22.4380]))])
epoch：366	 i:0 	 global-step:7320	 l-p:0.09594260156154633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.6022, 4.6026, 4.6022],
        [4.6022, 4.6397, 4.6120],
        [4.6022, 4.7308, 4.6729],
        [4.6022, 4.8039, 4.7468]], grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.09578884392976761 
model_pd.l_d.mean(): -11.995280265808105 
model_pd.lagr.mean(): -11.899491310119629 
model_pd.lambdas: dict_items([('pout', tensor([1.3579])), ('power', tensor([0.5798]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7643])), ('power', tensor([-22.4337]))])
epoch：367	 i:0 	 global-step:7340	 l-p:0.09578884392976761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[4.6085, 4.7097, 4.6567],
        [4.6085, 4.9565, 4.9451],
        [4.6085, 4.8312, 4.7774],
        [4.6085, 4.8105, 4.7533]], grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.09563441574573517 
model_pd.l_d.mean(): -11.969414710998535 
model_pd.lagr.mean(): -11.873780250549316 
model_pd.lambdas: dict_items([('pout', tensor([1.3587])), ('power', tensor([0.5787]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7625])), ('power', tensor([-22.4293]))])
epoch：368	 i:0 	 global-step:7360	 l-p:0.09563441574573517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[4.6149, 5.1053, 5.1773],
        [4.6149, 4.8378, 4.7839],
        [4.6149, 5.2516, 5.4364],
        [4.6149, 4.7021, 4.6528]], grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.09547942876815796 
model_pd.l_d.mean(): -11.943561553955078 
model_pd.lagr.mean(): -11.848082542419434 
model_pd.lambdas: dict_items([('pout', tensor([1.3595])), ('power', tensor([0.5776]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7607])), ('power', tensor([-22.4249]))])
epoch：369	 i:0 	 global-step:7380	 l-p:0.09547942876815796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.6213, 4.6326, 4.6227],
        [4.6213, 4.7503, 4.6922],
        [4.6213, 5.8575, 6.6777],
        [4.6213, 4.6589, 4.6311]], grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.09532387554645538 
model_pd.l_d.mean(): -11.917720794677734 
model_pd.lagr.mean(): -11.822397232055664 
model_pd.lambdas: dict_items([('pout', tensor([1.3602])), ('power', tensor([0.5764]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7589])), ('power', tensor([-22.4204]))])
epoch：370	 i:0 	 global-step:7400	 l-p:0.09532387554645538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[4.6277, 4.7737, 4.7141],
        [4.6277, 4.8401, 4.7841],
        [4.6277, 5.2663, 5.4516],
        [4.6277, 4.7971, 4.7371]], grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.09516774117946625 
model_pd.l_d.mean(): -11.891890525817871 
model_pd.lagr.mean(): -11.796722412109375 
model_pd.lambdas: dict_items([('pout', tensor([1.3610])), ('power', tensor([0.5753]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7571])), ('power', tensor([-22.4159]))])
epoch：371	 i:0 	 global-step:7420	 l-p:0.09516774117946625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.6341, 4.6345, 4.6341],
        [4.6341, 4.6341, 4.6341],
        [4.6341, 4.6382, 4.6344],
        [4.6341, 4.6341, 4.6341]], grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.0950108990073204 
model_pd.l_d.mean(): -11.866071701049805 
model_pd.lagr.mean(): -11.771060943603516 
model_pd.lambdas: dict_items([('pout', tensor([1.3617])), ('power', tensor([0.5742]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7552])), ('power', tensor([-22.4114]))])
epoch：372	 i:0 	 global-step:7440	 l-p:0.0950108990073204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.6406, 4.6410, 4.6406],
        [4.6406, 4.8647, 4.8104],
        [4.6406, 4.7701, 4.7118],
        [4.6406, 5.2061, 5.3311]], grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.0948532372713089 
model_pd.l_d.mean(): -11.840266227722168 
model_pd.lagr.mean(): -11.745412826538086 
model_pd.lambdas: dict_items([('pout', tensor([1.3625])), ('power', tensor([0.5731]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7534])), ('power', tensor([-22.4069]))])
epoch：373	 i:0 	 global-step:7460	 l-p:0.0948532372713089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.6471, 4.9300, 4.8909],
        [4.6471, 4.6475, 4.6471],
        [4.6471, 4.6585, 4.6485],
        [4.6471, 5.7959, 6.5059]], grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.09469454735517502 
model_pd.l_d.mean(): -11.814472198486328 
model_pd.lagr.mean(): -11.719778060913086 
model_pd.lambdas: dict_items([('pout', tensor([1.3632])), ('power', tensor([0.5720]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7516])), ('power', tensor([-22.4023]))])
epoch：374	 i:0 	 global-step:7480	 l-p:0.09469454735517502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.6537, 5.0051, 4.9934],
        [4.6537, 4.6591, 4.6541],
        [4.6537, 5.2209, 5.3462],
        [4.6537, 4.6733, 4.6571]], grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.09453469514846802 
model_pd.l_d.mean(): -11.788690567016602 
model_pd.lagr.mean(): -11.6941556930542 
model_pd.lambdas: dict_items([('pout', tensor([1.3640])), ('power', tensor([0.5708]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7497])), ('power', tensor([-22.3977]))])
epoch：375	 i:0 	 global-step:7500	 l-p:0.09453469514846802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.6603, 4.9440, 4.9047],
        [4.6603, 4.6603, 4.6603],
        [4.6603, 4.8308, 4.7703],
        [4.6603, 5.9086, 6.7364]], grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.09437352418899536 
model_pd.l_d.mean(): -11.762918472290039 
model_pd.lagr.mean(): -11.66854476928711 
model_pd.lambdas: dict_items([('pout', tensor([1.3647])), ('power', tensor([0.5697]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7479])), ('power', tensor([-22.3931]))])
epoch：376	 i:0 	 global-step:7520	 l-p:0.09437352418899536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[4.6670, 4.8924, 4.8377],
        [4.6670, 5.0318, 5.0259],
        [4.6670, 5.3776, 5.6224],
        [4.6670, 4.6711, 4.6673]], grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.09421085566282272 
model_pd.l_d.mean(): -11.737157821655273 
model_pd.lagr.mean(): -11.64294719696045 
model_pd.lambdas: dict_items([('pout', tensor([1.3655])), ('power', tensor([0.5686]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7460])), ('power', tensor([-22.3883]))])
epoch：377	 i:0 	 global-step:7540	 l-p:0.09421085566282272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.6737, 4.9910, 4.9635],
        [4.6737, 4.6738, 4.6737],
        [4.6737, 4.7619, 4.7120],
        [4.6737, 4.8784, 4.8203]], grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.09404663741588593 
model_pd.l_d.mean(): -11.711407661437988 
model_pd.lagr.mean(): -11.617361068725586 
model_pd.lambdas: dict_items([('pout', tensor([1.3662])), ('power', tensor([0.5675]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7441])), ('power', tensor([-22.3836]))])
epoch：378	 i:0 	 global-step:7560	 l-p:0.09404663741588593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[4.6805, 5.9351, 6.7670],
        [4.6805, 4.9066, 4.8517],
        [4.6805, 5.3271, 5.5144],
        [4.6805, 5.7032, 6.2628]], grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.09388093650341034 
model_pd.l_d.mean(): -11.685670852661133 
model_pd.lagr.mean(): -11.591790199279785 
model_pd.lambdas: dict_items([('pout', tensor([1.3670])), ('power', tensor([0.5664]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7422])), ('power', tensor([-22.3788]))])
epoch：379	 i:0 	 global-step:7580	 l-p:0.09388093650341034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[4.6873, 4.7246, 4.6969],
        [4.6873, 5.0799, 5.0873],
        [4.6873, 5.3960, 5.6369],
        [4.6873, 4.7611, 4.7160]], grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.093714639544487 
model_pd.l_d.mean(): -11.659945487976074 
model_pd.lagr.mean(): -11.566230773925781 
model_pd.lambdas: dict_items([('pout', tensor([1.3677])), ('power', tensor([0.5653]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7403])), ('power', tensor([-22.3740]))])
epoch：380	 i:0 	 global-step:7600	 l-p:0.093714639544487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.6941, 4.6983, 4.6944],
        [4.6941, 5.2334, 5.3344],
        [4.6941, 4.6941, 4.6941],
        [4.6941, 4.6941, 4.6941]], grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.09354839473962784 
model_pd.l_d.mean(): -11.634234428405762 
model_pd.lagr.mean(): -11.540685653686523 
model_pd.lambdas: dict_items([('pout', tensor([1.3684])), ('power', tensor([0.5641]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7384])), ('power', tensor([-22.3691]))])
epoch：381	 i:0 	 global-step:7620	 l-p:0.09354839473962784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.7009, 5.0562, 5.0442],
        [4.7009, 4.7039, 4.7011],
        [4.7009, 5.0444, 5.0270],
        [4.7009, 4.9069, 4.8484]], grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.09338225424289703 
model_pd.l_d.mean(): -11.608534812927246 
model_pd.lagr.mean(): -11.515152931213379 
model_pd.lambdas: dict_items([('pout', tensor([1.3692])), ('power', tensor([0.5630]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7365])), ('power', tensor([-22.3643]))])
epoch：382	 i:0 	 global-step:7640	 l-p:0.09338225424289703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[4.7077, 5.4263, 5.6741],
        [4.7077, 4.7119, 4.7080],
        [4.7077, 4.8801, 4.8189],
        [4.7077, 4.9239, 4.8667]], grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.09321584552526474 
model_pd.l_d.mean(): -11.582846641540527 
model_pd.lagr.mean(): -11.489630699157715 
model_pd.lambdas: dict_items([('pout', tensor([1.3699])), ('power', tensor([0.5619]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7346])), ('power', tensor([-22.3594]))])
epoch：383	 i:0 	 global-step:7660	 l-p:0.09321584552526474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.7145, 4.7532, 4.7246],
        [4.7145, 4.9212, 4.8625],
        [4.7145, 5.0592, 5.0417],
        [4.7145, 4.7156, 4.7146]], grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.09304855018854141 
model_pd.l_d.mean(): -11.557172775268555 
model_pd.lagr.mean(): -11.46412467956543 
model_pd.lambdas: dict_items([('pout', tensor([1.3707])), ('power', tensor([0.5608]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7327])), ('power', tensor([-22.3546]))])
epoch：384	 i:0 	 global-step:7680	 l-p:0.09304855018854141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.7214, 4.7414, 4.7249],
        [4.7214, 4.7216, 4.7214],
        [4.7214, 4.7214, 4.7214],
        [4.7214, 4.8533, 4.7938]], grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.0928795263171196 
model_pd.l_d.mean(): -11.531513214111328 
model_pd.lagr.mean(): -11.438633918762207 
model_pd.lambdas: dict_items([('pout', tensor([1.3714])), ('power', tensor([0.5597]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7308])), ('power', tensor([-22.3497]))])
epoch：385	 i:0 	 global-step:7700	 l-p:0.0928795263171196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[4.7284, 5.0167, 4.9767],
        [4.7284, 4.8777, 4.8166],
        [4.7284, 4.9358, 4.8768],
        [4.7284, 4.7286, 4.7284]], grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.0927080363035202 
model_pd.l_d.mean(): -11.505860328674316 
model_pd.lagr.mean(): -11.413152694702148 
model_pd.lambdas: dict_items([('pout', tensor([1.3721])), ('power', tensor([0.5585]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7289])), ('power', tensor([-22.3447]))])
epoch：386	 i:0 	 global-step:7720	 l-p:0.0927080363035202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.7355, 4.7357, 4.7355],
        [4.7355, 4.7365, 4.7355],
        [4.7355, 4.7355, 4.7355],
        [4.7355, 5.3909, 5.5805]], grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.09253337979316711 
model_pd.l_d.mean(): -11.48022174835205 
model_pd.lagr.mean(): -11.387688636779785 
model_pd.lambdas: dict_items([('pout', tensor([1.3728])), ('power', tensor([0.5574]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7269])), ('power', tensor([-22.3396]))])
epoch：387	 i:0 	 global-step:7740	 l-p:0.09253337979316711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.7427, 5.2482, 5.3218],
        [4.7427, 4.7427, 4.7427],
        [4.7427, 4.7431, 4.7427],
        [4.7427, 4.7427, 4.7427]], grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.09235507994890213 
model_pd.l_d.mean(): -11.45459270477295 
model_pd.lagr.mean(): -11.362237930297852 
model_pd.lambdas: dict_items([('pout', tensor([1.3736])), ('power', tensor([0.5563]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7249])), ('power', tensor([-22.3344]))])
epoch：388	 i:0 	 global-step:7760	 l-p:0.09235507994890213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[4.7500, 6.0268, 6.8731],
        [4.7500, 4.7502, 4.7500],
        [4.7500, 5.7907, 6.3599],
        [4.7500, 4.9800, 4.9241]], grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.09217292815446854 
model_pd.l_d.mean(): -11.428972244262695 
model_pd.lagr.mean(): -11.336799621582031 
model_pd.lambdas: dict_items([('pout', tensor([1.3743])), ('power', tensor([0.5552]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7229])), ('power', tensor([-22.3291]))])
epoch：389	 i:0 	 global-step:7780	 l-p:0.09217292815446854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[4.7575, 4.9320, 4.8700],
        [4.7575, 5.1570, 5.1644],
        [4.7575, 4.7575, 4.7575],
        [4.7575, 4.7803, 4.7618]], grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.09198696166276932 
model_pd.l_d.mean(): -11.403363227844238 
model_pd.lagr.mean(): -11.311376571655273 
model_pd.lambdas: dict_items([('pout', tensor([1.3750])), ('power', tensor([0.5541]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7208])), ('power', tensor([-22.3237]))])
epoch：390	 i:0 	 global-step:7800	 l-p:0.09198696166276932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.7651, 4.7652, 4.7652],
        [4.7651, 4.8043, 4.7754],
        [4.7651, 4.7694, 4.7655],
        [4.7651, 4.8768, 4.8203]], grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.09179728478193283 
model_pd.l_d.mean(): -11.377768516540527 
model_pd.lagr.mean(): -11.285971641540527 
model_pd.lambdas: dict_items([('pout', tensor([1.3757])), ('power', tensor([0.5530]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7187])), ('power', tensor([-22.3182]))])
epoch：391	 i:0 	 global-step:7820	 l-p:0.09179728478193283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.7729, 5.7484, 6.2432],
        [4.7729, 4.9926, 4.9344],
        [4.7729, 4.7739, 4.7729],
        [4.7729, 5.1226, 5.1048]], grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.0916043072938919 
model_pd.l_d.mean(): -11.352182388305664 
model_pd.lagr.mean(): -11.260578155517578 
model_pd.lambdas: dict_items([('pout', tensor([1.3764])), ('power', tensor([0.5518]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7166])), ('power', tensor([-22.3126]))])
epoch：392	 i:0 	 global-step:7840	 l-p:0.0916043072938919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[4.7807, 5.3659, 5.4948],
        [4.7807, 5.3317, 5.4349],
        [4.7807, 5.5118, 5.7632],
        [4.7807, 4.7863, 4.7811]], grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.09140834212303162 
model_pd.l_d.mean(): -11.32660961151123 
model_pd.lagr.mean(): -11.235200881958008 
model_pd.lambdas: dict_items([('pout', tensor([1.3772])), ('power', tensor([0.5507]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7145])), ('power', tensor([-22.3069]))])
epoch：393	 i:0 	 global-step:7860	 l-p:0.09140834212303162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.7886, 4.9991, 4.9393],
        [4.7886, 4.7886, 4.7886],
        [4.7886, 5.1644, 5.1582],
        [4.7886, 5.5155, 5.7624]], grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.09120974689722061 
model_pd.l_d.mean(): -11.301047325134277 
model_pd.lagr.mean(): -11.209837913513184 
model_pd.lambdas: dict_items([('pout', tensor([1.3779])), ('power', tensor([0.5496]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7123])), ('power', tensor([-22.3012]))])
epoch：394	 i:0 	 global-step:7880	 l-p:0.09120974689722061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.7965, 4.7965, 4.7965],
        [4.7965, 5.0898, 5.0490],
        [4.7965, 4.7965, 4.7965],
        [4.7965, 5.3842, 5.5136]], grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.09100862592458725 
model_pd.l_d.mean(): -11.275501251220703 
model_pd.lagr.mean(): -11.184493064880371 
model_pd.lambdas: dict_items([('pout', tensor([1.3786])), ('power', tensor([0.5485]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7101])), ('power', tensor([-22.2954]))])
epoch：395	 i:0 	 global-step:7900	 l-p:0.09100862592458725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.8046, 5.1819, 5.1757],
        [4.8046, 4.8957, 4.8441],
        [4.8046, 4.8441, 4.8149],
        [4.8046, 6.0993, 6.9574]], grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.09080508351325989 
model_pd.l_d.mean(): -11.249968528747559 
model_pd.lagr.mean(): -11.159163475036621 
model_pd.lambdas: dict_items([('pout', tensor([1.3793])), ('power', tensor([0.5474]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7079])), ('power', tensor([-22.2896]))])
epoch：396	 i:0 	 global-step:7920	 l-p:0.09080508351325989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.8127, 4.8127, 4.8127],
        [4.8127, 5.8197, 6.3427],
        [4.8127, 5.4811, 5.6744],
        [4.8127, 4.8127, 4.8127]], grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.09059889614582062 
model_pd.l_d.mean(): -11.224444389343262 
model_pd.lagr.mean(): -11.133845329284668 
model_pd.lambdas: dict_items([('pout', tensor([1.3800])), ('power', tensor([0.5463]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7057])), ('power', tensor([-22.2837]))])
epoch：397	 i:0 	 global-step:7940	 l-p:0.09059889614582062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.8208, 5.5603, 5.8152],
        [4.8208, 6.0212, 6.7623],
        [4.8208, 4.8608, 4.8313],
        [4.8208, 5.8084, 6.3093]], grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.09038982540369034 
model_pd.l_d.mean(): -11.198938369750977 
model_pd.lagr.mean(): -11.108548164367676 
model_pd.lambdas: dict_items([('pout', tensor([1.3807])), ('power', tensor([0.5452]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7035])), ('power', tensor([-22.2777]))])
epoch：398	 i:0 	 global-step:7960	 l-p:0.09038982540369034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.8291, 5.5636, 5.8130],
        [4.8291, 4.8322, 4.8293],
        [4.8291, 4.8293, 4.8291],
        [4.8291, 6.0320, 6.7747]], grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.09017739444971085 
model_pd.l_d.mean(): -11.173443794250488 
model_pd.lagr.mean(): -11.083266258239746 
model_pd.lambdas: dict_items([('pout', tensor([1.3814])), ('power', tensor([0.5440]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7012])), ('power', tensor([-22.2717]))])
epoch：399	 i:0 	 global-step:7980	 l-p:0.09017739444971085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.8374, 4.8493, 4.8389],
        [4.8374, 4.8775, 4.8479],
        [4.8374, 4.8374, 4.8374],
        [4.8374, 4.8761, 4.8473]], grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.089960977435112 
model_pd.l_d.mean(): -11.147961616516113 
model_pd.lagr.mean(): -11.058000564575195 
model_pd.lambdas: dict_items([('pout', tensor([1.3821])), ('power', tensor([0.5429]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6990])), ('power', tensor([-22.2656]))])
epoch：400	 i:0 	 global-step:8000	 l-p:0.089960977435112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.8459, 4.8470, 4.8459],
        [4.8459, 4.8578, 4.8474],
        [4.8459, 4.8459, 4.8459],
        [4.8459, 4.9528, 4.8967]], grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.08973989635705948 
model_pd.l_d.mean(): -11.122491836547852 
model_pd.lagr.mean(): -11.03275203704834 
model_pd.lambdas: dict_items([('pout', tensor([1.3828])), ('power', tensor([0.5418]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6967])), ('power', tensor([-22.2594]))])
epoch：401	 i:0 	 global-step:8020	 l-p:0.08973989635705948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.8545, 4.8602, 4.8550],
        [4.8545, 5.0334, 4.9699],
        [4.8545, 4.8547, 4.8545],
        [4.8545, 5.0687, 5.0078]], grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.08951348066329956 
model_pd.l_d.mean(): -11.09703254699707 
model_pd.lagr.mean(): -11.007518768310547 
model_pd.lambdas: dict_items([('pout', tensor([1.3835])), ('power', tensor([0.5407]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6943])), ('power', tensor([-22.2530]))])
epoch：402	 i:0 	 global-step:8040	 l-p:0.08951348066329956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.8633, 5.5405, 5.7365],
        [4.8633, 4.8644, 4.8634],
        [4.8633, 4.8690, 4.8638],
        [4.8633, 4.8633, 4.8633]], grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.08928097039461136 
model_pd.l_d.mean(): -11.071586608886719 
model_pd.lagr.mean(): -10.982305526733398 
model_pd.lambdas: dict_items([('pout', tensor([1.3842])), ('power', tensor([0.5396]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6919])), ('power', tensor([-22.2466]))])
epoch：403	 i:0 	 global-step:8060	 l-p:0.08928097039461136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[4.8723, 5.4715, 5.6035],
        [4.8723, 4.8723, 4.8723],
        [4.8723, 4.9126, 4.8828],
        [4.8723, 5.0977, 5.0380]], grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.08904168754816055 
model_pd.l_d.mean(): -11.046151161193848 
model_pd.lagr.mean(): -10.957109451293945 
model_pd.lambdas: dict_items([('pout', tensor([1.3849])), ('power', tensor([0.5385]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6895])), ('power', tensor([-22.2400]))])
epoch：404	 i:0 	 global-step:8080	 l-p:0.08904168754816055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.8815, 4.9894, 4.9327],
        [4.8815, 5.6260, 5.8789],
        [4.8815, 4.9966, 4.9383],
        [4.8815, 4.8815, 4.8815]], grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.08879495412111282 
model_pd.l_d.mean(): -11.020726203918457 
model_pd.lagr.mean(): -10.931931495666504 
model_pd.lambdas: dict_items([('pout', tensor([1.3856])), ('power', tensor([0.5374]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6870])), ('power', tensor([-22.2332]))])
epoch：405	 i:0 	 global-step:8100	 l-p:0.08879495412111282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[4.8909, 5.2514, 5.2330],
        [4.8909, 4.8953, 4.8912],
        [4.8909, 4.8909, 4.8909],
        [4.8909, 5.6439, 5.9035]], grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.08854003995656967 
model_pd.l_d.mean(): -10.99531078338623 
model_pd.lagr.mean(): -10.906770706176758 
model_pd.lambdas: dict_items([('pout', tensor([1.3862])), ('power', tensor([0.5363]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6845])), ('power', tensor([-22.2262]))])
epoch：406	 i:0 	 global-step:8120	 l-p:0.08854003995656967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.9005, 4.9005, 4.9005],
        [4.9005, 5.0162, 4.9576],
        [4.9005, 4.9007, 4.9005],
        [4.9005, 5.2619, 5.2436]], grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.08827625960111618 
model_pd.l_d.mean(): -10.9699068069458 
model_pd.lagr.mean(): -10.881630897521973 
model_pd.lambdas: dict_items([('pout', tensor([1.3869])), ('power', tensor([0.5351]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6819])), ('power', tensor([-22.2191]))])
epoch：407	 i:0 	 global-step:8140	 l-p:0.08827625960111618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228]])
 pt:tensor([[4.9104, 5.2983, 5.2919],
        [4.9104, 5.5156, 5.6489],
        [4.9104, 5.2727, 5.2543],
        [4.9104, 5.3259, 5.3336]], grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.08800279349088669 
model_pd.l_d.mean(): -10.944511413574219 
model_pd.lagr.mean(): -10.856508255004883 
model_pd.lambdas: dict_items([('pout', tensor([1.3876])), ('power', tensor([0.5340]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6793])), ('power', tensor([-22.2118]))])
epoch：408	 i:0 	 global-step:8160	 l-p:0.08800279349088669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.9205, 5.5273, 5.6611],
        [4.9205, 5.9559, 6.4938],
        [4.9205, 5.0774, 5.0132],
        [4.9205, 4.9609, 4.9310]], grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.08771869540214539 
model_pd.l_d.mean(): -10.919126510620117 
model_pd.lagr.mean(): -10.83140754699707 
model_pd.lambdas: dict_items([('pout', tensor([1.3883])), ('power', tensor([0.5329]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6766])), ('power', tensor([-22.2043]))])
epoch：409	 i:0 	 global-step:8180	 l-p:0.08771869540214539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[4.9309, 5.0883, 5.0239],
        [4.9309, 5.0476, 4.9885],
        [4.9309, 5.5394, 5.6735],
        [4.9309, 5.1494, 5.0874]], grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.08742273598909378 
model_pd.l_d.mean(): -10.89375114440918 
model_pd.lagr.mean(): -10.806328773498535 
model_pd.lambdas: dict_items([('pout', tensor([1.3890])), ('power', tensor([0.5318]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6738])), ('power', tensor([-22.1965]))])
epoch：410	 i:0 	 global-step:8200	 l-p:0.08742273598909378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[4.9416, 5.0204, 4.9722],
        [4.9416, 4.9461, 4.9420],
        [4.9416, 5.7047, 5.9679],
        [4.9416, 5.1608, 5.0986]], grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.08711346983909607 
model_pd.l_d.mean(): -10.868383407592773 
model_pd.lagr.mean(): -10.781270027160645 
model_pd.lambdas: dict_items([('pout', tensor([1.3896])), ('power', tensor([0.5307]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6709])), ('power', tensor([-22.1885]))])
epoch：411	 i:0 	 global-step:8220	 l-p:0.08711346983909607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.9527, 4.9926, 4.9630],
        [4.9527, 6.1945, 6.9616],
        [4.9527, 5.7180, 5.9820],
        [4.9527, 5.2934, 5.2639]], grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.08678898215293884 
model_pd.l_d.mean(): -10.843023300170898 
model_pd.lagr.mean(): -10.756234169006348 
model_pd.lambdas: dict_items([('pout', tensor([1.3903])), ('power', tensor([0.5296]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6679])), ('power', tensor([-22.1802]))])
epoch：412	 i:0 	 global-step:8240	 l-p:0.08678898215293884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.9642, 4.9687, 4.9645],
        [4.9642, 6.3131, 7.2075],
        [4.9642, 5.3445, 5.3317],
        [4.9642, 5.3059, 5.2764]], grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.08644727617502213 
model_pd.l_d.mean(): -10.817669868469238 
model_pd.lagr.mean(): -10.731222152709961 
model_pd.lambdas: dict_items([('pout', tensor([1.3910])), ('power', tensor([0.5285]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6649])), ('power', tensor([-22.1716]))])
epoch：413	 i:0 	 global-step:8260	 l-p:0.08644727617502213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.9761, 5.7462, 6.0119],
        [4.9761, 4.9793, 4.9763],
        [4.9761, 6.2254, 6.9973],
        [4.9761, 4.9761, 4.9761]], grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.08608552068471909 
model_pd.l_d.mean(): -10.792319297790527 
model_pd.lagr.mean(): -10.706233978271484 
model_pd.lambdas: dict_items([('pout', tensor([1.3916])), ('power', tensor([0.5274]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6617])), ('power', tensor([-22.1627]))])
epoch：414	 i:0 	 global-step:8280	 l-p:0.08608552068471909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.9885, 5.6884, 5.8912],
        [4.9885, 5.0010, 4.9901],
        [4.9885, 5.1071, 5.0471],
        [4.9885, 5.0304, 4.9995]], grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.08569972217082977 
model_pd.l_d.mean(): -10.766969680786133 
model_pd.lagr.mean(): -10.681269645690918 
model_pd.lambdas: dict_items([('pout', tensor([1.3923])), ('power', tensor([0.5263]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6584])), ('power', tensor([-22.1533]))])
epoch：415	 i:0 	 global-step:8300	 l-p:0.08569972217082977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[5.0016, 5.0434, 5.0125],
        [5.0016, 5.7769, 6.0446],
        [5.0016, 5.2245, 5.1613],
        [5.0016, 5.7039, 5.9075]], grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.08528459817171097 
model_pd.l_d.mean(): -10.741621971130371 
model_pd.lagr.mean(): -10.65633773803711 
model_pd.lambdas: dict_items([('pout', tensor([1.3929])), ('power', tensor([0.5252]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6550])), ('power', tensor([-22.1435]))])
epoch：416	 i:0 	 global-step:8320	 l-p:0.08528459817171097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.0155, 5.7204, 5.9248],
        [5.0155, 5.0157, 5.0155],
        [5.0155, 5.2622, 5.2024],
        [5.0155, 5.0570, 5.0263]], grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.08483293652534485 
model_pd.l_d.mean(): -10.716264724731445 
model_pd.lagr.mean(): -10.631431579589844 
model_pd.lambdas: dict_items([('pout', tensor([1.3936])), ('power', tensor([0.5241]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6513])), ('power', tensor([-22.1330]))])
epoch：417	 i:0 	 global-step:8340	 l-p:0.08483293652534485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[5.0303, 5.3784, 5.3484],
        [5.0303, 5.8106, 6.0796],
        [5.0303, 5.2550, 5.1913],
        [5.0303, 5.1428, 5.0838]], grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.0843348577618599 
model_pd.l_d.mean(): -10.69089412689209 
model_pd.lagr.mean(): -10.606558799743652 
model_pd.lambdas: dict_items([('pout', tensor([1.3942])), ('power', tensor([0.5230]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6474])), ('power', tensor([-22.1218]))])
epoch：418	 i:0 	 global-step:8360	 l-p:0.0843348577618599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[5.0463, 5.1902, 5.1254],
        [5.0463, 5.0681, 5.0501],
        [5.0463, 5.0589, 5.0479],
        [5.0463, 5.3960, 5.3660]], grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.08377624303102493 
model_pd.l_d.mean(): -10.66550064086914 
model_pd.lagr.mean(): -10.581724166870117 
model_pd.lambdas: dict_items([('pout', tensor([1.3949])), ('power', tensor([0.5218]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6432])), ('power', tensor([-22.1096]))])
epoch：419	 i:0 	 global-step:8380	 l-p:0.08377624303102493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.0639, 5.0651, 5.0639],
        [5.0639, 5.1067, 5.0751],
        [5.0639, 5.0640, 5.0639],
        [5.0639, 5.0643, 5.0639]], grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.08313655853271484 
model_pd.l_d.mean(): -10.640070915222168 
model_pd.lagr.mean(): -10.556934356689453 
model_pd.lambdas: dict_items([('pout', tensor([1.3955])), ('power', tensor([0.5207]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6386])), ('power', tensor([-22.0962]))])
epoch：420	 i:0 	 global-step:8400	 l-p:0.08313655853271484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.0836, 6.4747, 7.3983],
        [5.0836, 5.1264, 5.0948],
        [5.0836, 5.0838, 5.0836],
        [5.0836, 5.4905, 5.4843]], grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.08238386362791061 
model_pd.l_d.mean(): -10.614580154418945 
model_pd.lagr.mean(): -10.532196044921875 
model_pd.lambdas: dict_items([('pout', tensor([1.3961])), ('power', tensor([0.5196]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6335])), ('power', tensor([-22.0812]))])
epoch：421	 i:0 	 global-step:8420	 l-p:0.08238386362791061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.1061, 5.1063, 5.1061],
        [5.1061, 6.1700, 6.7110],
        [5.1061, 5.1888, 5.1383],
        [5.1061, 5.1066, 5.1061]], grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.08146552741527557 
model_pd.l_d.mean(): -10.588995933532715 
model_pd.lagr.mean(): -10.507530212402344 
model_pd.lambdas: dict_items([('pout', tensor([1.3968])), ('power', tensor([0.5185]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6277])), ('power', tensor([-22.0639]))])
epoch：422	 i:0 	 global-step:8440	 l-p:0.08146552741527557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.1328, 5.1328, 5.1328],
        [5.1328, 5.5183, 5.4995],
        [5.1328, 5.1552, 5.1367],
        [5.1328, 5.9350, 6.2123]], grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.08028687536716461 
model_pd.l_d.mean(): -10.563257217407227 
model_pd.lagr.mean(): -10.482970237731934 
model_pd.lambdas: dict_items([('pout', tensor([1.3974])), ('power', tensor([0.5174]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6208])), ('power', tensor([-22.0434]))])
epoch：423	 i:0 	 global-step:8460	 l-p:0.08028687536716461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.1657, 5.6116, 5.6211],
        [5.1657, 6.2464, 6.7966],
        [5.1657, 5.5548, 5.5360],
        [5.1657, 5.1657, 5.1657]], grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.07865696400403976 
model_pd.l_d.mean(): -10.537256240844727 
model_pd.lagr.mean(): -10.458599090576172 
model_pd.lambdas: dict_items([('pout', tensor([1.3980])), ('power', tensor([0.5163]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6123])), ('power', tensor([-22.0180]))])
epoch：424	 i:0 	 global-step:8480	 l-p:0.07865696400403976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[5.2087, 5.2092, 5.2087],
        [5.2087, 5.2535, 5.2205],
        [5.2087, 5.2221, 5.2104],
        [5.2087, 6.6455, 7.6008]], grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.07612055540084839 
model_pd.l_d.mean(): -10.510761260986328 
model_pd.lagr.mean(): -10.434640884399414 
model_pd.lambdas: dict_items([('pout', tensor([1.3986])), ('power', tensor([0.5152]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6013])), ('power', tensor([-21.9846]))])
epoch：425	 i:0 	 global-step:8500	 l-p:0.07612055540084839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.2702, 5.3993, 5.3343],
        [5.2702, 5.2702, 5.2702],
        [5.2702, 5.6048, 5.5597],
        [5.2702, 6.1026, 6.3916]], grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.07122545689344406 
model_pd.l_d.mean(): -10.483245849609375 
model_pd.lagr.mean(): -10.412020683288574 
model_pd.lambdas: dict_items([('pout', tensor([1.3992])), ('power', tensor([0.5141]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5858])), ('power', tensor([-21.9367]))])
epoch：426	 i:0 	 global-step:8520	 l-p:0.07122545689344406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[5.3697, 5.7821, 5.7634],
        [5.3697, 5.5283, 5.4575],
        [5.3697, 5.6433, 5.5783],
        [5.3697, 5.4773, 5.4168]], grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.05500838905572891 
model_pd.l_d.mean(): -10.453221321105957 
model_pd.lagr.mean(): -10.398213386535645 
model_pd.lambdas: dict_items([('pout', tensor([1.3998])), ('power', tensor([0.5130]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5610])), ('power', tensor([-21.8583]))])
epoch：427	 i:0 	 global-step:8540	 l-p:0.05500838905572891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.5559, 5.5559, 5.5559],
        [5.5559, 5.6050, 5.5688],
        [5.5559, 5.5561, 5.5559],
        [5.5559, 5.5559, 5.5559]], grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.16817016899585724 
model_pd.l_d.mean(): -10.415953636169434 
model_pd.lagr.mean(): -10.247783660888672 
model_pd.lambdas: dict_items([('pout', tensor([1.4003])), ('power', tensor([0.5120]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5159])), ('power', tensor([-21.7097]))])
epoch：428	 i:0 	 global-step:8560	 l-p:0.16817016899585724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[5.8109, 6.5326, 6.6747],
        [5.8109, 6.3400, 6.3555],
        [5.8109, 7.4702, 8.5817],
        [5.8109, 6.2733, 6.2549]], grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.09990478307008743 
model_pd.l_d.mean(): -10.36910629272461 
model_pd.lagr.mean(): -10.269201278686523 
model_pd.lambdas: dict_items([('pout', tensor([1.4007])), ('power', tensor([0.5109]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4565])), ('power', tensor([-21.5024]))])
epoch：429	 i:0 	 global-step:8580	 l-p:0.09990478307008743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.0080, 6.0083, 6.0081],
        [6.0080, 6.0125, 6.0083],
        [6.0080, 6.0081, 6.0080],
        [6.0080, 6.0249, 6.0102]], grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.09262928366661072 
model_pd.l_d.mean(): -10.324277877807617 
model_pd.lagr.mean(): -10.231648445129395 
model_pd.lambdas: dict_items([('pout', tensor([1.4011])), ('power', tensor([0.5098]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4124])), ('power', tensor([-21.3393]))])
epoch：430	 i:0 	 global-step:8600	 l-p:0.09262928366661072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[6.1189, 6.1748, 6.1335],
        [6.1189, 6.5351, 6.4831],
        [6.1189, 7.1321, 7.4875],
        [6.1189, 6.6331, 6.6228]], grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.09018050879240036 
model_pd.l_d.mean(): -10.287969589233398 
model_pd.lagr.mean(): -10.197789192199707 
model_pd.lambdas: dict_items([('pout', tensor([1.4015])), ('power', tensor([0.5088]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3882])), ('power', tensor([-21.2467]))])
epoch：431	 i:0 	 global-step:8620	 l-p:0.09018050879240036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.1466, 6.1469, 6.1466],
        [6.1466, 6.5655, 6.5132],
        [6.1466, 6.1466, 6.1466],
        [6.1466, 7.5101, 8.2152]], grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.0896616205573082 
model_pd.l_d.mean(): -10.26179313659668 
model_pd.lagr.mean(): -10.172131538391113 
model_pd.lambdas: dict_items([('pout', tensor([1.4019])), ('power', tensor([0.5077]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3823])), ('power', tensor([-21.2234]))])
epoch：432	 i:0 	 global-step:8640	 l-p:0.0896616205573082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.1031, 6.1600, 6.1182],
        [6.1031, 6.1031, 6.1031],
        [6.1031, 6.5989, 6.5806],
        [6.1031, 6.3205, 6.2341]], grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.09048987179994583 
model_pd.l_d.mean(): -10.24448013305664 
model_pd.lagr.mean(): -10.153989791870117 
model_pd.lambdas: dict_items([('pout', tensor([1.4023])), ('power', tensor([0.5066]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3916])), ('power', tensor([-21.2599]))])
epoch：433	 i:0 	 global-step:8660	 l-p:0.09048987179994583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.0037, 6.5047, 6.4942],
        [6.0037, 6.5579, 6.5754],
        [6.0037, 6.0081, 6.0040],
        [6.0037, 6.4883, 6.4701]], grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.09273514151573181 
model_pd.l_d.mean(): -10.2333345413208 
model_pd.lagr.mean(): -10.140599250793457 
model_pd.lambdas: dict_items([('pout', tensor([1.4027])), ('power', tensor([0.5056]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4133])), ('power', tensor([-21.3428]))])
epoch：434	 i:0 	 global-step:8680	 l-p:0.09273514151573181
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[5.8652, 5.9195, 5.8796],
        [5.8652, 5.8667, 5.8652],
        [5.8652, 5.9176, 5.8788],
        [5.8652, 6.5439, 6.6501]], grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.09726268798112869 
model_pd.l_d.mean(): -10.225102424621582 
model_pd.lagr.mean(): -10.127840042114258 
model_pd.lambdas: dict_items([('pout', tensor([1.4032])), ('power', tensor([0.5045]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4442])), ('power', tensor([-21.4575]))])
epoch：435	 i:0 	 global-step:8700	 l-p:0.09726268798112869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[5.7087, 5.7087, 5.7087],
        [5.7087, 6.1755, 6.1645],
        [5.7087, 5.7605, 5.7224],
        [5.7087, 5.7090, 5.7087]], grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10785620659589767 
model_pd.l_d.mean(): -10.21639633178711 
model_pd.lagr.mean(): -10.108540534973145 
model_pd.lambdas: dict_items([('pout', tensor([1.4036])), ('power', tensor([0.5034]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4799])), ('power', tensor([-21.5857]))])
epoch：436	 i:0 	 global-step:8720	 l-p:0.10785620659589767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[5.5837, 6.2178, 6.3153],
        [5.5837, 5.6984, 5.6341],
        [5.5837, 5.5877, 5.5839],
        [5.5837, 5.7168, 5.6478]], grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.14074738323688507 
model_pd.l_d.mean(): -10.202753067016602 
model_pd.lagr.mean(): -10.062005996704102 
model_pd.lambdas: dict_items([('pout', tensor([1.4041])), ('power', tensor([0.5023]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5092])), ('power', tensor([-21.6869]))])
epoch：437	 i:0 	 global-step:8740	 l-p:0.14074738323688507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[5.5629, 6.8456, 7.5545],
        [5.5629, 5.7040, 5.6335],
        [5.5629, 5.8381, 5.7675],
        [5.5629, 5.5643, 5.5630]], grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.15809808671474457 
model_pd.l_d.mean(): -10.180452346801758 
model_pd.lagr.mean(): -10.022354125976562 
model_pd.lambdas: dict_items([('pout', tensor([1.4047])), ('power', tensor([0.5012]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5141])), ('power', tensor([-21.7036]))])
epoch：438	 i:0 	 global-step:8760	 l-p:0.15809808671474457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[5.6267, 6.0385, 6.0067],
        [5.6267, 5.6267, 5.6267],
        [5.6267, 5.6307, 5.6269],
        [5.6267, 5.6269, 5.6267]], grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.12238243967294693 
model_pd.l_d.mean(): -10.152018547058105 
model_pd.lagr.mean(): -10.02963638305664 
model_pd.lambdas: dict_items([('pout', tensor([1.4052])), ('power', tensor([0.5002]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4990])), ('power', tensor([-21.6522]))])
epoch：439	 i:0 	 global-step:8780	 l-p:0.12238243967294693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[5.7604, 6.2336, 6.2228],
        [5.7604, 7.0140, 7.6593],
        [5.7604, 5.9376, 5.8593],
        [5.7604, 5.7646, 5.7607]], grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.10309761762619019 
model_pd.l_d.mean(): -10.117563247680664 
model_pd.lagr.mean(): -10.01446533203125 
model_pd.lambdas: dict_items([('pout', tensor([1.4056])), ('power', tensor([0.4991]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680])), ('power', tensor([-21.5433]))])
epoch：440	 i:0 	 global-step:8800	 l-p:0.10309761762619019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[5.8755, 5.9036, 5.8805],
        [5.8755, 7.4332, 8.4074],
        [5.8755, 5.9298, 5.8899],
        [5.8755, 6.0815, 5.9993]], grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.09682221710681915 
model_pd.l_d.mean(): -10.083727836608887 
model_pd.lagr.mean(): -9.986906051635742 
model_pd.lambdas: dict_items([('pout', tensor([1.4061])), ('power', tensor([0.4980]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4418])), ('power', tensor([-21.4488]))])
epoch：441	 i:0 	 global-step:8820	 l-p:0.09682221710681915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[5.9514, 6.1945, 6.1110],
        [5.9514, 6.3524, 6.3019],
        [5.9514, 6.1077, 6.0301],
        [5.9514, 5.9575, 5.9518]], grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.09417787194252014 
model_pd.l_d.mean(): -10.053119659423828 
model_pd.lagr.mean(): -9.958941459655762 
model_pd.lambdas: dict_items([('pout', tensor([1.4065])), ('power', tensor([0.4969]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4248])), ('power', tensor([-21.3860]))])
epoch：442	 i:0 	 global-step:8840	 l-p:0.09417787194252014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[5.9844, 5.9848, 5.9844],
        [5.9844, 5.9845, 5.9844],
        [5.9844, 6.1323, 6.0560],
        [5.9844, 6.0910, 6.0266]], grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.09323134273290634 
model_pd.l_d.mean(): -10.026732444763184 
model_pd.lagr.mean(): -9.933501243591309 
model_pd.lambdas: dict_items([('pout', tensor([1.4069])), ('power', tensor([0.4959]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4175])), ('power', tensor([-21.3586]))])
epoch：443	 i:0 	 global-step:8860	 l-p:0.09323134273290634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[5.9768, 5.9768, 5.9768],
        [5.9768, 6.2215, 6.1375],
        [5.9768, 6.7285, 6.8782],
        [5.9768, 6.1641, 6.0817]], grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.09343719482421875 
model_pd.l_d.mean(): -10.004508018493652 
model_pd.lagr.mean(): -9.911070823669434 
model_pd.lambdas: dict_items([('pout', tensor([1.4073])), ('power', tensor([0.4948]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4192])), ('power', tensor([-21.3648]))])
epoch：444	 i:0 	 global-step:8880	 l-p:0.09343719482421875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[5.9345, 7.2666, 7.9702],
        [5.9345, 5.9345, 5.9345],
        [5.9345, 6.7242, 6.9072],
        [5.9345, 6.0903, 6.0129]], grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.09469563513994217 
model_pd.l_d.mean(): -9.985655784606934 
model_pd.lagr.mean(): -9.890959739685059 
model_pd.lambdas: dict_items([('pout', tensor([1.4078])), ('power', tensor([0.4937]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4286])), ('power', tensor([-21.3999]))])
epoch：445	 i:0 	 global-step:8900	 l-p:0.09469563513994217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[5.8657, 5.8659, 5.8657],
        [5.8657, 5.8659, 5.8657],
        [5.8657, 5.8660, 5.8657],
        [5.8657, 7.1503, 7.8128]], grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.09720873087644577 
model_pd.l_d.mean(): -9.968935012817383 
model_pd.lagr.mean(): -9.871726036071777 
model_pd.lambdas: dict_items([('pout', tensor([1.4082])), ('power', tensor([0.4927]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4440])), ('power', tensor([-21.4567]))])
epoch：446	 i:0 	 global-step:8920	 l-p:0.09720873087644577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[5.7813, 7.0688, 7.7476],
        [5.7813, 6.1666, 6.1175],
        [5.7813, 6.3083, 6.3242],
        [5.7813, 6.7306, 7.0665]], grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.1015988364815712 
model_pd.l_d.mean(): -9.952890396118164 
model_pd.lagr.mean(): -9.85129165649414 
model_pd.lambdas: dict_items([('pout', tensor([1.4087])), ('power', tensor([0.4916]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631])), ('power', tensor([-21.5260]))])
epoch：447	 i:0 	 global-step:8940	 l-p:0.1015988364815712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[5.6979, 7.1959, 8.1312],
        [5.6979, 6.9347, 7.5712],
        [5.6979, 6.5427, 6.7947],
        [5.6979, 6.1807, 6.1779]], grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.10899344086647034 
model_pd.l_d.mean(): -9.935904502868652 
model_pd.lagr.mean(): -9.826910972595215 
model_pd.lambdas: dict_items([('pout', tensor([1.4091])), ('power', tensor([0.4905]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4823])), ('power', tensor([-21.5941]))])
epoch：448	 i:0 	 global-step:8960	 l-p:0.10899344086647034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[5.6463, 5.6976, 5.6599],
        [5.6463, 5.8188, 5.7425],
        [5.6463, 5.6728, 5.6510],
        [5.6463, 6.8685, 7.4970]], grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.11729001253843307 
model_pd.l_d.mean(): -9.915945053100586 
model_pd.lagr.mean(): -9.79865550994873 
model_pd.lambdas: dict_items([('pout', tensor([1.4096])), ('power', tensor([0.4894]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943])), ('power', tensor([-21.6359]))])
epoch：449	 i:0 	 global-step:8980	 l-p:0.11729001253843307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[5.6681, 6.9230, 7.5837],
        [5.6681, 6.1316, 6.1208],
        [5.6681, 5.8949, 5.8165],
        [5.6681, 5.6948, 5.6729]], grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.11318649351596832 
model_pd.l_d.mean(): -9.89082145690918 
model_pd.lagr.mean(): -9.777634620666504 
model_pd.lambdas: dict_items([('pout', tensor([1.4101])), ('power', tensor([0.4883]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4892])), ('power', tensor([-21.6182]))])
epoch：450	 i:0 	 global-step:9000	 l-p:0.11318649351596832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[5.7312, 6.2522, 6.2677],
        [5.7312, 6.9779, 7.6198],
        [5.7312, 5.7312, 5.7312],
        [5.7312, 6.4849, 6.6584]], grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.10544704645872116 
model_pd.l_d.mean(): -9.862787246704102 
model_pd.lagr.mean(): -9.757340431213379 
model_pd.lambdas: dict_items([('pout', tensor([1.4106])), ('power', tensor([0.4873]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746])), ('power', tensor([-21.5669]))])
epoch：451	 i:0 	 global-step:9020	 l-p:0.10544704645872116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.7966, 5.7966, 5.7966],
        [5.7966, 5.7966, 5.7966],
        [5.7966, 5.9991, 5.9183],
        [5.7966, 5.8500, 5.8107]], grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.10062593221664429 
model_pd.l_d.mean(): -9.834400177001953 
model_pd.lagr.mean(): -9.733774185180664 
model_pd.lambdas: dict_items([('pout', tensor([1.4111])), ('power', tensor([0.4862]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4596])), ('power', tensor([-21.5133]))])
epoch：452	 i:0 	 global-step:9040	 l-p:0.10062593221664429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[5.8471, 7.1546, 7.8447],
        [5.8471, 5.9504, 5.8880],
        [5.8471, 5.8471, 5.8471],
        [5.8471, 6.3831, 6.3997]], grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.0979982316493988 
model_pd.l_d.mean(): -9.807052612304688 
model_pd.lagr.mean(): -9.709053993225098 
model_pd.lambdas: dict_items([('pout', tensor([1.4115])), ('power', tensor([0.4851]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4481])), ('power', tensor([-21.4718]))])
epoch：453	 i:0 	 global-step:9060	 l-p:0.0979982316493988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[5.8764, 6.8474, 7.1918],
        [5.8764, 7.1925, 7.8874],
        [5.8764, 6.2711, 6.2214],
        [5.8764, 6.3642, 6.3539]], grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.09674940258264542 
model_pd.l_d.mean(): -9.781417846679688 
model_pd.lagr.mean(): -9.68466854095459 
model_pd.lambdas: dict_items([('pout', tensor([1.4120])), ('power', tensor([0.4840]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4415])), ('power', tensor([-21.4477]))])
epoch：454	 i:0 	 global-step:9080	 l-p:0.09674940258264542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[5.8831, 5.8832, 5.8831],
        [5.8831, 7.2658, 8.0340],
        [5.8831, 6.6198, 6.7663],
        [5.8831, 6.1692, 6.0914]], grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.09648101776838303 
model_pd.l_d.mean(): -9.75766658782959 
model_pd.lagr.mean(): -9.661185264587402 
model_pd.lambdas: dict_items([('pout', tensor([1.4124])), ('power', tensor([0.4830]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4400])), ('power', tensor([-21.4421]))])
epoch：455	 i:0 	 global-step:9100	 l-p:0.09648101776838303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.8694, 5.8754, 5.8698],
        [5.8694, 5.9733, 5.9105],
        [5.8694, 6.1834, 6.1110],
        [5.8694, 5.8697, 5.8694]], grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.09702753275632858 
model_pd.l_d.mean(): -9.735588073730469 
model_pd.lagr.mean(): -9.63856029510498 
model_pd.lambdas: dict_items([('pout', tensor([1.4128])), ('power', tensor([0.4819]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431])), ('power', tensor([-21.4534]))])
epoch：456	 i:0 	 global-step:9120	 l-p:0.09702753275632858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[5.8393, 6.2307, 6.1813],
        [5.8393, 6.3231, 6.3127],
        [5.8393, 5.8453, 5.8398],
        [5.8393, 5.9424, 5.8801]], grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.09834922105073929 
model_pd.l_d.mean(): -9.714713096618652 
model_pd.lagr.mean(): -9.616363525390625 
model_pd.lambdas: dict_items([('pout', tensor([1.4133])), ('power', tensor([0.4808]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4498])), ('power', tensor([-21.4781]))])
epoch：457	 i:0 	 global-step:9140	 l-p:0.09834922105073929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[5.7990, 5.7991, 5.7990],
        [5.7990, 6.7533, 7.0914],
        [5.7990, 6.2781, 6.2677],
        [5.7990, 5.8005, 5.7991]], grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.10046043992042542 
model_pd.l_d.mean(): -9.694416999816895 
model_pd.lagr.mean(): -9.59395694732666 
model_pd.lambdas: dict_items([('pout', tensor([1.4137])), ('power', tensor([0.4797]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4590])), ('power', tensor([-21.5112]))])
epoch：458	 i:0 	 global-step:9160	 l-p:0.10046043992042542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.7566, 5.8079, 5.7699],
        [5.7566, 6.1836, 6.1516],
        [5.7566, 5.9573, 5.8772],
        [5.7566, 5.7566, 5.7566]], grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.10328137129545212 
model_pd.l_d.mean(): -9.674007415771484 
model_pd.lagr.mean(): -9.57072639465332 
model_pd.lambdas: dict_items([('pout', tensor([1.4142])), ('power', tensor([0.4787]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4687])), ('power', tensor([-21.5459]))])
epoch：459	 i:0 	 global-step:9180	 l-p:0.10328137129545212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[5.7223, 5.8703, 5.7966],
        [5.7223, 6.6599, 6.9915],
        [5.7223, 5.7228, 5.7223],
        [5.7223, 5.7223, 5.7223]], grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.10623390227556229 
model_pd.l_d.mean(): -9.65279769897461 
model_pd.lagr.mean(): -9.546564102172852 
model_pd.lambdas: dict_items([('pout', tensor([1.4147])), ('power', tensor([0.4776]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766])), ('power', tensor([-21.5739]))])
epoch：460	 i:0 	 global-step:9200	 l-p:0.10623390227556229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[5.7071, 5.7071, 5.7071],
        [5.7071, 6.1919, 6.1893],
        [5.7071, 5.7128, 5.7075],
        [5.7071, 5.9812, 5.9063]], grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.1078147292137146 
model_pd.l_d.mean(): -9.630232810974121 
model_pd.lagr.mean(): -9.522418022155762 
model_pd.lambdas: dict_items([('pout', tensor([1.4152])), ('power', tensor([0.4765]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4801])), ('power', tensor([-21.5862]))])
epoch：461	 i:0 	 global-step:9220	 l-p:0.1078147292137146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.7161, 5.7162, 5.7161],
        [5.7161, 5.8921, 5.8144],
        [5.7161, 5.7161, 5.7161],
        [5.7161, 6.1858, 6.1753]], grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.10684875398874283 
model_pd.l_d.mean(): -9.606159210205078 
model_pd.lagr.mean(): -9.499310493469238 
model_pd.lambdas: dict_items([('pout', tensor([1.4157])), ('power', tensor([0.4754]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780])), ('power', tensor([-21.5789]))])
epoch：462	 i:0 	 global-step:9240	 l-p:0.10684875398874283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[5.7431, 5.9432, 5.8634],
        [5.7431, 5.7959, 5.7571],
        [5.7431, 6.0328, 5.9594],
        [5.7431, 6.5983, 6.8541]], grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.10433797538280487 
model_pd.l_d.mean(): -9.580995559692383 
model_pd.lagr.mean(): -9.47665786743164 
model_pd.lambdas: dict_items([('pout', tensor([1.4161])), ('power', tensor([0.4744]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4718])), ('power', tensor([-21.5568]))])
epoch：463	 i:0 	 global-step:9260	 l-p:0.10433797538280487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.7770, 6.3046, 6.3208],
        [5.7770, 5.7773, 5.7770],
        [5.7770, 5.8305, 5.7913],
        [5.7770, 6.4963, 6.6387]], grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.10181155800819397 
model_pd.l_d.mean(): -9.555408477783203 
model_pd.lagr.mean(): -9.453597068786621 
model_pd.lambdas: dict_items([('pout', tensor([1.4166])), ('power', tensor([0.4733]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640])), ('power', tensor([-21.5291]))])
epoch：464	 i:0 	 global-step:9280	 l-p:0.10181155800819397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[5.8086, 5.8626, 5.8230],
        [5.8086, 5.9889, 5.9094],
        [5.8086, 5.8399, 5.8146],
        [5.8086, 6.3403, 6.3567]], grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.09989883750677109 
model_pd.l_d.mean(): -9.529977798461914 
model_pd.lagr.mean(): -9.430078506469727 
model_pd.lambdas: dict_items([('pout', tensor([1.4170])), ('power', tensor([0.4722]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4568])), ('power', tensor([-21.5032]))])
epoch：465	 i:0 	 global-step:9300	 l-p:0.09989883750677109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[5.8322, 6.0369, 5.9554],
        [5.8322, 5.8846, 5.8459],
        [5.8322, 5.8857, 5.8463],
        [5.8322, 5.8322, 5.8322]], grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.09867267310619354 
model_pd.l_d.mean(): -9.505091667175293 
model_pd.lagr.mean(): -9.406418800354004 
model_pd.lambdas: dict_items([('pout', tensor([1.4175])), ('power', tensor([0.4711]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4514])), ('power', tensor([-21.4838]))])
epoch：466	 i:0 	 global-step:9320	 l-p:0.09867267310619354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[5.8452, 5.8452, 5.8452],
        [5.8452, 6.3816, 6.3984],
        [5.8452, 7.3954, 8.3653],
        [5.8452, 5.8456, 5.8452]], grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.09805624186992645 
model_pd.l_d.mean(): -9.48094367980957 
model_pd.lagr.mean(): -9.382887840270996 
model_pd.lambdas: dict_items([('pout', tensor([1.4179])), ('power', tensor([0.4701]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4485])), ('power', tensor([-21.4731]))])
epoch：467	 i:0 	 global-step:9340	 l-p:0.09805624186992645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[5.8472, 6.2841, 6.2518],
        [5.8472, 6.5258, 6.6326],
        [5.8472, 5.8636, 5.8493],
        [5.8472, 7.1559, 7.8470]], grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.0979619026184082 
model_pd.l_d.mean(): -9.457549095153809 
model_pd.lagr.mean(): -9.359586715698242 
model_pd.lambdas: dict_items([('pout', tensor([1.4184])), ('power', tensor([0.4690]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4480])), ('power', tensor([-21.4714]))])
epoch：468	 i:0 	 global-step:9360	 l-p:0.0979619026184082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[5.8396, 5.8398, 5.8396],
        [5.8396, 7.1188, 7.7788],
        [5.8396, 5.8473, 5.8402],
        [5.8396, 6.5169, 6.6235]], grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09831514209508896 
model_pd.l_d.mean(): -9.434794425964355 
model_pd.lagr.mean(): -9.336479187011719 
model_pd.lambdas: dict_items([('pout', tensor([1.4188])), ('power', tensor([0.4679]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497])), ('power', tensor([-21.4777]))])
epoch：469	 i:0 	 global-step:9380	 l-p:0.09831514209508896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[5.8246, 5.8249, 5.8246],
        [5.8246, 6.3072, 6.2969],
        [5.8246, 5.9767, 5.9012],
        [5.8246, 5.8323, 5.8253]], grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.09904180467128754 
model_pd.l_d.mean(): -9.412467956542969 
model_pd.lagr.mean(): -9.31342601776123 
model_pd.lambdas: dict_items([('pout', tensor([1.4193])), ('power', tensor([0.4668]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4531])), ('power', tensor([-21.4900]))])
epoch：470	 i:0 	 global-step:9400	 l-p:0.09904180467128754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[5.8059, 5.9861, 5.9067],
        [5.8059, 5.8220, 5.8079],
        [5.8059, 6.2706, 6.2528],
        [5.8059, 5.9573, 5.8820]], grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.10003914684057236 
model_pd.l_d.mean(): -9.39030647277832 
model_pd.lagr.mean(): -9.290266990661621 
model_pd.lambdas: dict_items([('pout', tensor([1.4198])), ('power', tensor([0.4658]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4574])), ('power', tensor([-21.5054]))])
epoch：471	 i:0 	 global-step:9420	 l-p:0.10003914684057236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[5.7872, 6.2179, 6.1858],
        [5.7872, 6.3164, 6.3327],
        [5.7872, 5.9898, 5.9090],
        [5.7872, 5.9286, 5.8556]], grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.10113777220249176 
model_pd.l_d.mean(): -9.36805248260498 
model_pd.lagr.mean(): -9.266914367675781 
model_pd.lambdas: dict_items([('pout', tensor([1.4202])), ('power', tensor([0.4647]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4616])), ('power', tensor([-21.5206]))])
epoch：472	 i:0 	 global-step:9440	 l-p:0.10113777220249176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.7727, 5.8255, 5.7867],
        [5.7727, 5.8243, 5.7862],
        [5.7727, 6.6344, 6.8924],
        [5.7727, 5.7727, 5.7727]], grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.10207945853471756 
model_pd.l_d.mean(): -9.345488548278809 
model_pd.lagr.mean(): -9.243409156799316 
model_pd.lambdas: dict_items([('pout', tensor([1.4207])), ('power', tensor([0.4636]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649])), ('power', tensor([-21.5325]))])
epoch：473	 i:0 	 global-step:9460	 l-p:0.10207945853471756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.7656, 6.1506, 6.1018],
        [5.7656, 5.8670, 5.8057],
        [5.7656, 6.7052, 7.0333],
        [5.7656, 5.7656, 5.7656]], grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.10257294774055481 
model_pd.l_d.mean(): -9.322470664978027 
model_pd.lagr.mean(): -9.219897270202637 
model_pd.lambdas: dict_items([('pout', tensor([1.4211])), ('power', tensor([0.4625]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4666])), ('power', tensor([-21.5383]))])
epoch：474	 i:0 	 global-step:9480	 l-p:0.10257294774055481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[5.7673, 5.7679, 5.7673],
        [5.7673, 6.1959, 6.1640],
        [5.7673, 5.7673, 5.7673],
        [5.7673, 6.5290, 6.7049]], grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.10245025902986526 
model_pd.l_d.mean(): -9.298967361450195 
model_pd.lagr.mean(): -9.196516990661621 
model_pd.lambdas: dict_items([('pout', tensor([1.4216])), ('power', tensor([0.4615]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4662])), ('power', tensor([-21.5369]))])
epoch：475	 i:0 	 global-step:9500	 l-p:0.10245025902986526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[5.7770, 6.0562, 5.9801],
        [5.7770, 7.1287, 7.8790],
        [5.7770, 5.7846, 5.7776],
        [5.7770, 5.8987, 5.8307]], grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.10178767889738083 
model_pd.l_d.mean(): -9.275053024291992 
model_pd.lagr.mean(): -9.17326545715332 
model_pd.lambdas: dict_items([('pout', tensor([1.4221])), ('power', tensor([0.4604]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640])), ('power', tensor([-21.5290]))])
epoch：476	 i:0 	 global-step:9520	 l-p:0.10178767889738083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[5.7920, 5.8232, 5.7980],
        [5.7920, 6.5582, 6.7353],
        [5.7920, 6.3219, 6.3383],
        [5.7920, 5.7920, 5.7920]], grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.10083560645580292 
model_pd.l_d.mean(): -9.250882148742676 
model_pd.lagr.mean(): -9.150046348571777 
model_pd.lambdas: dict_items([('pout', tensor([1.4225])), ('power', tensor([0.4593]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4605])), ('power', tensor([-21.5167]))])
epoch：477	 i:0 	 global-step:9540	 l-p:0.10083560645580292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.8091, 6.1039, 6.0294],
        [5.8091, 6.0449, 5.9638],
        [5.8091, 5.9116, 5.8497],
        [5.8091, 5.8629, 5.8234]], grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.09984917938709259 
model_pd.l_d.mean(): -9.22662353515625 
model_pd.lagr.mean(): -9.126774787902832 
model_pd.lambdas: dict_items([('pout', tensor([1.4230])), ('power', tensor([0.4582]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4566])), ('power', tensor([-21.5027]))])
epoch：478	 i:0 	 global-step:9560	 l-p:0.09984917938709259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[5.8253, 7.1919, 7.9509],
        [5.8253, 5.8268, 5.8254],
        [5.8253, 5.9775, 5.9019],
        [5.8253, 6.1080, 6.0311]], grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.09899501502513885 
model_pd.l_d.mean(): -9.202439308166504 
model_pd.lagr.mean(): -9.10344409942627 
model_pd.lambdas: dict_items([('pout', tensor([1.4234])), ('power', tensor([0.4572]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4529])), ('power', tensor([-21.4893]))])
epoch：479	 i:0 	 global-step:9580	 l-p:0.09899501502513885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[5.8385, 6.1507, 6.0787],
        [5.8385, 7.3870, 8.3559],
        [5.8385, 7.2091, 7.9705],
        [5.8385, 5.8930, 5.8530]], grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.09835140407085419 
model_pd.l_d.mean(): -9.17844295501709 
model_pd.lagr.mean(): -9.08009147644043 
model_pd.lambdas: dict_items([('pout', tensor([1.4239])), ('power', tensor([0.4561]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4500])), ('power', tensor([-21.4785]))])
epoch：480	 i:0 	 global-step:9600	 l-p:0.09835140407085419
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[5.8474, 6.8123, 7.1541],
        [5.8474, 5.9000, 5.8611],
        [5.8474, 5.8474, 5.8474],
        [5.8474, 5.8474, 5.8474]], grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.09793929010629654 
model_pd.l_d.mean(): -9.15470027923584 
model_pd.lagr.mean(): -9.056760787963867 
model_pd.lambdas: dict_items([('pout', tensor([1.4243])), ('power', tensor([0.4550]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4479])), ('power', tensor([-21.4712]))])
epoch：481	 i:0 	 global-step:9620	 l-p:0.09793929010629654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[5.8517, 6.3542, 6.3524],
        [5.8517, 6.0341, 5.9537],
        [5.8517, 6.2449, 6.1954],
        [5.8517, 5.9043, 5.8654]], grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.09774845838546753 
model_pd.l_d.mean(): -9.131226539611816 
model_pd.lagr.mean(): -9.033477783203125 
model_pd.lambdas: dict_items([('pout', tensor([1.4248])), ('power', tensor([0.4539]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4470])), ('power', tensor([-21.4677]))])
epoch：482	 i:0 	 global-step:9640	 l-p:0.09774845838546753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.8516, 5.8516, 5.8516],
        [5.8516, 6.2448, 6.1953],
        [5.8516, 7.2263, 7.9900],
        [5.8516, 5.8521, 5.8516]], grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.0977511778473854 
model_pd.l_d.mean(): -9.107986450195312 
model_pd.lagr.mean(): -9.010234832763672 
model_pd.lambdas: dict_items([('pout', tensor([1.4252])), ('power', tensor([0.4529]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4470])), ('power', tensor([-21.4677]))])
epoch：483	 i:0 	 global-step:9660	 l-p:0.0977511778473854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[5.8481, 6.0862, 6.0044],
        [5.8481, 5.8486, 5.8481],
        [5.8481, 5.8481, 5.8481],
        [5.8481, 6.3178, 6.3000]], grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.09790760278701782 
model_pd.l_d.mean(): -9.084919929504395 
model_pd.lagr.mean(): -8.987011909484863 
model_pd.lambdas: dict_items([('pout', tensor([1.4257])), ('power', tensor([0.4518]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4478])), ('power', tensor([-21.4706]))])
epoch：484	 i:0 	 global-step:9680	 l-p:0.09790760278701782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[5.8424, 6.2348, 6.1854],
        [5.8424, 5.8966, 5.8568],
        [5.8424, 6.1396, 6.0646],
        [5.8424, 6.7990, 7.1337]], grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.09816660732030869 
model_pd.l_d.mean(): -9.06195068359375 
model_pd.lagr.mean(): -8.963784217834473 
model_pd.lambdas: dict_items([('pout', tensor([1.4261])), ('power', tensor([0.4507]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4491])), ('power', tensor([-21.4753]))])
epoch：485	 i:0 	 global-step:9700	 l-p:0.09816660732030869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.8360, 5.8640, 5.8410],
        [5.8360, 6.3044, 6.2866],
        [5.8360, 5.9596, 5.8906],
        [5.8360, 5.8360, 5.8360]], grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.09846697002649307 
model_pd.l_d.mean(): -9.038993835449219 
model_pd.lagr.mean(): -8.940526962280273 
model_pd.lambdas: dict_items([('pout', tensor([1.4266])), ('power', tensor([0.4496]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505])), ('power', tensor([-21.4806]))])
epoch：486	 i:0 	 global-step:9720	 l-p:0.09846697002649307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[5.8303, 5.8305, 5.8303],
        [5.8303, 5.9334, 5.8711],
        [5.8303, 5.8306, 5.8303],
        [5.8303, 6.6035, 6.7825]], grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.09874098747968674 
model_pd.l_d.mean(): -9.015983581542969 
model_pd.lagr.mean(): -8.917243003845215 
model_pd.lambdas: dict_items([('pout', tensor([1.4270])), ('power', tensor([0.4486]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4518])), ('power', tensor([-21.4852]))])
epoch：487	 i:0 	 global-step:9740	 l-p:0.09874098747968674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[5.8266, 6.7871, 7.1271],
        [5.8266, 5.8267, 5.8266],
        [5.8266, 5.8809, 5.8411],
        [5.8266, 6.0313, 5.9497]], grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.0989241898059845 
model_pd.l_d.mean(): -8.992864608764648 
model_pd.lagr.mean(): -8.893939971923828 
model_pd.lambdas: dict_items([('pout', tensor([1.4275])), ('power', tensor([0.4475]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4527])), ('power', tensor([-21.4883]))])
epoch：488	 i:0 	 global-step:9760	 l-p:0.0989241898059845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.8257, 6.2165, 6.1672],
        [5.8257, 6.1217, 6.0470],
        [5.8257, 5.8419, 5.8278],
        [5.8257, 5.8257, 5.8257]], grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.0989692211151123 
model_pd.l_d.mean(): -8.969613075256348 
model_pd.lagr.mean(): -8.870643615722656 
model_pd.lambdas: dict_items([('pout', tensor([1.4279])), ('power', tensor([0.4464]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4529])), ('power', tensor([-21.4890]))])
epoch：489	 i:0 	 global-step:9780	 l-p:0.0989692211151123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.8279, 5.8441, 5.8300],
        [5.8279, 6.2631, 6.2309],
        [5.8279, 5.8558, 5.8329],
        [5.8279, 5.8282, 5.8279]], grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.09885800629854202 
model_pd.l_d.mean(): -8.946222305297852 
model_pd.lagr.mean(): -8.84736442565918 
model_pd.lambdas: dict_items([('pout', tensor([1.4284])), ('power', tensor([0.4453]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4524])), ('power', tensor([-21.4872]))])
epoch：490	 i:0 	 global-step:9800	 l-p:0.09885800629854202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.8330, 5.8871, 5.8474],
        [5.8330, 6.7072, 6.9694],
        [5.8330, 6.2687, 6.2365],
        [5.8330, 5.8333, 5.8330]], grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.09860524535179138 
model_pd.l_d.mean(): -8.92271614074707 
model_pd.lagr.mean(): -8.824110984802246 
model_pd.lambdas: dict_items([('pout', tensor([1.4288])), ('power', tensor([0.4443]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512])), ('power', tensor([-21.4830]))])
epoch：491	 i:0 	 global-step:9820	 l-p:0.09860524535179138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.8405, 5.8408, 5.8405],
        [5.8405, 5.9642, 5.8952],
        [5.8405, 5.8685, 5.8455],
        [5.8405, 6.3418, 6.3400]], grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.09825017303228378 
model_pd.l_d.mean(): -8.899125099182129 
model_pd.lagr.mean(): -8.800874710083008 
model_pd.lambdas: dict_items([('pout', tensor([1.4293])), ('power', tensor([0.4432]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4495])), ('power', tensor([-21.4768]))])
epoch：492	 i:0 	 global-step:9840	 l-p:0.09825017303228378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[5.8494, 5.8499, 5.8494],
        [5.8494, 6.8161, 7.1592],
        [5.8494, 6.8077, 7.1430],
        [5.8494, 7.1595, 7.8515]], grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.09784243255853653 
model_pd.l_d.mean(): -8.87549114227295 
model_pd.lagr.mean(): -8.77764892578125 
model_pd.lambdas: dict_items([('pout', tensor([1.4297])), ('power', tensor([0.4421]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4475])), ('power', tensor([-21.4695]))])
epoch：493	 i:0 	 global-step:9860	 l-p:0.09784243255853653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.8588, 6.3457, 6.3356],
        [5.8588, 5.8588, 5.8588],
        [5.8588, 6.8192, 7.1553],
        [5.8588, 5.8594, 5.8588]], grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.0974288359284401 
model_pd.l_d.mean(): -8.851856231689453 
model_pd.lagr.mean(): -8.754426956176758 
model_pd.lambdas: dict_items([('pout', tensor([1.4302])), ('power', tensor([0.4410]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4454])), ('power', tensor([-21.4617]))])
epoch：494	 i:0 	 global-step:9880	 l-p:0.0974288359284401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[5.8680, 5.8680, 5.8680],
        [5.8680, 6.0748, 5.9925],
        [5.8680, 6.1825, 6.1101],
        [5.8680, 6.1073, 6.0251]], grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.09704546630382538 
model_pd.l_d.mean(): -8.828252792358398 
model_pd.lagr.mean(): -8.731206893920898 
model_pd.lambdas: dict_items([('pout', tensor([1.4306])), ('power', tensor([0.4400]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4433])), ('power', tensor([-21.4542]))])
epoch：495	 i:0 	 global-step:9900	 l-p:0.09704546630382538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.8761, 5.9311, 5.8908],
        [5.8761, 5.8839, 5.8768],
        [5.8761, 6.4169, 6.4341],
        [5.8761, 5.8762, 5.8761]], grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.09671561419963837 
model_pd.l_d.mean(): -8.80471134185791 
model_pd.lagr.mean(): -8.707995414733887 
model_pd.lambdas: dict_items([('pout', tensor([1.4311])), ('power', tensor([0.4389]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4415])), ('power', tensor([-21.4475]))])
epoch：496	 i:0 	 global-step:9920	 l-p:0.09671561419963837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[5.8829, 6.3238, 6.2914],
        [5.8829, 6.6656, 6.8472],
        [5.8829, 5.8832, 5.8829],
        [5.8829, 6.0278, 5.9531]], grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.09645073115825653 
model_pd.l_d.mean(): -8.781243324279785 
model_pd.lagr.mean(): -8.684792518615723 
model_pd.lambdas: dict_items([('pout', tensor([1.4315])), ('power', tensor([0.4378]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4399])), ('power', tensor([-21.4419]))])
epoch：497	 i:0 	 global-step:9940	 l-p:0.09645073115825653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.8880, 6.2041, 6.1314],
        [5.8880, 6.0427, 5.9659],
        [5.8880, 5.8880, 5.8880],
        [5.8880, 6.3951, 6.3934]], grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.09625263512134552 
model_pd.l_d.mean(): -8.757855415344238 
model_pd.lagr.mean(): -8.661602973937988 
model_pd.lambdas: dict_items([('pout', tensor([1.4320])), ('power', tensor([0.4368]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4388])), ('power', tensor([-21.4377]))])
epoch：498	 i:0 	 global-step:9960	 l-p:0.09625263512134552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[5.8917, 5.8917, 5.8917],
        [5.8917, 5.9964, 5.9332],
        [5.8917, 6.1790, 6.1010],
        [5.8917, 6.1324, 6.0498]], grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.09611596912145615 
model_pd.l_d.mean(): -8.734541893005371 
model_pd.lagr.mean(): -8.638425827026367 
model_pd.lambdas: dict_items([('pout', tensor([1.4324])), ('power', tensor([0.4357]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4380])), ('power', tensor([-21.4347]))])
epoch：499	 i:0 	 global-step:9980	 l-p:0.09611596912145615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[5.8940, 5.8940, 5.8940],
        [5.8940, 6.4017, 6.4001],
        [5.8940, 5.9260, 5.9001],
        [5.8940, 5.9473, 5.9079]], grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.09603022783994675 
model_pd.l_d.mean(): -8.71129035949707 
model_pd.lagr.mean(): -8.615260124206543 
model_pd.lambdas: dict_items([('pout', tensor([1.4328])), ('power', tensor([0.4346]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4375])), ('power', tensor([-21.4327]))])
epoch：500	 i:0 	 global-step:10000	 l-p:0.09603022783994675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[5.8953, 6.0798, 5.9986],
        [5.8953, 7.5911, 8.7295],
        [5.8953, 6.0407, 5.9658],
        [5.8953, 6.1363, 6.0536]], grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.09598129242658615 
model_pd.l_d.mean(): -8.688079833984375 
model_pd.lagr.mean(): -8.592098236083984 
model_pd.lambdas: dict_items([('pout', tensor([1.4333])), ('power', tensor([0.4335]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4372])), ('power', tensor([-21.4317]))])
epoch：501	 i:0 	 global-step:10020	 l-p:0.09598129242658615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[5.8961, 5.9039, 5.8967],
        [5.8961, 5.9004, 5.8963],
        [5.8961, 5.8964, 5.8961],
        [5.8961, 6.1043, 6.0215]], grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.09595336765050888 
model_pd.l_d.mean(): -8.664896011352539 
model_pd.lagr.mean(): -8.56894302368164 
model_pd.lambdas: dict_items([('pout', tensor([1.4337])), ('power', tensor([0.4325]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4370])), ('power', tensor([-21.4310]))])
epoch：502	 i:0 	 global-step:10040	 l-p:0.09595336765050888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[5.8967, 5.9288, 5.9028],
        [5.8967, 5.8970, 5.8967],
        [5.8967, 6.3880, 6.3779],
        [5.8967, 7.1931, 7.8626]], grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.0959300696849823 
model_pd.l_d.mean(): -8.641718864440918 
model_pd.lagr.mean(): -8.545788764953613 
model_pd.lambdas: dict_items([('pout', tensor([1.4341])), ('power', tensor([0.4314]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4368])), ('power', tensor([-21.4305]))])
epoch：503	 i:0 	 global-step:10060	 l-p:0.0959300696849823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[5.8976, 6.0527, 5.9757],
        [5.8976, 5.9036, 5.8980],
        [5.8976, 5.8976, 5.8976],
        [5.8976, 7.1943, 7.8639]], grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.095896415412426 
model_pd.l_d.mean(): -8.61853313446045 
model_pd.lagr.mean(): -8.522636413574219 
model_pd.lambdas: dict_items([('pout', tensor([1.4346])), ('power', tensor([0.4303]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366])), ('power', tensor([-21.4297]))])
epoch：504	 i:0 	 global-step:10080	 l-p:0.095896415412426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.8992, 5.9537, 5.9136],
        [5.8992, 7.2887, 8.0612],
        [5.8992, 5.9313, 5.9053],
        [5.8992, 5.8992, 5.8992]], grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.09584018588066101 
model_pd.l_d.mean(): -8.59532642364502 
model_pd.lagr.mean(): -8.499485969543457 
model_pd.lambdas: dict_items([('pout', tensor([1.4350])), ('power', tensor([0.4293]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4363])), ('power', tensor([-21.4285]))])
epoch：505	 i:0 	 global-step:10100	 l-p:0.09584018588066101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.9016, 6.0568, 5.9797],
        [5.9016, 6.5893, 6.6980],
        [5.9016, 6.8798, 7.2274],
        [5.9016, 5.9561, 5.9160]], grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.09575306624174118 
model_pd.l_d.mean(): -8.572096824645996 
model_pd.lagr.mean(): -8.476344108581543 
model_pd.lambdas: dict_items([('pout', tensor([1.4354])), ('power', tensor([0.4282]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358])), ('power', tensor([-21.4265]))])
epoch：506	 i:0 	 global-step:10120	 l-p:0.09575306624174118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.9050, 5.9050, 5.9050],
        [5.9050, 5.9601, 5.9197],
        [5.9050, 6.4141, 6.4125],
        [5.9050, 5.9055, 5.9050]], grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.09563135355710983 
model_pd.l_d.mean(): -8.548842430114746 
model_pd.lagr.mean(): -8.453210830688477 
model_pd.lambdas: dict_items([('pout', tensor([1.4359])), ('power', tensor([0.4271]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4350])), ('power', tensor([-21.4236]))])
epoch：507	 i:0 	 global-step:10140	 l-p:0.09563135355710983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.9094, 5.9094, 5.9094],
        [5.9094, 5.9380, 5.9145],
        [5.9094, 6.3083, 6.2583],
        [5.9094, 5.9260, 5.9116]], grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.09547588229179382 
model_pd.l_d.mean(): -8.52556324005127 
model_pd.lagr.mean(): -8.430087089538574 
model_pd.lambdas: dict_items([('pout', tensor([1.4363])), ('power', tensor([0.4260]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4340])), ('power', tensor([-21.4200]))])
epoch：508	 i:0 	 global-step:10160	 l-p:0.09547588229179382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.9148, 5.9163, 5.9148],
        [5.9148, 5.9703, 5.9296],
        [5.9148, 6.1002, 6.0186],
        [5.9148, 5.9151, 5.9148]], grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.09529104828834534 
model_pd.l_d.mean(): -8.50226879119873 
model_pd.lagr.mean(): -8.406977653503418 
model_pd.lambdas: dict_items([('pout', tensor([1.4367])), ('power', tensor([0.4250]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4328])), ('power', tensor([-21.4156]))])
epoch：509	 i:0 	 global-step:10180	 l-p:0.09529104828834534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[5.9209, 6.9035, 7.2527],
        [5.9209, 5.9209, 5.9209],
        [5.9209, 5.9270, 5.9214],
        [5.9209, 5.9213, 5.9209]], grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.09508395940065384 
model_pd.l_d.mean(): -8.478961944580078 
model_pd.lagr.mean(): -8.383877754211426 
model_pd.lambdas: dict_items([('pout', tensor([1.4372])), ('power', tensor([0.4239]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4314])), ('power', tensor([-21.4105]))])
epoch：510	 i:0 	 global-step:10200	 l-p:0.09508395940065384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[5.9277, 6.1705, 6.0873],
        [5.9277, 6.9105, 7.2592],
        [5.9277, 6.2312, 6.1548],
        [5.9277, 5.9600, 5.9339]], grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.0948629304766655 
model_pd.l_d.mean(): -8.455652236938477 
model_pd.lagr.mean(): -8.36078929901123 
model_pd.lambdas: dict_items([('pout', tensor([1.4376])), ('power', tensor([0.4228]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4299])), ('power', tensor([-21.4049]))])
epoch：511	 i:0 	 global-step:10220	 l-p:0.0948629304766655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.9347, 6.8294, 7.0985],
        [5.9347, 5.9348, 5.9347],
        [5.9347, 6.0406, 5.9767],
        [5.9347, 6.4474, 6.4459]], grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.09463610500097275 
model_pd.l_d.mean(): -8.43234920501709 
model_pd.lagr.mean(): -8.337713241577148 
model_pd.lambdas: dict_items([('pout', tensor([1.4380])), ('power', tensor([0.4218]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4283])), ('power', tensor([-21.3991]))])
epoch：512	 i:0 	 global-step:10240	 l-p:0.09463610500097275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[5.9419, 5.9425, 5.9419],
        [5.9419, 6.9279, 7.2778],
        [5.9419, 6.2328, 6.1539],
        [5.9419, 5.9480, 5.9424]], grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.09441063553094864 
model_pd.l_d.mean(): -8.409058570861816 
model_pd.lagr.mean(): -8.314647674560547 
model_pd.lambdas: dict_items([('pout', tensor([1.4385])), ('power', tensor([0.4207]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267])), ('power', tensor([-21.3931]))])
epoch：513	 i:0 	 global-step:10260	 l-p:0.09441063553094864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[5.9491, 6.0048, 5.9639],
        [5.9491, 6.4634, 6.4620],
        [5.9491, 5.9491, 5.9491],
        [5.9491, 6.1061, 6.0282]], grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.09419237077236176 
model_pd.l_d.mean(): -8.385787010192871 
model_pd.lagr.mean(): -8.291594505310059 
model_pd.lambdas: dict_items([('pout', tensor([1.4389])), ('power', tensor([0.4196]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4252])), ('power', tensor([-21.3872]))])
epoch：514	 i:0 	 global-step:10280	 l-p:0.09419237077236176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.9560, 6.9451, 7.2962],
        [5.9560, 6.7519, 6.9370],
        [5.9560, 6.1036, 6.0276],
        [5.9560, 6.0118, 5.9709]], grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.09398528933525085 
model_pd.l_d.mean(): -8.362537384033203 
model_pd.lagr.mean(): -8.26855182647705 
model_pd.lambdas: dict_items([('pout', tensor([1.4393])), ('power', tensor([0.4186]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4236])), ('power', tensor([-21.3814]))])
epoch：515	 i:0 	 global-step:10300	 l-p:0.09398528933525085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[5.9627, 5.9629, 5.9627],
        [5.9627, 6.2687, 6.1918],
        [5.9627, 6.4786, 6.4773],
        [5.9627, 5.9627, 5.9627]], grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.0937916487455368 
model_pd.l_d.mean(): -8.339314460754395 
model_pd.lagr.mean(): -8.245522499084473 
model_pd.lambdas: dict_items([('pout', tensor([1.4397])), ('power', tensor([0.4175]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4221])), ('power', tensor([-21.3759]))])
epoch：516	 i:0 	 global-step:10320	 l-p:0.0937916487455368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[5.9689, 6.4686, 6.4586],
        [5.9689, 5.9689, 5.9689],
        [5.9689, 6.5216, 6.5395],
        [5.9689, 6.9620, 7.3153]], grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.09361208230257034 
model_pd.l_d.mean(): -8.316117286682129 
model_pd.lagr.mean(): -8.222505569458008 
model_pd.lambdas: dict_items([('pout', tensor([1.4402])), ('power', tensor([0.4164]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4208])), ('power', tensor([-21.3707]))])
epoch：517	 i:0 	 global-step:10340	 l-p:0.09361208230257034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[5.9749, 5.9750, 5.9749],
        [5.9749, 6.4752, 6.4652],
        [5.9749, 7.5705, 8.5702],
        [5.9749, 6.1028, 6.0315]], grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.09344598650932312 
model_pd.l_d.mean(): -8.292943954467773 
model_pd.lagr.mean(): -8.199498176574707 
model_pd.lambdas: dict_items([('pout', tensor([1.4406])), ('power', tensor([0.4153]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4194])), ('power', tensor([-21.3658]))])
epoch：518	 i:0 	 global-step:10360	 l-p:0.09344598650932312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[5.9805, 6.1290, 6.0525],
        [5.9805, 6.2740, 6.1945],
        [5.9805, 6.3860, 6.3354],
        [5.9805, 6.2878, 6.2106]], grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.09329172223806381 
model_pd.l_d.mean(): -8.269795417785645 
model_pd.lagr.mean(): -8.176504135131836 
model_pd.lambdas: dict_items([('pout', tensor([1.4410])), ('power', tensor([0.4143]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4182])), ('power', tensor([-21.3611]))])
epoch：519	 i:0 	 global-step:10380	 l-p:0.09329172223806381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[5.9858, 7.7149, 8.8766],
        [5.9858, 5.9920, 5.9863],
        [5.9858, 5.9861, 5.9858],
        [5.9858, 6.2797, 6.2002]], grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.09314696490764618 
model_pd.l_d.mean(): -8.24666690826416 
model_pd.lagr.mean(): -8.153519630432129 
model_pd.lambdas: dict_items([('pout', tensor([1.4414])), ('power', tensor([0.4132]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4170])), ('power', tensor([-21.3567]))])
epoch：520	 i:0 	 global-step:10400	 l-p:0.09314696490764618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.9910, 6.9889, 7.3441],
        [5.9910, 7.7220, 8.8850],
        [5.9910, 6.8970, 7.1698],
        [5.9910, 5.9916, 5.9910]], grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.09300892055034637 
model_pd.l_d.mean(): -8.223555564880371 
model_pd.lagr.mean(): -8.130546569824219 
model_pd.lambdas: dict_items([('pout', tensor([1.4418])), ('power', tensor([0.4121]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159])), ('power', tensor([-21.3524]))])
epoch：521	 i:0 	 global-step:10420	 l-p:0.09300892055034637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.9961, 6.0528, 6.0113],
        [5.9961, 6.4489, 6.4161],
        [5.9961, 6.0253, 6.0013],
        [5.9961, 5.9961, 5.9961]], grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.09287470579147339 
model_pd.l_d.mean(): -8.200461387634277 
model_pd.lagr.mean(): -8.107586860656738 
model_pd.lambdas: dict_items([('pout', tensor([1.4422])), ('power', tensor([0.4111]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4148])), ('power', tensor([-21.3481]))])
epoch：522	 i:0 	 global-step:10440	 l-p:0.09287470579147339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[6.0013, 6.8053, 6.9925],
        [6.0013, 6.9093, 7.1829],
        [6.0013, 6.3260, 6.2516],
        [6.0013, 6.4088, 6.3580]], grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.09274163842201233 
model_pd.l_d.mean(): -8.177377700805664 
model_pd.lagr.mean(): -8.084635734558105 
model_pd.lambdas: dict_items([('pout', tensor([1.4427])), ('power', tensor([0.4100]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4136])), ('power', tensor([-21.3439]))])
epoch：523	 i:0 	 global-step:10460	 l-p:0.09274163842201233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[6.0066, 7.0067, 7.3621],
        [6.0066, 7.3626, 8.0802],
        [6.0066, 6.0110, 6.0068],
        [6.0066, 6.5640, 6.5822]], grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.09260719269514084 
model_pd.l_d.mean(): -8.154302597045898 
model_pd.lagr.mean(): -8.061695098876953 
model_pd.lambdas: dict_items([('pout', tensor([1.4431])), ('power', tensor([0.4089]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4125])), ('power', tensor([-21.3395]))])
epoch：524	 i:0 	 global-step:10480	 l-p:0.09260719269514084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.0121, 6.0414, 6.0173],
        [6.0121, 6.7172, 6.8293],
        [6.0121, 6.2599, 6.1750],
        [6.0121, 6.0121, 6.0121]], grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.09246952086687088 
model_pd.l_d.mean(): -8.13123893737793 
model_pd.lagr.mean(): -8.038769721984863 
model_pd.lambdas: dict_items([('pout', tensor([1.4435])), ('power', tensor([0.4079]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113])), ('power', tensor([-21.3349]))])
epoch：525	 i:0 	 global-step:10500	 l-p:0.09246952086687088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[6.0178, 6.2078, 6.1244],
        [6.0178, 7.6281, 8.6373],
        [6.0178, 6.2323, 6.1471],
        [6.0178, 6.4729, 6.4400]], grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.09232709556818008 
model_pd.l_d.mean(): -8.10818099975586 
model_pd.lagr.mean(): -8.015853881835938 
model_pd.lambdas: dict_items([('pout', tensor([1.4439])), ('power', tensor([0.4068]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4100])), ('power', tensor([-21.3301]))])
epoch：526	 i:0 	 global-step:10520	 l-p:0.09232709556818008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[6.0239, 6.3344, 6.2565],
        [6.0239, 6.0242, 6.0239],
        [6.0239, 6.0239, 6.0239],
        [6.0239, 6.0807, 6.0390]], grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.09217912703752518 
model_pd.l_d.mean(): -8.085131645202637 
model_pd.lagr.mean(): -7.992952346801758 
model_pd.lambdas: dict_items([('pout', tensor([1.4443])), ('power', tensor([0.4057]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4087])), ('power', tensor([-21.3250]))])
epoch：527	 i:0 	 global-step:10540	 l-p:0.09217912703752518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[6.0304, 7.6449, 8.6569],
        [6.0304, 6.0872, 6.0455],
        [6.0304, 6.0304, 6.0304],
        [6.0304, 6.0304, 6.0304]], grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.09202533960342407 
model_pd.l_d.mean(): -8.062088012695312 
model_pd.lagr.mean(): -7.970062732696533 
model_pd.lambdas: dict_items([('pout', tensor([1.4447])), ('power', tensor([0.4047]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4072])), ('power', tensor([-21.3197]))])
epoch：528	 i:0 	 global-step:10560	 l-p:0.09202533960342407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[6.0372, 6.5619, 6.5609],
        [6.0372, 6.1975, 6.1181],
        [6.0372, 6.0374, 6.0372],
        [6.0372, 7.7850, 8.9599]], grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.09186583757400513 
model_pd.l_d.mean(): -8.039052963256836 
model_pd.lagr.mean(): -7.9471869468688965 
model_pd.lambdas: dict_items([('pout', tensor([1.4451])), ('power', tensor([0.4036]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4058])), ('power', tensor([-21.3140]))])
epoch：529	 i:0 	 global-step:10580	 l-p:0.09186583757400513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[6.0443, 6.0447, 6.0443],
        [6.0443, 7.3829, 8.0753],
        [6.0443, 6.0446, 6.0443],
        [6.0443, 6.0996, 6.0588]], grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.09170124679803848 
model_pd.l_d.mean(): -8.01602840423584 
model_pd.lagr.mean(): -7.924327373504639 
model_pd.lambdas: dict_items([('pout', tensor([1.4455])), ('power', tensor([0.4025]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4042])), ('power', tensor([-21.3081]))])
epoch：530	 i:0 	 global-step:10600	 l-p:0.09170124679803848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.0518, 7.4210, 8.1458],
        [6.0518, 7.3925, 8.0861],
        [6.0518, 7.0542, 7.4064],
        [6.0518, 6.1084, 6.0668]], grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.09153246879577637 
model_pd.l_d.mean(): -7.993011474609375 
model_pd.lagr.mean(): -7.9014787673950195 
model_pd.lambdas: dict_items([('pout', tensor([1.4459])), ('power', tensor([0.4015]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4026])), ('power', tensor([-21.3018]))])
epoch：531	 i:0 	 global-step:10620	 l-p:0.09153246879577637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.0595, 6.8739, 7.0639],
        [6.0595, 6.4725, 6.4212],
        [6.0595, 6.0768, 6.0618],
        [6.0595, 6.1163, 6.0746]], grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.09136054664850235 
model_pd.l_d.mean(): -7.970009803771973 
model_pd.lagr.mean(): -7.8786492347717285 
model_pd.lambdas: dict_items([('pout', tensor([1.4463])), ('power', tensor([0.4004]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4009])), ('power', tensor([-21.2954]))])
epoch：532	 i:0 	 global-step:10640	 l-p:0.09136054664850235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[6.0675, 6.0848, 6.0698],
        [6.0675, 6.3671, 6.2861],
        [6.0675, 6.0691, 6.0676],
        [6.0675, 6.1232, 6.0821]], grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.09118658304214478 
model_pd.l_d.mean(): -7.947019100189209 
model_pd.lagr.mean(): -7.855832576751709 
model_pd.lambdas: dict_items([('pout', tensor([1.4467])), ('power', tensor([0.3993]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3991])), ('power', tensor([-21.2887]))])
epoch：533	 i:0 	 global-step:10660	 l-p:0.09118658304214478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.0757, 6.3272, 6.2413],
        [6.0757, 6.6051, 6.6042],
        [6.0757, 6.0820, 6.0762],
        [6.0757, 6.0930, 6.0780]], grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.09101167321205139 
model_pd.l_d.mean(): -7.924046039581299 
model_pd.lagr.mean(): -7.833034515380859 
model_pd.lambdas: dict_items([('pout', tensor([1.4471])), ('power', tensor([0.3983]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3973])), ('power', tensor([-21.2818]))])
epoch：534	 i:0 	 global-step:10680	 l-p:0.09101167321205139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[6.0841, 6.0841, 6.0841],
        [6.0841, 6.5460, 6.5129],
        [6.0841, 6.0857, 6.0841],
        [6.0841, 7.5299, 8.3352]], grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.09083680063486099 
model_pd.l_d.mean(): -7.901087760925293 
model_pd.lagr.mean(): -7.810250759124756 
model_pd.lambdas: dict_items([('pout', tensor([1.4475])), ('power', tensor([0.3972]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3955])), ('power', tensor([-21.2748]))])
epoch：535	 i:0 	 global-step:10700	 l-p:0.09083680063486099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.0926, 6.1225, 6.0979],
        [6.0926, 6.9129, 7.1044],
        [6.0926, 6.0928, 6.0926],
        [6.0926, 6.0929, 6.0926]], grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.09066282957792282 
model_pd.l_d.mean(): -7.878144264221191 
model_pd.lagr.mean(): -7.787481307983398 
model_pd.lambdas: dict_items([('pout', tensor([1.4479])), ('power', tensor([0.3962]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3937])), ('power', tensor([-21.2678]))])
epoch：536	 i:0 	 global-step:10720	 l-p:0.09066282957792282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.1011, 6.1011, 6.1011],
        [6.1011, 6.5180, 6.4664],
        [6.1011, 6.1017, 6.1012],
        [6.1011, 6.1186, 6.1034]], grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.0904904380440712 
model_pd.l_d.mean(): -7.855222702026367 
model_pd.lagr.mean(): -7.764732360839844 
model_pd.lambdas: dict_items([('pout', tensor([1.4483])), ('power', tensor([0.3951]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3918])), ('power', tensor([-21.2606]))])
epoch：537	 i:0 	 global-step:10740	 l-p:0.0904904380440712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.1098, 6.9331, 7.1255],
        [6.1098, 6.4264, 6.3472],
        [6.1098, 6.1098, 6.1098],
        [6.1098, 6.3289, 6.2420]], grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.0903201475739479 
model_pd.l_d.mean(): -7.832318305969238 
model_pd.lagr.mean(): -7.741998195648193 
model_pd.lambdas: dict_items([('pout', tensor([1.4487])), ('power', tensor([0.3940]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3900])), ('power', tensor([-21.2534]))])
epoch：538	 i:0 	 global-step:10760	 l-p:0.0903201475739479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[6.1184, 6.6352, 6.6255],
        [6.1184, 6.1768, 6.1341],
        [6.1184, 6.1185, 6.1184],
        [6.1184, 6.1184, 6.1184]], grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.09015221893787384 
model_pd.l_d.mean(): -7.809432506561279 
model_pd.lagr.mean(): -7.719280242919922 
model_pd.lambdas: dict_items([('pout', tensor([1.4491])), ('power', tensor([0.3930]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3881])), ('power', tensor([-21.2461]))])
epoch：539	 i:0 	 global-step:10780	 l-p:0.09015221893787384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[6.1271, 6.1611, 6.1337],
        [6.1271, 7.5861, 8.3990],
        [6.1271, 6.3816, 6.2947],
        [6.1271, 6.1857, 6.1428]], grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.08998677879571915 
model_pd.l_d.mean(): -7.786565780639648 
model_pd.lagr.mean(): -7.6965789794921875 
model_pd.lambdas: dict_items([('pout', tensor([1.4495])), ('power', tensor([0.3919]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3862])), ('power', tensor([-21.2388]))])
epoch：540	 i:0 	 global-step:10800	 l-p:0.08998677879571915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[6.1359, 6.6546, 6.6450],
        [6.1359, 6.2688, 6.1949],
        [6.1359, 6.1359, 6.1359],
        [6.1359, 6.1942, 6.1514]], grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.08982383459806442 
model_pd.l_d.mean(): -7.763718605041504 
model_pd.lagr.mean(): -7.673894882202148 
model_pd.lambdas: dict_items([('pout', tensor([1.4498])), ('power', tensor([0.3908]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3843])), ('power', tensor([-21.2315]))])
epoch：541	 i:0 	 global-step:10820	 l-p:0.08982383459806442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.1446, 6.1450, 6.1446],
        [6.1446, 6.6821, 6.6815],
        [6.1446, 6.6644, 6.6548],
        [6.1446, 6.2025, 6.1600]], grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.08966325968503952 
model_pd.l_d.mean(): -7.740890979766846 
model_pd.lagr.mean(): -7.651227951049805 
model_pd.lambdas: dict_items([('pout', tensor([1.4502])), ('power', tensor([0.3898]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3825])), ('power', tensor([-21.2242]))])
epoch：542	 i:0 	 global-step:10840	 l-p:0.08966325968503952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[6.1534, 6.3182, 6.2367],
        [6.1534, 6.4896, 6.4130],
        [6.1534, 6.8808, 6.9972],
        [6.1534, 6.1534, 6.1534]], grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.08950480818748474 
model_pd.l_d.mean(): -7.718082427978516 
model_pd.lagr.mean(): -7.628577709197998 
model_pd.lambdas: dict_items([('pout', tensor([1.4506])), ('power', tensor([0.3887]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3806])), ('power', tensor([-21.2168]))])
epoch：543	 i:0 	 global-step:10860	 l-p:0.08950480818748474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.1623, 6.1965, 6.1689],
        [6.1623, 6.1927, 6.1677],
        [6.1623, 6.1687, 6.1627],
        [6.1623, 6.6671, 6.6493]], grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.0893481895327568 
model_pd.l_d.mean(): -7.6952948570251465 
model_pd.lagr.mean(): -7.6059465408325195 
model_pd.lambdas: dict_items([('pout', tensor([1.4510])), ('power', tensor([0.3877]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3787])), ('power', tensor([-21.2094]))])
epoch：544	 i:0 	 global-step:10880	 l-p:0.0893481895327568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[6.1712, 6.3052, 6.2307],
        [6.1712, 6.6422, 6.6087],
        [6.1712, 6.1728, 6.1712],
        [6.1712, 6.7118, 6.7113]], grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.08919308334589005 
model_pd.l_d.mean(): -7.672524452209473 
model_pd.lagr.mean(): -7.58333158493042 
model_pd.lambdas: dict_items([('pout', tensor([1.4514])), ('power', tensor([0.3866]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3768])), ('power', tensor([-21.2019]))])
epoch：545	 i:0 	 global-step:10900	 l-p:0.08919308334589005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[6.1802, 6.1886, 6.1809],
        [6.1802, 6.3358, 6.2558],
        [6.1802, 6.2386, 6.1957],
        [6.1802, 6.9685, 7.1272]], grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.08903919160366058 
model_pd.l_d.mean(): -7.649774551391602 
model_pd.lagr.mean(): -7.56073522567749 
model_pd.lambdas: dict_items([('pout', tensor([1.4517])), ('power', tensor([0.3855]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3748])), ('power', tensor([-21.1943]))])
epoch：546	 i:0 	 global-step:10920	 l-p:0.08903919160366058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[6.1893, 6.1899, 6.1893],
        [6.1893, 6.6621, 6.6286],
        [6.1893, 6.1909, 6.1893],
        [6.1893, 7.2294, 7.6001]], grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.08888613432645798 
model_pd.l_d.mean(): -7.627042293548584 
model_pd.lagr.mean(): -7.538156032562256 
model_pd.lambdas: dict_items([('pout', tensor([1.4521])), ('power', tensor([0.3845]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3729])), ('power', tensor([-21.1867]))])
epoch：547	 i:0 	 global-step:10940	 l-p:0.08888613432645798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[6.1985, 6.1985, 6.1985],
        [6.1985, 6.3967, 6.3098],
        [6.1985, 6.7074, 6.6896],
        [6.1985, 8.0054, 9.2215]], grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.08873366564512253 
model_pd.l_d.mean(): -7.604330539703369 
model_pd.lagr.mean(): -7.515596866607666 
model_pd.lambdas: dict_items([('pout', tensor([1.4525])), ('power', tensor([0.3834]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3709])), ('power', tensor([-21.1789]))])
epoch：548	 i:0 	 global-step:10960	 l-p:0.08873366564512253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[6.2078, 6.2672, 6.2237],
        [6.2078, 6.2078, 6.2078],
        [6.2078, 6.7178, 6.7000],
        [6.2078, 7.2442, 7.6093]], grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.08858151733875275 
model_pd.l_d.mean(): -7.581636428833008 
model_pd.lagr.mean(): -7.4930548667907715 
model_pd.lambdas: dict_items([('pout', tensor([1.4529])), ('power', tensor([0.3824]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3690])), ('power', tensor([-21.1710]))])
epoch：549	 i:0 	 global-step:10980	 l-p:0.08858151733875275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[6.2173, 7.0599, 7.2573],
        [6.2173, 7.7037, 8.5325],
        [6.2173, 6.2259, 6.2181],
        [6.2173, 6.7454, 6.7360]], grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.08842947334051132 
model_pd.l_d.mean(): -7.558963298797607 
model_pd.lagr.mean(): -7.470533847808838 
model_pd.lambdas: dict_items([('pout', tensor([1.4532])), ('power', tensor([0.3813]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3669])), ('power', tensor([-21.1630]))])
epoch：550	 i:0 	 global-step:11000	 l-p:0.08842947334051132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.2270, 7.2765, 7.6516],
        [6.2270, 6.2317, 6.2273],
        [6.2270, 6.4264, 6.3391],
        [6.2270, 7.0713, 7.2692]], grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.0882774367928505 
model_pd.l_d.mean(): -7.5363078117370605 
model_pd.lagr.mean(): -7.448030471801758 
model_pd.lambdas: dict_items([('pout', tensor([1.4536])), ('power', tensor([0.3803]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3649])), ('power', tensor([-21.1549]))])
epoch：551	 i:0 	 global-step:11020	 l-p:0.0882774367928505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[6.2368, 6.3506, 6.2821],
        [6.2368, 6.2368, 6.2368],
        [6.2368, 6.4367, 6.3492],
        [6.2368, 8.0578, 9.2836]], grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.08812527358531952 
model_pd.l_d.mean(): -7.513671875 
model_pd.lagr.mean(): -7.425546646118164 
model_pd.lambdas: dict_items([('pout', tensor([1.4539])), ('power', tensor([0.3792]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3628])), ('power', tensor([-21.1466]))])
epoch：552	 i:0 	 global-step:11040	 l-p:0.08812527358531952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[6.2468, 6.5733, 6.4920],
        [6.2468, 6.2778, 6.2524],
        [6.2468, 6.5901, 6.5121],
        [6.2468, 7.6727, 8.4291]], grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.08797294646501541 
model_pd.l_d.mean(): -7.491055488586426 
model_pd.lagr.mean(): -7.403082370758057 
model_pd.lambdas: dict_items([('pout', tensor([1.4543])), ('power', tensor([0.3781]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3607])), ('power', tensor([-21.1382]))])
epoch：553	 i:0 	 global-step:11060	 l-p:0.08797294646501541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[6.2570, 6.2570, 6.2570],
        [6.2570, 6.3152, 6.2722],
        [6.2570, 6.6885, 6.6356],
        [6.2570, 7.6858, 8.4438]], grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.08782047033309937 
model_pd.l_d.mean(): -7.4684600830078125 
model_pd.lagr.mean(): -7.380639553070068 
model_pd.lambdas: dict_items([('pout', tensor([1.4547])), ('power', tensor([0.3771]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3585])), ('power', tensor([-21.1296]))])
epoch：554	 i:0 	 global-step:11080	 l-p:0.08782047033309937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[6.2673, 6.7483, 6.7144],
        [6.2673, 6.4686, 6.3805],
        [6.2673, 6.2985, 6.2729],
        [6.2673, 6.2676, 6.2674]], grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.08766791224479675 
model_pd.l_d.mean(): -7.445883274078369 
model_pd.lagr.mean(): -7.35821533203125 
model_pd.lambdas: dict_items([('pout', tensor([1.4550])), ('power', tensor([0.3760]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3564])), ('power', tensor([-21.1208]))])
epoch：555	 i:0 	 global-step:11100	 l-p:0.08766791224479675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[6.2779, 6.5055, 6.4155],
        [6.2779, 6.4370, 6.3553],
        [6.2779, 7.1312, 7.3315],
        [6.2779, 6.2795, 6.2779]], grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.08751526474952698 
model_pd.l_d.mean(): -7.423327922821045 
model_pd.lagr.mean(): -7.335812568664551 
model_pd.lambdas: dict_items([('pout', tensor([1.4554])), ('power', tensor([0.3750]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3541])), ('power', tensor([-21.1119]))])
epoch：556	 i:0 	 global-step:11120	 l-p:0.08751526474952698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[6.2886, 6.2888, 6.2886],
        [6.2886, 6.3484, 6.3045],
        [6.2886, 8.1285, 9.3675],
        [6.2886, 6.2902, 6.2886]], grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.08736272901296616 
model_pd.l_d.mean(): -7.400794982910156 
model_pd.lagr.mean(): -7.313432216644287 
model_pd.lambdas: dict_items([('pout', tensor([1.4557])), ('power', tensor([0.3739]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3519])), ('power', tensor([-21.1029]))])
epoch：557	 i:0 	 global-step:11140	 l-p:0.08736272901296616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[6.2994, 6.5639, 6.4739],
        [6.2994, 6.3061, 6.2999],
        [6.2994, 7.1566, 7.3579],
        [6.2994, 6.7837, 6.7497]], grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.08721035718917847 
model_pd.l_d.mean(): -7.378281593322754 
model_pd.lagr.mean(): -7.29107141494751 
model_pd.lambdas: dict_items([('pout', tensor([1.4561])), ('power', tensor([0.3729]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3496])), ('power', tensor([-21.0937]))])
epoch：558	 i:0 	 global-step:11160	 l-p:0.08721035718917847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[6.3104, 6.3108, 6.3104],
        [6.3104, 6.3104, 6.3104],
        [6.3104, 6.6267, 6.5418],
        [6.3104, 6.6585, 6.5795]], grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.08705830574035645 
model_pd.l_d.mean(): -7.355791091918945 
model_pd.lagr.mean(): -7.268733024597168 
model_pd.lambdas: dict_items([('pout', tensor([1.4564])), ('power', tensor([0.3718]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3473])), ('power', tensor([-21.0844]))])
epoch：559	 i:0 	 global-step:11180	 l-p:0.08705830574035645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[6.3216, 6.5253, 6.4361],
        [6.3216, 6.3216, 6.3216],
        [6.3216, 6.6386, 6.5535],
        [6.3216, 7.3918, 7.7749]], grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.08690667152404785 
model_pd.l_d.mean(): -7.333322525024414 
model_pd.lagr.mean(): -7.246416091918945 
model_pd.lambdas: dict_items([('pout', tensor([1.4568])), ('power', tensor([0.3708]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3450])), ('power', tensor([-21.0750]))])
epoch：560	 i:0 	 global-step:11200	 l-p:0.08690667152404785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[6.3328, 6.8206, 6.7865],
        [6.3328, 7.7536, 8.4908],
        [6.3328, 7.1469, 7.3117],
        [6.3328, 6.5993, 6.5087]], grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.08675557374954224 
model_pd.l_d.mean(): -7.310876369476318 
model_pd.lagr.mean(): -7.224120616912842 
model_pd.lambdas: dict_items([('pout', tensor([1.4571])), ('power', tensor([0.3697]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3426])), ('power', tensor([-21.0654]))])
epoch：561	 i:0 	 global-step:11220	 l-p:0.08675557374954224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[6.3443, 6.8869, 6.8777],
        [6.3443, 6.4058, 6.3608],
        [6.3443, 6.5163, 6.4314],
        [6.3443, 8.2046, 9.4579]], grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.08660513162612915 
model_pd.l_d.mean(): -7.288453102111816 
model_pd.lagr.mean(): -7.201848030090332 
model_pd.lambdas: dict_items([('pout', tensor([1.4575])), ('power', tensor([0.3686]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3403])), ('power', tensor([-21.0557]))])
epoch：562	 i:0 	 global-step:11240	 l-p:0.08660513162612915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[6.3558, 6.3626, 6.3563],
        [6.3558, 6.3558, 6.3558],
        [6.3558, 7.4243, 7.8016],
        [6.3558, 6.3559, 6.3558]], grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.08645541965961456 
model_pd.l_d.mean(): -7.266053676605225 
model_pd.lagr.mean(): -7.179598331451416 
model_pd.lambdas: dict_items([('pout', tensor([1.4578])), ('power', tensor([0.3676]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3379])), ('power', tensor([-21.0459]))])
epoch：563	 i:0 	 global-step:11260	 l-p:0.08645541965961456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.3675, 6.3993, 6.3732],
        [6.3675, 7.3487, 7.6466],
        [6.3675, 7.7981, 8.5407],
        [6.3675, 6.8951, 6.8773]], grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.08630653470754623 
model_pd.l_d.mean(): -7.243675708770752 
model_pd.lagr.mean(): -7.157369136810303 
model_pd.lambdas: dict_items([('pout', tensor([1.4581])), ('power', tensor([0.3665]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3354])), ('power', tensor([-21.0360]))])
epoch：564	 i:0 	 global-step:11280	 l-p:0.08630653470754623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[6.3792, 6.9837, 7.0052],
        [6.3792, 6.3796, 6.3792],
        [6.3792, 6.5855, 6.4953],
        [6.3792, 7.4609, 7.8476]], grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.08615852147340775 
model_pd.l_d.mean(): -7.221323013305664 
model_pd.lagr.mean(): -7.135164260864258 
model_pd.lambdas: dict_items([('pout', tensor([1.4585])), ('power', tensor([0.3655]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3330])), ('power', tensor([-21.0260]))])
epoch：565	 i:0 	 global-step:11300	 l-p:0.08615852147340775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[6.3911, 6.5650, 6.4791],
        [6.3911, 6.7130, 6.6268],
        [6.3911, 7.9302, 8.7899],
        [6.3911, 6.9392, 6.9300]], grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.08601143211126328 
model_pd.l_d.mean(): -7.1989946365356445 
model_pd.lagr.mean(): -7.112983226776123 
model_pd.lambdas: dict_items([('pout', tensor([1.4588])), ('power', tensor([0.3644]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3306])), ('power', tensor([-21.0159]))])
epoch：566	 i:0 	 global-step:11320	 l-p:0.08601143211126328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228]])
 pt:tensor([[6.4031, 8.2849, 9.5533],
        [6.4031, 6.9712, 6.9716],
        [6.4031, 6.6370, 6.5447],
        [6.4031, 7.0105, 7.0322]], grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.08586529642343521 
model_pd.l_d.mean(): -7.176689147949219 
model_pd.lagr.mean(): -7.090823650360107 
model_pd.lambdas: dict_items([('pout', tensor([1.4591])), ('power', tensor([0.3634]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3281])), ('power', tensor([-21.0057]))])
epoch：567	 i:0 	 global-step:11340	 l-p:0.08586529642343521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[6.4152, 6.9847, 6.9852],
        [6.4152, 6.4777, 6.4319],
        [6.4152, 6.9660, 6.9569],
        [6.4152, 6.5900, 6.5037]], grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.08572012186050415 
model_pd.l_d.mean(): -7.154408931732178 
model_pd.lagr.mean(): -7.068688869476318 
model_pd.lambdas: dict_items([('pout', tensor([1.4594])), ('power', tensor([0.3623]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3256])), ('power', tensor([-20.9954]))])
epoch：568	 i:0 	 global-step:11360	 l-p:0.08572012186050415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[6.4273, 6.7669, 6.6827],
        [6.4273, 6.4291, 6.4274],
        [6.4273, 6.6625, 6.5698],
        [6.4273, 6.4273, 6.4273]], grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.08557592332363129 
model_pd.l_d.mean(): -7.132153511047363 
model_pd.lagr.mean(): -7.046577453613281 
model_pd.lambdas: dict_items([('pout', tensor([1.4598])), ('power', tensor([0.3613]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3231])), ('power', tensor([-20.9850]))])
epoch：569	 i:0 	 global-step:11380	 l-p:0.08557592332363129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[6.4396, 6.4396, 6.4396],
        [6.4396, 7.5357, 7.9288],
        [6.4396, 6.4413, 6.4397],
        [6.4396, 6.4396, 6.4396]], grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.08543266355991364 
model_pd.l_d.mean(): -7.109921932220459 
model_pd.lagr.mean(): -7.024489402770996 
model_pd.lambdas: dict_items([('pout', tensor([1.4601])), ('power', tensor([0.3602]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3206])), ('power', tensor([-20.9746]))])
epoch：570	 i:0 	 global-step:11400	 l-p:0.08543266355991364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.4520, 8.0096, 8.8801],
        [6.4520, 7.5508, 7.9449],
        [6.4520, 6.4520, 6.4520],
        [6.4520, 6.4709, 6.4544]], grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.0852903202176094 
model_pd.l_d.mean(): -7.087716102600098 
model_pd.lagr.mean(): -7.002425670623779 
model_pd.lambdas: dict_items([('pout', tensor([1.4604])), ('power', tensor([0.3592]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3181])), ('power', tensor([-20.9640]))])
epoch：571	 i:0 	 global-step:11420	 l-p:0.0852903202176094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[6.4644, 6.5273, 6.4813],
        [6.4644, 6.6411, 6.5540],
        [6.4644, 7.4651, 7.7695],
        [6.4644, 6.4644, 6.4644]], grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.0851488709449768 
model_pd.l_d.mean(): -7.065535545349121 
model_pd.lagr.mean(): -6.980386734008789 
model_pd.lambdas: dict_items([('pout', tensor([1.4607])), ('power', tensor([0.3581]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3155])), ('power', tensor([-20.9534]))])
epoch：572	 i:0 	 global-step:11440	 l-p:0.0851488709449768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[6.4770, 6.4770, 6.4770],
        [6.4770, 6.7519, 6.6587],
        [6.4770, 7.5718, 7.9593],
        [6.4770, 6.4771, 6.4770]], grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.08500828593969345 
model_pd.l_d.mean(): -7.043380260467529 
model_pd.lagr.mean(): -6.958372116088867 
model_pd.lambdas: dict_items([('pout', tensor([1.4610])), ('power', tensor([0.3571]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3129])), ('power', tensor([-20.9426]))])
epoch：573	 i:0 	 global-step:11460	 l-p:0.08500828593969345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[6.4897, 6.5510, 6.5058],
        [6.4897, 7.3303, 7.5013],
        [6.4897, 7.0310, 7.0132],
        [6.4897, 6.9431, 6.8882]], grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.08486852794885635 
model_pd.l_d.mean(): -7.021249294281006 
model_pd.lagr.mean(): -6.936380863189697 
model_pd.lambdas: dict_items([('pout', tensor([1.4613])), ('power', tensor([0.3561]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3104])), ('power', tensor([-20.9318]))])
epoch：574	 i:0 	 global-step:11480	 l-p:0.08486852794885635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[6.5025, 6.5353, 6.5083],
        [6.5025, 6.5031, 6.5025],
        [6.5025, 6.5027, 6.5025],
        [6.5025, 6.5028, 6.5025]], grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.08472954481840134 
model_pd.l_d.mean(): -6.999145030975342 
model_pd.lagr.mean(): -6.91441535949707 
model_pd.lambdas: dict_items([('pout', tensor([1.4617])), ('power', tensor([0.3550]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3078])), ('power', tensor([-20.9208]))])
epoch：575	 i:0 	 global-step:11500	 l-p:0.08472954481840134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[6.5153, 6.5156, 6.5153],
        [6.5153, 7.0968, 7.0977],
        [6.5153, 6.8613, 6.7757],
        [6.5153, 7.6281, 8.0276]], grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.08459131419658661 
model_pd.l_d.mean(): -6.977066516876221 
model_pd.lagr.mean(): -6.892475128173828 
model_pd.lambdas: dict_items([('pout', tensor([1.4620])), ('power', tensor([0.3540]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3052])), ('power', tensor([-20.9098]))])
epoch：576	 i:0 	 global-step:11520	 l-p:0.08459131419658661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[6.5283, 6.5902, 6.5446],
        [6.5283, 8.3124, 9.4353],
        [6.5283, 7.1517, 7.1745],
        [6.5283, 8.0361, 8.8381]], grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.08445379137992859 
model_pd.l_d.mean(): -6.955014228820801 
model_pd.lagr.mean(): -6.870560646057129 
model_pd.lambdas: dict_items([('pout', tensor([1.4623])), ('power', tensor([0.3529]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3025])), ('power', tensor([-20.8987]))])
epoch：577	 i:0 	 global-step:11540	 l-p:0.08445379137992859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[6.5414, 6.5485, 6.5420],
        [6.5414, 6.5607, 6.5439],
        [6.5414, 6.7211, 6.6325],
        [6.5414, 7.6503, 8.0431]], grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.08431693911552429 
model_pd.l_d.mean(): -6.932987213134766 
model_pd.lagr.mean(): -6.848670482635498 
model_pd.lambdas: dict_items([('pout', tensor([1.4626])), ('power', tensor([0.3519]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2999])), ('power', tensor([-20.8874]))])
epoch：578	 i:0 	 global-step:11560	 l-p:0.08431693911552429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[6.5547, 7.4063, 7.5798],
        [6.5547, 7.1813, 7.2044],
        [6.5547, 6.5553, 6.5547],
        [6.5547, 6.6767, 6.6034]], grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.08418072760105133 
model_pd.l_d.mean(): -6.91098690032959 
model_pd.lagr.mean(): -6.82680606842041 
model_pd.lambdas: dict_items([('pout', tensor([1.4629])), ('power', tensor([0.3508]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2972])), ('power', tensor([-20.8761]))])
epoch：579	 i:0 	 global-step:11580	 l-p:0.08418072760105133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[6.5680, 6.5751, 6.5685],
        [6.5680, 6.6327, 6.5854],
        [6.5680, 6.5686, 6.5680],
        [6.5680, 7.1964, 7.2195]], grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.08404514938592911 
model_pd.l_d.mean(): -6.889013767242432 
model_pd.lagr.mean(): -6.80496883392334 
model_pd.lambdas: dict_items([('pout', tensor([1.4632])), ('power', tensor([0.3498]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2945])), ('power', tensor([-20.8647]))])
epoch：580	 i:0 	 global-step:11600	 l-p:0.08404514938592911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[6.5815, 6.7626, 6.6734],
        [6.5815, 6.5815, 6.5815],
        [6.5815, 7.6990, 8.0951],
        [6.5815, 7.2115, 7.2348]], grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.08391016721725464 
model_pd.l_d.mean(): -6.867065906524658 
model_pd.lagr.mean(): -6.783155918121338 
model_pd.lambdas: dict_items([('pout', tensor([1.4634])), ('power', tensor([0.3487]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2918])), ('power', tensor([-20.8531]))])
epoch：581	 i:0 	 global-step:11620	 l-p:0.08391016721725464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.5950, 7.1860, 7.1872],
        [6.5950, 6.5950, 6.5950],
        [6.5950, 6.5950, 6.5950],
        [6.5950, 7.5051, 7.7205]], grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.08377576619386673 
model_pd.l_d.mean(): -6.845145225524902 
model_pd.lagr.mean(): -6.761369228363037 
model_pd.lambdas: dict_items([('pout', tensor([1.4637])), ('power', tensor([0.3477]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2891])), ('power', tensor([-20.8414]))])
epoch：582	 i:0 	 global-step:11640	 l-p:0.08377576619386673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[6.6087, 6.6088, 6.6087],
        [6.6087, 6.8532, 6.7570],
        [6.6087, 6.8914, 6.7957],
        [6.6087, 6.6087, 6.6087]], grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.08364196121692657 
model_pd.l_d.mean(): -6.823251247406006 
model_pd.lagr.mean(): -6.739609241485596 
model_pd.lambdas: dict_items([('pout', tensor([1.4640])), ('power', tensor([0.3467]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2864])), ('power', tensor([-20.8296]))])
epoch：583	 i:0 	 global-step:11660	 l-p:0.08364196121692657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.6225, 6.6297, 6.6230],
        [6.6225, 8.4387, 9.5826],
        [6.6225, 7.2578, 7.2814],
        [6.6225, 7.5375, 7.7542]], grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.08350870758295059 
model_pd.l_d.mean(): -6.801385402679443 
model_pd.lagr.mean(): -6.71787691116333 
model_pd.lambdas: dict_items([('pout', tensor([1.4643])), ('power', tensor([0.3456]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2836])), ('power', tensor([-20.8178]))])
epoch：584	 i:0 	 global-step:11680	 l-p:0.08350870758295059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[6.6364, 7.0093, 6.9255],
        [6.6364, 6.7017, 6.6540],
        [6.6364, 7.1559, 7.1207],
        [6.6364, 6.6365, 6.6364]], grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.08337602019309998 
model_pd.l_d.mean(): -6.779547214508057 
model_pd.lagr.mean(): -6.696171283721924 
model_pd.lambdas: dict_items([('pout', tensor([1.4646])), ('power', tensor([0.3446]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2809])), ('power', tensor([-20.8058]))])
epoch：585	 i:0 	 global-step:11700	 l-p:0.08337602019309998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.6505, 8.1938, 9.0156],
        [6.6505, 6.6505, 6.6505],
        [6.6505, 6.6505, 6.6505],
        [6.6505, 6.7154, 6.6678]], grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.08324390649795532 
model_pd.l_d.mean(): -6.7577362060546875 
model_pd.lagr.mean(): -6.674492359161377 
model_pd.lambdas: dict_items([('pout', tensor([1.4649])), ('power', tensor([0.3435]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2781])), ('power', tensor([-20.7937]))])
epoch：586	 i:0 	 global-step:11720	 l-p:0.08324390649795532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[6.6646, 6.6651, 6.6647],
        [6.6646, 8.6425, 9.9779],
        [6.6646, 6.7283, 6.6814],
        [6.6646, 6.8377, 6.7492]], grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.08311235159635544 
model_pd.l_d.mean(): -6.735953330993652 
model_pd.lagr.mean(): -6.652841091156006 
model_pd.lambdas: dict_items([('pout', tensor([1.4651])), ('power', tensor([0.3425]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2753])), ('power', tensor([-20.7814]))])
epoch：587	 i:0 	 global-step:11740	 l-p:0.08311235159635544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[6.6789, 6.8525, 6.7637],
        [6.6789, 8.5144, 9.6709],
        [6.6789, 8.2306, 9.0570],
        [6.6789, 6.8988, 6.8030]], grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.08298137784004211 
model_pd.l_d.mean(): -6.714199066162109 
model_pd.lagr.mean(): -6.6312174797058105 
model_pd.lambdas: dict_items([('pout', tensor([1.4654])), ('power', tensor([0.3415]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2725])), ('power', tensor([-20.7691]))])
epoch：588	 i:0 	 global-step:11760	 l-p:0.08298137784004211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[6.6933, 7.7400, 8.0597],
        [6.6933, 6.7276, 6.6995],
        [6.6933, 6.6933, 6.6933],
        [6.6933, 6.6933, 6.6933]], grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.08285096287727356 
model_pd.l_d.mean(): -6.692471504211426 
model_pd.lagr.mean(): -6.609620571136475 
model_pd.lambdas: dict_items([('pout', tensor([1.4657])), ('power', tensor([0.3404]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2696])), ('power', tensor([-20.7567]))])
epoch：589	 i:0 	 global-step:11780	 l-p:0.08285096287727356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[6.7078, 6.7078, 6.7078],
        [6.7078, 6.7735, 6.7253],
        [6.7078, 6.7741, 6.7256],
        [6.7078, 7.0677, 6.9791]], grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.08272112905979156 
model_pd.l_d.mean(): -6.670773029327393 
model_pd.lagr.mean(): -6.588051795959473 
model_pd.lambdas: dict_items([('pout', tensor([1.4659])), ('power', tensor([0.3394]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2668])), ('power', tensor([-20.7442]))])
epoch：590	 i:0 	 global-step:11800	 l-p:0.08272112905979156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[6.7224, 6.7224, 6.7224],
        [6.7224, 6.7613, 6.7299],
        [6.7224, 6.9443, 6.8477],
        [6.7224, 6.7277, 6.7227]], grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.08259187638759613 
model_pd.l_d.mean(): -6.649102210998535 
model_pd.lagr.mean(): -6.566510200500488 
model_pd.lambdas: dict_items([('pout', tensor([1.4662])), ('power', tensor([0.3383]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2639])), ('power', tensor([-20.7315]))])
epoch：591	 i:0 	 global-step:11820	 l-p:0.08259187638759613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.7372, 7.0831, 6.9911],
        [6.7372, 6.8042, 6.7552],
        [6.7372, 6.9597, 6.8628],
        [6.7372, 6.7376, 6.7372]], grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.08246322721242905 
model_pd.l_d.mean(): -6.6274614334106445 
model_pd.lagr.mean(): -6.5449981689453125 
model_pd.lambdas: dict_items([('pout', tensor([1.4665])), ('power', tensor([0.3373]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2610])), ('power', tensor([-20.7188]))])
epoch：592	 i:0 	 global-step:11840	 l-p:0.08246322721242905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[6.7520, 6.8793, 6.8029],
        [6.7520, 6.9282, 6.8381],
        [6.7520, 6.7538, 6.7521],
        [6.7520, 6.9752, 6.8781]], grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.08233514428138733 
model_pd.l_d.mean(): -6.6058502197265625 
model_pd.lagr.mean(): -6.523515224456787 
model_pd.lambdas: dict_items([('pout', tensor([1.4667])), ('power', tensor([0.3363]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2581])), ('power', tensor([-20.7059]))])
epoch：593	 i:0 	 global-step:11860	 l-p:0.08233514428138733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.7669, 7.3393, 7.3215],
        [6.7669, 7.5912, 7.7263],
        [6.7669, 6.7872, 6.7696],
        [6.7669, 7.0195, 6.9203]], grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.08220767229795456 
model_pd.l_d.mean(): -6.584266185760498 
model_pd.lagr.mean(): -6.502058506011963 
model_pd.lambdas: dict_items([('pout', tensor([1.4670])), ('power', tensor([0.3352]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2552])), ('power', tensor([-20.6930]))])
epoch：594	 i:0 	 global-step:11880	 l-p:0.08220767229795456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.7820, 8.3637, 9.2069],
        [6.7820, 7.9534, 8.3756],
        [6.7820, 6.9709, 6.8780],
        [6.7820, 6.7824, 6.7820]], grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.08208079636096954 
model_pd.l_d.mean(): -6.562712669372559 
model_pd.lagr.mean(): -6.4806318283081055 
model_pd.lambdas: dict_items([('pout', tensor([1.4672])), ('power', tensor([0.3342]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2523])), ('power', tensor([-20.6799]))])
epoch：595	 i:0 	 global-step:11900	 l-p:0.08208079636096954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[6.7971, 7.6899, 7.8731],
        [6.7971, 7.9618, 8.3759],
        [6.7971, 6.7978, 6.7972],
        [6.7971, 7.0909, 6.9918]], grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.08195451647043228 
model_pd.l_d.mean(): -6.541188716888428 
model_pd.lagr.mean(): -6.459234237670898 
model_pd.lambdas: dict_items([('pout', tensor([1.4675])), ('power', tensor([0.3332]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2493])), ('power', tensor([-20.6667]))])
epoch：596	 i:0 	 global-step:11920	 l-p:0.08195451647043228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[6.8124, 7.3503, 7.3144],
        [6.8124, 6.8805, 6.8308],
        [6.8124, 6.9665, 6.8812],
        [6.8124, 6.9413, 6.8639]], grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.08182883262634277 
model_pd.l_d.mean(): -6.519695281982422 
model_pd.lagr.mean(): -6.4378662109375 
model_pd.lambdas: dict_items([('pout', tensor([1.4677])), ('power', tensor([0.3321]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2464])), ('power', tensor([-20.6535]))])
epoch：597	 i:0 	 global-step:11940	 l-p:0.08182883262634277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.8278, 6.8675, 6.8355],
        [6.8278, 7.4465, 7.4487],
        [6.8278, 6.8961, 6.8462],
        [6.8278, 6.8282, 6.8278]], grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.08170375227928162 
model_pd.l_d.mean(): -6.498230934143066 
model_pd.lagr.mean(): -6.416527271270752 
model_pd.lambdas: dict_items([('pout', tensor([1.4680])), ('power', tensor([0.3311]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2434])), ('power', tensor([-20.6401]))])
epoch：598	 i:0 	 global-step:11960	 l-p:0.08170375227928162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[6.8432, 6.8831, 6.8509],
        [6.8432, 7.7978, 8.0252],
        [6.8432, 6.8437, 6.8433],
        [6.8432, 7.7439, 7.9289]], grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.08157924562692642 
model_pd.l_d.mean(): -6.476797103881836 
model_pd.lagr.mean(): -6.3952178955078125 
model_pd.lambdas: dict_items([('pout', tensor([1.4682])), ('power', tensor([0.3301]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2404])), ('power', tensor([-20.6267]))])
epoch：599	 i:0 	 global-step:11980	 l-p:0.08157924562692642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[6.8588, 8.0369, 8.4562],
        [6.8588, 7.0869, 6.9878],
        [6.8588, 6.9276, 6.8774],
        [6.8588, 7.2297, 7.1387]], grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.08145532757043839 
model_pd.l_d.mean(): -6.455392837524414 
model_pd.lagr.mean(): -6.373937606811523 
model_pd.lambdas: dict_items([('pout', tensor([1.4685])), ('power', tensor([0.3290]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2374])), ('power', tensor([-20.6131]))])
epoch：600	 i:0 	 global-step:12000	 l-p:0.08145532757043839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[6.8745, 7.9576, 8.2895],
        [6.8745, 6.8844, 6.8753],
        [6.8745, 6.9146, 6.8823],
        [6.8745, 6.8800, 6.8748]], grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.08133197575807571 
model_pd.l_d.mean(): -6.434020042419434 
model_pd.lagr.mean(): -6.352687835693359 
model_pd.lambdas: dict_items([('pout', tensor([1.4687])), ('power', tensor([0.3280]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2344])), ('power', tensor([-20.5995]))])
epoch：601	 i:0 	 global-step:12020	 l-p:0.08133197575807571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[6.8903, 7.2635, 7.1720],
        [6.8903, 6.8903, 6.8903],
        [6.8903, 8.4704, 9.2944],
        [6.8903, 6.9260, 6.8967]], grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.0812092125415802 
model_pd.l_d.mean(): -6.41267728805542 
model_pd.lagr.mean(): -6.331468105316162 
model_pd.lambdas: dict_items([('pout', tensor([1.4689])), ('power', tensor([0.3270]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2314])), ('power', tensor([-20.5857]))])
epoch：602	 i:0 	 global-step:12040	 l-p:0.0812092125415802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.9062, 6.9069, 6.9062],
        [6.9062, 6.9117, 6.9065],
        [6.9062, 6.9756, 6.9249],
        [6.9062, 7.1658, 7.0641]], grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.08108700066804886 
model_pd.l_d.mean(): -6.3913655281066895 
model_pd.lagr.mean(): -6.310278415679932 
model_pd.lambdas: dict_items([('pout', tensor([1.4691])), ('power', tensor([0.3260]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2283])), ('power', tensor([-20.5719]))])
epoch：603	 i:0 	 global-step:12060	 l-p:0.08108700066804886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.9222, 6.9222, 6.9222],
        [6.9222, 7.4716, 7.4353],
        [6.9222, 6.9241, 6.9222],
        [6.9222, 6.9222, 6.9222]], grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.08096534758806229 
model_pd.l_d.mean(): -6.370084285736084 
model_pd.lagr.mean(): -6.289118766784668 
model_pd.lambdas: dict_items([('pout', tensor([1.4694])), ('power', tensor([0.3249]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2253])), ('power', tensor([-20.5579]))])
epoch：604	 i:0 	 global-step:12080	 l-p:0.08096534758806229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[6.9383, 6.9460, 6.9388],
        [6.9383, 6.9383, 6.9383],
        [6.9383, 8.5321, 9.3637],
        [6.9383, 8.1337, 8.5597]], grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.08084423094987869 
model_pd.l_d.mean(): -6.34883451461792 
model_pd.lagr.mean(): -6.2679901123046875 
model_pd.lambdas: dict_items([('pout', tensor([1.4696])), ('power', tensor([0.3239]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2222])), ('power', tensor([-20.5438]))])
epoch：605	 i:0 	 global-step:12100	 l-p:0.08084423094987869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.9545, 7.1381, 7.0444],
        [6.9545, 8.0537, 8.3909],
        [6.9545, 6.9545, 6.9545],
        [6.9545, 7.3155, 7.2200]], grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.08072363585233688 
model_pd.l_d.mean(): -6.327615737915039 
model_pd.lagr.mean(): -6.246891975402832 
model_pd.lambdas: dict_items([('pout', tensor([1.4698])), ('power', tensor([0.3229]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2191])), ('power', tensor([-20.5297]))])
epoch：606	 i:0 	 global-step:12120	 l-p:0.08072363585233688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[6.9708, 7.5858, 7.5778],
        [6.9708, 8.6076, 9.4816],
        [6.9708, 6.9708, 6.9708],
        [6.9708, 7.0408, 6.9896]], grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.08060357719659805 
model_pd.l_d.mean(): -6.306429386138916 
model_pd.lagr.mean(): -6.225825786590576 
model_pd.lambdas: dict_items([('pout', tensor([1.4700])), ('power', tensor([0.3219]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2161])), ('power', tensor([-20.5154]))])
epoch：607	 i:0 	 global-step:12140	 l-p:0.08060357719659805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.9872, 6.9872, 6.9872],
        [6.9872, 8.6289, 9.5055],
        [6.9872, 7.4877, 7.4287],
        [6.9872, 6.9872, 6.9872]], grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.08048404008150101 
model_pd.l_d.mean(): -6.285274505615234 
model_pd.lagr.mean(): -6.2047905921936035 
model_pd.lambdas: dict_items([('pout', tensor([1.4702])), ('power', tensor([0.3208]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2129])), ('power', tensor([-20.5010]))])
epoch：608	 i:0 	 global-step:12160	 l-p:0.08048404008150101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[7.0037, 7.0746, 7.0229],
        [7.0037, 7.8656, 8.0079],
        [7.0037, 8.2240, 8.6651],
        [7.0037, 7.5058, 7.4467]], grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.08036497980356216 
model_pd.l_d.mean(): -6.264150619506836 
model_pd.lagr.mean(): -6.183785438537598 
model_pd.lambdas: dict_items([('pout', tensor([1.4705])), ('power', tensor([0.3198]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2098])), ('power', tensor([-20.4866]))])
epoch：609	 i:0 	 global-step:12180	 l-p:0.08036497980356216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[ 7.0204,  7.0889,  7.0385],
        [ 7.0204,  9.1290, 10.5558],
        [ 7.0204,  7.0207,  7.0204],
        [ 7.0204,  7.0417,  7.0232]], grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.08024642616510391 
model_pd.l_d.mean(): -6.243060111999512 
model_pd.lagr.mean(): -6.162813663482666 
model_pd.lambdas: dict_items([('pout', tensor([1.4707])), ('power', tensor([0.3188]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2067])), ('power', tensor([-20.4720]))])
epoch：610	 i:0 	 global-step:12200	 l-p:0.08024642616510391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[7.0371, 8.6593, 9.5063],
        [7.0371, 8.2633, 8.7060],
        [7.0371, 7.3035, 7.1993],
        [7.0371, 7.1059, 7.0553]], grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.08012836426496506 
model_pd.l_d.mean(): -6.2220001220703125 
model_pd.lagr.mean(): -6.141871929168701 
model_pd.lambdas: dict_items([('pout', tensor([1.4709])), ('power', tensor([0.3178]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2036])), ('power', tensor([-20.4573]))])
epoch：611	 i:0 	 global-step:12220	 l-p:0.08012836426496506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[7.0540, 7.4587, 7.3687],
        [7.0540, 7.0540, 7.0540],
        [7.0540, 7.9904, 8.1840],
        [7.0540, 7.1255, 7.0733]], grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.080010786652565 
model_pd.l_d.mean(): -6.200972557067871 
model_pd.lagr.mean(): -6.120961666107178 
model_pd.lambdas: dict_items([('pout', tensor([1.4711])), ('power', tensor([0.3167]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2004])), ('power', tensor([-20.4425]))])
epoch：612	 i:0 	 global-step:12240	 l-p:0.080010786652565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[7.0709, 7.0925, 7.0737],
        [7.0709, 7.0788, 7.0715],
        [7.0709, 7.9434, 8.0879],
        [7.0709, 7.1417, 7.0899]], grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.07989367097616196 
model_pd.l_d.mean(): -6.179978370666504 
model_pd.lagr.mean(): -6.1000847816467285 
model_pd.lambdas: dict_items([('pout', tensor([1.4713])), ('power', tensor([0.3157]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1972])), ('power', tensor([-20.4276]))])
epoch：613	 i:0 	 global-step:12260	 l-p:0.07989367097616196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[7.0880, 8.0303, 8.2252],
        [7.0880, 8.0866, 8.3257],
        [7.0880, 7.0880, 7.0880],
        [7.0880, 7.0884, 7.0880]], grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.07977701723575592 
model_pd.l_d.mean(): -6.159017086029053 
model_pd.lagr.mean(): -6.079239845275879 
model_pd.lambdas: dict_items([('pout', tensor([1.4715])), ('power', tensor([0.3147]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1941])), ('power', tensor([-20.4126]))])
epoch：614	 i:0 	 global-step:12280	 l-p:0.07977701723575592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[7.1052, 7.9831, 8.1287],
        [7.1052, 7.1156, 7.1061],
        [7.1052, 7.2418, 7.1600],
        [7.1052, 8.7469, 9.6046]], grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.0796608179807663 
model_pd.l_d.mean(): -6.138088226318359 
model_pd.lagr.mean(): -6.058427333831787 
model_pd.lambdas: dict_items([('pout', tensor([1.4716])), ('power', tensor([0.3137]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1909])), ('power', tensor([-20.3975]))])
epoch：615	 i:0 	 global-step:12300	 l-p:0.0796608179807663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[7.1225, 8.3582, 8.7996],
        [7.1225, 8.3675, 8.8174],
        [7.1225, 7.1940, 7.1416],
        [7.1225, 7.1225, 7.1225]], grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.07954507321119308 
model_pd.l_d.mean(): -6.117192268371582 
model_pd.lagr.mean(): -6.037647247314453 
model_pd.lambdas: dict_items([('pout', tensor([1.4718])), ('power', tensor([0.3127]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1877])), ('power', tensor([-20.3823]))])
epoch：616	 i:0 	 global-step:12320	 l-p:0.07954507321119308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[7.1399, 7.2116, 7.1591],
        [7.1399, 7.7960, 7.7994],
        [7.1399, 8.8262, 9.7277],
        [7.1399, 7.7541, 7.7364]], grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.07942976802587509 
model_pd.l_d.mean(): -6.096329689025879 
model_pd.lagr.mean(): -6.016900062561035 
model_pd.lambdas: dict_items([('pout', tensor([1.4720])), ('power', tensor([0.3116]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1845])), ('power', tensor([-20.3670]))])
epoch：617	 i:0 	 global-step:12340	 l-p:0.07942976802587509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[7.1574, 7.6741, 7.6137],
        [7.1574, 7.2301, 7.1770],
        [7.1574, 8.1115, 8.3092],
        [7.1574, 7.1578, 7.1574]], grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.07931490242481232 
model_pd.l_d.mean(): -6.075500965118408 
model_pd.lagr.mean(): -5.996186256408691 
model_pd.lambdas: dict_items([('pout', tensor([1.4722])), ('power', tensor([0.3106]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1812])), ('power', tensor([-20.3515]))])
epoch：618	 i:0 	 global-step:12360	 l-p:0.07931490242481232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[7.1751, 7.2457, 7.1937],
        [7.1751, 7.2483, 7.1949],
        [7.1751, 8.3187, 8.6709],
        [7.1751, 7.1856, 7.1760]], grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.07920046895742416 
model_pd.l_d.mean(): -6.054705619812012 
model_pd.lagr.mean(): -5.975505352020264 
model_pd.lambdas: dict_items([('pout', tensor([1.4724])), ('power', tensor([0.3096]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1780])), ('power', tensor([-20.3360]))])
epoch：619	 i:0 	 global-step:12380	 l-p:0.07920046895742416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[7.1928, 7.2009, 7.1934],
        [7.1928, 7.7706, 7.7333],
        [7.1928, 8.4547, 8.9121],
        [7.1928, 7.2356, 7.2011]], grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.07908646762371063 
model_pd.l_d.mean(): -6.033944129943848 
model_pd.lagr.mean(): -5.95485782623291 
model_pd.lambdas: dict_items([('pout', tensor([1.4726])), ('power', tensor([0.3086]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1747])), ('power', tensor([-20.3203]))])
epoch：620	 i:0 	 global-step:12400	 l-p:0.07908646762371063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[7.2107, 7.2107, 7.2107],
        [7.2107, 7.2114, 7.2107],
        [7.2107, 7.2108, 7.2107],
        [7.2107, 7.2188, 7.2113]], grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.07897287607192993 
model_pd.l_d.mean(): -6.013216495513916 
model_pd.lagr.mean(): -5.934243679046631 
model_pd.lambdas: dict_items([('pout', tensor([1.4727])), ('power', tensor([0.3076]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1715])), ('power', tensor([-20.3046]))])
epoch：621	 i:0 	 global-step:12420	 l-p:0.07897287607192993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[7.2287, 7.3027, 7.2487],
        [7.2287, 7.2287, 7.2287],
        [7.2287, 8.1949, 8.3954],
        [7.2287, 8.3831, 8.7389]], grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.07885971665382385 
model_pd.l_d.mean(): -5.992523193359375 
model_pd.lagr.mean(): -5.913663387298584 
model_pd.lambdas: dict_items([('pout', tensor([1.4729])), ('power', tensor([0.3066]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1682])), ('power', tensor([-20.2887]))])
epoch：622	 i:0 	 global-step:12440	 l-p:0.07885971665382385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[7.2468, 7.9156, 7.9195],
        [7.2468, 7.5239, 7.4157],
        [7.2468, 7.4409, 7.3420],
        [7.2468, 7.2469, 7.2468]], grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.07874695956707001 
model_pd.l_d.mean(): -5.971864223480225 
model_pd.lagr.mean(): -5.893117427825928 
model_pd.lambdas: dict_items([('pout', tensor([1.4731])), ('power', tensor([0.3055]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1649])), ('power', tensor([-20.2727]))])
epoch：623	 i:0 	 global-step:12460	 l-p:0.07874695956707001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[7.2650, 7.2732, 7.2656],
        [7.2650, 7.3369, 7.2840],
        [7.2650, 7.8504, 7.8128],
        [7.2650, 7.2650, 7.2650]], grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.07863461226224899 
model_pd.l_d.mean(): -5.951239109039307 
model_pd.lagr.mean(): -5.8726043701171875 
model_pd.lambdas: dict_items([('pout', tensor([1.4732])), ('power', tensor([0.3045]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1616])), ('power', tensor([-20.2566]))])
epoch：624	 i:0 	 global-step:12480	 l-p:0.07863461226224899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[7.2833, 7.6852, 7.5875],
        [7.2833, 7.7056, 7.6122],
        [7.2833, 7.3570, 7.3031],
        [7.2833, 8.9761, 9.8616]], grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.078522689640522 
model_pd.l_d.mean(): -5.930649757385254 
model_pd.lagr.mean(): -5.8521270751953125 
model_pd.lambdas: dict_items([('pout', tensor([1.4734])), ('power', tensor([0.3035]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1582])), ('power', tensor([-20.2404]))])
epoch：625	 i:0 	 global-step:12500	 l-p:0.078522689640522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[ 7.3017,  8.3388,  8.5882],
        [ 7.3017,  8.2109,  8.3625],
        [ 7.3017,  7.3017,  7.3018],
        [ 7.3017,  9.1188, 10.1409]], grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.07841116935014725 
model_pd.l_d.mean(): -5.910093307495117 
model_pd.lagr.mean(): -5.831682205200195 
model_pd.lambdas: dict_items([('pout', tensor([1.4735])), ('power', tensor([0.3025]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1549])), ('power', tensor([-20.2241]))])
epoch：626	 i:0 	 global-step:12520	 l-p:0.07841116935014725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[ 7.3203,  7.8524,  7.7907],
        [ 7.3203,  7.3203,  7.3203],
        [ 7.3203,  7.3952,  7.3405],
        [ 7.3203,  9.3752, 10.6752]], grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.07830002903938293 
model_pd.l_d.mean(): -5.8895745277404785 
model_pd.lagr.mean(): -5.811274528503418 
model_pd.lambdas: dict_items([('pout', tensor([1.4737])), ('power', tensor([0.3015]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1516])), ('power', tensor([-20.2076]))])
epoch：627	 i:0 	 global-step:12540	 l-p:0.07830002903938293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[7.3390, 8.6316, 9.1001],
        [7.3390, 7.8729, 7.8110],
        [7.3390, 7.3450, 7.3393],
        [7.3390, 7.3411, 7.3391]], grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.07818931341171265 
model_pd.l_d.mean(): -5.869089126586914 
model_pd.lagr.mean(): -5.790899753570557 
model_pd.lambdas: dict_items([('pout', tensor([1.4738])), ('power', tensor([0.3005]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1482])), ('power', tensor([-20.1911]))])
epoch：628	 i:0 	 global-step:12560	 l-p:0.07818931341171265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[7.3578, 7.6086, 7.5001],
        [7.3578, 7.3578, 7.3578],
        [7.3578, 8.2758, 8.4291],
        [7.3578, 7.5688, 7.4655]], grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.0780789777636528 
model_pd.l_d.mean(): -5.848640441894531 
model_pd.lagr.mean(): -5.770561695098877 
model_pd.lambdas: dict_items([('pout', tensor([1.4740])), ('power', tensor([0.2995]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1448])), ('power', tensor([-20.1744]))])
epoch：629	 i:0 	 global-step:12580	 l-p:0.0780789777636528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 7.3767,  7.7854,  7.6862],
        [ 7.3767,  8.6791,  9.1522],
        [ 7.3767,  9.1322, 10.0724],
        [ 7.3767,  7.4160,  7.3838]], grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.077969029545784 
model_pd.l_d.mean(): -5.828225612640381 
model_pd.lagr.mean(): -5.750256538391113 
model_pd.lambdas: dict_items([('pout', tensor([1.4741])), ('power', tensor([0.2985]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1415])), ('power', tensor([-20.1576]))])
epoch：630	 i:0 	 global-step:12600	 l-p:0.077969029545784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[ 7.3957,  7.9948,  7.9568],
        [ 7.3957,  7.3962,  7.3957],
        [ 7.3957,  9.1207, 10.0239],
        [ 7.3957,  8.0387,  8.0210]], grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.07785947620868683 
model_pd.l_d.mean(): -5.807847499847412 
model_pd.lagr.mean(): -5.729988098144531 
model_pd.lambdas: dict_items([('pout', tensor([1.4743])), ('power', tensor([0.2975]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1381])), ('power', tensor([-20.1407]))])
epoch：631	 i:0 	 global-step:12620	 l-p:0.07785947620868683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[7.4148, 8.7242, 9.1992],
        [7.4148, 7.4915, 7.4356],
        [7.4148, 7.4149, 7.4148],
        [7.4148, 7.6151, 7.5132]], grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.0777503028512001 
model_pd.l_d.mean(): -5.787506580352783 
model_pd.lagr.mean(): -5.709756374359131 
model_pd.lambdas: dict_items([('pout', tensor([1.4744])), ('power', tensor([0.2965]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1347])), ('power', tensor([-20.1237]))])
epoch：632	 i:0 	 global-step:12640	 l-p:0.0777503028512001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[7.4341, 7.6885, 7.5785],
        [7.4341, 7.5106, 7.4547],
        [7.4341, 8.1028, 8.0958],
        [7.4341, 7.4341, 7.4341]], grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.07764150202274323 
model_pd.l_d.mean(): -5.767200469970703 
model_pd.lagr.mean(): -5.689558982849121 
model_pd.lambdas: dict_items([('pout', tensor([1.4745])), ('power', tensor([0.2955]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1312])), ('power', tensor([-20.1066]))])
epoch：633	 i:0 	 global-step:12660	 l-p:0.07764150202274323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[ 7.4535,  9.5540, 10.8839],
        [ 7.4535,  7.6552,  7.5526],
        [ 7.4535,  7.4767,  7.4565],
        [ 7.4535,  7.4535,  7.4535]], grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.07753308862447739 
model_pd.l_d.mean(): -5.746931076049805 
model_pd.lagr.mean(): -5.669397830963135 
model_pd.lambdas: dict_items([('pout', tensor([1.4746])), ('power', tensor([0.2944]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1278])), ('power', tensor([-20.0893]))])
epoch：634	 i:0 	 global-step:12680	 l-p:0.07753308862447739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[7.4730, 8.1689, 8.1738],
        [7.4730, 7.7291, 7.6184],
        [7.4730, 7.4730, 7.4730],
        [7.4730, 7.4751, 7.4731]], grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.0774250477552414 
model_pd.l_d.mean(): -5.726698398590088 
model_pd.lagr.mean(): -5.64927339553833 
model_pd.lambdas: dict_items([('pout', tensor([1.4748])), ('power', tensor([0.2934]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1244])), ('power', tensor([-20.0719]))])
epoch：635	 i:0 	 global-step:12700	 l-p:0.0774250477552414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[7.4926, 7.5676, 7.5125],
        [7.4926, 8.8191, 9.3007],
        [7.4926, 7.4930, 7.4926],
        [7.4926, 7.5700, 7.5135]], grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.07731737196445465 
model_pd.l_d.mean(): -5.706502437591553 
model_pd.lagr.mean(): -5.629185199737549 
model_pd.lambdas: dict_items([('pout', tensor([1.4749])), ('power', tensor([0.2924]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1209])), ('power', tensor([-20.0545]))])
epoch：636	 i:0 	 global-step:12720	 l-p:0.07731737196445465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[7.5124, 8.1684, 8.1507],
        [7.5124, 7.5129, 7.5124],
        [7.5124, 7.5904, 7.5335],
        [7.5124, 7.6884, 7.5914]], grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.07721006125211716 
model_pd.l_d.mean(): -5.686342716217041 
model_pd.lagr.mean(): -5.609132766723633 
model_pd.lambdas: dict_items([('pout', tensor([1.4750])), ('power', tensor([0.2914]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1175])), ('power', tensor([-20.0368]))])
epoch：637	 i:0 	 global-step:12740	 l-p:0.07721006125211716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[ 7.5322,  8.2834,  8.3151],
        [ 7.5322,  9.4197, 10.4831],
        [ 7.5322,  7.8693,  7.7567],
        [ 7.5322,  7.6078,  7.5522]], grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.07710311561822891 
model_pd.l_d.mean(): -5.666220188140869 
model_pd.lagr.mean(): -5.589117050170898 
model_pd.lambdas: dict_items([('pout', tensor([1.4751])), ('power', tensor([0.2904]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1140])), ('power', tensor([-20.0191]))])
epoch：638	 i:0 	 global-step:12760	 l-p:0.07710311561822891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[7.5522, 7.5522, 7.5522],
        [7.5522, 8.6344, 8.8958],
        [7.5522, 7.5527, 7.5522],
        [7.5522, 7.5522, 7.5522]], grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.07699653506278992 
model_pd.l_d.mean(): -5.646135330200195 
model_pd.lagr.mean(): -5.569139003753662 
model_pd.lambdas: dict_items([('pout', tensor([1.4752])), ('power', tensor([0.2894]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1105])), ('power', tensor([-20.0013]))])
epoch：639	 i:0 	 global-step:12780	 l-p:0.07699653506278992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[7.5723, 7.6508, 7.5935],
        [7.5723, 7.5723, 7.5723],
        [7.5723, 8.2801, 8.2854],
        [7.5723, 7.6484, 7.5925]], grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.07689028978347778 
model_pd.l_d.mean(): -5.6260881423950195 
model_pd.lagr.mean(): -5.549197673797607 
model_pd.lambdas: dict_items([('pout', tensor([1.4753])), ('power', tensor([0.2884]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1070])), ('power', tensor([-19.9833]))])
epoch：640	 i:0 	 global-step:12800	 l-p:0.07689028978347778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[7.5925, 7.6386, 7.6015],
        [7.5925, 7.5925, 7.5925],
        [7.5925, 7.5926, 7.5925],
        [7.5925, 7.5925, 7.5925]], grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.0767844021320343 
model_pd.l_d.mean(): -5.606078624725342 
model_pd.lagr.mean(): -5.529294013977051 
model_pd.lambdas: dict_items([('pout', tensor([1.4754])), ('power', tensor([0.2874]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1035])), ('power', tensor([-19.9652]))])
epoch：641	 i:0 	 global-step:12820	 l-p:0.0767844021320343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[7.6129, 8.8449, 9.2265],
        [7.6129, 8.0605, 7.9622],
        [7.6129, 7.6217, 7.6135],
        [7.6129, 7.6129, 7.6129]], grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.07667886465787888 
model_pd.l_d.mean(): -5.58610725402832 
model_pd.lagr.mean(): -5.50942850112915 
model_pd.lambdas: dict_items([('pout', tensor([1.4755])), ('power', tensor([0.2864]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0999])), ('power', tensor([-19.9470]))])
epoch：642	 i:0 	 global-step:12840	 l-p:0.07667886465787888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[7.6334, 8.2575, 8.2186],
        [7.6334, 7.6334, 7.6334],
        [7.6334, 7.6338, 7.6334],
        [7.6334, 7.8551, 7.7468]], grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.07657365500926971 
model_pd.l_d.mean(): -5.5661725997924805 
model_pd.lagr.mean(): -5.489598751068115 
model_pd.lambdas: dict_items([('pout', tensor([1.4756])), ('power', tensor([0.2854]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0964])), ('power', tensor([-19.9287]))])
epoch：643	 i:0 	 global-step:12860	 l-p:0.07657365500926971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[7.6540, 8.3716, 8.3773],
        [7.6540, 7.6540, 7.6540],
        [7.6540, 7.8051, 7.7148],
        [7.6540, 7.6548, 7.6540]], grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.0764688029885292 
model_pd.l_d.mean(): -5.546276092529297 
model_pd.lagr.mean(): -5.469807147979736 
model_pd.lambdas: dict_items([('pout', tensor([1.4757])), ('power', tensor([0.2845]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0928])), ('power', tensor([-19.9102]))])
epoch：644	 i:0 	 global-step:12880	 l-p:0.0764688029885292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[7.6747, 8.3032, 8.2641],
        [7.6747, 7.6747, 7.6747],
        [7.6747, 9.0414, 9.5385],
        [7.6747, 8.1052, 8.0013]], grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.07636428624391556 
model_pd.l_d.mean(): -5.526419162750244 
model_pd.lagr.mean(): -5.45005464553833 
model_pd.lambdas: dict_items([('pout', tensor([1.4758])), ('power', tensor([0.2835]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0893])), ('power', tensor([-19.8916]))])
epoch：645	 i:0 	 global-step:12900	 l-p:0.07636428624391556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[ 7.6956,  7.9061,  7.7992],
        [ 7.6956,  7.7425,  7.7047],
        [ 7.6956,  9.8791, 11.2632],
        [ 7.6956,  8.4676,  8.5008]], grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.07626008242368698 
model_pd.l_d.mean(): -5.5065999031066895 
model_pd.lagr.mean(): -5.430339813232422 
model_pd.lambdas: dict_items([('pout', tensor([1.4759])), ('power', tensor([0.2825]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0857])), ('power', tensor([-19.8729]))])
epoch：646	 i:0 	 global-step:12920	 l-p:0.07626008242368698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[7.7166, 7.7282, 7.7176],
        [7.7166, 8.4417, 8.4476],
        [7.7166, 7.7255, 7.7172],
        [7.7166, 7.7975, 7.7385]], grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.07615622878074646 
model_pd.l_d.mean(): -5.486820697784424 
model_pd.lagr.mean(): -5.4106645584106445 
model_pd.lambdas: dict_items([('pout', tensor([1.4760])), ('power', tensor([0.2815]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0821])), ('power', tensor([-19.8541]))])
epoch：647	 i:0 	 global-step:12940	 l-p:0.07615622878074646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[7.7377, 7.7494, 7.7387],
        [7.7377, 7.7377, 7.7377],
        [7.7377, 7.7382, 7.7377],
        [7.7377, 7.9497, 7.8421]], grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.07605268806219101 
model_pd.l_d.mean(): -5.467079162597656 
model_pd.lagr.mean(): -5.391026496887207 
model_pd.lambdas: dict_items([('pout', tensor([1.4761])), ('power', tensor([0.2805]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0785])), ('power', tensor([-19.8351]))])
epoch：648	 i:0 	 global-step:12960	 l-p:0.07605268806219101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[7.7590, 9.1442, 9.6485],
        [7.7590, 8.5391, 8.5729],
        [7.7590, 7.7590, 7.7590],
        [7.7590, 9.0204, 9.4119]], grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.07594949007034302 
model_pd.l_d.mean(): -5.447377681732178 
model_pd.lagr.mean(): -5.3714280128479 
model_pd.lambdas: dict_items([('pout', tensor([1.4761])), ('power', tensor([0.2795]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0749])), ('power', tensor([-19.8160]))])
epoch：649	 i:0 	 global-step:12980	 l-p:0.07594949007034302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[7.7803, 9.0461, 9.4390],
        [7.7803, 8.8406, 9.0632],
        [7.7803, 8.1991, 8.0899],
        [7.7803, 8.2185, 8.1129]], grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.0758465826511383 
model_pd.l_d.mean(): -5.427716255187988 
model_pd.lagr.mean(): -5.351869583129883 
model_pd.lambdas: dict_items([('pout', tensor([1.4762])), ('power', tensor([0.2785]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0713])), ('power', tensor([-19.7968]))])
epoch：650	 i:0 	 global-step:13000	 l-p:0.0758465826511383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[7.8019, 7.8019, 7.8019],
        [7.8019, 7.8019, 7.8019],
        [7.8019, 7.8840, 7.8241],
        [7.8019, 8.1074, 7.9888]], grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.07574401050806046 
model_pd.l_d.mean(): -5.408092975616455 
model_pd.lagr.mean(): -5.332348823547363 
model_pd.lambdas: dict_items([('pout', tensor([1.4763])), ('power', tensor([0.2775]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0677])), ('power', tensor([-19.7775]))])
epoch：651	 i:0 	 global-step:13020	 l-p:0.07574401050806046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[ 7.8235,  7.8235,  7.8235],
        [ 7.8235, 10.2279, 11.8617],
        [ 7.8235,  7.8240,  7.8235],
        [ 7.8235,  7.9048,  7.8453]], grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.07564175873994827 
model_pd.l_d.mean(): -5.388510227203369 
model_pd.lagr.mean(): -5.312868595123291 
model_pd.lambdas: dict_items([('pout', tensor([1.4763])), ('power', tensor([0.2765]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0640])), ('power', tensor([-19.7580]))])
epoch：652	 i:0 	 global-step:13040	 l-p:0.07564175873994827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[ 7.8453,  9.8286, 10.9480],
        [ 7.8453,  8.6364,  8.6710],
        [ 7.8453,  7.8453,  7.8453],
        [ 7.8453,  8.9166,  9.1419]], grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.07553980499505997 
model_pd.l_d.mean(): -5.3689680099487305 
model_pd.lagr.mean(): -5.293428421020508 
model_pd.lambdas: dict_items([('pout', tensor([1.4764])), ('power', tensor([0.2755]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0603])), ('power', tensor([-19.7384]))])
epoch：653	 i:0 	 global-step:13060	 l-p:0.07553980499505997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[7.8672, 8.6611, 8.6959],
        [7.8672, 9.2763, 9.7899],
        [7.8672, 7.8675, 7.8672],
        [7.8672, 8.2920, 8.1814]], grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.07543816417455673 
model_pd.l_d.mean(): -5.349463939666748 
model_pd.lagr.mean(): -5.274025917053223 
model_pd.lambdas: dict_items([('pout', tensor([1.4765])), ('power', tensor([0.2745]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0567])), ('power', tensor([-19.7186]))])
epoch：654	 i:0 	 global-step:13080	 l-p:0.07543816417455673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[ 7.8892,  8.3156,  8.2047],
        [ 7.8892, 10.1391, 11.5667],
        [ 7.8892,  7.8916,  7.8893],
        [ 7.8892,  8.2474,  8.1283]], grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.07533684372901917 
model_pd.l_d.mean(): -5.330002307891846 
model_pd.lagr.mean(): -5.254665374755859 
model_pd.lambdas: dict_items([('pout', tensor([1.4765])), ('power', tensor([0.2736]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0530])), ('power', tensor([-19.6987]))])
epoch：655	 i:0 	 global-step:13100	 l-p:0.07533684372901917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[ 7.9114,  9.8235, 10.8510],
        [ 7.9114,  7.9939,  7.9336],
        [ 7.9114,  8.3818,  8.2791],
        [ 7.9114,  8.5648,  8.5248]], grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.07523580640554428 
model_pd.l_d.mean(): -5.310580730438232 
model_pd.lagr.mean(): -5.235344886779785 
model_pd.lambdas: dict_items([('pout', tensor([1.4766])), ('power', tensor([0.2726]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0493])), ('power', tensor([-19.6787]))])
epoch：656	 i:0 	 global-step:13120	 l-p:0.07523580640554428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[7.9337, 8.2945, 8.1746],
        [7.9337, 9.3576, 9.8768],
        [7.9337, 8.4059, 8.3028],
        [7.9337, 9.0201, 9.2490]], grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.07513508200645447 
model_pd.l_d.mean(): -5.291200637817383 
model_pd.lagr.mean(): -5.216065406799316 
model_pd.lambdas: dict_items([('pout', tensor([1.4766])), ('power', tensor([0.2716]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0456])), ('power', tensor([-19.6586]))])
epoch：657	 i:0 	 global-step:13140	 l-p:0.07513508200645447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[7.9562, 7.9562, 7.9562],
        [7.9562, 9.3745, 9.8858],
        [7.9562, 7.9585, 7.9563],
        [7.9562, 8.1153, 8.0203]], grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.07503465563058853 
model_pd.l_d.mean(): -5.271860599517822 
model_pd.lagr.mean(): -5.196825981140137 
model_pd.lambdas: dict_items([('pout', tensor([1.4767])), ('power', tensor([0.2706]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0419])), ('power', tensor([-19.6383]))])
epoch：658	 i:0 	 global-step:13160	 l-p:0.07503465563058853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[7.9788, 7.9910, 7.9798],
        [7.9788, 9.1378, 9.4198],
        [7.9788, 8.0226, 7.9867],
        [7.9788, 8.0635, 8.0018]], grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.07493452727794647 
model_pd.l_d.mean(): -5.252561092376709 
model_pd.lagr.mean(): -5.177626609802246 
model_pd.lambdas: dict_items([('pout', tensor([1.4767])), ('power', tensor([0.2696]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0381])), ('power', tensor([-19.6179]))])
epoch：659	 i:0 	 global-step:13180	 l-p:0.07493452727794647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[8.0015, 8.0019, 8.0015],
        [8.0015, 8.0455, 8.0095],
        [8.0015, 8.6644, 8.6241],
        [8.0015, 8.1930, 8.0878]], grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.07483468949794769 
model_pd.l_d.mean(): -5.233304023742676 
model_pd.lagr.mean(): -5.158469200134277 
model_pd.lambdas: dict_items([('pout', tensor([1.4767])), ('power', tensor([0.2687]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0344])), ('power', tensor([-19.5973]))])
epoch：660	 i:0 	 global-step:13200	 l-p:0.07483468949794769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[8.0244, 9.4576, 9.9747],
        [8.0244, 8.0244, 8.0244],
        [8.0244, 8.3905, 8.2690],
        [8.0244, 8.0312, 8.0248]], grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.07473514974117279 
model_pd.l_d.mean(): -5.214087963104248 
model_pd.lagr.mean(): -5.139352798461914 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2677]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0306])), ('power', tensor([-19.5766]))])
epoch：661	 i:0 	 global-step:13220	 l-p:0.07473514974117279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[ 8.0474,  8.1331,  8.0707],
        [ 8.0474,  8.0542,  8.0478],
        [ 8.0474, 10.0926, 11.2482],
        [ 8.0474,  8.0474,  8.0474]], grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.07463590055704117 
model_pd.l_d.mean(): -5.1949143409729 
model_pd.lagr.mean(): -5.120278358459473 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2667]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0269])), ('power', tensor([-19.5558]))])
epoch：662	 i:0 	 global-step:13240	 l-p:0.07463590055704117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[ 8.0706,  9.1803,  9.4146],
        [ 8.0706,  8.0730,  8.0707],
        [ 8.0706,  8.3900,  8.2663],
        [ 8.0706,  9.5246, 10.0555]], grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.07453693449497223 
model_pd.l_d.mean(): -5.175781726837158 
model_pd.lagr.mean(): -5.101244926452637 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2657]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0231])), ('power', tensor([-19.5348]))])
epoch：663	 i:0 	 global-step:13260	 l-p:0.07453693449497223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[8.0939, 8.5549, 8.4444],
        [8.0939, 8.2883, 8.1815],
        [8.0939, 8.3784, 8.2560],
        [8.0939, 8.1790, 8.1168]], grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.07443825900554657 
model_pd.l_d.mean(): -5.1566925048828125 
model_pd.lagr.mean(): -5.082254409790039 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2647]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0193])), ('power', tensor([-19.5136]))])
epoch：664	 i:0 	 global-step:13280	 l-p:0.07443825900554657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[8.1173, 8.1197, 8.1174],
        [8.1173, 8.1179, 8.1173],
        [8.1173, 8.1182, 8.1174],
        [8.1173, 8.4391, 8.3146]], grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.07433987408876419 
model_pd.l_d.mean(): -5.137645244598389 
model_pd.lagr.mean(): -5.063305377960205 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2638]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0155])), ('power', tensor([-19.4924]))])
epoch：665	 i:0 	 global-step:13300	 l-p:0.07433987408876419
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[ 8.1409,  9.2627,  9.4998],
        [ 8.1409, 10.4770, 11.9611],
        [ 8.1409,  8.8677,  8.8501],
        [ 8.1409,  8.5139,  8.3902]], grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.0742417648434639 
model_pd.l_d.mean(): -5.118640422821045 
model_pd.lagr.mean(): -5.044398784637451 
model_pd.lambdas: dict_items([('pout', tensor([1.4769])), ('power', tensor([0.2628]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0116])), ('power', tensor([-19.4710]))])
epoch：666	 i:0 	 global-step:13320	 l-p:0.0742417648434639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[8.1647, 9.2904, 9.5285],
        [8.1647, 8.1647, 8.1647],
        [8.1647, 8.6545, 8.5480],
        [8.1647, 8.2515, 8.1882]], grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.07414395362138748 
model_pd.l_d.mean(): -5.0996785163879395 
model_pd.lagr.mean(): -5.025534629821777 
model_pd.lambdas: dict_items([('pout', tensor([1.4769])), ('power', tensor([0.2618]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0078])), ('power', tensor([-19.4494]))])
epoch：667	 i:0 	 global-step:13340	 l-p:0.07414395362138748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[8.1885, 8.1889, 8.1886],
        [8.1885, 8.9448, 8.9394],
        [8.1885, 8.6565, 8.5445],
        [8.1885, 8.2396, 8.1985]], grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.07404640316963196 
model_pd.l_d.mean(): -5.080760478973389 
model_pd.lagr.mean(): -5.0067138671875 
model_pd.lambdas: dict_items([('pout', tensor([1.4769])), ('power', tensor([0.2609]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0040])), ('power', tensor([-19.4277]))])
epoch：668	 i:0 	 global-step:13360	 l-p:0.07404640316963196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 8.2126,  8.8295,  8.7603],
        [ 8.2126,  8.2130,  8.2126],
        [ 8.2126,  8.5898,  8.4648],
        [ 8.2126, 10.7604, 12.4946]], grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.07394912838935852 
model_pd.l_d.mean(): -5.06188440322876 
model_pd.lagr.mean(): -4.9879350662231445 
model_pd.lambdas: dict_items([('pout', tensor([1.4769])), ('power', tensor([0.2599]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0001])), ('power', tensor([-19.4059]))])
epoch：669	 i:0 	 global-step:13380	 l-p:0.07394912838935852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[ 8.2368,  8.2465,  8.2375],
        [ 8.2368, 10.2032, 11.2382],
        [ 8.2368,  8.2495,  8.2378],
        [ 8.2368,  8.2882,  8.2468]], grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.07385215908288956 
model_pd.l_d.mean(): -5.0430521965026855 
model_pd.lagr.mean(): -4.969200134277344 
model_pd.lambdas: dict_items([('pout', tensor([1.4769])), ('power', tensor([0.2589]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0038])), ('power', tensor([-19.3839]))])
epoch：670	 i:0 	 global-step:13400	 l-p:0.07385215908288956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[ 8.2611,  8.2709,  8.2618],
        [ 8.2611,  8.5071,  8.3873],
        [ 8.2611,  8.3466,  8.2838],
        [ 8.2611, 10.2346, 11.2733]], grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.07375544309616089 
model_pd.l_d.mean(): -5.024263381958008 
model_pd.lagr.mean(): -4.950508117675781 
model_pd.lambdas: dict_items([('pout', tensor([1.4769])), ('power', tensor([0.2579]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0076])), ('power', tensor([-19.3617]))])
epoch：671	 i:0 	 global-step:13420	 l-p:0.07375544309616089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[ 8.2856, 10.3072, 11.3958],
        [ 8.2856,  8.3374,  8.2957],
        [ 8.2856,  8.5788,  8.4529],
        [ 8.2856,  9.6533, 10.0803]], grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.0736590027809143 
model_pd.l_d.mean(): -5.005518436431885 
model_pd.lagr.mean(): -4.931859493255615 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2570]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0115])), ('power', tensor([-19.3394]))])
epoch：672	 i:0 	 global-step:13440	 l-p:0.0736590027809143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 8.3102,  8.3996,  8.3345],
        [ 8.3102,  9.0806,  9.0754],
        [ 8.3102,  9.0055,  8.9641],
        [ 8.3102, 10.3391, 11.4316]], grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.07356283068656921 
model_pd.l_d.mean(): -4.986818790435791 
model_pd.lagr.mean(): -4.9132561683654785 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2560]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0154])), ('power', tensor([-19.3170]))])
epoch：673	 i:0 	 global-step:13460	 l-p:0.07356283068656921
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[8.3350, 8.3350, 8.3350],
        [8.3350, 8.3449, 8.3357],
        [8.3350, 8.3350, 8.3350],
        [8.3350, 8.3872, 8.3452]], grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.07346692681312561 
model_pd.l_d.mean(): -4.9681620597839355 
model_pd.lagr.mean(): -4.894695281982422 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2550]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0194])), ('power', tensor([-19.2944]))])
epoch：674	 i:0 	 global-step:13480	 l-p:0.07346692681312561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[ 8.3599,  9.5190,  9.7649],
        [ 8.3599, 10.3618, 11.4161],
        [ 8.3599,  8.4495,  8.3842],
        [ 8.3599,  9.8666, 10.4118]], grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.07337129861116409 
model_pd.l_d.mean(): -4.949550151824951 
model_pd.lagr.mean(): -4.876178741455078 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2541]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0233])), ('power', tensor([-19.2716]))])
epoch：675	 i:0 	 global-step:13500	 l-p:0.07337129861116409
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[8.3850, 8.5555, 8.4539],
        [8.3850, 9.1641, 9.1591],
        [8.3850, 8.3854, 8.3850],
        [8.3850, 9.6172, 9.9188]], grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.07327594608068466 
model_pd.l_d.mean(): -4.930984020233154 
model_pd.lagr.mean(): -4.857707977294922 
model_pd.lambdas: dict_items([('pout', tensor([1.4768])), ('power', tensor([0.2531]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0272])), ('power', tensor([-19.2487]))])
epoch：676	 i:0 	 global-step:13520	 l-p:0.07327594608068466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 8.4103,  8.5814,  8.4794],
        [ 8.4103,  9.1923,  9.1873],
        [ 8.4103, 11.0309, 12.8161],
        [ 8.4103,  8.4572,  8.4188]], grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.07318083941936493 
model_pd.l_d.mean(): -4.912461280822754 
model_pd.lagr.mean(): -4.839280605316162 
model_pd.lambdas: dict_items([('pout', tensor([1.4767])), ('power', tensor([0.2522]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0312])), ('power', tensor([-19.2257]))])
epoch：677	 i:0 	 global-step:13540	 l-p:0.07318083941936493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[ 8.4357,  8.7358,  8.6070],
        [ 8.4357,  8.8260,  8.6970],
        [ 8.4357, 10.8728, 12.4230],
        [ 8.4357, 10.5013, 11.6143]], grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.07308600097894669 
model_pd.l_d.mean(): -4.893984794616699 
model_pd.lagr.mean(): -4.82089900970459 
model_pd.lambdas: dict_items([('pout', tensor([1.4767])), ('power', tensor([0.2512]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0352])), ('power', tensor([-19.2024]))])
epoch：678	 i:0 	 global-step:13560	 l-p:0.07308600097894669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[ 8.4612, 10.6331, 11.8627],
        [ 8.4612,  8.4613,  8.4612],
        [ 8.4612,  8.6995,  8.5790],
        [ 8.4612,  9.9901, 10.5438]], grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.07299143075942993 
model_pd.l_d.mean(): -4.87555456161499 
model_pd.lagr.mean(): -4.802563190460205 
model_pd.lambdas: dict_items([('pout', tensor([1.4767])), ('power', tensor([0.2502]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0391])), ('power', tensor([-19.1791]))])
epoch：679	 i:0 	 global-step:13580	 l-p:0.07299143075942993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[ 8.4870, 10.0346, 10.6027],
        [ 8.4870,  8.6601,  8.5569],
        [ 8.4870,  8.4870,  8.4870],
        [ 8.4870,  9.2526,  9.2351]], grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.07289712131023407 
model_pd.l_d.mean(): -4.857168197631836 
model_pd.lagr.mean(): -4.784271240234375 
model_pd.lambdas: dict_items([('pout', tensor([1.4766])), ('power', tensor([0.2493]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0431])), ('power', tensor([-19.1555]))])
epoch：680	 i:0 	 global-step:13600	 l-p:0.07289712131023407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[8.5128, 9.1583, 9.0866],
        [8.5128, 8.5134, 8.5128],
        [8.5128, 9.2814, 9.2639],
        [8.5128, 8.5666, 8.5233]], grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.0728030800819397 
model_pd.l_d.mean(): -4.8388285636901855 
model_pd.lagr.mean(): -4.766025543212891 
model_pd.lambdas: dict_items([('pout', tensor([1.4766])), ('power', tensor([0.2483]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0471])), ('power', tensor([-19.1318]))])
epoch：681	 i:0 	 global-step:13620	 l-p:0.0728030800819397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[ 8.5389, 10.7345, 11.9780],
        [ 8.5389,  8.5389,  8.5389],
        [ 8.5389, 11.0114, 12.5848],
        [ 8.5389,  8.5463,  8.5393]], grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.07270929217338562 
model_pd.l_d.mean(): -4.820535182952881 
model_pd.lagr.mean(): -4.747826099395752 
model_pd.lambdas: dict_items([('pout', tensor([1.4765])), ('power', tensor([0.2474]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0512])), ('power', tensor([-19.1080]))])
epoch：682	 i:0 	 global-step:13640	 l-p:0.07270929217338562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[ 8.5651,  9.3396,  9.3220],
        [ 8.5651,  8.5657,  8.5651],
        [ 8.5651,  8.5754,  8.5658],
        [ 8.5651, 10.7687, 12.0170]], grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.07261575758457184 
model_pd.l_d.mean(): -4.802287578582764 
model_pd.lagr.mean(): -4.729671955108643 
model_pd.lambdas: dict_items([('pout', tensor([1.4765])), ('power', tensor([0.2464]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0552])), ('power', tensor([-19.0840]))])
epoch：683	 i:0 	 global-step:13660	 l-p:0.07261575758457184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[ 8.5915,  8.5924,  8.5915],
        [ 8.5915,  8.6018,  8.5922],
        [ 8.5915,  9.7900, 10.0452],
        [ 8.5915,  8.5919,  8.5915]], grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.07252248376607895 
model_pd.l_d.mean(): -4.784086227416992 
model_pd.lagr.mean(): -4.711563587188721 
model_pd.lambdas: dict_items([('pout', tensor([1.4764])), ('power', tensor([0.2455]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0592])), ('power', tensor([-19.0598]))])
epoch：684	 i:0 	 global-step:13680	 l-p:0.07252248376607895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[8.6180, 8.6184, 8.6180],
        [8.6180, 9.4511, 9.4608],
        [8.6180, 9.7363, 9.9287],
        [8.6180, 8.6181, 8.6180]], grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.07242946326732635 
model_pd.l_d.mean(): -4.765933036804199 
model_pd.lagr.mean(): -4.693503379821777 
model_pd.lambdas: dict_items([('pout', tensor([1.4763])), ('power', tensor([0.2445]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0633])), ('power', tensor([-19.0354]))])
epoch：685	 i:0 	 global-step:13700	 l-p:0.07242946326732635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 8.6447,  9.3751,  9.3324],
        [ 8.6447,  8.6995,  8.6554],
        [ 8.6447,  8.6934,  8.6535],
        [ 8.6447, 10.7715, 11.9187]], grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.07233669608831406 
model_pd.l_d.mean(): -4.747825622558594 
model_pd.lagr.mean(): -4.6754889488220215 
model_pd.lambdas: dict_items([('pout', tensor([1.4763])), ('power', tensor([0.2436]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0674])), ('power', tensor([-19.0109]))])
epoch：686	 i:0 	 global-step:13720	 l-p:0.07233669608831406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[ 8.6716, 11.1896, 12.7927],
        [ 8.6716,  8.6717,  8.6716],
        [ 8.6716,  9.4048,  9.3620],
        [ 8.6716,  8.6725,  8.6716]], grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.07224417477846146 
model_pd.l_d.mean(): -4.72976541519165 
model_pd.lagr.mean(): -4.6575212478637695 
model_pd.lambdas: dict_items([('pout', tensor([1.4762])), ('power', tensor([0.2426]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0714])), ('power', tensor([-18.9863]))])
epoch：687	 i:0 	 global-step:13740	 l-p:0.07224417477846146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[ 8.6986,  8.7934,  8.7244],
        [ 8.6986,  8.7901,  8.7230],
        [ 8.6986,  9.8298, 10.0246],
        [ 8.6986,  8.6987,  8.6986]], grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.07215191423892975 
model_pd.l_d.mean(): -4.711752414703369 
model_pd.lagr.mean(): -4.6396002769470215 
model_pd.lambdas: dict_items([('pout', tensor([1.4761])), ('power', tensor([0.2417]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0755])), ('power', tensor([-18.9614]))])
epoch：688	 i:0 	 global-step:13760	 l-p:0.07215191423892975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[ 8.7258,  8.8197,  8.7511],
        [ 8.7258,  8.7284,  8.7259],
        [ 8.7258,  9.9472, 10.2078],
        [ 8.7258,  8.7259,  8.7258]], grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.07205990701913834 
model_pd.l_d.mean(): -4.693788051605225 
model_pd.lagr.mean(): -4.62172794342041 
model_pd.lambdas: dict_items([('pout', tensor([1.4760])), ('power', tensor([0.2407]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0796])), ('power', tensor([-18.9364]))])
epoch：689	 i:0 	 global-step:13780	 l-p:0.07205990701913834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[ 8.7532,  8.7532,  8.7532],
        [ 8.7532,  8.8488,  8.7792],
        [ 8.7532,  9.6024,  9.6127],
        [ 8.7532, 10.3596, 10.9505]], grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.07196813821792603 
model_pd.l_d.mean(): -4.675870895385742 
model_pd.lagr.mean(): -4.603902816772461 
model_pd.lambdas: dict_items([('pout', tensor([1.4760])), ('power', tensor([0.2398]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0838])), ('power', tensor([-18.9112]))])
epoch：690	 i:0 	 global-step:13800	 l-p:0.07196813821792603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[8.7807, 8.7807, 8.7807],
        [8.7807, 8.7807, 8.7807],
        [8.7807, 9.6333, 9.6436],
        [8.7807, 8.7811, 8.7807]], grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.0718766301870346 
model_pd.l_d.mean(): -4.658001899719238 
model_pd.lagr.mean(): -4.586125373840332 
model_pd.lambdas: dict_items([('pout', tensor([1.4759])), ('power', tensor([0.2388]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0879])), ('power', tensor([-18.8859]))])
epoch：691	 i:0 	 global-step:13820	 l-p:0.0718766301870346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[8.8084, 9.3216, 9.1998],
        [8.8084, 8.8191, 8.8092],
        [8.8084, 8.9901, 8.8820],
        [8.8084, 9.3475, 9.2315]], grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.07178536057472229 
model_pd.l_d.mean(): -4.640181541442871 
model_pd.lagr.mean(): -4.568396091461182 
model_pd.lambdas: dict_items([('pout', tensor([1.4758])), ('power', tensor([0.2379]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0920])), ('power', tensor([-18.8603]))])
epoch：692	 i:0 	 global-step:13840	 l-p:0.07178536057472229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[ 8.8363,  8.9326,  8.8625],
        [ 8.8363,  8.8363,  8.8363],
        [ 8.8363,  9.3288,  9.2022],
        [ 8.8363, 10.4611, 11.0591]], grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.07169434428215027 
model_pd.l_d.mean(): -4.622408390045166 
model_pd.lagr.mean(): -4.550714015960693 
model_pd.lambdas: dict_items([('pout', tensor([1.4757])), ('power', tensor([0.2369]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0962])), ('power', tensor([-18.8346]))])
epoch：693	 i:0 	 global-step:13860	 l-p:0.07169434428215027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[8.8644, 8.8644, 8.8644],
        [8.8644, 8.8644, 8.8644],
        [8.8644, 9.6179, 9.5744],
        [8.8644, 8.8751, 8.8652]], grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.07160357385873795 
model_pd.l_d.mean(): -4.604685306549072 
model_pd.lagr.mean(): -4.533081531524658 
model_pd.lambdas: dict_items([('pout', tensor([1.4756])), ('power', tensor([0.2360]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1004])), ('power', tensor([-18.8088]))])
epoch：694	 i:0 	 global-step:13880	 l-p:0.07160357385873795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[ 8.8926,  8.8931,  8.8926],
        [ 8.8926, 10.5280, 11.1291],
        [ 8.8926, 11.1965, 12.5034],
        [ 8.8926,  8.9221,  8.8965]], grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.07151305675506592 
model_pd.l_d.mean(): -4.587010860443115 
model_pd.lagr.mean(): -4.51549768447876 
model_pd.lambdas: dict_items([('pout', tensor([1.4755])), ('power', tensor([0.2351]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1045])), ('power', tensor([-18.7827]))])
epoch：695	 i:0 	 global-step:13900	 l-p:0.07151305675506592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[ 8.9211, 11.1288, 12.3211],
        [ 8.9211,  9.0156,  8.9462],
        [ 8.9211,  9.7623,  9.7584],
        [ 8.9211, 11.0840, 12.2262]], grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.0714227706193924 
model_pd.l_d.mean(): -4.569385528564453 
model_pd.lagr.mean(): -4.497962951660156 
model_pd.lambdas: dict_items([('pout', tensor([1.4754])), ('power', tensor([0.2341]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1087])), ('power', tensor([-18.7565]))])
epoch：696	 i:0 	 global-step:13920	 l-p:0.0714227706193924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[ 8.9497,  9.2057,  9.0764],
        [ 8.9497,  9.1710,  9.0498],
        [ 8.9497,  9.0475,  8.9762],
        [ 8.9497, 10.5975, 11.2035]], grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.07133273780345917 
model_pd.l_d.mean(): -4.551809787750244 
model_pd.lagr.mean(): -4.4804768562316895 
model_pd.lambdas: dict_items([('pout', tensor([1.4753])), ('power', tensor([0.2332]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1129])), ('power', tensor([-18.7301]))])
epoch：697	 i:0 	 global-step:13940	 l-p:0.07133273780345917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[ 8.9784,  8.9927,  8.9796],
        [ 8.9784,  9.4008,  9.2618],
        [ 8.9784, 10.1541, 10.3575],
        [ 8.9784,  8.9786,  8.9784]], grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.07124295085668564 
model_pd.l_d.mean(): -4.534283638000488 
model_pd.lagr.mean(): -4.463040828704834 
model_pd.lambdas: dict_items([('pout', tensor([1.4751])), ('power', tensor([0.2323]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1172])), ('power', tensor([-18.7035]))])
epoch：698	 i:0 	 global-step:13960	 l-p:0.07124295085668564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 9.0074, 11.2404, 12.4468],
        [ 9.0074,  9.5118,  9.3824],
        [ 9.0074,  9.0074,  9.0074],
        [ 9.0074, 11.8482, 13.7874]], grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.07115339487791061 
model_pd.l_d.mean(): -4.516807556152344 
model_pd.lagr.mean(): -4.445654392242432 
model_pd.lambdas: dict_items([('pout', tensor([1.4750])), ('power', tensor([0.2313]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1214])), ('power', tensor([-18.6767]))])
epoch：699	 i:0 	 global-step:13980	 l-p:0.07115339487791061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[9.0365, 9.3124, 9.1786],
        [9.0365, 9.0666, 9.0405],
        [9.0365, 9.0946, 9.0479],
        [9.0365, 9.0365, 9.0365]], grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.07106408476829529 
model_pd.l_d.mean(): -4.499382019042969 
model_pd.lagr.mean(): -4.428318023681641 
model_pd.lambdas: dict_items([('pout', tensor([1.4749])), ('power', tensor([0.2304]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1257])), ('power', tensor([-18.6497]))])
epoch：700	 i:0 	 global-step:14000	 l-p:0.07106408476829529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[ 9.0659, 10.7413, 11.3590],
        [ 9.0659,  9.1659,  9.0932],
        [ 9.0659,  9.0660,  9.0659],
        [ 9.0659,  9.0803,  9.0671]], grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.07097502052783966 
model_pd.l_d.mean(): -4.482006072998047 
model_pd.lagr.mean(): -4.411031246185303 
model_pd.lambdas: dict_items([('pout', tensor([1.4748])), ('power', tensor([0.2295]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1299])), ('power', tensor([-18.6226]))])
epoch：701	 i:0 	 global-step:14020	 l-p:0.07097502052783966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[9.0954, 9.4673, 9.3243],
        [9.0954, 9.6060, 9.4751],
        [9.0954, 9.1065, 9.0962],
        [9.0954, 9.0954, 9.0954]], grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.07088617980480194 
model_pd.l_d.mean(): -4.464681625366211 
model_pd.lagr.mean(): -4.393795490264893 
model_pd.lambdas: dict_items([('pout', tensor([1.4746])), ('power', tensor([0.2285]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1342])), ('power', tensor([-18.5953]))])
epoch：702	 i:0 	 global-step:14040	 l-p:0.07088617980480194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[ 9.1251,  9.5561,  9.4144],
        [ 9.1251,  9.1251,  9.1251],
        [ 9.1251, 10.4905, 10.8277],
        [ 9.1251,  9.6614,  9.5346]], grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.07079758495092392 
model_pd.l_d.mean(): -4.447407245635986 
model_pd.lagr.mean(): -4.376609802246094 
model_pd.lambdas: dict_items([('pout', tensor([1.4745])), ('power', tensor([0.2276]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1385])), ('power', tensor([-18.5678]))])
epoch：703	 i:0 	 global-step:14060	 l-p:0.07079758495092392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[ 9.1550,  9.2563,  9.1826],
        [ 9.1550, 10.8358, 11.4476],
        [ 9.1550,  9.2558,  9.1824],
        [ 9.1550,  9.1631,  9.1555]], grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.0707092359662056 
model_pd.l_d.mean(): -4.4301838874816895 
model_pd.lagr.mean(): -4.3594746589660645 
model_pd.lambdas: dict_items([('pout', tensor([1.4743])), ('power', tensor([0.2267]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1428])), ('power', tensor([-18.5401]))])
epoch：704	 i:0 	 global-step:14080	 l-p:0.0707092359662056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[ 9.1850, 10.4847, 10.7637],
        [ 9.1850, 12.0913, 14.0763],
        [ 9.1850,  9.1850,  9.1850],
        [ 9.1850,  9.1861,  9.1851]], grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.07062114030122757 
model_pd.l_d.mean(): -4.413013458251953 
model_pd.lagr.mean(): -4.342392444610596 
model_pd.lambdas: dict_items([('pout', tensor([1.4742])), ('power', tensor([0.2257]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1471])), ('power', tensor([-18.5122]))])
epoch：705	 i:0 	 global-step:14100	 l-p:0.07062114030122757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[9.2153, 9.3175, 9.2432],
        [9.2153, 9.2160, 9.2153],
        [9.2153, 9.3169, 9.2430],
        [9.2153, 9.2158, 9.2153]], grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.07053325325250626 
model_pd.l_d.mean(): -4.395893096923828 
model_pd.lagr.mean(): -4.32535982131958 
model_pd.lambdas: dict_items([('pout', tensor([1.4740])), ('power', tensor([0.2248]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1514])), ('power', tensor([-18.4841]))])
epoch：706	 i:0 	 global-step:14120	 l-p:0.07053325325250626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[ 9.2458,  9.7909,  9.6622],
        [ 9.2458, 10.1247, 10.1214],
        [ 9.2458, 10.0967, 10.0792],
        [ 9.2458, 12.1745, 14.1751]], grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.07044562697410583 
model_pd.l_d.mean(): -4.378824710845947 
model_pd.lagr.mean(): -4.308379173278809 
model_pd.lambdas: dict_items([('pout', tensor([1.4739])), ('power', tensor([0.2239]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1557])), ('power', tensor([-18.4558]))])
epoch：707	 i:0 	 global-step:14140	 l-p:0.07044562697410583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[ 9.2764,  9.6576,  9.5112],
        [ 9.2764,  9.2793,  9.2765],
        [ 9.2764, 10.9964, 11.6302],
        [ 9.2764,  9.3795,  9.3045]], grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.07035823166370392 
model_pd.l_d.mean(): -4.3618083000183105 
model_pd.lagr.mean(): -4.291450023651123 
model_pd.lambdas: dict_items([('pout', tensor([1.4737])), ('power', tensor([0.2230]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1601])), ('power', tensor([-18.4274]))])
epoch：708	 i:0 	 global-step:14160	 l-p:0.07035823166370392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[ 9.3073,  9.8845,  9.7610],
        [ 9.3073, 11.0360, 11.6743],
        [ 9.3073,  9.3073,  9.3073],
        [ 9.3073,  9.3078,  9.3073]], grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.07027105242013931 
model_pd.l_d.mean(): -4.344844341278076 
model_pd.lagr.mean(): -4.27457332611084 
model_pd.lambdas: dict_items([('pout', tensor([1.4736])), ('power', tensor([0.2221]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1645])), ('power', tensor([-18.3987]))])
epoch：709	 i:0 	 global-step:14180	 l-p:0.07027105242013931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[ 9.3383, 10.9184, 11.4164],
        [ 9.3383,  9.3390,  9.3383],
        [ 9.3383, 10.5711, 10.7857],
        [ 9.3383, 10.6641, 10.9492]], grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.0701841413974762 
model_pd.l_d.mean(): -4.327933311462402 
model_pd.lagr.mean(): -4.257749080657959 
model_pd.lambdas: dict_items([('pout', tensor([1.4734])), ('power', tensor([0.2211]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1688])), ('power', tensor([-18.3699]))])
epoch：710	 i:0 	 global-step:14200	 l-p:0.0701841413974762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 9.3696,  9.4724,  9.3974],
        [ 9.3696,  9.9516,  9.8272],
        [ 9.3696,  9.6041,  9.4758],
        [ 9.3696, 11.7086, 12.9741]], grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.0700974389910698 
model_pd.l_d.mean(): -4.311074256896973 
model_pd.lagr.mean(): -4.240976810455322 
model_pd.lambdas: dict_items([('pout', tensor([1.4732])), ('power', tensor([0.2202]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1732])), ('power', tensor([-18.3408]))])
epoch：711	 i:0 	 global-step:14220	 l-p:0.0700974389910698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[ 9.4010,  9.4161,  9.4023],
        [ 9.4010,  9.4011,  9.4010],
        [ 9.4010, 10.2108, 10.1653],
        [ 9.4010,  9.4010,  9.4010]], grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.07001098990440369 
model_pd.l_d.mean(): -4.294268608093262 
model_pd.lagr.mean(): -4.224257469177246 
model_pd.lambdas: dict_items([('pout', tensor([1.4730])), ('power', tensor([0.2193]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1776])), ('power', tensor([-18.3116]))])
epoch：712	 i:0 	 global-step:14240	 l-p:0.07001098990440369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[ 9.4326, 11.1890, 11.8381],
        [ 9.4326, 10.1654, 10.0860],
        [ 9.4326,  9.4443,  9.4335],
        [ 9.4326,  9.4331,  9.4326]], grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.06992477178573608 
model_pd.l_d.mean(): -4.2775163650512695 
model_pd.lagr.mean(): -4.207591533660889 
model_pd.lambdas: dict_items([('pout', tensor([1.4729])), ('power', tensor([0.2184]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1820])), ('power', tensor([-18.2821]))])
epoch：713	 i:0 	 global-step:14260	 l-p:0.06992477178573608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[ 9.4645, 10.8910, 11.2445],
        [ 9.4645,  9.4645,  9.4645],
        [ 9.4645, 10.0255,  9.8933],
        [ 9.4645,  9.6634,  9.5452]], grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.06983878463506699 
model_pd.l_d.mean(): -4.260817527770996 
model_pd.lagr.mean(): -4.190978527069092 
model_pd.lambdas: dict_items([('pout', tensor([1.4727])), ('power', tensor([0.2175]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1865])), ('power', tensor([-18.2524]))])
epoch：714	 i:0 	 global-step:14280	 l-p:0.06983878463506699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 9.4965,  9.5118,  9.4978],
        [ 9.4965, 12.2973, 14.0854],
        [ 9.4965, 10.8493, 11.1407],
        [ 9.4965,  9.5515,  9.5065]], grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.06975302845239639 
model_pd.l_d.mean(): -4.2441725730896 
model_pd.lagr.mean(): -4.174419403076172 
model_pd.lambdas: dict_items([('pout', tensor([1.4725])), ('power', tensor([0.2166]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1909])), ('power', tensor([-18.2226]))])
epoch：715	 i:0 	 global-step:14300	 l-p:0.06975302845239639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[ 9.5288,  9.6316,  9.5562],
        [ 9.5288, 10.1229,  9.9962],
        [ 9.5288, 11.3064, 11.9636],
        [ 9.5288, 11.1472, 11.6581]], grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.0696675106883049 
model_pd.l_d.mean(): -4.2275824546813965 
model_pd.lagr.mean(): -4.157915115356445 
model_pd.lambdas: dict_items([('pout', tensor([1.4723])), ('power', tensor([0.2157]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1954])), ('power', tensor([-18.1925]))])
epoch：716	 i:0 	 global-step:14320	 l-p:0.0696675106883049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[ 9.5612,  9.5612,  9.5612],
        [ 9.5612, 10.0179,  9.8682],
        [ 9.5612,  9.5612,  9.5612],
        [ 9.5612,  9.6683,  9.5905]], grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.06958221644163132 
model_pd.l_d.mean(): -4.211044788360596 
model_pd.lagr.mean(): -4.141462802886963 
model_pd.lambdas: dict_items([('pout', tensor([1.4721])), ('power', tensor([0.2147]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1998])), ('power', tensor([-18.1622]))])
epoch：717	 i:0 	 global-step:14340	 l-p:0.06958221644163132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[ 9.5939, 12.6509, 14.7412],
        [ 9.5939, 12.4281, 14.2380],
        [ 9.5939, 12.1124, 13.5446],
        [ 9.5939, 10.4240, 10.3777]], grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.06949716806411743 
model_pd.l_d.mean(): -4.1945624351501465 
model_pd.lagr.mean(): -4.125065326690674 
model_pd.lambdas: dict_items([('pout', tensor([1.4719])), ('power', tensor([0.2138]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2043])), ('power', tensor([-18.1318]))])
epoch：718	 i:0 	 global-step:14360	 l-p:0.06949716806411743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[9.6267, 9.9254, 9.7809],
        [9.6267, 9.7332, 9.6556],
        [9.6267, 9.7342, 9.6560],
        [9.6267, 9.6594, 9.6310]], grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.06941234320402145 
model_pd.l_d.mean(): -4.178134441375732 
model_pd.lagr.mean(): -4.10872220993042 
model_pd.lambdas: dict_items([('pout', tensor([1.4717])), ('power', tensor([0.2129]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2088])), ('power', tensor([-18.1011]))])
epoch：719	 i:0 	 global-step:14380	 l-p:0.06941234320402145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 9.6598, 10.6823, 10.7330],
        [ 9.6598, 10.5867, 10.5842],
        [ 9.6598,  9.6599,  9.6598],
        [ 9.6598,  9.7160,  9.6700]], grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.06932775676250458 
model_pd.l_d.mean(): -4.161762237548828 
model_pd.lagr.mean(): -4.092434406280518 
model_pd.lambdas: dict_items([('pout', tensor([1.4715])), ('power', tensor([0.2120]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2133])), ('power', tensor([-18.0702]))])
epoch：720	 i:0 	 global-step:14400	 l-p:0.06932775676250458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[ 9.6931, 12.0777, 13.3408],
        [ 9.6931, 10.6238, 10.6214],
        [ 9.6931, 10.6548, 10.6690],
        [ 9.6931,  9.6942,  9.6931]], grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.0692434087395668 
model_pd.l_d.mean(): -4.145444393157959 
model_pd.lagr.mean(): -4.07620096206665 
model_pd.lambdas: dict_items([('pout', tensor([1.4712])), ('power', tensor([0.2111]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2178])), ('power', tensor([-18.0391]))])
epoch：721	 i:0 	 global-step:14420	 l-p:0.0692434087395668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[ 9.7266,  9.7266,  9.7266],
        [ 9.7266, 10.0856,  9.9326],
        [ 9.7266,  9.7271,  9.7266],
        [ 9.7266,  9.7271,  9.7266]], grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.06915928423404694 
model_pd.l_d.mean(): -4.129182815551758 
model_pd.lagr.mean(): -4.060023307800293 
model_pd.lambdas: dict_items([('pout', tensor([1.4710])), ('power', tensor([0.2102]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2224])), ('power', tensor([-18.0078]))])
epoch：722	 i:0 	 global-step:14440	 l-p:0.06915928423404694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[ 9.7603,  9.7609,  9.7603],
        [ 9.7603, 12.2137, 13.5429],
        [ 9.7603,  9.7634,  9.7604],
        [ 9.7603,  9.9671,  9.8443]], grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.06907539069652557 
model_pd.l_d.mean(): -4.112976551055908 
model_pd.lagr.mean(): -4.043900966644287 
model_pd.lambdas: dict_items([('pout', tensor([1.4708])), ('power', tensor([0.2093]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2269])), ('power', tensor([-17.9762]))])
epoch：723	 i:0 	 global-step:14460	 l-p:0.06907539069652557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[ 9.7942, 10.2019, 10.0458],
        [ 9.7942,  9.9007,  9.8227],
        [ 9.7942, 10.3793, 10.2419],
        [ 9.7942, 10.7068, 10.6893]], grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.06899173557758331 
model_pd.l_d.mean(): -4.096826076507568 
model_pd.lagr.mean(): -4.027834415435791 
model_pd.lambdas: dict_items([('pout', tensor([1.4706])), ('power', tensor([0.2084]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2315])), ('power', tensor([-17.9445]))])
epoch：724	 i:0 	 global-step:14480	 l-p:0.06899173557758331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[ 9.8284, 10.1162,  9.9714],
        [ 9.8284, 10.0370,  9.9131],
        [ 9.8284,  9.8284,  9.8284],
        [ 9.8284, 10.2378, 10.0810]], grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.06890831142663956 
model_pd.l_d.mean(): -4.0807318687438965 
model_pd.lagr.mean(): -4.011823654174805 
model_pd.lambdas: dict_items([('pout', tensor([1.4703])), ('power', tensor([0.2075]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2360])), ('power', tensor([-17.9125]))])
epoch：725	 i:0 	 global-step:14500	 l-p:0.06890831142663956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 9.8628,  9.8628,  9.8628],
        [ 9.8628, 11.5486, 12.0819],
        [ 9.8628, 10.1127,  9.9762],
        [ 9.8628, 12.3461, 13.6921]], grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.06882511079311371 
model_pd.l_d.mean(): -4.064693927764893 
model_pd.lagr.mean(): -3.9958689212799072 
model_pd.lambdas: dict_items([('pout', tensor([1.4701])), ('power', tensor([0.2066]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2406])), ('power', tensor([-17.8803]))])
epoch：726	 i:0 	 global-step:14520	 l-p:0.06882511079311371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[ 9.8973,  9.8973,  9.8973],
        [ 9.8973, 11.5901, 12.1258],
        [ 9.8973,  9.8973,  9.8973],
        [ 9.8973, 10.4899, 10.3509]], grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.06874215602874756 
model_pd.l_d.mean(): -4.048712253570557 
model_pd.lagr.mean(): -3.9799699783325195 
model_pd.lambdas: dict_items([('pout', tensor([1.4698])), ('power', tensor([0.2058]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2452])), ('power', tensor([-17.8479]))])
epoch：727	 i:0 	 global-step:14540	 l-p:0.06874215602874756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[ 9.9321,  9.9321,  9.9322],
        [ 9.9321, 10.0405,  9.9611],
        [ 9.9321, 11.2592, 11.4921],
        [ 9.9321, 11.6320, 12.1699]], grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.06865942478179932 
model_pd.l_d.mean(): -4.032788276672363 
model_pd.lagr.mean(): -3.9641289710998535 
model_pd.lambdas: dict_items([('pout', tensor([1.4696])), ('power', tensor([0.2049]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2498])), ('power', tensor([-17.8152]))])
epoch：728	 i:0 	 global-step:14560	 l-p:0.06865942478179932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[ 9.9672,  9.9679,  9.9672],
        [ 9.9672, 10.7506, 10.6668],
        [ 9.9672, 12.9293, 14.8229],
        [ 9.9672,  9.9677,  9.9672]], grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.06857693940401077 
model_pd.l_d.mean(): -4.016921043395996 
model_pd.lagr.mean(): -3.9483439922332764 
model_pd.lambdas: dict_items([('pout', tensor([1.4693])), ('power', tensor([0.2040]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2545])), ('power', tensor([-17.7824]))])
epoch：729	 i:0 	 global-step:14580	 l-p:0.06857693940401077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[10.0024, 11.0011, 11.0166],
        [10.0024, 11.5257, 11.9051],
        [10.0024, 10.0611, 10.0131],
        [10.0024, 10.0367, 10.0069]], grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.06849468499422073 
model_pd.l_d.mean(): -4.00111198425293 
model_pd.lagr.mean(): -3.9326171875 
model_pd.lambdas: dict_items([('pout', tensor([1.4691])), ('power', tensor([0.2031]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2591])), ('power', tensor([-17.7493]))])
epoch：730	 i:0 	 global-step:14600	 l-p:0.06849468499422073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.0379, 11.5676, 11.9486],
        [10.0379, 11.9257, 12.6244],
        [10.0379, 10.6407, 10.4995],
        [10.0379, 10.0384, 10.0379]], grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.0684126541018486 
model_pd.l_d.mean(): -3.9853601455688477 
model_pd.lagr.mean(): -3.916947603225708 
model_pd.lambdas: dict_items([('pout', tensor([1.4688])), ('power', tensor([0.2022]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2638])), ('power', tensor([-17.7160]))])
epoch：731	 i:0 	 global-step:14620	 l-p:0.0684126541018486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[10.0736, 11.9714, 12.6753],
        [10.0736, 11.5246, 11.8391],
        [10.0736, 10.0768, 10.0737],
        [10.0736, 10.3302, 10.1901]], grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.06833086162805557 
model_pd.l_d.mean(): -3.9696664810180664 
model_pd.lagr.mean(): -3.9013357162475586 
model_pd.lambdas: dict_items([('pout', tensor([1.4685])), ('power', tensor([0.2013]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2684])), ('power', tensor([-17.6825]))])
epoch：732	 i:0 	 global-step:14640	 l-p:0.06833086162805557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[10.1095, 10.1101, 10.1095],
        [10.1095, 10.7176, 10.5752],
        [10.1095, 10.1100, 10.1095],
        [10.1095, 10.1127, 10.1096]], grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.06824929267168045 
model_pd.l_d.mean(): -3.954030752182007 
model_pd.lagr.mean(): -3.8857815265655518 
model_pd.lambdas: dict_items([('pout', tensor([1.4683])), ('power', tensor([0.2004]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2731])), ('power', tensor([-17.6487]))])
epoch：733	 i:0 	 global-step:14660	 l-p:0.06824929267168045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[10.1457, 10.1457, 10.1457],
        [10.1457, 11.5067, 11.7462],
        [10.1457, 11.8886, 12.4410],
        [10.1457, 11.6948, 12.0810]], grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.06816795468330383 
model_pd.l_d.mean(): -3.9384536743164062 
model_pd.lagr.mean(): -3.870285749435425 
model_pd.lambdas: dict_items([('pout', tensor([1.4680])), ('power', tensor([0.1996]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2778])), ('power', tensor([-17.6147]))])
epoch：734	 i:0 	 global-step:14680	 l-p:0.06816795468330383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[10.1821, 12.1017, 12.8128],
        [10.1821, 10.5021, 10.3475],
        [10.1821, 10.1833, 10.1821],
        [10.1821, 11.7378, 12.1258]], grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.06808685511350632 
model_pd.l_d.mean(): -3.9229354858398438 
model_pd.lagr.mean(): -3.854848623275757 
model_pd.lambdas: dict_items([('pout', tensor([1.4677])), ('power', tensor([0.1987]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2825])), ('power', tensor([-17.5805]))])
epoch：735	 i:0 	 global-step:14700	 l-p:0.06808685511350632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[10.2187, 13.2671, 15.2171],
        [10.2187, 10.4376, 10.3077],
        [10.2187, 10.2539, 10.2233],
        [10.2187, 10.2355, 10.2201]], grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.06800597161054611 
model_pd.l_d.mean(): -3.9074759483337402 
model_pd.lagr.mean(): -3.8394699096679688 
model_pd.lambdas: dict_items([('pout', tensor([1.4674])), ('power', tensor([0.1978]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2872])), ('power', tensor([-17.5460]))])
epoch：736	 i:0 	 global-step:14720	 l-p:0.06800597161054611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[10.2556, 10.6387, 10.4757],
        [10.2556, 10.2561, 10.2556],
        [10.2556, 10.4754, 10.3450],
        [10.2556, 11.1552, 11.1065]], grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.0679253414273262 
model_pd.l_d.mean(): -3.892075538635254 
model_pd.lagr.mean(): -3.8241500854492188 
model_pd.lambdas: dict_items([('pout', tensor([1.4671])), ('power', tensor([0.1969]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2919])), ('power', tensor([-17.5113]))])
epoch：737	 i:0 	 global-step:14740	 l-p:0.0679253414273262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[10.2927, 10.3096, 10.2941],
        [10.2927, 10.4059, 10.3230],
        [10.2927, 11.2613, 11.2438],
        [10.2927, 10.2931, 10.2927]], grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.0678449273109436 
model_pd.l_d.mean(): -3.8767340183258057 
model_pd.lagr.mean(): -3.808889150619507 
model_pd.lambdas: dict_items([('pout', tensor([1.4668])), ('power', tensor([0.1960]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2967])), ('power', tensor([-17.4763]))])
epoch：738	 i:0 	 global-step:14760	 l-p:0.0678449273109436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[10.3300, 10.3300, 10.3300],
        [10.3300, 11.1480, 11.0611],
        [10.3300, 12.9503, 14.3725],
        [10.3300, 12.1102, 12.6750]], grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.0677647590637207 
model_pd.l_d.mean(): -3.861452579498291 
model_pd.lagr.mean(): -3.7936878204345703 
model_pd.lambdas: dict_items([('pout', tensor([1.4665])), ('power', tensor([0.1952]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3014])), ('power', tensor([-17.4411]))])
epoch：739	 i:0 	 global-step:14780	 l-p:0.0677647590637207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.3676, 11.3446, 11.3272],
        [10.3676, 10.8718, 10.7074],
        [10.3676, 10.4819, 10.3982],
        [10.3676, 10.3681, 10.3676]], grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.0676848292350769 
model_pd.l_d.mean(): -3.84623122215271 
model_pd.lagr.mean(): -3.7785463333129883 
model_pd.lambdas: dict_items([('pout', tensor([1.4662])), ('power', tensor([0.1943]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3062])), ('power', tensor([-17.4057]))])
epoch：740	 i:0 	 global-step:14800	 l-p:0.0676848292350769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[10.4054, 11.9131, 12.2409],
        [10.4054, 10.4186, 10.4063],
        [10.4054, 12.9946, 14.3692],
        [10.4054, 13.1723, 14.7498]], grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.0676051452755928 
model_pd.l_d.mean(): -3.8310697078704834 
model_pd.lagr.mean(): -3.7634644508361816 
model_pd.lambdas: dict_items([('pout', tensor([1.4659])), ('power', tensor([0.1934]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3110])), ('power', tensor([-17.3701]))])
epoch：741	 i:0 	 global-step:14820	 l-p:0.0676051452755928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[10.4434, 11.4612, 11.4603],
        [10.4434, 10.5132, 10.4571],
        [10.4434, 12.4208, 13.1542],
        [10.4434, 10.4467, 10.4435]], grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.06752569228410721 
model_pd.l_d.mean(): -3.8159682750701904 
model_pd.lagr.mean(): -3.7484426498413086 
model_pd.lambdas: dict_items([('pout', tensor([1.4656])), ('power', tensor([0.1926]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3158])), ('power', tensor([-17.3342]))])
epoch：742	 i:0 	 global-step:14840	 l-p:0.06752569228410721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[10.4817, 10.6001, 10.5138],
        [10.4817, 13.1464, 14.5934],
        [10.4817, 12.4697, 13.2086],
        [10.4817, 11.1170, 10.9687]], grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.06744649261236191 
model_pd.l_d.mean(): -3.8009276390075684 
model_pd.lagr.mean(): -3.7334811687469482 
model_pd.lambdas: dict_items([('pout', tensor([1.4653])), ('power', tensor([0.1917]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3206])), ('power', tensor([-17.2980]))])
epoch：743	 i:0 	 global-step:14860	 l-p:0.06744649261236191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[10.5202, 10.5202, 10.5202],
        [10.5202, 10.5299, 10.5208],
        [10.5202, 10.8332, 10.6760],
        [10.5202, 10.7471, 10.6126]], grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.06736753135919571 
model_pd.l_d.mean(): -3.7859482765197754 
model_pd.lagr.mean(): -3.718580722808838 
model_pd.lambdas: dict_items([('pout', tensor([1.4650])), ('power', tensor([0.1908]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3254])), ('power', tensor([-17.2617]))])
epoch：744	 i:0 	 global-step:14880	 l-p:0.06736753135919571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[10.5590, 10.8936, 10.7321],
        [10.5590, 10.7869, 10.6517],
        [10.5590, 10.5724, 10.5599],
        [10.5590, 11.3987, 11.3099]], grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.0672888234257698 
model_pd.l_d.mean(): -3.771029472351074 
model_pd.lagr.mean(): -3.7037405967712402 
model_pd.lambdas: dict_items([('pout', tensor([1.4646])), ('power', tensor([0.1900]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3302])), ('power', tensor([-17.2250]))])
epoch：745	 i:0 	 global-step:14900	 l-p:0.0672888234257698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[10.5980, 11.7402, 11.7993],
        [10.5980, 12.6094, 13.3561],
        [10.5980, 12.1385, 12.4741],
        [10.5980, 13.2968, 14.7628]], grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.067210353910923 
model_pd.l_d.mean(): -3.756171941757202 
model_pd.lagr.mean(): -3.6889615058898926 
model_pd.lambdas: dict_items([('pout', tensor([1.4643])), ('power', tensor([0.1891]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3351])), ('power', tensor([-17.1882]))])
epoch：746	 i:0 	 global-step:14920	 l-p:0.067210353910923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[10.6372, 11.6775, 11.6770],
        [10.6372, 10.6372, 10.6372],
        [10.6372, 11.0882, 10.9161],
        [10.6372, 10.7552, 10.6688]], grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.0671321302652359 
model_pd.l_d.mean(): -3.7413759231567383 
model_pd.lagr.mean(): -3.674243688583374 
model_pd.lambdas: dict_items([('pout', tensor([1.4640])), ('power', tensor([0.1883]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3399])), ('power', tensor([-17.1511]))])
epoch：747	 i:0 	 global-step:14940	 l-p:0.0671321302652359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[10.6767, 11.1992, 11.0291],
        [10.6767, 11.6886, 11.6711],
        [10.6767, 11.0159, 10.8523],
        [10.6767, 12.7055, 13.4589]], grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.0670541524887085 
model_pd.l_d.mean(): -3.7266416549682617 
model_pd.lagr.mean(): -3.6595873832702637 
model_pd.lambdas: dict_items([('pout', tensor([1.4636])), ('power', tensor([0.1874]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3448])), ('power', tensor([-17.1137]))])
epoch：748	 i:0 	 global-step:14960	 l-p:0.0670541524887085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[10.7164, 10.8392, 10.7500],
        [10.7164, 10.7164, 10.7164],
        [10.7164, 13.5787, 15.2119],
        [10.7164, 13.3950, 14.8185]], grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.06697642058134079 
model_pd.l_d.mean(): -3.7119698524475098 
model_pd.lagr.mean(): -3.644993543624878 
model_pd.lambdas: dict_items([('pout', tensor([1.4633])), ('power', tensor([0.1866]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3497])), ('power', tensor([-17.0761]))])
epoch：749	 i:0 	 global-step:14980	 l-p:0.06697642058134079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[10.7564, 13.4465, 14.8762],
        [10.7564, 11.0780, 10.9166],
        [10.7564, 10.7939, 10.7613],
        [10.7564, 13.5017, 14.9936]], grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.06689893454313278 
model_pd.l_d.mean(): -3.697359561920166 
model_pd.lagr.mean(): -3.630460739135742 
model_pd.lambdas: dict_items([('pout', tensor([1.4629])), ('power', tensor([0.1857]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3545])), ('power', tensor([-17.0383]))])
epoch：750	 i:0 	 global-step:15000	 l-p:0.06689893454313278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[10.7966, 12.8520, 13.6156],
        [10.7966, 13.5537, 15.0522],
        [10.7966, 12.8543, 13.6201],
        [10.7966, 12.4632, 12.8808]], grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.06682168692350388 
model_pd.l_d.mean(): -3.682811975479126 
model_pd.lagr.mean(): -3.615990400314331 
model_pd.lambdas: dict_items([('pout', tensor([1.4625])), ('power', tensor([0.1849]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3594])), ('power', tensor([-17.0001]))])
epoch：751	 i:0 	 global-step:15020	 l-p:0.06682168692350388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[10.8371, 13.7364, 15.3912],
        [10.8371, 11.9007, 11.9006],
        [10.8371, 12.4185, 12.7636],
        [10.8371, 10.8379, 10.8371]], grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.06674467772245407 
model_pd.l_d.mean(): -3.6683273315429688 
model_pd.lagr.mean(): -3.6015827655792236 
model_pd.lambdas: dict_items([('pout', tensor([1.4622])), ('power', tensor([0.1840]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3644])), ('power', tensor([-16.9618]))])
epoch：752	 i:0 	 global-step:15040	 l-p:0.06674467772245407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[10.8779, 11.4123, 11.2384],
        [10.8779, 11.1598, 11.0062],
        [10.8779, 11.2038, 11.0403],
        [10.8779, 10.9159, 10.8829]], grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.06666790693998337 
model_pd.l_d.mean(): -3.6539058685302734 
model_pd.lagr.mean(): -3.587238073348999 
model_pd.lambdas: dict_items([('pout', tensor([1.4618])), ('power', tensor([0.1832]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3693])), ('power', tensor([-16.9232]))])
epoch：753	 i:0 	 global-step:15060	 l-p:0.06666790693998337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[10.9189, 11.0452, 10.9535],
        [10.9189, 10.9189, 10.9189],
        [10.9189, 11.2674, 11.0994],
        [10.9189, 10.9843, 10.9308]], grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.06659137457609177 
model_pd.l_d.mean(): -3.6395468711853027 
model_pd.lagr.mean(): -3.57295560836792 
model_pd.lambdas: dict_items([('pout', tensor([1.4614])), ('power', tensor([0.1823]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3742])), ('power', tensor([-16.8843]))])
epoch：754	 i:0 	 global-step:15080	 l-p:0.06659137457609177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[10.9601, 13.0364, 13.7996],
        [10.9601, 10.9601, 10.9601],
        [10.9601, 10.9608, 10.9601],
        [10.9601, 11.8380, 11.7459]], grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.06651508063077927 
model_pd.l_d.mean(): -3.6252522468566895 
model_pd.lagr.mean(): -3.558737277984619 
model_pd.lambdas: dict_items([('pout', tensor([1.4611])), ('power', tensor([0.1815]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3792])), ('power', tensor([-16.8451]))])
epoch：755	 i:0 	 global-step:15100	 l-p:0.06651508063077927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[11.0017, 12.1202, 12.1398],
        [11.0017, 11.0017, 11.0017],
        [11.0017, 11.3321, 11.1664],
        [11.0017, 11.0017, 11.0017]], grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.06643902510404587 
model_pd.l_d.mean(): -3.611020565032959 
model_pd.lagr.mean(): -3.544581651687622 
model_pd.lambdas: dict_items([('pout', tensor([1.4607])), ('power', tensor([0.1806]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3841])), ('power', tensor([-16.8057]))])
epoch：756	 i:0 	 global-step:15120	 l-p:0.06643902510404587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[11.0434, 11.0470, 11.0436],
        [11.0434, 11.1098, 11.0555],
        [11.0434, 11.0537, 11.0441],
        [11.0434, 12.7544, 13.1839]], grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.06636320054531097 
model_pd.l_d.mean(): -3.5968523025512695 
model_pd.lagr.mean(): -3.530489206314087 
model_pd.lambdas: dict_items([('pout', tensor([1.4603])), ('power', tensor([0.1798]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3891])), ('power', tensor([-16.7660]))])
epoch：757	 i:0 	 global-step:15140	 l-p:0.06636320054531097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[11.0855, 12.0724, 12.0206],
        [11.0855, 11.5595, 11.3790],
        [11.0855, 11.0855, 11.0855],
        [11.0855, 11.2142, 11.1208]], grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.06628762930631638 
model_pd.l_d.mean(): -3.582749366760254 
model_pd.lagr.mean(): -3.5164618492126465 
model_pd.lambdas: dict_items([('pout', tensor([1.4599])), ('power', tensor([0.1790]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3941])), ('power', tensor([-16.7261]))])
epoch：758	 i:0 	 global-step:15160	 l-p:0.06628762930631638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[11.1278, 11.1948, 11.1400],
        [11.1278, 11.1278, 11.1278],
        [11.1278, 11.1285, 11.1278],
        [11.1278, 12.0216, 11.9281]], grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.06621226668357849 
model_pd.l_d.mean(): -3.5687098503112793 
model_pd.lagr.mean(): -3.502497673034668 
model_pd.lambdas: dict_items([('pout', tensor([1.4595])), ('power', tensor([0.1781]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3991])), ('power', tensor([-16.6859]))])
epoch：759	 i:0 	 global-step:15180	 l-p:0.06621226668357849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[11.1704, 11.1710, 11.1704],
        [11.1704, 11.1892, 11.1720],
        [11.1704, 12.3091, 12.3294],
        [11.1704, 12.6945, 12.9657]], grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.0661371573805809 
model_pd.l_d.mean(): -3.5547356605529785 
model_pd.lagr.mean(): -3.488598585128784 
model_pd.lambdas: dict_items([('pout', tensor([1.4591])), ('power', tensor([0.1773]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4041])), ('power', tensor([-16.6454]))])
epoch：760	 i:0 	 global-step:15200	 l-p:0.0661371573805809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[11.2133, 13.3605, 14.1597],
        [11.2133, 11.2139, 11.2133],
        [11.2133, 11.4585, 11.3133],
        [11.2133, 11.5731, 11.3998]], grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.06606227159500122 
model_pd.l_d.mean(): -3.5408248901367188 
model_pd.lagr.mean(): -3.4747626781463623 
model_pd.lambdas: dict_items([('pout', tensor([1.4587])), ('power', tensor([0.1765]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4091])), ('power', tensor([-16.6046]))])
epoch：761	 i:0 	 global-step:15220	 l-p:0.06606227159500122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[11.2564, 13.4156, 14.2208],
        [11.2564, 11.2570, 11.2564],
        [11.2564, 11.3868, 11.2921],
        [11.2564, 11.2564, 11.2564]], grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.06598762422800064 
model_pd.l_d.mean(): -3.5269806385040283 
model_pd.lagr.mean(): -3.4609930515289307 
model_pd.lambdas: dict_items([('pout', tensor([1.4583])), ('power', tensor([0.1756]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4142])), ('power', tensor([-16.5636]))])
epoch：762	 i:0 	 global-step:15240	 l-p:0.06598762422800064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[11.2998, 11.4315, 11.3360],
        [11.2998, 13.4661, 14.2727],
        [11.2998, 11.2998, 11.2998],
        [11.2998, 11.3104, 11.3005]], grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.06591320782899857 
model_pd.l_d.mean(): -3.5132012367248535 
model_pd.lagr.mean(): -3.4472880363464355 
model_pd.lambdas: dict_items([('pout', tensor([1.4578])), ('power', tensor([0.1748]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4192])), ('power', tensor([-16.5223]))])
epoch：763	 i:0 	 global-step:15260	 l-p:0.06591320782899857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[11.3435, 11.3435, 11.3435],
        [11.3435, 12.4658, 12.4667],
        [11.3435, 11.6401, 11.4787],
        [11.3435, 11.3441, 11.3435]], grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.065839022397995 
model_pd.l_d.mean(): -3.4994876384735107 
model_pd.lagr.mean(): -3.4336485862731934 
model_pd.lambdas: dict_items([('pout', tensor([1.4574])), ('power', tensor([0.1740]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4243])), ('power', tensor([-16.4806]))])
epoch：764	 i:0 	 global-step:15280	 l-p:0.065839022397995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[11.3875, 14.4552, 16.2085],
        [11.3875, 11.3889, 11.3875],
        [11.3875, 14.8368, 17.0491],
        [11.3875, 11.7541, 11.5776]], grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.06576507538557053 
model_pd.l_d.mean(): -3.485839366912842 
model_pd.lagr.mean(): -3.420074224472046 
model_pd.lambdas: dict_items([('pout', tensor([1.4570])), ('power', tensor([0.1732]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4294])), ('power', tensor([-16.4388]))])
epoch：765	 i:0 	 global-step:15300	 l-p:0.06576507538557053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[11.4318, 11.4355, 11.4319],
        [11.4318, 11.4324, 11.4318],
        [11.4318, 12.5642, 12.5654],
        [11.4318, 12.1055, 11.9360]], grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.06569135934114456 
model_pd.l_d.mean(): -3.472256898880005 
model_pd.lagr.mean(): -3.4065654277801514 
model_pd.lambdas: dict_items([('pout', tensor([1.4566])), ('power', tensor([0.1723]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4345])), ('power', tensor([-16.3966]))])
epoch：766	 i:0 	 global-step:15320	 l-p:0.06569135934114456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[11.4763, 13.6815, 14.5031],
        [11.4763, 14.5712, 16.3403],
        [11.4763, 12.6139, 12.6151],
        [11.4763, 11.4763, 11.4763]], grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.0656178742647171 
model_pd.l_d.mean(): -3.458740711212158 
model_pd.lagr.mean(): -3.393122911453247 
model_pd.lambdas: dict_items([('pout', tensor([1.4561])), ('power', tensor([0.1715]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4396])), ('power', tensor([-16.3541]))])
epoch：767	 i:0 	 global-step:15340	 l-p:0.0656178742647171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[11.5211, 14.4308, 15.9803],
        [11.5211, 11.5212, 11.5211],
        [11.5211, 13.2190, 13.5915],
        [11.5211, 11.5211, 11.5211]], grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.06554463505744934 
model_pd.l_d.mean(): -3.445291519165039 
model_pd.lagr.mean(): -3.379746913909912 
model_pd.lambdas: dict_items([('pout', tensor([1.4557])), ('power', tensor([0.1707]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4447])), ('power', tensor([-16.3114]))])
epoch：768	 i:0 	 global-step:15360	 l-p:0.06554463505744934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[11.5662, 12.0648, 11.8753],
        [11.5662, 11.9172, 11.7414],
        [11.5662, 13.5959, 14.2441],
        [11.5662, 11.5671, 11.5662]], grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.06547162681818008 
model_pd.l_d.mean(): -3.43190860748291 
model_pd.lagr.mean(): -3.3664369583129883 
model_pd.lambdas: dict_items([('pout', tensor([1.4552])), ('power', tensor([0.1699]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4498])), ('power', tensor([-16.2684]))])
epoch：769	 i:0 	 global-step:15380	 l-p:0.06547162681818008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[11.6116, 11.8673, 11.7160],
        [11.6116, 14.6073, 16.2388],
        [11.6116, 13.8492, 14.6847],
        [11.6116, 11.6911, 11.6272]], grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.06539884209632874 
model_pd.l_d.mean(): -3.4185922145843506 
model_pd.lagr.mean(): -3.3531932830810547 
model_pd.lambdas: dict_items([('pout', tensor([1.4548])), ('power', tensor([0.1691]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4550])), ('power', tensor([-16.2251]))])
epoch：770	 i:0 	 global-step:15400	 l-p:0.06539884209632874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[11.6573, 15.4752, 18.0977],
        [11.6573, 11.6573, 11.6573],
        [11.6573, 11.6771, 11.6590],
        [11.6573, 12.3783, 12.2115]], grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.06532630324363708 
model_pd.l_d.mean(): -3.405343532562256 
model_pd.lagr.mean(): -3.340017318725586 
model_pd.lambdas: dict_items([('pout', tensor([1.4543])), ('power', tensor([0.1683]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4601])), ('power', tensor([-16.1814]))])
epoch：771	 i:0 	 global-step:15420	 l-p:0.06532630324363708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[11.7033, 11.7745, 11.7163],
        [11.7033, 12.6517, 12.5533],
        [11.7033, 11.7033, 11.7033],
        [11.7033, 13.7606, 14.4180]], grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.06525400280952454 
model_pd.l_d.mean(): -3.3921616077423096 
model_pd.lagr.mean(): -3.3269076347351074 
model_pd.lambdas: dict_items([('pout', tensor([1.4538])), ('power', tensor([0.1675]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4653])), ('power', tensor([-16.1375]))])
epoch：772	 i:0 	 global-step:15440	 l-p:0.06525400280952454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[11.7496, 14.0175, 14.8648],
        [11.7496, 11.7502, 11.7496],
        [11.7496, 12.7023, 12.6036],
        [11.7496, 12.2008, 12.0098]], grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.06518194079399109 
model_pd.l_d.mean(): -3.379047393798828 
model_pd.lagr.mean(): -3.3138654232025146 
model_pd.lambdas: dict_items([('pout', tensor([1.4534])), ('power', tensor([0.1667]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4704])), ('power', tensor([-16.0933]))])
epoch：773	 i:0 	 global-step:15460	 l-p:0.06518194079399109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[11.7961, 12.5638, 12.4033],
        [11.7961, 11.8072, 11.7968],
        [11.7961, 11.7961, 11.7961],
        [11.7961, 11.7967, 11.7961]], grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.06511012464761734 
model_pd.l_d.mean(): -3.3660011291503906 
model_pd.lagr.mean(): -3.3008909225463867 
model_pd.lambdas: dict_items([('pout', tensor([1.4529])), ('power', tensor([0.1659]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4756])), ('power', tensor([-16.0488]))])
epoch：774	 i:0 	 global-step:15480	 l-p:0.06511012464761734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[11.8430, 11.8432, 11.8430],
        [11.8430, 12.2271, 12.0423],
        [11.8430, 13.9284, 14.5952],
        [11.8430, 11.8541, 11.8436]], grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.06503854691982269 
model_pd.l_d.mean(): -3.353022575378418 
model_pd.lagr.mean(): -3.2879841327667236 
model_pd.lambdas: dict_items([('pout', tensor([1.4524])), ('power', tensor([0.1651]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4808])), ('power', tensor([-16.0041]))])
epoch：775	 i:0 	 global-step:15500	 l-p:0.06503854691982269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[11.8901, 12.3478, 12.1542],
        [11.8901, 12.2039, 12.0332],
        [11.8901, 13.1971, 13.2677],
        [11.8901, 13.9850, 14.6550]], grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.06496720016002655 
model_pd.l_d.mean(): -3.3401119709014893 
model_pd.lagr.mean(): -3.2751448154449463 
model_pd.lambdas: dict_items([('pout', tensor([1.4519])), ('power', tensor([0.1643]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4860])), ('power', tensor([-15.9590]))])
epoch：776	 i:0 	 global-step:15520	 l-p:0.06496720016002655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[11.9376, 15.5753, 17.9109],
        [11.9376, 12.9082, 12.8078],
        [11.9376, 13.7063, 14.0955],
        [11.9376, 12.3019, 12.1195]], grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.0648960992693901 
model_pd.l_d.mean(): -3.327270030975342 
model_pd.lagr.mean(): -3.262373924255371 
model_pd.lambdas: dict_items([('pout', tensor([1.4514])), ('power', tensor([0.1635]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4913])), ('power', tensor([-15.9136]))])
epoch：777	 i:0 	 global-step:15540	 l-p:0.0648960992693901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[11.9853, 12.0678, 12.0015],
        [11.9853, 13.8657, 14.3404],
        [11.9853, 13.3044, 13.3758],
        [11.9853, 12.5849, 12.3908]], grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.06482525169849396 
model_pd.l_d.mean(): -3.3144967555999756 
model_pd.lagr.mean(): -3.249671459197998 
model_pd.lambdas: dict_items([('pout', tensor([1.4509])), ('power', tensor([0.1627]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4965])), ('power', tensor([-15.8679]))])
epoch：778	 i:0 	 global-step:15560	 l-p:0.06482525169849396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[12.0333, 12.4248, 12.2366],
        [12.0333, 15.1524, 16.8526],
        [12.0333, 12.1704, 12.0702],
        [12.0333, 15.2986, 17.1673]], grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.06475463509559631 
model_pd.l_d.mean(): -3.3017916679382324 
model_pd.lagr.mean(): -3.237036943435669 
model_pd.lambdas: dict_items([('pout', tensor([1.4504])), ('power', tensor([0.1619]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5018])), ('power', tensor([-15.8219]))])
epoch：779	 i:0 	 global-step:15580	 l-p:0.06475463509559631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[12.0817, 12.6066, 12.4074],
        [12.0817, 15.2148, 16.9230],
        [12.0817, 14.2152, 14.8980],
        [12.0817, 12.8007, 12.6205]], grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.06468427181243896 
model_pd.l_d.mean(): -3.289155960083008 
model_pd.lagr.mean(): -3.2244715690612793 
model_pd.lambdas: dict_items([('pout', tensor([1.4499])), ('power', tensor([0.1611]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5070])), ('power', tensor([-15.7756]))])
epoch：780	 i:0 	 global-step:15600	 l-p:0.06468427181243896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[12.1303, 15.2777, 16.9937],
        [12.1303, 13.1192, 13.0172],
        [12.1303, 12.1463, 12.1315],
        [12.1303, 12.3997, 12.2404]], grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.06461415439844131 
model_pd.l_d.mean(): -3.276589870452881 
model_pd.lagr.mean(): -3.2119758129119873 
model_pd.lambdas: dict_items([('pout', tensor([1.4494])), ('power', tensor([0.1603]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5123])), ('power', tensor([-15.7290]))])
epoch：781	 i:0 	 global-step:15620	 l-p:0.06461415439844131
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[12.1793, 12.5763, 12.3855],
        [12.1793, 12.6501, 12.4510],
        [12.1793, 13.4386, 13.4630],
        [12.1793, 14.5223, 15.3875]], grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.06454429775476456 
model_pd.l_d.mean(): -3.264092206954956 
model_pd.lagr.mean(): -3.1995480060577393 
model_pd.lambdas: dict_items([('pout', tensor([1.4489])), ('power', tensor([0.1595]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5176])), ('power', tensor([-15.6821]))])
epoch：782	 i:0 	 global-step:15640	 l-p:0.06454429775476456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[12.2285, 12.2287, 12.2285],
        [12.2285, 14.0468, 14.4475],
        [12.2285, 12.3733, 12.2683],
        [12.2285, 12.2286, 12.2285]], grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.0644746720790863 
model_pd.l_d.mean(): -3.251664638519287 
model_pd.lagr.mean(): -3.187190055847168 
model_pd.lambdas: dict_items([('pout', tensor([1.4484])), ('power', tensor([0.1587]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5229])), ('power', tensor([-15.6349]))])
epoch：783	 i:0 	 global-step:15660	 l-p:0.0644746720790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[12.2780, 13.2809, 13.1777],
        [12.2780, 12.2781, 12.2780],
        [12.2780, 13.4698, 13.4524],
        [12.2780, 12.6547, 12.4663]], grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.06440529972314835 
model_pd.l_d.mean(): -3.239306926727295 
model_pd.lagr.mean(): -3.1749017238616943 
model_pd.lambdas: dict_items([('pout', tensor([1.4479])), ('power', tensor([0.1580]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5282])), ('power', tensor([-15.5874]))])
epoch：784	 i:0 	 global-step:15680	 l-p:0.06440529972314835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[12.3279, 12.4133, 12.3447],
        [12.3279, 13.5641, 13.5670],
        [12.3279, 12.3281, 12.3279],
        [12.3279, 12.4689, 12.3658]], grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.06433617323637009 
model_pd.l_d.mean(): -3.227018356323242 
model_pd.lagr.mean(): -3.162682294845581 
model_pd.lambdas: dict_items([('pout', tensor([1.4473])), ('power', tensor([0.1572]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5335])), ('power', tensor([-15.5395]))])
epoch：785	 i:0 	 global-step:15700	 l-p:0.06433617323637009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[12.3781, 12.7827, 12.5883],
        [12.3781, 12.3781, 12.3781],
        [12.3781, 16.4614, 19.2695],
        [12.3781, 14.3291, 14.8226]], grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.06426730751991272 
model_pd.l_d.mean(): -3.2148003578186035 
model_pd.lagr.mean(): -3.1505329608917236 
model_pd.lambdas: dict_items([('pout', tensor([1.4468])), ('power', tensor([0.1564]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5388])), ('power', tensor([-15.4914]))])
epoch：786	 i:0 	 global-step:15720	 l-p:0.06426730751991272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[12.4285, 12.5709, 12.4668],
        [12.4285, 13.2057, 13.0266],
        [12.4285, 12.4285, 12.4285],
        [12.4285, 12.4295, 12.4285]], grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.06419868767261505 
model_pd.l_d.mean(): -3.2026519775390625 
model_pd.lagr.mean(): -3.138453245162964 
model_pd.lambdas: dict_items([('pout', tensor([1.4462])), ('power', tensor([0.1556]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5441])), ('power', tensor([-15.4430]))])
epoch：787	 i:0 	 global-step:15740	 l-p:0.06419868767261505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[12.4793, 12.4800, 12.4793],
        [12.4793, 15.6638, 17.3632],
        [12.4793, 12.4803, 12.4793],
        [12.4793, 14.9054, 15.8124]], grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.06413032859563828 
model_pd.l_d.mean(): -3.1905746459960938 
model_pd.lagr.mean(): -3.1264443397521973 
model_pd.lambdas: dict_items([('pout', tensor([1.4457])), ('power', tensor([0.1549]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5495])), ('power', tensor([-15.3942]))])
epoch：788	 i:0 	 global-step:15760	 l-p:0.06413032859563828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[12.5304, 13.9189, 13.9950],
        [12.5304, 15.9476, 17.9051],
        [12.5304, 14.9705, 15.8845],
        [12.5304, 12.5319, 12.5304]], grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.0640622079372406 
model_pd.l_d.mean(): -3.178567886352539 
model_pd.lagr.mean(): -3.1145057678222656 
model_pd.lambdas: dict_items([('pout', tensor([1.4451])), ('power', tensor([0.1541]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5548])), ('power', tensor([-15.3451]))])
epoch：789	 i:0 	 global-step:15780	 l-p:0.0640622079372406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[12.5818, 12.7294, 12.6220],
        [12.5818, 14.4602, 14.8750],
        [12.5818, 13.4094, 13.2372],
        [12.5818, 12.7315, 12.6229]], grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.06399434804916382 
model_pd.l_d.mean(): -3.1666319370269775 
model_pd.lagr.mean(): -3.102637529373169 
model_pd.lambdas: dict_items([('pout', tensor([1.4446])), ('power', tensor([0.1533]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5602])), ('power', tensor([-15.2957]))])
epoch：790	 i:0 	 global-step:15800	 l-p:0.06399434804916382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[12.6335, 12.6344, 12.6335],
        [12.6335, 13.9470, 13.9733],
        [12.6335, 12.6553, 12.6353],
        [12.6335, 12.6350, 12.6335]], grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.06392675638198853 
model_pd.l_d.mean(): -3.154766798019409 
model_pd.lagr.mean(): -3.0908401012420654 
model_pd.lambdas: dict_items([('pout', tensor([1.4440])), ('power', tensor([0.1526]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5656])), ('power', tensor([-15.2460]))])
epoch：791	 i:0 	 global-step:15820	 l-p:0.06392675638198853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[12.6855, 13.2413, 13.0307],
        [12.6855, 12.6865, 12.6855],
        [12.6855, 16.5794, 19.0824],
        [12.6855, 14.0052, 14.0318]], grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.06385941058397293 
model_pd.l_d.mean(): -3.142972469329834 
model_pd.lagr.mean(): -3.079113006591797 
model_pd.lambdas: dict_items([('pout', tensor([1.4434])), ('power', tensor([0.1518]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5710])), ('power', tensor([-15.1960]))])
epoch：792	 i:0 	 global-step:15840	 l-p:0.06385941058397293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[12.7378, 12.7378, 12.7378],
        [12.7378, 12.8844, 12.7772],
        [12.7378, 12.8166, 12.7522],
        [12.7378, 14.7535, 15.2643]], grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.06379231810569763 
model_pd.l_d.mean(): -3.1312499046325684 
model_pd.lagr.mean(): -3.067457675933838 
model_pd.lambdas: dict_items([('pout', tensor([1.4429])), ('power', tensor([0.1511]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5764])), ('power', tensor([-15.1457]))])
epoch：793	 i:0 	 global-step:15860	 l-p:0.06379231810569763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[12.7904, 15.2879, 16.2240],
        [12.7904, 12.7905, 12.7904],
        [12.7904, 12.9424, 12.8321],
        [12.7904, 14.0802, 14.0840]], grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.06372549384832382 
model_pd.l_d.mean(): -3.119598150253296 
model_pd.lagr.mean(): -3.055872678756714 
model_pd.lambdas: dict_items([('pout', tensor([1.4423])), ('power', tensor([0.1503]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5818])), ('power', tensor([-15.0950]))])
epoch：794	 i:0 	 global-step:15880	 l-p:0.06372549384832382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[12.8434, 14.8781, 15.3939],
        [12.8434, 12.8441, 12.8434],
        [12.8434, 14.7663, 15.1916],
        [12.8434, 12.8434, 12.8434]], grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.06365892291069031 
model_pd.l_d.mean(): -3.108018398284912 
model_pd.lagr.mean(): -3.0443594455718994 
model_pd.lambdas: dict_items([('pout', tensor([1.4417])), ('power', tensor([0.1495]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5872])), ('power', tensor([-15.0441]))])
epoch：795	 i:0 	 global-step:15900	 l-p:0.06365892291069031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[12.8966, 14.9409, 15.4593],
        [12.8966, 15.3965, 16.3217],
        [12.8966, 13.2957, 13.0963],
        [12.8966, 14.6946, 15.0189]], grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.06359261274337769 
model_pd.l_d.mean(): -3.0965099334716797 
model_pd.lagr.mean(): -3.0329172611236572 
model_pd.lambdas: dict_items([('pout', tensor([1.4411])), ('power', tensor([0.1488]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5926])), ('power', tensor([-14.9928]))])
epoch：796	 i:0 	 global-step:15920	 l-p:0.06359261274337769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[12.9502, 13.1030, 12.9919],
        [12.9502, 13.1052, 12.9929],
        [12.9502, 15.2587, 15.9999],
        [12.9502, 13.1044, 12.9925]], grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.06352655589580536 
model_pd.l_d.mean(): -3.085073709487915 
model_pd.lagr.mean(): -3.0215470790863037 
model_pd.lambdas: dict_items([('pout', tensor([1.4405])), ('power', tensor([0.1481]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5980])), ('power', tensor([-14.9412]))])
epoch：797	 i:0 	 global-step:15940	 l-p:0.06352655589580536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[13.0041, 16.5662, 18.6083],
        [13.0041, 14.9544, 15.3861],
        [13.0041, 13.0513, 13.0104],
        [13.0041, 13.0041, 13.0041]], grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.06346075981855392 
model_pd.l_d.mean(): -3.073709011077881 
model_pd.lagr.mean(): -3.0102481842041016 
model_pd.lambdas: dict_items([('pout', tensor([1.4399])), ('power', tensor([0.1473]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6035])), ('power', tensor([-14.8893]))])
epoch：798	 i:0 	 global-step:15960	 l-p:0.06346075981855392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[13.0583, 13.1395, 13.0732],
        [13.0583, 15.1317, 15.6578],
        [13.0583, 15.3886, 16.1371],
        [13.0583, 13.8454, 13.6490]], grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.06339523196220398 
model_pd.l_d.mean(): -3.0624165534973145 
model_pd.lagr.mean(): -2.999021291732788 
model_pd.lambdas: dict_items([('pout', tensor([1.4393])), ('power', tensor([0.1466]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6089])), ('power', tensor([-14.8370]))])
epoch：799	 i:0 	 global-step:15980	 l-p:0.06339523196220398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[13.1129, 17.1532, 19.7519],
        [13.1129, 13.2693, 13.1558],
        [13.1129, 13.1255, 13.1136],
        [13.1129, 15.1960, 15.7248]], grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.06332995742559433 
model_pd.l_d.mean(): -3.051196575164795 
model_pd.lagr.mean(): -2.9878666400909424 
model_pd.lambdas: dict_items([('pout', tensor([1.4387])), ('power', tensor([0.1458]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6144])), ('power', tensor([-14.7844]))])
epoch：800	 i:0 	 global-step:16000	 l-p:0.06332995742559433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[13.1677, 13.3250, 13.2109],
        [13.1677, 14.5451, 14.5736],
        [13.1677, 13.4644, 13.2892],
        [13.1677, 13.5766, 13.3724]], grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.06326495856046677 
model_pd.l_d.mean(): -3.040048837661743 
model_pd.lagr.mean(): -2.9767839908599854 
model_pd.lambdas: dict_items([('pout', tensor([1.4381])), ('power', tensor([0.1451]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6199])), ('power', tensor([-14.7315]))])
epoch：801	 i:0 	 global-step:16020	 l-p:0.06326495856046677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[13.2229, 13.3809, 13.2662],
        [13.2229, 13.6601, 13.4504],
        [13.2229, 13.2357, 13.2237],
        [13.2229, 15.5863, 16.3458]], grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.0632002130150795 
model_pd.l_d.mean(): -3.028973340988159 
model_pd.lagr.mean(): -2.965773105621338 
model_pd.lambdas: dict_items([('pout', tensor([1.4374])), ('power', tensor([0.1444]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6254])), ('power', tensor([-14.6783]))])
epoch：802	 i:0 	 global-step:16040	 l-p:0.0632002130150795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[13.2784, 13.2800, 13.2784],
        [13.2784, 13.2784, 13.2784],
        [13.2784, 14.1592, 13.9767],
        [13.2784, 14.6690, 14.6979]], grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.06313575059175491 
model_pd.l_d.mean(): -3.017970323562622 
model_pd.lagr.mean(): -2.954834461212158 
model_pd.lambdas: dict_items([('pout', tensor([1.4368])), ('power', tensor([0.1436]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6308])), ('power', tensor([-14.6248]))])
epoch：803	 i:0 	 global-step:16060	 l-p:0.06313575059175491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[13.3342, 13.3387, 13.3344],
        [13.3342, 13.3828, 13.3406],
        [13.3342, 13.3350, 13.3342],
        [13.3342, 13.3344, 13.3342]], grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.06307153403759003 
model_pd.l_d.mean(): -3.007040023803711 
model_pd.lagr.mean(): -2.9439685344696045 
model_pd.lambdas: dict_items([('pout', tensor([1.4362])), ('power', tensor([0.1429]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6363])), ('power', tensor([-14.5709]))])
epoch：804	 i:0 	 global-step:16080	 l-p:0.06307153403759003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[13.3903, 13.5515, 13.4347],
        [13.3903, 14.2797, 14.0955],
        [13.3903, 17.0705, 19.1815],
        [13.3903, 13.5507, 13.4343]], grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.06300760060548782 
model_pd.l_d.mean(): -2.996182680130005 
model_pd.lagr.mean(): -2.9331750869750977 
model_pd.lambdas: dict_items([('pout', tensor([1.4355])), ('power', tensor([0.1422]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6419])), ('power', tensor([-14.5167]))])
epoch：805	 i:0 	 global-step:16100	 l-p:0.06300760060548782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[13.4468, 14.6813, 14.6206],
        [13.4468, 15.8553, 16.6298],
        [13.4468, 13.7508, 13.5713],
        [13.4468, 14.8575, 14.8871]], grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.06294392794370651 
model_pd.l_d.mean(): -2.985398054122925 
model_pd.lagr.mean(): -2.9224541187286377 
model_pd.lambdas: dict_items([('pout', tensor([1.4349])), ('power', tensor([0.1414]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6474])), ('power', tensor([-14.4622]))])
epoch：806	 i:0 	 global-step:16120	 l-p:0.06294392794370651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[13.5036, 14.1925, 13.9706],
        [13.5036, 16.9818, 18.8413],
        [13.5036, 15.6570, 16.2044],
        [13.5036, 13.6606, 13.5459]], grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.0628805160522461 
model_pd.l_d.mean(): -2.974686622619629 
model_pd.lagr.mean(): -2.911806106567383 
model_pd.lambdas: dict_items([('pout', tensor([1.4342])), ('power', tensor([0.1407]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6529])), ('power', tensor([-14.4073]))])
epoch：807	 i:0 	 global-step:16140	 l-p:0.0628805160522461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[13.5607, 14.1612, 13.9341],
        [13.5607, 13.7185, 13.6032],
        [13.5607, 15.7243, 16.2745],
        [13.5607, 13.5789, 13.5620]], grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.06281737983226776 
model_pd.l_d.mean(): -2.964047908782959 
model_pd.lagr.mean(): -2.901230573654175 
model_pd.lambdas: dict_items([('pout', tensor([1.4336])), ('power', tensor([0.1400]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6584])), ('power', tensor([-14.3521]))])
epoch：808	 i:0 	 global-step:16160	 l-p:0.06281737983226776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[13.6181, 13.6227, 13.6183],
        [13.6181, 14.3137, 14.0897],
        [13.6181, 16.0611, 16.8471],
        [13.6181, 14.0705, 13.8536]], grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.06275451928377151 
model_pd.l_d.mean(): -2.9534826278686523 
model_pd.lagr.mean(): -2.890727996826172 
model_pd.lambdas: dict_items([('pout', tensor([1.4329])), ('power', tensor([0.1393]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6640])), ('power', tensor([-14.2966]))])
epoch：809	 i:0 	 global-step:16180	 l-p:0.06275451928377151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[13.6758, 13.6759, 13.6758],
        [13.6758, 14.8111, 14.6962],
        [13.6758, 17.2753, 19.2431],
        [13.6758, 18.2372, 21.3794]], grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.06269192695617676 
model_pd.l_d.mean(): -2.942990303039551 
model_pd.lagr.mean(): -2.880298376083374 
model_pd.lambdas: dict_items([('pout', tensor([1.4322])), ('power', tensor([0.1386]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6695])), ('power', tensor([-14.2408]))])
epoch：810	 i:0 	 global-step:16200	 l-p:0.06269192695617676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[13.7339, 14.9985, 14.9367],
        [13.7339, 14.1055, 13.9039],
        [13.7339, 14.8747, 14.7593],
        [13.7339, 15.9287, 16.4872]], grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.0626295953989029 
model_pd.l_d.mean(): -2.9325714111328125 
model_pd.lagr.mean(): -2.8699417114257812 
model_pd.lambdas: dict_items([('pout', tensor([1.4316])), ('power', tensor([0.1379]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6751])), ('power', tensor([-14.1846]))])
epoch：811	 i:0 	 global-step:16220	 l-p:0.0626295953989029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[13.7923, 14.1054, 13.9206],
        [13.7923, 17.5954, 19.7782],
        [13.7923, 15.1538, 15.1364],
        [13.7923, 13.7925, 13.7923]], grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.06256754696369171 
model_pd.l_d.mean(): -2.9222259521484375 
model_pd.lagr.mean(): -2.8596584796905518 
model_pd.lambdas: dict_items([('pout', tensor([1.4309])), ('power', tensor([0.1372]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6806])), ('power', tensor([-14.1281]))])
epoch：812	 i:0 	 global-step:16240	 l-p:0.06256754696369171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[13.8510, 17.4289, 19.3427],
        [13.8510, 13.9380, 13.8669],
        [13.8510, 14.2846, 14.0682],
        [13.8510, 13.8518, 13.8510]], grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.06250577419996262 
model_pd.l_d.mean(): -2.911953926086426 
model_pd.lagr.mean(): -2.8494482040405273 
model_pd.lambdas: dict_items([('pout', tensor([1.4302])), ('power', tensor([0.1365]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6862])), ('power', tensor([-14.0713]))])
epoch：813	 i:0 	 global-step:16260	 l-p:0.06250577419996262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[13.9100, 13.9101, 13.9100],
        [13.9100, 15.0675, 14.9506],
        [13.9100, 13.9118, 13.9101],
        [13.9100, 14.7950, 14.5925]], grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.06244426593184471 
model_pd.l_d.mean(): -2.9017553329467773 
model_pd.lagr.mean(): -2.839311122894287 
model_pd.lambdas: dict_items([('pout', tensor([1.4295])), ('power', tensor([0.1358]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6918])), ('power', tensor([-14.0142]))])
epoch：814	 i:0 	 global-step:16280	 l-p:0.06244426593184471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[13.9694, 14.2872, 14.0996],
        [13.9694, 17.6547, 19.6703],
        [13.9694, 14.0573, 13.9855],
        [13.9694, 13.9702, 13.9694]], grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.06238304078578949 
model_pd.l_d.mean(): -2.891630172729492 
model_pd.lagr.mean(): -2.829247236251831 
model_pd.lambdas: dict_items([('pout', tensor([1.4288])), ('power', tensor([0.1351]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6973])), ('power', tensor([-13.9567]))])
epoch：815	 i:0 	 global-step:16300	 l-p:0.06238304078578949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[14.0291, 14.6535, 14.4177],
        [14.0291, 15.4621, 15.4683],
        [14.0291, 17.9046, 20.1297],
        [14.0291, 14.7489, 14.5173]], grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.062322091311216354 
model_pd.l_d.mean(): -2.8815789222717285 
model_pd.lagr.mean(): -2.8192567825317383 
model_pd.lambdas: dict_items([('pout', tensor([1.4281])), ('power', tensor([0.1344]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7029])), ('power', tensor([-13.8989]))])
epoch：816	 i:0 	 global-step:16320	 l-p:0.062322091311216354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[14.0891, 14.1081, 14.0905],
        [14.0891, 15.3909, 15.3278],
        [14.0891, 14.2541, 14.1335],
        [14.0891, 16.8497, 17.8745]], grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.062261417508125305 
model_pd.l_d.mean(): -2.87160062789917 
model_pd.lagr.mean(): -2.8093392848968506 
model_pd.lambdas: dict_items([('pout', tensor([1.4274])), ('power', tensor([0.1337]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7085])), ('power', tensor([-13.8408]))])
epoch：817	 i:0 	 global-step:16340	 l-p:0.062261417508125305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[14.1494, 15.0967, 14.9012],
        [14.1494, 15.5964, 15.6028],
        [14.1494, 14.1495, 14.1494],
        [14.1494, 14.1494, 14.1494]], grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.06220102682709694 
model_pd.l_d.mean(): -2.861696481704712 
model_pd.lagr.mean(): -2.7994954586029053 
model_pd.lambdas: dict_items([('pout', tensor([1.4267])), ('power', tensor([0.1330]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7141])), ('power', tensor([-13.7823]))])
epoch：818	 i:0 	 global-step:16360	 l-p:0.06220102682709694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[14.2101, 14.2293, 14.2115],
        [14.2101, 16.3655, 16.8452],
        [14.2101, 14.2101, 14.2101],
        [14.2101, 14.5965, 14.3870]], grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.06214091181755066 
model_pd.l_d.mean(): -2.851865291595459 
model_pd.lagr.mean(): -2.789724349975586 
model_pd.lambdas: dict_items([('pout', tensor([1.4259])), ('power', tensor([0.1323]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7197])), ('power', tensor([-13.7235]))])
epoch：819	 i:0 	 global-step:16380	 l-p:0.06214091181755066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[14.2710, 16.2870, 16.6535],
        [14.2710, 15.7321, 15.7387],
        [14.2710, 14.5968, 14.4045],
        [14.2710, 14.9078, 14.6674]], grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.06208107993006706 
model_pd.l_d.mean(): -2.842108964920044 
model_pd.lagr.mean(): -2.7800278663635254 
model_pd.lambdas: dict_items([('pout', tensor([1.4252])), ('power', tensor([0.1316]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7253])), ('power', tensor([-13.6644]))])
epoch：820	 i:0 	 global-step:16400	 l-p:0.06208107993006706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[14.3323, 16.3580, 16.7264],
        [14.3323, 15.2936, 15.0953],
        [14.3323, 14.8121, 14.5823],
        [14.3323, 18.0483, 20.0373]], grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.06202153116464615 
model_pd.l_d.mean(): -2.832425594329834 
model_pd.lagr.mean(): -2.770404100418091 
model_pd.lambdas: dict_items([('pout', tensor([1.4245])), ('power', tensor([0.1309]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7309])), ('power', tensor([-13.6050]))])
epoch：821	 i:0 	 global-step:16420	 l-p:0.06202153116464615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[14.3939, 14.4080, 14.3948],
        [14.3939, 14.3957, 14.3939],
        [14.3939, 15.7277, 15.6634],
        [14.3939, 14.4849, 14.4106]], grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.06196225807070732 
model_pd.l_d.mean(): -2.822815418243408 
model_pd.lagr.mean(): -2.7608530521392822 
model_pd.lambdas: dict_items([('pout', tensor([1.4238])), ('power', tensor([0.1303]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7366])), ('power', tensor([-13.5452]))])
epoch：822	 i:0 	 global-step:16440	 l-p:0.06196225807070732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[14.4558, 14.5473, 14.4726],
        [14.4558, 14.4558, 14.4558],
        [14.4558, 14.5093, 14.4629],
        [14.4558, 14.4608, 14.4560]], grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.061903271824121475 
model_pd.l_d.mean(): -2.813279628753662 
model_pd.lagr.mean(): -2.7513763904571533 
model_pd.lambdas: dict_items([('pout', tensor([1.4230])), ('power', tensor([0.1296]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7422])), ('power', tensor([-13.4851]))])
epoch：823	 i:0 	 global-step:16460	 l-p:0.061903271824121475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[14.5181, 17.3964, 18.4799],
        [14.5181, 14.5181, 14.5181],
        [14.5181, 14.5199, 14.5181],
        [14.5181, 14.5188, 14.5181]], grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.06184457242488861 
model_pd.l_d.mean(): -2.8038170337677 
model_pd.lagr.mean(): -2.7419724464416504 
model_pd.lambdas: dict_items([('pout', tensor([1.4223])), ('power', tensor([0.1289]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7478])), ('power', tensor([-13.4247]))])
epoch：824	 i:0 	 global-step:16480	 l-p:0.06184457242488861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[14.5806, 16.0775, 16.0848],
        [14.5806, 14.7577, 14.6293],
        [14.5806, 19.4753, 22.8505],
        [14.5806, 16.6457, 17.0217]], grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.06178615614771843 
model_pd.l_d.mean(): -2.7944283485412598 
model_pd.lagr.mean(): -2.73264217376709 
model_pd.lambdas: dict_items([('pout', tensor([1.4215])), ('power', tensor([0.1282]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7534])), ('power', tensor([-13.3640]))])
epoch：825	 i:0 	 global-step:16500	 l-p:0.06178615614771843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[14.6435, 14.7363, 14.6605],
        [14.6435, 16.8727, 17.3697],
        [14.6435, 17.5462, 18.6373],
        [14.6435, 14.6435, 14.6435]], grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.06172801926732063 
model_pd.l_d.mean(): -2.7851133346557617 
model_pd.lagr.mean(): -2.7233853340148926 
model_pd.lambdas: dict_items([('pout', tensor([1.4208])), ('power', tensor([0.1276]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7591])), ('power', tensor([-13.3029]))])
epoch：826	 i:0 	 global-step:16520	 l-p:0.06172801926732063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[14.7067, 14.8864, 14.7563],
        [14.7067, 17.0764, 17.6814],
        [14.7067, 19.2929, 22.2485],
        [14.7067, 14.8000, 14.7238]], grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.06167018786072731 
model_pd.l_d.mean(): -2.775871753692627 
model_pd.lagr.mean(): -2.7142014503479004 
model_pd.lambdas: dict_items([('pout', tensor([1.4200])), ('power', tensor([0.1269]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7647])), ('power', tensor([-13.2415]))])
epoch：827	 i:0 	 global-step:16540	 l-p:0.06167018786072731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[14.7702, 14.7702, 14.7702],
        [14.7702, 14.7703, 14.7702],
        [14.7702, 15.1091, 14.9092],
        [14.7702, 15.5335, 15.2885]], grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.061612628400325775 
model_pd.l_d.mean(): -2.7667036056518555 
model_pd.lagr.mean(): -2.7050909996032715 
model_pd.lambdas: dict_items([('pout', tensor([1.4192])), ('power', tensor([0.1263]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7704])), ('power', tensor([-13.1798]))])
epoch：828	 i:0 	 global-step:16560	 l-p:0.061612628400325775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[14.8340, 15.0146, 14.8836],
        [14.8340, 16.3602, 16.3680],
        [14.8340, 15.0130, 14.8829],
        [14.8340, 14.8352, 14.8340]], grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.06155536323785782 
model_pd.l_d.mean(): -2.7576091289520264 
model_pd.lagr.mean(): -2.6960537433624268 
model_pd.lambdas: dict_items([('pout', tensor([1.4184])), ('power', tensor([0.1256]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7760])), ('power', tensor([-13.1178]))])
epoch：829	 i:0 	 global-step:16580	 l-p:0.06155536323785782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[14.8981, 14.8993, 14.8981],
        [14.8981, 19.0394, 21.4197],
        [14.8981, 17.5990, 18.4711],
        [14.8981, 17.3022, 17.9165]], grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.06149838864803314 
model_pd.l_d.mean(): -2.7485880851745605 
model_pd.lagr.mean(): -2.687089681625366 
model_pd.lambdas: dict_items([('pout', tensor([1.4177])), ('power', tensor([0.1250]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7817])), ('power', tensor([-13.0555]))])
epoch：830	 i:0 	 global-step:16600	 l-p:0.06149838864803314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[14.9626, 15.1433, 15.0120],
        [14.9626, 15.1459, 15.0131],
        [14.9626, 19.6365, 22.6494],
        [14.9626, 14.9626, 14.9626]], grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.061441708356142044 
model_pd.l_d.mean(): -2.739640712738037 
model_pd.lagr.mean(): -2.678199052810669 
model_pd.lambdas: dict_items([('pout', tensor([1.4169])), ('power', tensor([0.1243]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7873])), ('power', tensor([-12.9928]))])
epoch：831	 i:0 	 global-step:16620	 l-p:0.061441708356142044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[15.0273, 15.2106, 15.0777],
        [15.0273, 15.0292, 15.0273],
        [15.0273, 15.0273, 15.0273],
        [15.0273, 15.1350, 15.0485]], grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.06138532608747482 
model_pd.l_d.mean(): -2.7307662963867188 
model_pd.lagr.mean(): -2.6693809032440186 
model_pd.lambdas: dict_items([('pout', tensor([1.4161])), ('power', tensor([0.1237]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7930])), ('power', tensor([-12.9298]))])
epoch：832	 i:0 	 global-step:16640	 l-p:0.06138532608747482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[15.0923, 15.1485, 15.0998],
        [15.0923, 19.2930, 21.7081],
        [15.0923, 15.2775, 15.1434],
        [15.0923, 20.1756, 23.6826]], grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.061329226940870285 
model_pd.l_d.mean(): -2.7219648361206055 
model_pd.lagr.mean(): -2.6606357097625732 
model_pd.lambdas: dict_items([('pout', tensor([1.4153])), ('power', tensor([0.1230]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7986])), ('power', tensor([-12.8666]))])
epoch：833	 i:0 	 global-step:16660	 l-p:0.061329226940870285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[15.1577, 15.1784, 15.1592],
        [15.1577, 15.2142, 15.1652],
        [15.1577, 15.1577, 15.1577],
        [15.1577, 15.1577, 15.1577]], grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.061273425817489624 
model_pd.l_d.mean(): -2.7132368087768555 
model_pd.lagr.mean(): -2.651963472366333 
model_pd.lambdas: dict_items([('pout', tensor([1.4145])), ('power', tensor([0.1224]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8043])), ('power', tensor([-12.8030]))])
epoch：834	 i:0 	 global-step:16680	 l-p:0.061273425817489624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[15.2233, 15.3326, 15.2449],
        [15.2233, 19.4641, 21.9026],
        [15.2233, 15.2286, 15.2235],
        [15.2233, 15.2234, 15.2234]], grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.06121792271733284 
model_pd.l_d.mean(): -2.7045819759368896 
model_pd.lagr.mean(): -2.6433639526367188 
model_pd.lambdas: dict_items([('pout', tensor([1.4137])), ('power', tensor([0.1217]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8099])), ('power', tensor([-12.7391]))])
epoch：835	 i:0 	 global-step:16700	 l-p:0.06121792271733284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[15.2893, 15.7096, 15.4820],
        [15.2893, 15.3463, 15.2968],
        [15.2893, 15.4707, 15.3382],
        [15.2893, 16.8682, 16.8769]], grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.061162713915109634 
model_pd.l_d.mean(): -2.696000099182129 
model_pd.lagr.mean(): -2.6348373889923096 
model_pd.lambdas: dict_items([('pout', tensor([1.4129])), ('power', tensor([0.1211]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8156])), ('power', tensor([-12.6749]))])
epoch：836	 i:0 	 global-step:16720	 l-p:0.061162713915109634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[15.3555, 15.3565, 15.3556],
        [15.3555, 17.7059, 18.2314],
        [15.3555, 17.1037, 17.2047],
        [15.3555, 16.0478, 15.7870]], grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.061107806861400604 
model_pd.l_d.mean(): -2.687490463256836 
model_pd.lagr.mean(): -2.62638258934021 
model_pd.lambdas: dict_items([('pout', tensor([1.4120])), ('power', tensor([0.1205]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8213])), ('power', tensor([-12.6104]))])
epoch：837	 i:0 	 global-step:16740	 l-p:0.061107806861400604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[15.4221, 15.4274, 15.4223],
        [15.4221, 16.9665, 16.9491],
        [15.4221, 15.4221, 15.4221],
        [15.4221, 17.0164, 17.0253]], grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.06105319410562515 
model_pd.l_d.mean(): -2.679053783416748 
model_pd.lagr.mean(): -2.6180005073547363 
model_pd.lambdas: dict_items([('pout', tensor([1.4112])), ('power', tensor([0.1198]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8269])), ('power', tensor([-12.5455]))])
epoch：838	 i:0 	 global-step:16760	 l-p:0.06105319410562515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[15.4889, 15.5102, 15.4905],
        [15.4889, 15.4909, 15.4890],
        [15.4889, 17.0408, 17.0234],
        [15.4889, 15.4889, 15.4889]], grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.060998883098363876 
model_pd.l_d.mean(): -2.670689582824707 
model_pd.lagr.mean(): -2.6096906661987305 
model_pd.lambdas: dict_items([('pout', tensor([1.4104])), ('power', tensor([0.1192]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8326])), ('power', tensor([-12.4804]))])
epoch：839	 i:0 	 global-step:16780	 l-p:0.060998883098363876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[15.5561, 15.7451, 15.6078],
        [15.5561, 16.0513, 15.8046],
        [15.5561, 16.2586, 15.9940],
        [15.5561, 19.6232, 21.8036]], grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.060944877564907074 
model_pd.l_d.mean(): -2.662398338317871 
model_pd.lagr.mean(): -2.6014535427093506 
model_pd.lambdas: dict_items([('pout', tensor([1.4095])), ('power', tensor([0.1186]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8383])), ('power', tensor([-12.4150]))])
epoch：840	 i:0 	 global-step:16800	 l-p:0.060944877564907074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[15.6235, 15.7235, 15.6418],
        [15.6235, 20.5239, 23.6849],
        [15.6235, 18.7422, 19.9167],
        [15.6235, 17.2411, 17.2504]], grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 0.06089117005467415 
model_pd.l_d.mean(): -2.6541786193847656 
model_pd.lagr.mean(): -2.593287467956543 
model_pd.lambdas: dict_items([('pout', tensor([1.4087])), ('power', tensor([0.1180]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8439])), ('power', tensor([-12.3493]))])
epoch：841	 i:0 	 global-step:16820	 l-p:0.06089117005467415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[15.6912, 15.6912, 15.6912],
        [15.6912, 20.6148, 23.7910],
        [15.6912, 18.8023, 19.9611],
        [15.6912, 15.6913, 15.6912]], grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.0608377642929554 
model_pd.l_d.mean(): -2.646031379699707 
model_pd.lagr.mean(): -2.585193634033203 
model_pd.lambdas: dict_items([('pout', tensor([1.4078])), ('power', tensor([0.1174]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8496])), ('power', tensor([-12.2833]))])
epoch：842	 i:0 	 global-step:16840	 l-p:0.0608377642929554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[15.7592, 15.7592, 15.7592],
        [15.7592, 15.7592, 15.7592],
        [15.7592, 16.7787, 16.5470],
        [15.7592, 15.8182, 15.7670]], grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.06078465282917023 
model_pd.l_d.mean(): -2.637956142425537 
model_pd.lagr.mean(): -2.577171564102173 
model_pd.lambdas: dict_items([('pout', tensor([1.4070])), ('power', tensor([0.1168]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8553])), ('power', tensor([-12.2170]))])
epoch：843	 i:0 	 global-step:16860	 l-p:0.06078465282917023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[15.8274, 16.9029, 16.6824],
        [15.8274, 16.8074, 16.5652],
        [15.8274, 17.4687, 17.4784],
        [15.8274, 15.9417, 15.8500]], grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.060731858015060425 
model_pd.l_d.mean(): -2.629952907562256 
model_pd.lagr.mean(): -2.569221019744873 
model_pd.lambdas: dict_items([('pout', tensor([1.4061])), ('power', tensor([0.1162]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8609])), ('power', tensor([-12.1504]))])
epoch：844	 i:0 	 global-step:16880	 l-p:0.060731858015060425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[15.8960, 15.9980, 15.9147],
        [15.8960, 16.7256, 16.4599],
        [15.8960, 15.9556, 15.9039],
        [15.8960, 15.9015, 15.8962]], grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.060679372400045395 
model_pd.l_d.mean(): -2.622020721435547 
model_pd.lagr.mean(): -2.5613412857055664 
model_pd.lambdas: dict_items([('pout', tensor([1.4053])), ('power', tensor([0.1155]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8666])), ('power', tensor([-12.0836]))])
epoch：845	 i:0 	 global-step:16900	 l-p:0.060679372400045395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[15.9648, 16.0673, 15.9836],
        [15.9648, 19.1623, 20.3694],
        [15.9648, 21.3697, 25.1014],
        [15.9648, 18.4189, 18.9687]], grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.06062719225883484 
model_pd.l_d.mean(): -2.6141600608825684 
model_pd.lagr.mean(): -2.553532838821411 
model_pd.lambdas: dict_items([('pout', tensor([1.4044])), ('power', tensor([0.1149]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8722])), ('power', tensor([-12.0164]))])
epoch：846	 i:0 	 global-step:16920	 l-p:0.06062719225883484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[16.0339, 18.4998, 19.0523],
        [16.0339, 16.0941, 16.0418],
        [16.0339, 16.2324, 16.0887],
        [16.0339, 17.6991, 17.7092]], grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.06057531386613846 
model_pd.l_d.mean(): -2.606370687484741 
model_pd.lagr.mean(): -2.545795440673828 
model_pd.lambdas: dict_items([('pout', tensor([1.4035])), ('power', tensor([0.1143]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8779])), ('power', tensor([-11.9490]))])
epoch：847	 i:0 	 global-step:16940	 l-p:0.06057531386613846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[16.1032, 21.1681, 24.4367],
        [16.1032, 16.2999, 16.1570],
        [16.1032, 19.3046, 20.4979],
        [16.1032, 16.2068, 16.1222]], grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.06052374467253685 
model_pd.l_d.mean(): -2.5986523628234863 
model_pd.lagr.mean(): -2.538128614425659 
model_pd.lambdas: dict_items([('pout', tensor([1.4026])), ('power', tensor([0.1138]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8835])), ('power', tensor([-11.8813]))])
epoch：848	 i:0 	 global-step:16960	 l-p:0.06052374467253685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[16.1728, 16.2019, 16.1753],
        [16.1728, 17.0188, 16.7480],
        [16.1728, 16.1731, 16.1728],
        [16.1728, 16.7235, 16.4603]], grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.060472484678030014 
model_pd.l_d.mean(): -2.5910046100616455 
model_pd.lagr.mean(): -2.530532121658325 
model_pd.lambdas: dict_items([('pout', tensor([1.4017])), ('power', tensor([0.1132]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8892])), ('power', tensor([-11.8133]))])
epoch：849	 i:0 	 global-step:16980	 l-p:0.060472484678030014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[16.2427, 16.4431, 16.2979],
        [16.2427, 16.2430, 16.2427],
        [16.2427, 16.3039, 16.2508],
        [16.2427, 18.8888, 19.5676]], grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.06042153760790825 
model_pd.l_d.mean(): -2.5834274291992188 
model_pd.lagr.mean(): -2.523005962371826 
model_pd.lambdas: dict_items([('pout', tensor([1.4008])), ('power', tensor([0.1126]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8948])), ('power', tensor([-11.7451]))])
epoch：850	 i:0 	 global-step:17000	 l-p:0.06042153760790825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[16.3129, 17.3267, 17.0764],
        [16.3129, 17.6981, 17.5610],
        [16.3129, 19.5601, 20.7709],
        [16.3129, 18.6529, 19.0823]], grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.060370903462171555 
model_pd.l_d.mean(): -2.575920820236206 
model_pd.lagr.mean(): -2.515549898147583 
model_pd.lambdas: dict_items([('pout', tensor([1.3999])), ('power', tensor([0.1120]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9005])), ('power', tensor([-11.6766]))])
epoch：851	 i:0 	 global-step:17020	 l-p:0.060370903462171555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[16.3832, 17.0454, 16.7675],
        [16.3832, 19.6459, 20.8626],
        [16.3832, 16.9085, 16.6471],
        [16.3832, 18.7345, 19.1660]], grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.06032058596611023 
model_pd.l_d.mean(): -2.568483829498291 
model_pd.lagr.mean(): -2.5081632137298584 
model_pd.lambdas: dict_items([('pout', tensor([1.3990])), ('power', tensor([0.1114]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9061])), ('power', tensor([-11.6078]))])
epoch：852	 i:0 	 global-step:17040	 l-p:0.06032058596611023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[16.4539, 19.7320, 20.9546],
        [16.4539, 16.4539, 16.4539],
        [16.4539, 16.6511, 16.5071],
        [16.4539, 18.8163, 19.2501]], grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.06027057766914368 
model_pd.l_d.mean(): -2.561116933822632 
model_pd.lagr.mean(): -2.5008463859558105 
model_pd.lambdas: dict_items([('pout', tensor([1.3981])), ('power', tensor([0.1108]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9117])), ('power', tensor([-11.5388]))])
epoch：853	 i:0 	 global-step:17060	 l-p:0.06027057766914368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[16.5247, 16.5257, 16.5248],
        [16.5247, 20.8700, 23.2021],
        [16.5247, 17.6535, 17.4226],
        [16.5247, 22.1361, 26.0122]], grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.060220882296562195 
model_pd.l_d.mean(): -2.553819179534912 
model_pd.lagr.mean(): -2.493598222732544 
model_pd.lambdas: dict_items([('pout', tensor([1.3972])), ('power', tensor([0.1103]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9174])), ('power', tensor([-11.4696]))])
epoch：854	 i:0 	 global-step:17080	 l-p:0.060220882296562195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[16.5958, 19.9289, 21.1864],
        [16.5958, 17.0572, 16.8076],
        [16.5958, 16.5959, 16.5958],
        [16.5958, 19.9051, 21.1396]], grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.06017150357365608 
model_pd.l_d.mean(): -2.546590805053711 
model_pd.lagr.mean(): -2.486419200897217 
model_pd.lambdas: dict_items([('pout', tensor([1.3963])), ('power', tensor([0.1097]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9230])), ('power', tensor([-11.4000]))])
epoch：855	 i:0 	 global-step:17100	 l-p:0.06017150357365608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[16.6672, 16.6839, 16.6682],
        [16.6672, 20.0198, 21.2869],
        [16.6672, 21.0533, 23.4078],
        [16.6672, 17.2369, 16.9648]], grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.06012243777513504 
model_pd.l_d.mean(): -2.539431571960449 
model_pd.lagr.mean(): -2.47930908203125 
model_pd.lambdas: dict_items([('pout', tensor([1.3954])), ('power', tensor([0.1091]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9286])), ('power', tensor([-11.3303]))])
epoch：856	 i:0 	 global-step:17120	 l-p:0.06012243777513504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[16.7388, 20.1034, 21.3730],
        [16.7388, 18.4857, 18.4972],
        [16.7388, 16.9461, 16.7959],
        [16.7388, 17.7823, 17.5250]], grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.060073696076869965 
model_pd.l_d.mean(): -2.5323407649993896 
model_pd.lagr.mean(): -2.4722671508789062 
model_pd.lambdas: dict_items([('pout', tensor([1.3944])), ('power', tensor([0.1086]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9342])), ('power', tensor([-11.2603]))])
epoch：857	 i:0 	 global-step:17140	 l-p:0.060073696076869965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[16.8106, 17.2787, 17.0254],
        [16.8106, 17.8591, 17.6006],
        [16.8106, 18.3979, 18.3245],
        [16.8106, 16.8165, 16.8108]], grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.06002526357769966 
model_pd.l_d.mean(): -2.5253186225891113 
model_pd.lagr.mean(): -2.4652934074401855 
model_pd.lambdas: dict_items([('pout', tensor([1.3935])), ('power', tensor([0.1080]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9398])), ('power', tensor([-11.1901]))])
epoch：858	 i:0 	 global-step:17160	 l-p:0.06002526357769966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[16.8826, 18.7041, 18.7478],
        [16.8826, 18.8254, 18.9399],
        [16.8826, 16.8847, 16.8826],
        [16.8826, 16.8995, 16.8836]], grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.059977155178785324 
model_pd.l_d.mean(): -2.5183639526367188 
model_pd.lagr.mean(): -2.4583868980407715 
model_pd.lambdas: dict_items([('pout', tensor([1.3925])), ('power', tensor([0.1074]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9454])), ('power', tensor([-11.1196]))])
epoch：859	 i:0 	 global-step:17180	 l-p:0.059977155178785324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[16.9548, 17.4275, 17.1718],
        [16.9548, 16.9557, 16.9548],
        [16.9548, 16.9562, 16.9548],
        [16.9548, 20.3427, 21.6073]], grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.05992937088012695 
model_pd.l_d.mean(): -2.51147723197937 
model_pd.lagr.mean(): -2.451547861099243 
model_pd.lambdas: dict_items([('pout', tensor([1.3916])), ('power', tensor([0.1069]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9510])), ('power', tensor([-11.0489]))])
epoch：860	 i:0 	 global-step:17200	 l-p:0.05992937088012695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[17.0273, 20.4555, 21.7498],
        [17.0273, 18.1393, 17.8875],
        [17.0273, 17.0273, 17.0273],
        [17.0273, 18.1945, 17.9561]], grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.059881895780563354 
model_pd.l_d.mean(): -2.5046579837799072 
model_pd.lagr.mean(): -2.4447760581970215 
model_pd.lambdas: dict_items([('pout', tensor([1.3906])), ('power', tensor([0.1063]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9566])), ('power', tensor([-10.9780]))])
epoch：861	 i:0 	 global-step:17220	 l-p:0.059881895780563354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[17.0999, 22.9234, 26.9479],
        [17.0999, 20.5481, 21.8523],
        [17.0999, 21.6104, 24.0326],
        [17.0999, 20.2449, 21.2652]], grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.059834763407707214 
model_pd.l_d.mean(): -2.497905731201172 
model_pd.lagr.mean(): -2.4380710124969482 
model_pd.lambdas: dict_items([('pout', tensor([1.3897])), ('power', tensor([0.1058]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9622])), ('power', tensor([-10.9069]))])
epoch：862	 i:0 	 global-step:17240	 l-p:0.059834763407707214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[17.1728, 17.3874, 17.2321],
        [17.1728, 17.2842, 17.1932],
        [17.1728, 17.1749, 17.1728],
        [17.1728, 17.8710, 17.5782]], grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.05978793278336525 
model_pd.l_d.mean(): -2.491219997406006 
model_pd.lagr.mean(): -2.431432008743286 
model_pd.lambdas: dict_items([('pout', tensor([1.3887])), ('power', tensor([0.1053]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9677])), ('power', tensor([-10.8356]))])
epoch：863	 i:0 	 global-step:17260	 l-p:0.05978793278336525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[17.2458, 17.2458, 17.2458],
        [17.2458, 17.4584, 17.3040],
        [17.2458, 17.9473, 17.6532],
        [17.2458, 19.1108, 19.1560]], grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.05974142998456955 
model_pd.l_d.mean(): -2.4846010208129883 
model_pd.lagr.mean(): -2.4248595237731934 
model_pd.lambdas: dict_items([('pout', tensor([1.3877])), ('power', tensor([0.1047]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9733])), ('power', tensor([-10.7641]))])
epoch：864	 i:0 	 global-step:17280	 l-p:0.05974142998456955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[17.3190, 17.3251, 17.3192],
        [17.3190, 17.3212, 17.3191],
        [17.3190, 17.3190, 17.3190],
        [17.3190, 17.8782, 17.6001]], grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.059695255011320114 
model_pd.l_d.mean(): -2.4780473709106445 
model_pd.lagr.mean(): -2.4183521270751953 
model_pd.lambdas: dict_items([('pout', tensor([1.3867])), ('power', tensor([0.1042]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9788])), ('power', tensor([-10.6924]))])
epoch：865	 i:0 	 global-step:17300	 l-p:0.059695255011320114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[17.3924, 17.3924, 17.3924],
        [17.3924, 18.8802, 18.7341],
        [17.3924, 17.9901, 17.7048],
        [17.3924, 19.2750, 19.3208]], grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.059649400413036346 
model_pd.l_d.mean(): -2.471559524536133 
model_pd.lagr.mean(): -2.411910057067871 
model_pd.lambdas: dict_items([('pout', tensor([1.3858])), ('power', tensor([0.1037]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9844])), ('power', tensor([-10.6205]))])
epoch：866	 i:0 	 global-step:17320	 l-p:0.059649400413036346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[17.4660, 18.0305, 17.7498],
        [17.4660, 17.4663, 17.4660],
        [17.4660, 22.9983, 26.5726],
        [17.4660, 17.4674, 17.4660]], grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.05960388109087944 
model_pd.l_d.mean(): -2.4651365280151367 
model_pd.lagr.mean(): -2.4055325984954834 
model_pd.lambdas: dict_items([('pout', tensor([1.3848])), ('power', tensor([0.1031]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9899])), ('power', tensor([-10.5484]))])
epoch：867	 i:0 	 global-step:17340	 l-p:0.05960388109087944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[17.5397, 17.5412, 17.5398],
        [17.5397, 21.0558, 22.3695],
        [17.5397, 17.5643, 17.5415],
        [17.5397, 17.5398, 17.5397]], grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.0595586858689785 
model_pd.l_d.mean(): -2.4587783813476562 
model_pd.lagr.mean(): -2.3992197513580322 
model_pd.lambdas: dict_items([('pout', tensor([1.3838])), ('power', tensor([0.1026]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9954])), ('power', tensor([-10.4761]))])
epoch：868	 i:0 	 global-step:17360	 l-p:0.0595586858689785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[17.6136, 17.6159, 17.6137],
        [17.6136, 17.7428, 17.6392],
        [17.6136, 17.6314, 17.6147],
        [17.6136, 22.5863, 25.4524]], grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.05951381102204323 
model_pd.l_d.mean(): -2.4524848461151123 
model_pd.lagr.mean(): -2.3929710388183594 
model_pd.lambdas: dict_items([('pout', tensor([1.3828])), ('power', tensor([0.1021]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0009])), ('power', tensor([-10.4037]))])
epoch：869	 i:0 	 global-step:17380	 l-p:0.05951381102204323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[17.6877, 19.3673, 19.2906],
        [17.6877, 17.6900, 17.6877],
        [17.6877, 18.1834, 17.9154],
        [17.6877, 20.5941, 21.3424]], grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.05946926772594452 
model_pd.l_d.mean(): -2.4462552070617676 
model_pd.lagr.mean(): -2.3867859840393066 
model_pd.lambdas: dict_items([('pout', tensor([1.3818])), ('power', tensor([0.1016]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0064])), ('power', tensor([-10.3311]))])
epoch：870	 i:0 	 global-step:17400	 l-p:0.05946926772594452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[17.7619, 19.6887, 19.7361],
        [17.7619, 17.7629, 17.7619],
        [17.7619, 17.7630, 17.7619],
        [17.7619, 17.7634, 17.7619]], grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.059425052255392075 
model_pd.l_d.mean(): -2.4400887489318848 
model_pd.lagr.mean(): -2.3806636333465576 
model_pd.lambdas: dict_items([('pout', tensor([1.3808])), ('power', tensor([0.1011]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0119])), ('power', tensor([-10.2583]))])
epoch：871	 i:0 	 global-step:17420	 l-p:0.059425052255392075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[17.8363, 17.8363, 17.8363],
        [17.8363, 17.8363, 17.8363],
        [17.8363, 21.4472, 22.8144],
        [17.8363, 19.0074, 18.7428]], grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.05938117206096649 
model_pd.l_d.mean(): -2.433985710144043 
model_pd.lagr.mean(): -2.3746044635772705 
model_pd.lambdas: dict_items([('pout', tensor([1.3797])), ('power', tensor([0.1005]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0174])), ('power', tensor([-10.1854]))])
epoch：872	 i:0 	 global-step:17440	 l-p:0.05938117206096649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[17.9107, 18.3326, 18.0842],
        [17.9107, 17.9111, 17.9108],
        [17.9107, 19.9848, 20.1084],
        [17.9107, 17.9108, 17.9107]], grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.05933761224150658 
model_pd.l_d.mean(): -2.4279446601867676 
model_pd.lagr.mean(): -2.3686070442199707 
model_pd.lambdas: dict_items([('pout', tensor([1.3787])), ('power', tensor([0.1000]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0228])), ('power', tensor([-10.1124]))])
epoch：873	 i:0 	 global-step:17460	 l-p:0.05933761224150658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[17.9854, 20.5914, 21.0724],
        [17.9854, 18.0106, 17.9872],
        [17.9854, 20.9454, 21.7081],
        [17.9854, 17.9865, 17.9854]], grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.059294380247592926 
model_pd.l_d.mean(): -2.421966075897217 
model_pd.lagr.mean(): -2.3626716136932373 
model_pd.lambdas: dict_items([('pout', tensor([1.3777])), ('power', tensor([0.0995]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0282])), ('power', tensor([-10.0392]))])
epoch：874	 i:0 	 global-step:17480	 l-p:0.059294380247592926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[18.0601, 18.1781, 18.0818],
        [18.0601, 18.2861, 18.1224],
        [18.0601, 19.2475, 18.9794],
        [18.0601, 19.1960, 18.9168]], grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): 0.05925149470567703 
model_pd.l_d.mean(): -2.416049003601074 
model_pd.lagr.mean(): -2.356797456741333 
model_pd.lambdas: dict_items([('pout', tensor([1.3767])), ('power', tensor([0.0990]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0337])), ('power', tensor([-9.9659]))])
epoch：875	 i:0 	 global-step:17500	 l-p:0.05925149470567703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[18.1349, 18.1414, 18.1352],
        [18.1349, 20.1065, 20.1554],
        [18.1349, 18.1349, 18.1349],
        [18.1349, 18.3552, 18.1945]], grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.059208933264017105 
model_pd.l_d.mean(): -2.41019344329834 
model_pd.lagr.mean(): -2.350984573364258 
model_pd.lambdas: dict_items([('pout', tensor([1.3756])), ('power', tensor([0.0985]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0391])), ('power', tensor([-9.8925]))])
epoch：876	 i:0 	 global-step:17520	 l-p:0.059208933264017105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[18.2099, 18.2099, 18.2099],
        [18.2099, 18.2432, 18.2127],
        [18.2099, 19.3562, 19.0746],
        [18.2099, 18.9555, 18.6432]], grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.05916669964790344 
model_pd.l_d.mean(): -2.404397964477539 
model_pd.lagr.mean(): -2.345231294631958 
model_pd.lambdas: dict_items([('pout', tensor([1.3746])), ('power', tensor([0.0981]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0445])), ('power', tensor([-9.8189]))])
epoch：877	 i:0 	 global-step:17540	 l-p:0.05916669964790344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[18.2849, 19.0339, 18.7203],
        [18.2849, 20.4068, 20.5338],
        [18.2849, 18.2864, 18.2849],
        [18.2849, 21.6693, 22.7695]], grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.05912480503320694 
model_pd.l_d.mean(): -2.398663282394409 
model_pd.lagr.mean(): -2.33953857421875 
model_pd.lambdas: dict_items([('pout', tensor([1.3735])), ('power', tensor([0.0976]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0499])), ('power', tensor([-9.7453]))])
epoch：878	 i:0 	 global-step:17560	 l-p:0.05912480503320694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[18.3600, 18.9570, 18.6604],
        [18.3600, 18.3624, 18.3601],
        [18.3600, 18.3612, 18.3601],
        [18.3600, 19.9399, 19.7856]], grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.059083241969347 
model_pd.l_d.mean(): -2.3929882049560547 
model_pd.lagr.mean(): -2.333904981613159 
model_pd.lambdas: dict_items([('pout', tensor([1.3725])), ('power', tensor([0.0971]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0552])), ('power', tensor([-9.6715]))])
epoch：879	 i:0 	 global-step:17580	 l-p:0.059083241969347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[18.4353, 18.5712, 18.4621],
        [18.4353, 20.4428, 20.4930],
        [18.4353, 18.4376, 18.4353],
        [18.4353, 23.4284, 26.1728]], grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.05904201418161392 
model_pd.l_d.mean(): -2.3873722553253174 
model_pd.lagr.mean(): -2.3283302783966064 
model_pd.lambdas: dict_items([('pout', tensor([1.3714])), ('power', tensor([0.0966]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0606])), ('power', tensor([-9.5977]))])
epoch：880	 i:0 	 global-step:17600	 l-p:0.05904201418161392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[18.5105, 18.7408, 18.5737],
        [18.5105, 18.5121, 18.5106],
        [18.5105, 18.5815, 18.5199],
        [18.5105, 24.8546, 29.2434]], grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.059001125395298004 
model_pd.l_d.mean(): -2.381814956665039 
model_pd.lagr.mean(): -2.3228137493133545 
model_pd.lambdas: dict_items([('pout', tensor([1.3703])), ('power', tensor([0.0961]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0659])), ('power', tensor([-9.5237]))])
epoch：881	 i:0 	 global-step:17620	 l-p:0.059001125395298004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[18.5859, 19.8118, 19.5353],
        [18.5859, 23.6232, 26.3923],
        [18.5859, 19.4437, 19.1220],
        [18.5859, 18.8205, 18.6508]], grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.05896057188510895 
model_pd.l_d.mean(): -2.3763155937194824 
model_pd.lagr.mean(): -2.317354917526245 
model_pd.lambdas: dict_items([('pout', tensor([1.3693])), ('power', tensor([0.0957]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0712])), ('power', tensor([-9.4498]))])
epoch：882	 i:0 	 global-step:17640	 l-p:0.05896057188510895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[18.6613, 18.6679, 18.6615],
        [18.6613, 18.6613, 18.6613],
        [18.6613, 24.6039, 28.4467],
        [18.6613, 18.6613, 18.6613]], grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.058920346200466156 
model_pd.l_d.mean(): -2.3708744049072266 
model_pd.lagr.mean(): -2.3119540214538574 
model_pd.lambdas: dict_items([('pout', tensor([1.3682])), ('power', tensor([0.0952]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0765])), ('power', tensor([-9.3757]))])
epoch：883	 i:0 	 global-step:17660	 l-p:0.058920346200466156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[18.7367, 18.9701, 18.8007],
        [18.7367, 18.7391, 18.7368],
        [18.7367, 22.5468, 23.9913],
        [18.7367, 20.3524, 20.1950]], grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.058880459517240524 
model_pd.l_d.mean(): -2.3654897212982178 
model_pd.lagr.mean(): -2.3066091537475586 
model_pd.lambdas: dict_items([('pout', tensor([1.3671])), ('power', tensor([0.0947]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0818])), ('power', tensor([-9.3016]))])
epoch：884	 i:0 	 global-step:17680	 l-p:0.058880459517240524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[18.8122, 18.8387, 18.8141],
        [18.8122, 22.6347, 24.0815],
        [18.8122, 21.5499, 22.0565],
        [18.8122, 19.2579, 18.9956]], grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.05884091556072235 
model_pd.l_d.mean(): -2.360161781311035 
model_pd.lagr.mean(): -2.301320791244507 
model_pd.lambdas: dict_items([('pout', tensor([1.3660])), ('power', tensor([0.0943]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0871])), ('power', tensor([-9.2274]))])
epoch：885	 i:0 	 global-step:17700	 l-p:0.05884091556072235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[18.8877, 19.0274, 18.9153],
        [18.8877, 22.3939, 23.5349],
        [18.8877, 18.8893, 18.8877],
        [18.8877, 19.4212, 19.1330]], grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.05880169942975044 
model_pd.l_d.mean(): -2.3548898696899414 
model_pd.lagr.mean(): -2.296088218688965 
model_pd.lambdas: dict_items([('pout', tensor([1.3649])), ('power', tensor([0.0938]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0923])), ('power', tensor([-9.1532]))])
epoch：886	 i:0 	 global-step:17720	 l-p:0.05880169942975044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[18.9632, 18.9636, 18.9632],
        [18.9632, 18.9632, 18.9632],
        [18.9632, 19.4991, 19.2096],
        [18.9632, 19.5822, 19.2748]], grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.058762822300195694 
model_pd.l_d.mean(): -2.349673271179199 
model_pd.lagr.mean(): -2.290910482406616 
model_pd.lambdas: dict_items([('pout', tensor([1.3638])), ('power', tensor([0.0933]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0975])), ('power', tensor([-9.0790]))])
epoch：887	 i:0 	 global-step:17740	 l-p:0.058762822300195694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[19.0388, 25.1110, 29.0388],
        [19.0388, 19.1120, 19.0485],
        [19.0388, 22.8837, 24.3233],
        [19.0388, 22.5756, 23.7268]], grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.058724287897348404 
model_pd.l_d.mean(): -2.3445119857788086 
model_pd.lagr.mean(): -2.285787582397461 
model_pd.lambdas: dict_items([('pout', tensor([1.3627])), ('power', tensor([0.0929]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1027])), ('power', tensor([-9.0047]))])
epoch：888	 i:0 	 global-step:17760	 l-p:0.058724287897348404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[19.1143, 19.1143, 19.1143],
        [19.1143, 19.3564, 19.1813],
        [19.1143, 22.9758, 24.4218],
        [19.1143, 19.1143, 19.1143]], grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.058686092495918274 
model_pd.l_d.mean(): -2.339404582977295 
model_pd.lagr.mean(): -2.2807185649871826 
model_pd.lambdas: dict_items([('pout', tensor([1.3616])), ('power', tensor([0.0924]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1079])), ('power', tensor([-8.9305]))])
epoch：889	 i:0 	 global-step:17780	 l-p:0.058686092495918274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[19.1898, 19.4330, 19.2571],
        [19.1898, 21.9877, 22.5061],
        [19.1898, 19.2094, 19.1910],
        [19.1898, 19.7329, 19.4396]], grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.05864822864532471 
model_pd.l_d.mean(): -2.3343505859375 
model_pd.lagr.mean(): -2.275702476501465 
model_pd.lambdas: dict_items([('pout', tensor([1.3605])), ('power', tensor([0.0920]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1131])), ('power', tensor([-8.8562]))])
epoch：890	 i:0 	 global-step:17800	 l-p:0.05864822864532471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[19.2653, 19.8953, 19.5825],
        [19.2653, 20.1581, 19.8236],
        [19.2653, 19.3923, 19.2887],
        [19.2653, 24.3985, 27.1606]], grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.0586107075214386 
model_pd.l_d.mean(): -2.329349994659424 
model_pd.lagr.mean(): -2.2707393169403076 
model_pd.lambdas: dict_items([('pout', tensor([1.3594])), ('power', tensor([0.0916]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1182])), ('power', tensor([-8.7820]))])
epoch：891	 i:0 	 global-step:17820	 l-p:0.0586107075214386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[19.3408, 20.5664, 20.2660],
        [19.3408, 19.3418, 19.3408],
        [19.3408, 20.3738, 20.0449],
        [19.3408, 25.5168, 29.5125]], grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.058573536574840546 
model_pd.l_d.mean(): -2.32440185546875 
model_pd.lagr.mean(): -2.2658283710479736 
model_pd.lambdas: dict_items([('pout', tensor([1.3583])), ('power', tensor([0.0911]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1233])), ('power', tensor([-8.7078]))])
epoch：892	 i:0 	 global-step:17840	 l-p:0.058573536574840546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[19.4162, 23.3723, 24.8709],
        [19.4162, 21.0967, 20.9336],
        [19.4162, 22.6346, 23.4663],
        [19.4162, 23.0294, 24.2062]], grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.05853668227791786 
model_pd.l_d.mean(): -2.3195056915283203 
model_pd.lagr.mean(): -2.2609689235687256 
model_pd.lambdas: dict_items([('pout', tensor([1.3571])), ('power', tensor([0.0907]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1284])), ('power', tensor([-8.6335]))])
epoch：893	 i:0 	 global-step:17860	 l-p:0.05853668227791786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[19.4916, 19.4916, 19.4916],
        [19.4916, 19.4933, 19.4917],
        [19.4916, 23.4688, 24.9781],
        [19.4916, 21.5584, 21.5756]], grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.05850018188357353 
model_pd.l_d.mean(): -2.3146607875823975 
model_pd.lagr.mean(): -2.2561604976654053 
model_pd.lambdas: dict_items([('pout', tensor([1.3560])), ('power', tensor([0.0903]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1335])), ('power', tensor([-8.5594]))])
epoch：894	 i:0 	 global-step:17880	 l-p:0.05850018188357353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[19.5670, 19.5740, 19.5672],
        [19.5670, 22.4250, 22.9551],
        [19.5670, 20.6133, 20.2803],
        [19.5670, 19.8120, 19.6342]], grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.05846402049064636 
model_pd.l_d.mean(): -2.309866428375244 
model_pd.lagr.mean(): -2.2514023780822754 
model_pd.lambdas: dict_items([('pout', tensor([1.3549])), ('power', tensor([0.0898]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1386])), ('power', tensor([-8.4853]))])
epoch：895	 i:0 	 global-step:17900	 l-p:0.05846402049064636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[19.6422, 19.6448, 19.6423],
        [19.6422, 20.2859, 19.9664],
        [19.6422, 20.3269, 20.0007],
        [19.6422, 19.7181, 19.6523]], grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.05842819809913635 
model_pd.l_d.mean(): -2.3051223754882812 
model_pd.lagr.mean(): -2.2466940879821777 
model_pd.lambdas: dict_items([('pout', tensor([1.3537])), ('power', tensor([0.0894]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1436])), ('power', tensor([-8.4112]))])
epoch：896	 i:0 	 global-step:17920	 l-p:0.05842819809913635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[19.7174, 19.8477, 19.7414],
        [19.7174, 19.7175, 19.7174],
        [19.7174, 21.8790, 21.9346],
        [19.7174, 19.7174, 19.7174]], grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.058392710983753204 
model_pd.l_d.mean(): -2.3004276752471924 
model_pd.lagr.mean(): -2.242034912109375 
model_pd.lambdas: dict_items([('pout', tensor([1.3526])), ('power', tensor([0.0890]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1486])), ('power', tensor([-8.3372]))])
epoch：897	 i:0 	 global-step:17940	 l-p:0.058392710983753204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[19.7925, 19.8291, 19.7956],
        [19.7925, 22.1072, 22.2477],
        [19.7925, 21.1067, 20.8112],
        [19.7925, 22.6865, 23.2236]], grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.05835755914449692 
model_pd.l_d.mean(): -2.2957816123962402 
model_pd.lagr.mean(): -2.23742413520813 
model_pd.lambdas: dict_items([('pout', tensor([1.3514])), ('power', tensor([0.0886]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1535])), ('power', tensor([-8.2633]))])
epoch：898	 i:0 	 global-step:17960	 l-p:0.05835755914449692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[19.8675, 19.8675, 19.8675],
        [19.8675, 21.9122, 21.8952],
        [19.8675, 21.1872, 20.8905],
        [19.8675, 20.9317, 20.5931]], grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.05832275375723839 
model_pd.l_d.mean(): -2.2911839485168457 
model_pd.lagr.mean(): -2.232861280441284 
model_pd.lambdas: dict_items([('pout', tensor([1.3503])), ('power', tensor([0.0882]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1585])), ('power', tensor([-8.1895]))])
epoch：899	 i:0 	 global-step:17980	 l-p:0.05832275375723839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[19.9424, 25.3779, 28.3691],
        [19.9424, 20.4181, 20.1383],
        [19.9424, 21.9956, 21.9785],
        [19.9424, 26.8154, 31.5745]], grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.05828829109668732 
model_pd.l_d.mean(): -2.2866337299346924 
model_pd.lagr.mean(): -2.2283453941345215 
model_pd.lambdas: dict_items([('pout', tensor([1.3491])), ('power', tensor([0.0878]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1634])), ('power', tensor([-8.1158]))])
epoch：900	 i:0 	 global-step:18000	 l-p:0.05828829109668732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[20.0172, 20.0172, 20.0172],
        [20.0172, 20.4948, 20.2139],
        [20.0172, 20.0188, 20.0172],
        [20.0172, 20.2708, 20.0872]], grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.05825415626168251 
model_pd.l_d.mean(): -2.282130718231201 
model_pd.lagr.mean(): -2.223876476287842 
model_pd.lambdas: dict_items([('pout', tensor([1.3479])), ('power', tensor([0.0874]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1683])), ('power', tensor([-8.0421]))])
epoch：901	 i:0 	 global-step:18020	 l-p:0.05825415626168251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[20.0918, 21.0272, 20.6770],
        [20.0918, 25.4630, 28.3551],
        [20.0918, 20.1125, 20.0930],
        [20.0918, 24.2019, 25.7629]], grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.05822037160396576 
model_pd.l_d.mean(): -2.2776737213134766 
model_pd.lagr.mean(): -2.2194533348083496 
model_pd.lambdas: dict_items([('pout', tensor([1.3468])), ('power', tensor([0.0870]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1732])), ('power', tensor([-7.9686]))])
epoch：902	 i:0 	 global-step:18040	 l-p:0.05822037160396576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[20.1663, 20.7402, 20.4304],
        [20.1663, 23.1199, 23.6687],
        [20.1663, 22.3818, 22.4393],
        [20.1663, 20.2037, 20.1695]], grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.05818692594766617 
model_pd.l_d.mean(): -2.2732625007629395 
model_pd.lagr.mean(): -2.2150754928588867 
model_pd.lambdas: dict_items([('pout', tensor([1.3456])), ('power', tensor([0.0866]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1780])), ('power', tensor([-7.8953]))])
epoch：903	 i:0 	 global-step:18060	 l-p:0.05818692594766617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[20.2406, 25.7637, 28.8039],
        [20.2406, 26.7259, 30.9242],
        [20.2406, 23.2061, 23.7572],
        [20.2406, 20.9061, 20.5760]], grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.05815381184220314 
model_pd.l_d.mean(): -2.2688961029052734 
model_pd.lagr.mean(): -2.210742235183716 
model_pd.lambdas: dict_items([('pout', tensor([1.3444])), ('power', tensor([0.0862]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1828])), ('power', tensor([-7.8221]))])
epoch：904	 i:0 	 global-step:18080	 l-p:0.05815381184220314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[20.3148, 20.5740, 20.3866],
        [20.3148, 22.4775, 22.4964],
        [20.3148, 20.3221, 20.3151],
        [20.3148, 21.7344, 21.4470]], grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.05812103673815727 
model_pd.l_d.mean(): -2.2645740509033203 
model_pd.lagr.mean(): -2.2064530849456787 
model_pd.lambdas: dict_items([('pout', tensor([1.3432])), ('power', tensor([0.0858]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1876])), ('power', tensor([-7.7490]))])
epoch：905	 i:0 	 global-step:18100	 l-p:0.05812103673815727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[20.3888, 20.3905, 20.3889],
        [20.3888, 21.0597, 20.7269],
        [20.3888, 20.9698, 20.6562],
        [20.3888, 22.6311, 22.6896]], grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.05808860436081886 
model_pd.l_d.mean(): -2.2602956295013428 
model_pd.lagr.mean(): -2.202207088470459 
model_pd.lambdas: dict_items([('pout', tensor([1.3420])), ('power', tensor([0.0854]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1924])), ('power', tensor([-7.6761]))])
epoch：906	 i:0 	 global-step:18120	 l-p:0.05808860436081886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[20.4626, 20.5421, 20.4732],
        [20.4626, 24.2877, 25.5354],
        [20.4626, 20.4627, 20.4626],
        [20.4626, 20.7240, 20.5350]], grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.058056510984897614 
model_pd.l_d.mean(): -2.2560606002807617 
model_pd.lagr.mean(): -2.1980040073394775 
model_pd.lambdas: dict_items([('pout', tensor([1.3408])), ('power', tensor([0.0850]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1971])), ('power', tensor([-7.6033]))])
epoch：907	 i:0 	 global-step:18140	 l-p:0.058056510984897614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[20.5363, 20.5363, 20.5363],
        [20.5363, 20.5437, 20.5365],
        [20.5363, 20.5374, 20.5363],
        [20.5363, 20.7894, 20.6048]], grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.058024752885103226 
model_pd.l_d.mean(): -2.2518677711486816 
model_pd.lagr.mean(): -2.193843126296997 
model_pd.lambdas: dict_items([('pout', tensor([1.3396])), ('power', tensor([0.0847]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2018])), ('power', tensor([-7.5308]))])
epoch：908	 i:0 	 global-step:18160	 l-p:0.058024752885103226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[20.6097, 20.8639, 20.6785],
        [20.6097, 20.6110, 20.6097],
        [20.6097, 22.5973, 22.5098],
        [20.6097, 20.6097, 20.6097]], grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.0579933300614357 
model_pd.l_d.mean(): -2.2477164268493652 
model_pd.lagr.mean(): -2.189723014831543 
model_pd.lambdas: dict_items([('pout', tensor([1.3384])), ('power', tensor([0.0843]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2065])), ('power', tensor([-7.4584]))])
epoch：909	 i:0 	 global-step:18180	 l-p:0.0579933300614357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[20.6829, 27.3203, 31.6183],
        [20.6829, 20.6829, 20.6829],
        [20.6829, 21.7954, 21.4419],
        [20.6829, 22.9606, 23.0203]], grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.05796224996447563 
model_pd.l_d.mean(): -2.2436065673828125 
model_pd.lagr.mean(): -2.1856443881988525 
model_pd.lambdas: dict_items([('pout', tensor([1.3372])), ('power', tensor([0.0839]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2111])), ('power', tensor([-7.3863]))])
epoch：910	 i:0 	 global-step:18200	 l-p:0.05796224996447563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[20.7559, 20.7559, 20.7559],
        [20.7559, 21.4402, 21.1008],
        [20.7559, 20.7634, 20.7562],
        [20.7559, 22.5642, 22.3900]], grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.057931505143642426 
model_pd.l_d.mean(): -2.239536762237549 
model_pd.lagr.mean(): -2.181605339050293 
model_pd.lambdas: dict_items([('pout', tensor([1.3360])), ('power', tensor([0.0836]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2157])), ('power', tensor([-7.3143]))])
epoch：911	 i:0 	 global-step:18220	 l-p:0.057931505143642426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[20.8286, 24.1135, 24.8588],
        [20.8286, 20.8298, 20.8287],
        [20.8286, 28.0294, 33.0179],
        [20.8286, 26.7877, 30.2309]], grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.057901088148355484 
model_pd.l_d.mean(): -2.235507011413574 
model_pd.lagr.mean(): -2.1776058673858643 
model_pd.lambdas: dict_items([('pout', tensor([1.3348])), ('power', tensor([0.0832]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2203])), ('power', tensor([-7.2426]))])
epoch：912	 i:0 	 global-step:18240	 l-p:0.057901088148355484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[20.9011, 21.0402, 20.9267],
        [20.9011, 22.7233, 22.5479],
        [20.9011, 21.6347, 21.2855],
        [20.9011, 21.5907, 21.2487]], grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.0578710176050663 
model_pd.l_d.mean(): -2.231515645980835 
model_pd.lagr.mean(): -2.173644542694092 
model_pd.lambdas: dict_items([('pout', tensor([1.3335])), ('power', tensor([0.0828]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2249])), ('power', tensor([-7.1712]))])
epoch：913	 i:0 	 global-step:18260	 l-p:0.0578710176050663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[20.9734, 22.1031, 21.7442],
        [20.9734, 25.2742, 26.9064],
        [20.9734, 21.2325, 21.0436],
        [20.9734, 22.8025, 22.6264]], grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.05784127861261368 
model_pd.l_d.mean(): -2.2275636196136475 
model_pd.lagr.mean(): -2.169722318649292 
model_pd.lambdas: dict_items([('pout', tensor([1.3323])), ('power', tensor([0.0825]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2294])), ('power', tensor([-7.0999]))])
epoch：914	 i:0 	 global-step:18280	 l-p:0.05784127861261368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[21.0453, 22.0299, 21.6617],
        [21.0453, 25.3669, 27.0100],
        [21.0453, 21.0481, 21.0454],
        [21.0453, 21.1855, 21.0711]], grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.05781187117099762 
model_pd.l_d.mean(): -2.2236485481262207 
model_pd.lagr.mean(): -2.1658365726470947 
model_pd.lambdas: dict_items([('pout', tensor([1.3311])), ('power', tensor([0.0821]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2339])), ('power', tensor([-7.0290]))])
epoch：915	 i:0 	 global-step:18300	 l-p:0.05781187117099762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[21.1170, 21.1198, 21.1171],
        [21.1170, 21.8590, 21.5058],
        [21.1170, 21.1993, 21.1279],
        [21.1170, 26.7837, 29.8373]], grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.05778279900550842 
model_pd.l_d.mean(): -2.219771385192871 
model_pd.lagr.mean(): -2.1619884967803955 
model_pd.lambdas: dict_items([('pout', tensor([1.3298])), ('power', tensor([0.0818]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2383])), ('power', tensor([-6.9583]))])
epoch：916	 i:0 	 global-step:18320	 l-p:0.05778279900550842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[21.1884, 21.1884, 21.1884],
        [21.1884, 21.1902, 21.1884],
        [21.1884, 21.1884, 21.1884],
        [21.1884, 21.1884, 21.1884]], grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.05775405839085579 
model_pd.l_d.mean(): -2.2159299850463867 
model_pd.lagr.mean(): -2.1581759452819824 
model_pd.lambdas: dict_items([('pout', tensor([1.3286])), ('power', tensor([0.0814]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2427])), ('power', tensor([-6.8879]))])
epoch：917	 i:0 	 global-step:18340	 l-p:0.05775405839085579
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[21.2595, 21.2672, 21.2598],
        [21.2595, 22.6198, 22.2876],
        [21.2595, 21.2613, 21.2595],
        [21.2595, 22.6814, 22.3626]], grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.05772564932703972 
model_pd.l_d.mean(): -2.212124824523926 
model_pd.lagr.mean(): -2.1543991565704346 
model_pd.lambdas: dict_items([('pout', tensor([1.3273])), ('power', tensor([0.0811]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2471])), ('power', tensor([-6.8178]))])
epoch：918	 i:0 	 global-step:18360	 l-p:0.05772564932703972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[21.3303, 24.4698, 25.0549],
        [21.3303, 27.0584, 30.1457],
        [21.3303, 22.2189, 21.8479],
        [21.3303, 21.5944, 21.4018]], grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.05769757181406021 
model_pd.l_d.mean(): -2.2083544731140137 
model_pd.lagr.mean(): -2.1506569385528564 
model_pd.lambdas: dict_items([('pout', tensor([1.3261])), ('power', tensor([0.0808]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2514])), ('power', tensor([-6.7479]))])
epoch：919	 i:0 	 global-step:18380	 l-p:0.05769757181406021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[21.4007, 21.4843, 21.4118],
        [21.4007, 21.4229, 21.4021],
        [21.4007, 21.5614, 21.4326],
        [21.4007, 21.6754, 21.4769]], grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.05766982585191727 
model_pd.l_d.mean(): -2.2046189308166504 
model_pd.lagr.mean(): -2.146949052810669 
model_pd.lambdas: dict_items([('pout', tensor([1.3248])), ('power', tensor([0.0804]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2558])), ('power', tensor([-6.6784]))])
epoch：920	 i:0 	 global-step:18400	 l-p:0.05766982585191727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[21.4708, 22.1813, 21.8291],
        [21.4708, 24.0008, 24.1566],
        [21.4708, 21.7466, 21.5473],
        [21.4708, 27.6273, 31.1861]], grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.05764240399003029 
model_pd.l_d.mean(): -2.2009172439575195 
model_pd.lagr.mean(): -2.1432747840881348 
model_pd.lambdas: dict_items([('pout', tensor([1.3236])), ('power', tensor([0.0801]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2600])), ('power', tensor([-6.6093]))])
epoch：921	 i:0 	 global-step:18420	 l-p:0.05764240399003029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[21.5406, 21.5715, 21.5429],
        [21.5406, 21.5406, 21.5406],
        [21.5406, 21.7025, 21.5727],
        [21.5406, 25.9721, 27.6580]], grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.05761531740427017 
model_pd.l_d.mean(): -2.197248935699463 
model_pd.lagr.mean(): -2.1396336555480957 
model_pd.lambdas: dict_items([('pout', tensor([1.3223])), ('power', tensor([0.0798]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2643])), ('power', tensor([-6.5404]))])
epoch：922	 i:0 	 global-step:18440	 l-p:0.05761531740427017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[21.6100, 21.6100, 21.6101],
        [21.6100, 21.6114, 21.6101],
        [21.6100, 23.8518, 23.8351],
        [21.6100, 23.7034, 23.6123]], grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.05758855864405632 
model_pd.l_d.mean(): -2.193612575531006 
model_pd.lagr.mean(): -2.136023998260498 
model_pd.lambdas: dict_items([('pout', tensor([1.3210])), ('power', tensor([0.0794]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2685])), ('power', tensor([-6.4719]))])
epoch：923	 i:0 	 global-step:18460	 l-p:0.05758855864405632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[21.6791, 26.1047, 27.7670],
        [21.6791, 21.6791, 21.6791],
        [21.6791, 21.6803, 21.6791],
        [21.6791, 21.6792, 21.6791]], grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.057562123984098434 
model_pd.l_d.mean(): -2.1900088787078857 
model_pd.lagr.mean(): -2.132446765899658 
model_pd.lambdas: dict_items([('pout', tensor([1.3198])), ('power', tensor([0.0791]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2727])), ('power', tensor([-6.4037]))])
epoch：924	 i:0 	 global-step:18480	 l-p:0.057562123984098434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[21.7478, 29.2887, 34.5155],
        [21.7478, 21.7885, 21.7513],
        [21.7478, 21.7492, 21.7478],
        [21.7478, 28.7517, 33.2898]], grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.057536009699106216 
model_pd.l_d.mean(): -2.1864359378814697 
model_pd.lagr.mean(): -2.1288998126983643 
model_pd.lambdas: dict_items([('pout', tensor([1.3185])), ('power', tensor([0.0788]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2768])), ('power', tensor([-6.3360]))])
epoch：925	 i:0 	 global-step:18500	 l-p:0.057536009699106216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[21.8161, 21.8175, 21.8161],
        [21.8161, 27.8024, 31.1014],
        [21.8161, 24.2304, 24.2950],
        [21.8161, 22.4423, 22.1046]], grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.05751022696495056 
model_pd.l_d.mean(): -2.182893753051758 
model_pd.lagr.mean(): -2.1253836154937744 
model_pd.lambdas: dict_items([('pout', tensor([1.3172])), ('power', tensor([0.0785]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2809])), ('power', tensor([-6.2685]))])
epoch：926	 i:0 	 global-step:18520	 l-p:0.05751022696495056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[21.8840, 26.3547, 28.0344],
        [21.8840, 21.8852, 21.8840],
        [21.8840, 26.3918, 28.1073],
        [21.8840, 23.3519, 23.0231]], grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.05748476833105087 
model_pd.l_d.mean(): -2.1793816089630127 
model_pd.lagr.mean(): -2.121896743774414 
model_pd.lambdas: dict_items([('pout', tensor([1.3159])), ('power', tensor([0.0782]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2850])), ('power', tensor([-6.2015]))])
epoch：927	 i:0 	 global-step:18540	 l-p:0.05748476833105087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[21.9515, 21.9831, 21.9538],
        [21.9515, 23.4243, 23.0945],
        [21.9515, 25.4290, 26.2197],
        [21.9515, 24.3053, 24.3277]], grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.057459622621536255 
model_pd.l_d.mean(): -2.175898790359497 
model_pd.lagr.mean(): -2.118439197540283 
model_pd.lambdas: dict_items([('pout', tensor([1.3146])), ('power', tensor([0.0779]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2890])), ('power', tensor([-6.1348]))])
epoch：928	 i:0 	 global-step:18560	 l-p:0.057459622621536255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[22.0186, 23.0535, 22.6669],
        [22.0186, 22.2922, 22.0928],
        [22.0186, 22.1048, 22.0300],
        [22.0186, 24.1553, 24.0627]], grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.0574348010122776 
model_pd.l_d.mean(): -2.1724448204040527 
model_pd.lagr.mean(): -2.1150100231170654 
model_pd.lambdas: dict_items([('pout', tensor([1.3133])), ('power', tensor([0.0776]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2930])), ('power', tensor([-6.0686]))])
epoch：929	 i:0 	 global-step:18580	 l-p:0.0574348010122776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[22.0853, 28.1509, 31.4941],
        [22.0853, 24.3809, 24.3643],
        [22.0853, 22.0853, 22.0853],
        [22.0853, 22.7201, 22.3777]], grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.05741030350327492 
model_pd.l_d.mean(): -2.1690189838409424 
model_pd.lagr.mean(): -2.1116087436676025 
model_pd.lambdas: dict_items([('pout', tensor([1.3121])), ('power', tensor([0.0773]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2969])), ('power', tensor([-6.0028]))])
epoch：930	 i:0 	 global-step:18600	 l-p:0.05741030350327492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[22.1515, 26.7137, 28.4474],
        [22.1515, 22.1534, 22.1515],
        [22.1515, 24.6063, 24.6724],
        [22.1515, 22.1930, 22.1551]], grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.0573861226439476 
model_pd.l_d.mean(): -2.1656205654144287 
model_pd.lagr.mean(): -2.108234405517578 
model_pd.lambdas: dict_items([('pout', tensor([1.3108])), ('power', tensor([0.0770]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3009])), ('power', tensor([-5.9374]))])
epoch：931	 i:0 	 global-step:18620	 l-p:0.0573861226439476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[22.2173, 22.2202, 22.2174],
        [22.2173, 24.6022, 24.6252],
        [22.2173, 23.2625, 22.8721],
        [22.2173, 22.5037, 22.2967]], grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.057362254709005356 
model_pd.l_d.mean(): -2.1622495651245117 
model_pd.lagr.mean(): -2.1048872470855713 
model_pd.lambdas: dict_items([('pout', tensor([1.3094])), ('power', tensor([0.0767]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3047])), ('power', tensor([-5.8724]))])
epoch：932	 i:0 	 global-step:18640	 l-p:0.057362254709005356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[22.2827, 23.0229, 22.6561],
        [22.2827, 22.4508, 22.3160],
        [22.2827, 23.2152, 22.8261],
        [22.2827, 24.9170, 25.0802]], grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.05733870342373848 
model_pd.l_d.mean(): -2.158904552459717 
model_pd.lagr.mean(): -2.1015658378601074 
model_pd.lambdas: dict_items([('pout', tensor([1.3081])), ('power', tensor([0.0764]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3086])), ('power', tensor([-5.8078]))])
epoch：933	 i:0 	 global-step:18660	 l-p:0.05733870342373848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[22.3476, 28.7737, 32.4906],
        [22.3476, 22.6318, 22.4257],
        [22.3476, 24.3082, 24.1208],
        [22.3476, 23.8496, 23.5135]], grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.057315465062856674 
model_pd.l_d.mean(): -2.155585527420044 
model_pd.lagr.mean(): -2.0982701778411865 
model_pd.lambdas: dict_items([('pout', tensor([1.3068])), ('power', tensor([0.0761]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3124])), ('power', tensor([-5.7438]))])
epoch：934	 i:0 	 global-step:18680	 l-p:0.057315465062856674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[22.4120, 22.9535, 22.6354],
        [22.4120, 22.4132, 22.4120],
        [22.4120, 23.8535, 23.5022],
        [22.4120, 22.6997, 22.4915]], grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.05729254335165024 
model_pd.l_d.mean(): -2.1522912979125977 
model_pd.lagr.mean(): -2.094998836517334 
model_pd.lambdas: dict_items([('pout', tensor([1.3055])), ('power', tensor([0.0758]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3162])), ('power', tensor([-5.6801]))])
epoch：935	 i:0 	 global-step:18700	 l-p:0.05729254335165024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[22.4759, 22.6268, 22.5037],
        [22.4759, 23.4174, 23.0247],
        [22.4759, 22.4789, 22.4760],
        [22.4759, 22.4772, 22.4760]], grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.05726991593837738 
model_pd.l_d.mean(): -2.149021863937378 
model_pd.lagr.mean(): -2.091752052307129 
model_pd.lambdas: dict_items([('pout', tensor([1.3042])), ('power', tensor([0.0755]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3199])), ('power', tensor([-5.6169]))])
epoch：936	 i:0 	 global-step:18720	 l-p:0.05726991593837738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[22.5394, 24.1306, 23.8101],
        [22.5394, 28.7388, 32.1568],
        [22.5394, 23.0843, 22.7642],
        [22.5394, 26.1179, 26.9326]], grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.05724761635065079 
model_pd.l_d.mean(): -2.1457762718200684 
model_pd.lagr.mean(): -2.088528633117676 
model_pd.lambdas: dict_items([('pout', tensor([1.3029])), ('power', tensor([0.0753]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3236])), ('power', tensor([-5.5542]))])
epoch：937	 i:0 	 global-step:18740	 l-p:0.05724761635065079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[22.6024, 22.6024, 22.6024],
        [22.6024, 24.8010, 24.7063],
        [22.6024, 24.9567, 24.9402],
        [22.6024, 23.3544, 22.9818]], grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.05722561851143837 
model_pd.l_d.mean(): -2.142554521560669 
model_pd.lagr.mean(): -2.0853288173675537 
model_pd.lambdas: dict_items([('pout', tensor([1.3015])), ('power', tensor([0.0750]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3272])), ('power', tensor([-5.4920]))])
epoch：938	 i:0 	 global-step:18760	 l-p:0.05722561851143837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[22.6649, 24.2658, 23.9434],
        [22.6649, 24.1243, 23.7687],
        [22.6649, 22.6885, 22.6663],
        [22.6649, 29.9848, 34.7302]], grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.057203926146030426 
model_pd.l_d.mean(): -2.139354944229126 
model_pd.lagr.mean(): -2.082150936126709 
model_pd.lambdas: dict_items([('pout', tensor([1.3002])), ('power', tensor([0.0747]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3308])), ('power', tensor([-5.4303]))])
epoch：939	 i:0 	 global-step:18780	 l-p:0.057203926146030426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[22.7269, 23.6799, 23.2824],
        [22.7269, 22.7506, 22.7283],
        [22.7269, 22.7281, 22.7269],
        [22.7269, 29.2699, 33.0552]], grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.05718253552913666 
model_pd.l_d.mean(): -2.1361782550811768 
model_pd.lagr.mean(): -2.078995704650879 
model_pd.lambdas: dict_items([('pout', tensor([1.2989])), ('power', tensor([0.0745]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3344])), ('power', tensor([-5.3690]))])
epoch：940	 i:0 	 global-step:18800	 l-p:0.05718253552913666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[22.7883, 23.5471, 23.1712],
        [22.7883, 26.1614, 26.7923],
        [22.7883, 23.8631, 23.4619],
        [22.7883, 27.4920, 29.2807]], grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.05716145411133766 
model_pd.l_d.mean(): -2.133023262023926 
model_pd.lagr.mean(): -2.075861692428589 
model_pd.lambdas: dict_items([('pout', tensor([1.2975])), ('power', tensor([0.0742]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3379])), ('power', tensor([-5.3083]))])
epoch：941	 i:0 	 global-step:18820	 l-p:0.05716145411133766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[22.8493, 24.8581, 24.6665],
        [22.8493, 24.3217, 23.9631],
        [22.8493, 23.5084, 23.1531],
        [22.8493, 23.1432, 22.9305]], grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.05714065954089165 
model_pd.l_d.mean(): -2.1298890113830566 
model_pd.lagr.mean(): -2.0727484226226807 
model_pd.lambdas: dict_items([('pout', tensor([1.2962])), ('power', tensor([0.0739]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3414])), ('power', tensor([-5.2481]))])
epoch：942	 i:0 	 global-step:18840	 l-p:0.05714065954089165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[22.9097, 22.9528, 22.9134],
        [22.9097, 25.2989, 25.2825],
        [22.9097, 22.9097, 22.9097],
        [22.9097, 23.2018, 22.9900]], grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.057120174169540405 
model_pd.l_d.mean(): -2.1267759799957275 
model_pd.lagr.mean(): -2.0696558952331543 
model_pd.lambdas: dict_items([('pout', tensor([1.2948])), ('power', tensor([0.0737]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3449])), ('power', tensor([-5.1884]))])
epoch：943	 i:0 	 global-step:18860	 l-p:0.057120174169540405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[22.9696, 26.3717, 27.0084],
        [22.9696, 24.9900, 24.7974],
        [22.9696, 25.5233, 25.5930],
        [22.9696, 23.2652, 23.0513]], grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.05709998309612274 
model_pd.l_d.mean(): -2.123682975769043 
model_pd.lagr.mean(): -2.0665829181671143 
model_pd.lambdas: dict_items([('pout', tensor([1.2935])), ('power', tensor([0.0734]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3483])), ('power', tensor([-5.1292]))])
epoch：944	 i:0 	 global-step:18880	 l-p:0.05709998309612274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[23.0289, 23.0723, 23.0326],
        [23.0289, 25.5898, 25.6598],
        [23.0289, 23.3270, 23.1116],
        [23.0289, 23.0304, 23.0290]], grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.057080093771219254 
model_pd.l_d.mean(): -2.120609760284424 
model_pd.lagr.mean(): -2.0635297298431396 
model_pd.lambdas: dict_items([('pout', tensor([1.2921])), ('power', tensor([0.0732]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3517])), ('power', tensor([-5.0705]))])
epoch：945	 i:0 	 global-step:18900	 l-p:0.057080093771219254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[23.0877, 23.1312, 23.0915],
        [23.0877, 23.3851, 23.1699],
        [23.0877, 24.5770, 24.2145],
        [23.0877, 27.8633, 29.6830]], grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.05706048384308815 
model_pd.l_d.mean(): -2.1175553798675537 
model_pd.lagr.mean(): -2.060494899749756 
model_pd.lambdas: dict_items([('pout', tensor([1.2908])), ('power', tensor([0.0729]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3550])), ('power', tensor([-5.0124]))])
epoch：946	 i:0 	 global-step:18920	 l-p:0.05706048384308815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[23.1460, 23.8146, 23.4542],
        [23.1460, 23.4415, 23.2272],
        [23.1460, 23.1475, 23.1460],
        [23.1460, 25.7210, 25.7916]], grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.05704116448760033 
model_pd.l_d.mean(): -2.1145198345184326 
model_pd.lagr.mean(): -2.057478666305542 
model_pd.lambdas: dict_items([('pout', tensor([1.2894])), ('power', tensor([0.0727]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3583])), ('power', tensor([-4.9548]))])
epoch：947	 i:0 	 global-step:18940	 l-p:0.05704116448760033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[23.2037, 23.7664, 23.4359],
        [23.2037, 23.4937, 23.2823],
        [23.2037, 27.5855, 29.0200],
        [23.2037, 25.4662, 25.3694]], grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.05702213943004608 
model_pd.l_d.mean(): -2.111502170562744 
model_pd.lagr.mean(): -2.0544800758361816 
model_pd.lambdas: dict_items([('pout', tensor([1.2881])), ('power', tensor([0.0724]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3616])), ('power', tensor([-4.8977]))])
epoch：948	 i:0 	 global-step:18960	 l-p:0.05702213943004608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[23.2608, 24.2386, 23.8310],
        [23.2608, 26.7097, 27.3556],
        [23.2608, 23.2608, 23.2608],
        [23.2608, 24.7623, 24.3969]], grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.05700339376926422 
model_pd.l_d.mean(): -2.108502149581909 
model_pd.lagr.mean(): -2.0514986515045166 
model_pd.lambdas: dict_items([('pout', tensor([1.2867])), ('power', tensor([0.0722]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3648])), ('power', tensor([-4.8413]))])
epoch：949	 i:0 	 global-step:18980	 l-p:0.05700339376926422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[23.3174, 26.0851, 26.2579],
        [23.3174, 25.3712, 25.1758],
        [23.3174, 23.3178, 23.3174],
        [23.3174, 23.4745, 23.3463]], grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.05698494613170624 
model_pd.l_d.mean(): -2.1055197715759277 
model_pd.lagr.mean(): -2.048534870147705 
model_pd.lambdas: dict_items([('pout', tensor([1.2853])), ('power', tensor([0.0719]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3680])), ('power', tensor([-4.7853]))])
epoch：950	 i:0 	 global-step:19000	 l-p:0.05698494613170624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[23.3734, 24.2034, 23.8089],
        [23.3734, 30.9378, 35.8435],
        [23.3734, 23.3979, 23.3749],
        [23.3734, 23.4073, 23.3759]], grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.056966766715049744 
model_pd.l_d.mean(): -2.102553606033325 
model_pd.lagr.mean(): -2.0455868244171143 
model_pd.lambdas: dict_items([('pout', tensor([1.2840])), ('power', tensor([0.0717]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3711])), ('power', tensor([-4.7300]))])
epoch：951	 i:0 	 global-step:19020	 l-p:0.056966766715049744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[23.4288, 23.7326, 23.5131],
        [23.4288, 23.4288, 23.4288],
        [23.4288, 23.7310, 23.5124],
        [23.4288, 25.0107, 24.6575]], grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.05694887042045593 
model_pd.l_d.mean(): -2.0996036529541016 
model_pd.lagr.mean(): -2.0426547527313232 
model_pd.lambdas: dict_items([('pout', tensor([1.2826])), ('power', tensor([0.0715]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3742])), ('power', tensor([-4.6752]))])
epoch：952	 i:0 	 global-step:19040	 l-p:0.05694887042045593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[23.4836, 23.4836, 23.4837],
        [23.4836, 23.4837, 23.4836],
        [23.4836, 23.4868, 23.4837],
        [23.4836, 26.9684, 27.6212]], grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.056931257247924805 
model_pd.l_d.mean(): -2.0966696739196777 
model_pd.lagr.mean(): -2.039738416671753 
model_pd.lambdas: dict_items([('pout', tensor([1.2812])), ('power', tensor([0.0712]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3773])), ('power', tensor([-4.6209]))])
epoch：953	 i:0 	 global-step:19060	 l-p:0.056931257247924805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[23.5379, 23.7166, 23.5733],
        [23.5379, 24.2190, 23.8520],
        [23.5379, 23.8433, 23.6226],
        [23.5379, 23.5399, 23.5379]], grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.05691391974687576 
model_pd.l_d.mean(): -2.0937509536743164 
model_pd.lagr.mean(): -2.036837100982666 
model_pd.lambdas: dict_items([('pout', tensor([1.2798])), ('power', tensor([0.0710]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3803])), ('power', tensor([-4.5673]))])
epoch：954	 i:0 	 global-step:19080	 l-p:0.05691391974687576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[23.5916, 24.4302, 24.0317],
        [23.5916, 25.1855, 24.8297],
        [23.5916, 23.5916, 23.5916],
        [23.5916, 23.6361, 23.5954]], grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.05689685046672821 
model_pd.l_d.mean(): -2.0908472537994385 
model_pd.lagr.mean(): -2.0339503288269043 
model_pd.lambdas: dict_items([('pout', tensor([1.2785])), ('power', tensor([0.0708]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3833])), ('power', tensor([-4.5142]))])
epoch：955	 i:0 	 global-step:19100	 l-p:0.05689685046672821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[23.6447, 26.4546, 26.6305],
        [23.6447, 24.6402, 24.2253],
        [23.6447, 25.9541, 25.8558],
        [23.6447, 23.6462, 23.6447]], grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.05688004940748215 
model_pd.l_d.mean(): -2.0879573822021484 
model_pd.lagr.mean(): -2.0310773849487305 
model_pd.lambdas: dict_items([('pout', tensor([1.2771])), ('power', tensor([0.0706]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3862])), ('power', tensor([-4.4617]))])
epoch：956	 i:0 	 global-step:19120	 l-p:0.05688004940748215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[23.6971, 26.5139, 26.6902],
        [23.6971, 27.2162, 27.8758],
        [23.6971, 24.0005, 23.7806],
        [23.6971, 23.7419, 23.7010]], grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.05686352401971817 
model_pd.l_d.mean(): -2.0850820541381836 
model_pd.lagr.mean(): -2.0282185077667236 
model_pd.lambdas: dict_items([('pout', tensor([1.2757])), ('power', tensor([0.0703]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3891])), ('power', tensor([-4.4098]))])
epoch：957	 i:0 	 global-step:19140	 l-p:0.05686352401971817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[23.7490, 23.7494, 23.7490],
        [23.7490, 23.7939, 23.7529],
        [23.7490, 32.0318, 37.7790],
        [23.7490, 27.2764, 27.9377]], grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.056847263127565384 
model_pd.l_d.mean(): -2.0822203159332275 
model_pd.lagr.mean(): -2.0253729820251465 
model_pd.lambdas: dict_items([('pout', tensor([1.2743])), ('power', tensor([0.0701]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3920])), ('power', tensor([-4.3585]))])
epoch：958	 i:0 	 global-step:19160	 l-p:0.056847263127565384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[23.8003, 23.8349, 23.8029],
        [23.8003, 24.6471, 24.2448],
        [23.8003, 25.0987, 24.6877],
        [23.8003, 28.7292, 30.6056]], grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.05683125928044319 
model_pd.l_d.mean(): -2.079371690750122 
model_pd.lagr.mean(): -2.022540330886841 
model_pd.lambdas: dict_items([('pout', tensor([1.2729])), ('power', tensor([0.0699]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3948])), ('power', tensor([-4.3077]))])
epoch：959	 i:0 	 global-step:19180	 l-p:0.05683125928044319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[23.8510, 23.8510, 23.8510],
        [23.8510, 30.3077, 33.7938],
        [23.8510, 23.9453, 23.8635],
        [23.8510, 23.8760, 23.8525]], grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.05681552365422249 
model_pd.l_d.mean(): -2.076535701751709 
model_pd.lagr.mean(): -2.0197200775146484 
model_pd.lambdas: dict_items([('pout', tensor([1.2715])), ('power', tensor([0.0697]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3976])), ('power', tensor([-4.2576]))])
epoch：960	 i:0 	 global-step:19200	 l-p:0.05681552365422249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[23.9011, 30.3723, 33.8663],
        [23.9011, 23.9012, 23.9011],
        [23.9011, 23.9031, 23.9011],
        [23.9011, 26.7442, 26.9225]], grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.05680004134774208 
model_pd.l_d.mean(): -2.0737125873565674 
model_pd.lagr.mean(): -2.0169124603271484 
model_pd.lambdas: dict_items([('pout', tensor([1.2701])), ('power', tensor([0.0695]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4003])), ('power', tensor([-4.2080]))])
epoch：961	 i:0 	 global-step:19220	 l-p:0.05680004134774208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[23.9506, 24.6449, 24.2708],
        [23.9506, 27.9912, 29.0438],
        [23.9506, 25.5711, 25.2096],
        [23.9506, 25.6512, 25.3097]], grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.056784819811582565 
model_pd.l_d.mean(): -2.070901393890381 
model_pd.lagr.mean(): -2.0141165256500244 
model_pd.lambdas: dict_items([('pout', tensor([1.2687])), ('power', tensor([0.0693]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4030])), ('power', tensor([-4.1591]))])
epoch：962	 i:0 	 global-step:19240	 l-p:0.056784819811582565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[23.9994, 28.9783, 30.8773],
        [23.9994, 30.9350, 34.9506],
        [23.9994, 24.0007, 23.9994],
        [23.9994, 24.6953, 24.3204]], grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.05676984786987305 
model_pd.l_d.mean(): -2.0681018829345703 
model_pd.lagr.mean(): -2.0113320350646973 
model_pd.lambdas: dict_items([('pout', tensor([1.2673])), ('power', tensor([0.0691]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4057])), ('power', tensor([-4.1107]))])
epoch：963	 i:0 	 global-step:19260	 l-p:0.05676984786987305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[24.0477, 24.0730, 24.0492],
        [24.0477, 29.0373, 30.9406],
        [24.0477, 24.8529, 24.4543],
        [24.0477, 24.3495, 24.1296]], grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.05675513297319412 
model_pd.l_d.mean(): -2.0653135776519775 
model_pd.lagr.mean(): -2.008558511734009 
model_pd.lambdas: dict_items([('pout', tensor([1.2659])), ('power', tensor([0.0689]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4083])), ('power', tensor([-4.0630]))])
epoch：964	 i:0 	 global-step:19280	 l-p:0.05675513297319412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[24.0953, 27.9419, 28.8202],
        [24.0953, 28.6590, 30.1548],
        [24.0953, 31.0605, 35.0936],
        [24.0953, 24.1907, 24.1080]], grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.0567406602203846 
model_pd.l_d.mean(): -2.0625362396240234 
model_pd.lagr.mean(): -2.005795478820801 
model_pd.lambdas: dict_items([('pout', tensor([1.2645])), ('power', tensor([0.0687]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4109])), ('power', tensor([-4.0158]))])
epoch：965	 i:0 	 global-step:19300	 l-p:0.0567406602203846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[24.1424, 24.1512, 24.1427],
        [24.1424, 29.1532, 31.0647],
        [24.1424, 24.4521, 24.2276],
        [24.1424, 24.1424, 24.1424]], grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.05672644078731537 
model_pd.l_d.mean(): -2.059769630432129 
model_pd.lagr.mean(): -2.0030431747436523 
model_pd.lambdas: dict_items([('pout', tensor([1.2631])), ('power', tensor([0.0685]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4134])), ('power', tensor([-3.9693]))])
epoch：966	 i:0 	 global-step:19320	 l-p:0.05672644078731537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[24.1888, 24.1892, 24.1888],
        [24.1888, 27.7871, 28.4623],
        [24.1888, 26.7238, 26.7079],
        [24.1888, 25.5105, 25.0923]], grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.05671245604753494 
model_pd.l_d.mean(): -2.0570130348205566 
model_pd.lagr.mean(): -2.000300645828247 
model_pd.lambdas: dict_items([('pout', tensor([1.2616])), ('power', tensor([0.0683]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4160])), ('power', tensor([-3.9234]))])
epoch：967	 i:0 	 global-step:19340	 l-p:0.05671245604753494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[24.2346, 25.5591, 25.1400],
        [24.2346, 24.3987, 24.2648],
        [24.2346, 24.2379, 24.2347],
        [24.2346, 24.2435, 24.2349]], grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): 0.056698717176914215 
model_pd.l_d.mean(): -2.0542666912078857 
model_pd.lagr.mean(): -1.9975680112838745 
model_pd.lambdas: dict_items([('pout', tensor([1.2602])), ('power', tensor([0.0681]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4184])), ('power', tensor([-3.8780]))])
epoch：968	 i:0 	 global-step:19360	 l-p:0.056698717176914215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[24.2798, 27.8928, 28.5710],
        [24.2798, 29.2800, 31.1635],
        [24.2798, 24.5849, 24.3626],
        [24.2798, 24.2798, 24.2798]], grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.05668522045016289 
model_pd.l_d.mean(): -2.0515294075012207 
model_pd.lagr.mean(): -1.9948441982269287 
model_pd.lambdas: dict_items([('pout', tensor([1.2588])), ('power', tensor([0.0679]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4209])), ('power', tensor([-3.8333]))])
epoch：969	 i:0 	 global-step:19380	 l-p:0.05668522045016289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[24.3244, 26.0543, 25.7071],
        [24.3244, 24.3705, 24.3284],
        [24.3244, 29.3345, 31.2218],
        [24.3244, 24.6367, 24.4103]], grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.056671954691410065 
model_pd.l_d.mean(): -2.048801898956299 
model_pd.lagr.mean(): -1.992129921913147 
model_pd.lambdas: dict_items([('pout', tensor([1.2574])), ('power', tensor([0.0677]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4233])), ('power', tensor([-3.7891]))])
epoch：970	 i:0 	 global-step:19400	 l-p:0.056671954691410065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[24.3684, 24.3684, 24.3684],
        [24.3684, 27.0919, 27.1681],
        [24.3684, 31.4180, 35.5007],
        [24.3684, 26.0200, 25.6519]], grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.05665891617536545 
model_pd.l_d.mean(): -2.046083450317383 
model_pd.lagr.mean(): -1.9894245862960815 
model_pd.lambdas: dict_items([('pout', tensor([1.2559])), ('power', tensor([0.0675]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4256])), ('power', tensor([-3.7456]))])
epoch：971	 i:0 	 global-step:19420	 l-p:0.05665891617536545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[24.4118, 24.5979, 24.4487],
        [24.4118, 25.7470, 25.3246],
        [24.4118, 29.4772, 31.4068],
        [24.4118, 24.4119, 24.4118]], grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.056646112352609634 
model_pd.l_d.mean(): -2.0433735847473145 
model_pd.lagr.mean(): -1.9867274761199951 
model_pd.lambdas: dict_items([('pout', tensor([1.2545])), ('power', tensor([0.0673]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4280])), ('power', tensor([-3.7027]))])
epoch：972	 i:0 	 global-step:19440	 l-p:0.056646112352609634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[24.4546, 25.1651, 24.7824],
        [24.4546, 24.4902, 24.4572],
        [24.4546, 27.1886, 27.2651],
        [24.4546, 24.6410, 24.4916]], grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.056633539497852325 
model_pd.l_d.mean(): -2.0406723022460938 
model_pd.lagr.mean(): -1.9840387105941772 
model_pd.lambdas: dict_items([('pout', tensor([1.2531])), ('power', tensor([0.0671]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4303])), ('power', tensor([-3.6603]))])
epoch：973	 i:0 	 global-step:19460	 l-p:0.056633539497852325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[24.4967, 24.6836, 24.5338],
        [24.4967, 28.1447, 28.8298],
        [24.4967, 27.4171, 27.6010],
        [24.4967, 24.4967, 24.4967]], grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.05662118270993233 
model_pd.l_d.mean(): -2.0379786491394043 
model_pd.lagr.mean(): -1.981357455253601 
model_pd.lambdas: dict_items([('pout', tensor([1.2517])), ('power', tensor([0.0670]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4325])), ('power', tensor([-3.6186]))])
epoch：974	 i:0 	 global-step:19480	 l-p:0.05662118270993233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[24.5383, 24.5384, 24.5383],
        [24.5383, 26.2848, 25.9345],
        [24.5383, 31.3280, 35.0764],
        [24.5383, 26.7100, 26.5045]], grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.05660906434059143 
model_pd.l_d.mean(): -2.0352933406829834 
model_pd.lagr.mean(): -1.9786843061447144 
model_pd.lambdas: dict_items([('pout', tensor([1.2502])), ('power', tensor([0.0668]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4347])), ('power', tensor([-3.5775]))])
epoch：975	 i:0 	 global-step:19500	 l-p:0.05660906434059143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[24.5793, 24.5793, 24.5793],
        [24.5793, 24.5808, 24.5793],
        [24.5793, 24.5794, 24.5793],
        [24.5793, 24.8997, 24.6682]], grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.05659714713692665 
model_pd.l_d.mean(): -2.0326154232025146 
model_pd.lagr.mean(): -1.9760183095932007 
model_pd.lambdas: dict_items([('pout', tensor([1.2488])), ('power', tensor([0.0666]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4369])), ('power', tensor([-3.5369]))])
epoch：976	 i:0 	 global-step:19520	 l-p:0.05659714713692665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[24.6196, 24.8075, 24.6569],
        [24.6196, 24.6197, 24.6196],
        [24.6196, 24.6287, 24.6199],
        [24.6196, 24.6210, 24.6196]], grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.05658545345067978 
model_pd.l_d.mean(): -2.029944658279419 
model_pd.lagr.mean(): -1.973359227180481 
model_pd.lambdas: dict_items([('pout', tensor([1.2473])), ('power', tensor([0.0664]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4390])), ('power', tensor([-3.4970]))])
epoch：977	 i:0 	 global-step:19540	 l-p:0.05658545345067978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[24.6594, 26.0095, 25.5825],
        [24.6594, 33.2807, 39.2655],
        [24.6594, 27.3313, 27.3601],
        [24.6594, 24.7572, 24.6724]], grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.05657397583127022 
model_pd.l_d.mean(): -2.0272812843322754 
model_pd.lagr.mean(): -1.9707072973251343 
model_pd.lambdas: dict_items([('pout', tensor([1.2459])), ('power', tensor([0.0662]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4412])), ('power', tensor([-3.4576]))])
epoch：978	 i:0 	 global-step:19560	 l-p:0.05657397583127022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[24.6985, 26.0510, 25.6233],
        [24.6985, 27.3751, 27.4040],
        [24.6985, 24.6985, 24.6985],
        [24.6985, 31.8503, 35.9931]], grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.05656271055340767 
model_pd.l_d.mean(): -2.0246243476867676 
model_pd.lagr.mean(): -1.9680616855621338 
model_pd.lambdas: dict_items([('pout', tensor([1.2445])), ('power', tensor([0.0661]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4432])), ('power', tensor([-3.4188]))])
epoch：979	 i:0 	 global-step:19580	 l-p:0.05656271055340767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[24.7371, 29.8810, 31.8446],
        [24.7371, 24.7462, 24.7374],
        [24.7371, 24.7371, 24.7371],
        [24.7371, 25.5680, 25.1569]], grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.056551650166511536 
model_pd.l_d.mean(): -2.0219743251800537 
model_pd.lagr.mean(): -1.9654226303100586 
model_pd.lambdas: dict_items([('pout', tensor([1.2430])), ('power', tensor([0.0659]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4453])), ('power', tensor([-3.3806]))])
epoch：980	 i:0 	 global-step:19600	 l-p:0.056551650166511536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[24.7751, 27.5482, 27.6263],
        [24.7751, 24.8112, 24.7777],
        [24.7751, 28.4680, 29.1621],
        [24.7751, 25.8233, 25.3869]], grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.05654079467058182 
model_pd.l_d.mean(): -2.0193307399749756 
model_pd.lagr.mean(): -1.9627898931503296 
model_pd.lambdas: dict_items([('pout', tensor([1.2416])), ('power', tensor([0.0657]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4473])), ('power', tensor([-3.3431]))])
epoch：981	 i:0 	 global-step:19620	 l-p:0.05654079467058182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[24.8125, 26.4243, 26.0331],
        [24.8125, 24.8129, 24.8125],
        [24.8125, 25.0020, 24.8501],
        [24.8125, 26.4972, 26.1220]], grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.05653015151619911 
model_pd.l_d.mean(): -2.016692876815796 
model_pd.lagr.mean(): -1.9601627588272095 
model_pd.lambdas: dict_items([('pout', tensor([1.2401])), ('power', tensor([0.0656]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4493])), ('power', tensor([-3.3060]))])
epoch：982	 i:0 	 global-step:19640	 l-p:0.05653015151619911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[24.8493, 24.8526, 24.8493],
        [24.8493, 31.7311, 35.5313],
        [24.8493, 25.1691, 24.9373],
        [24.8493, 24.8493, 24.8493]], grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.05651969835162163 
model_pd.l_d.mean(): -2.014061450958252 
model_pd.lagr.mean(): -1.9575417041778564 
model_pd.lambdas: dict_items([('pout', tensor([1.2387])), ('power', tensor([0.0654]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4512])), ('power', tensor([-3.2696]))])
epoch：983	 i:0 	 global-step:19660	 l-p:0.05651969835162163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[24.8855, 24.9218, 24.8882],
        [24.8855, 24.8855, 24.8855],
        [24.8855, 25.7751, 25.3527],
        [24.8855, 24.9844, 24.8986]], grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.05650944635272026 
model_pd.l_d.mean(): -2.011435031890869 
model_pd.lagr.mean(): -1.954925537109375 
model_pd.lambdas: dict_items([('pout', tensor([1.2372])), ('power', tensor([0.0652]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4531])), ('power', tensor([-3.2337]))])
epoch：984	 i:0 	 global-step:19680	 l-p:0.05650944635272026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[24.9211, 28.9107, 29.8232],
        [24.9211, 26.2870, 25.8552],
        [24.9211, 24.9245, 24.9212],
        [24.9211, 24.9211, 24.9211]], grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.05649939179420471 
model_pd.l_d.mean(): -2.008814573287964 
model_pd.lagr.mean(): -1.9523152112960815 
model_pd.lambdas: dict_items([('pout', tensor([1.2358])), ('power', tensor([0.0651]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4550])), ('power', tensor([-3.1984]))])
epoch：985	 i:0 	 global-step:19700	 l-p:0.05649939179420471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[24.9562, 32.1879, 36.3776],
        [24.9562, 25.0554, 24.9694],
        [24.9562, 26.0130, 25.5730],
        [24.9562, 24.9825, 24.9578]], grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.056489523500204086 
model_pd.l_d.mean(): -2.006199359893799 
model_pd.lagr.mean(): -1.9497098922729492 
model_pd.lambdas: dict_items([('pout', tensor([1.2343])), ('power', tensor([0.0649]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4568])), ('power', tensor([-3.1637]))])
epoch：986	 i:0 	 global-step:19720	 l-p:0.056489523500204086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[24.9907, 24.9908, 24.9907],
        [24.9907, 33.7355, 39.8072],
        [24.9907, 25.0382, 24.9948],
        [24.9907, 26.6153, 26.2211]], grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.05647985637187958 
model_pd.l_d.mean(): -2.003588914871216 
model_pd.lagr.mean(): -1.9471091032028198 
model_pd.lambdas: dict_items([('pout', tensor([1.2328])), ('power', tensor([0.0648]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4586])), ('power', tensor([-3.1295]))])
epoch：987	 i:0 	 global-step:19740	 l-p:0.05647985637187958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.0246, 28.0137, 28.2027],
        [25.0246, 25.9198, 25.4948],
        [25.0246, 25.0612, 25.0273],
        [25.0246, 25.0248, 25.0246]], grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.056470371782779694 
model_pd.l_d.mean(): -2.000983953475952 
model_pd.lagr.mean(): -1.9445135593414307 
model_pd.lambdas: dict_items([('pout', tensor([1.2314])), ('power', tensor([0.0646]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4604])), ('power', tensor([-3.0959]))])
epoch：988	 i:0 	 global-step:19760	 l-p:0.056470371782779694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[25.0580, 30.2312, 32.1816],
        [25.0580, 25.0845, 25.0596],
        [25.0580, 26.4322, 25.9978],
        [25.0580, 25.3837, 25.1481]], grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.056461066007614136 
model_pd.l_d.mean(): -1.9983831644058228 
model_pd.lagr.mean(): -1.9419220685958862 
model_pd.lambdas: dict_items([('pout', tensor([1.2299])), ('power', tensor([0.0645]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4621])), ('power', tensor([-3.0629]))])
epoch：989	 i:0 	 global-step:19780	 l-p:0.056461066007614136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[25.0908, 33.2496, 38.5459],
        [25.0908, 25.7045, 25.3444],
        [25.0908, 28.8350, 29.5392],
        [25.0908, 25.2613, 25.1222]], grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): 0.056451939046382904 
model_pd.l_d.mean(): -1.995787262916565 
model_pd.lagr.mean(): -1.9393353462219238 
model_pd.lambdas: dict_items([('pout', tensor([1.2285])), ('power', tensor([0.0643]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4639])), ('power', tensor([-3.0304]))])
epoch：990	 i:0 	 global-step:19800	 l-p:0.056451939046382904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[25.1231, 28.1250, 28.3149],
        [25.1231, 25.1247, 25.1231],
        [25.1231, 25.3153, 25.1612],
        [25.1231, 33.2930, 38.5968]], grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.056442998349666595 
model_pd.l_d.mean(): -1.993195652961731 
model_pd.lagr.mean(): -1.9367526769638062 
model_pd.lambdas: dict_items([('pout', tensor([1.2270])), ('power', tensor([0.0642]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4655])), ('power', tensor([-2.9985]))])
epoch：991	 i:0 	 global-step:19820	 l-p:0.056442998349666595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[25.1548, 25.1548, 25.1548],
        [25.1548, 29.1850, 30.1073],
        [25.1548, 26.2209, 25.7771],
        [25.1548, 25.3258, 25.1863]], grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.056434229016304016 
model_pd.l_d.mean(): -1.9906080961227417 
model_pd.lagr.mean(): -1.934173822402954 
model_pd.lambdas: dict_items([('pout', tensor([1.2255])), ('power', tensor([0.0640]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4672])), ('power', tensor([-2.9671]))])
epoch：992	 i:0 	 global-step:19840	 l-p:0.056434229016304016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[25.1859, 25.1952, 25.1863],
        [25.1859, 25.1859, 25.1859],
        [25.1859, 25.9202, 25.5248],
        [25.1859, 25.1864, 25.1859]], grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.05642563849687576 
model_pd.l_d.mean(): -1.9880245923995972 
model_pd.lagr.mean(): -1.9315989017486572 
model_pd.lambdas: dict_items([('pout', tensor([1.2241])), ('power', tensor([0.0639]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4688])), ('power', tensor([-2.9362]))])
epoch：993	 i:0 	 global-step:19860	 l-p:0.05642563849687576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[25.2166, 26.0653, 25.6455],
        [25.2166, 25.5348, 25.3029],
        [25.2166, 25.2179, 25.2166],
        [25.2166, 25.9518, 25.5559]], grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.05641721561551094 
model_pd.l_d.mean(): -1.9854451417922974 
model_pd.lagr.mean(): -1.9290279150009155 
model_pd.lambdas: dict_items([('pout', tensor([1.2226])), ('power', tensor([0.0637]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4704])), ('power', tensor([-2.9059]))])
epoch：994	 i:0 	 global-step:19880	 l-p:0.05641721561551094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[25.2466, 32.5686, 36.8114],
        [25.2466, 25.2483, 25.2467],
        [25.2466, 25.5653, 25.3332],
        [25.2466, 25.2947, 25.2508]], grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.05640896409749985 
model_pd.l_d.mean(): -1.9828693866729736 
model_pd.lagr.mean(): -1.9264603853225708 
model_pd.lambdas: dict_items([('pout', tensor([1.2211])), ('power', tensor([0.0636]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4720])), ('power', tensor([-2.8760]))])
epoch：995	 i:0 	 global-step:19900	 l-p:0.05640896409749985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[25.2762, 25.3768, 25.2896],
        [25.2762, 27.7605, 27.6566],
        [25.2762, 25.2763, 25.2762],
        [25.2762, 25.6022, 25.3659]], grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.056400880217552185 
model_pd.l_d.mean(): -1.9802969694137573 
model_pd.lagr.mean(): -1.923896074295044 
model_pd.lambdas: dict_items([('pout', tensor([1.2196])), ('power', tensor([0.0634]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4735])), ('power', tensor([-2.8468]))])
epoch：996	 i:0 	 global-step:19920	 l-p:0.056400880217552185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[25.3052, 25.3053, 25.3052],
        [25.3052, 28.1434, 28.2241],
        [25.3052, 29.5940, 30.7139],
        [25.3052, 25.4774, 25.3370]], grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.05639295652508736 
model_pd.l_d.mean(): -1.9777287244796753 
model_pd.lagr.mean(): -1.9213358163833618 
model_pd.lambdas: dict_items([('pout', tensor([1.2182])), ('power', tensor([0.0633]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4750])), ('power', tensor([-2.8180]))])
epoch：997	 i:0 	 global-step:19940	 l-p:0.05639295652508736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[25.3338, 30.1515, 31.7331],
        [25.3338, 25.3338, 25.3338],
        [25.3338, 25.3338, 25.3338],
        [25.3338, 25.3352, 25.3338]], grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.05638519302010536 
model_pd.l_d.mean(): -1.9751631021499634 
model_pd.lagr.mean(): -1.9187779426574707 
model_pd.lambdas: dict_items([('pout', tensor([1.2167])), ('power', tensor([0.0632]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4765])), ('power', tensor([-2.7898]))])
epoch：998	 i:0 	 global-step:19960	 l-p:0.05638519302010536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[25.3618, 25.3989, 25.3645],
        [25.3618, 25.9829, 25.6185],
        [25.3618, 30.6403, 32.6532],
        [25.3618, 32.2579, 35.9851]], grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.056377582252025604 
model_pd.l_d.mean(): -1.9726011753082275 
model_pd.lagr.mean(): -1.9162236452102661 
model_pd.lambdas: dict_items([('pout', tensor([1.2152])), ('power', tensor([0.0630]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4779])), ('power', tensor([-2.7620]))])
epoch：999	 i:0 	 global-step:19980	 l-p:0.056377582252025604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.3893, 28.2378, 28.3189],
        [25.3893, 30.2184, 31.8040],
        [25.3893, 29.6935, 30.8177],
        [25.3893, 25.3894, 25.3893]], grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.05637013539671898 
model_pd.l_d.mean(): -1.970042109489441 
model_pd.lagr.mean(): -1.9136719703674316 
model_pd.lambdas: dict_items([('pout', tensor([1.2137])), ('power', tensor([0.0629]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4793])), ('power', tensor([-2.7348]))])
epoch：1000	 i:0 	 global-step:20000	 l-p:0.05637013539671898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[25.4163, 26.4947, 26.0459],
        [25.4163, 26.0389, 25.6736],
        [25.4163, 30.2510, 31.8385],
        [25.4163, 25.4535, 25.4190]], grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.056362833827733994 
model_pd.l_d.mean(): -1.967485785484314 
model_pd.lagr.mean(): -1.9111229181289673 
model_pd.lambdas: dict_items([('pout', tensor([1.2123])), ('power', tensor([0.0627]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4807])), ('power', tensor([-2.7080]))])
epoch：1001	 i:0 	 global-step:20020	 l-p:0.056362833827733994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[25.4428, 27.9451, 27.8407],
        [25.4428, 25.4428, 25.4428],
        [25.4428, 25.5441, 25.4562],
        [25.4428, 25.4449, 25.4428]], grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.05635569244623184 
model_pd.l_d.mean(): -1.9649327993392944 
model_pd.lagr.mean(): -1.9085770845413208 
model_pd.lambdas: dict_items([('pout', tensor([1.2108])), ('power', tensor([0.0626]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4821])), ('power', tensor([-2.6818]))])
epoch：1002	 i:0 	 global-step:20040	 l-p:0.05635569244623184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[25.4688, 25.4688, 25.4688],
        [25.4688, 34.3922, 40.5897],
        [25.4688, 26.0929, 25.7267],
        [25.4688, 30.7773, 32.8054]], grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.05634869635105133 
model_pd.l_d.mean(): -1.962382197380066 
model_pd.lagr.mean(): -1.9060335159301758 
model_pd.lambdas: dict_items([('pout', tensor([1.2093])), ('power', tensor([0.0625]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4834])), ('power', tensor([-2.6560]))])
epoch：1003	 i:0 	 global-step:20060	 l-p:0.05634869635105133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[25.4943, 25.5037, 25.4946],
        [25.4943, 30.8027, 32.8274],
        [25.4943, 32.8934, 37.1817],
        [25.4943, 25.4959, 25.4943]], grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.05634183809161186 
model_pd.l_d.mean(): -1.959834337234497 
model_pd.lagr.mean(): -1.9034924507141113 
model_pd.lambdas: dict_items([('pout', tensor([1.2078])), ('power', tensor([0.0623]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4847])), ('power', tensor([-2.6307]))])
epoch：1004	 i:0 	 global-step:20080	 l-p:0.05634183809161186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[25.5193, 25.6211, 25.5329],
        [25.5193, 25.5680, 25.5235],
        [25.5193, 29.6133, 30.5509],
        [25.5193, 29.8476, 30.9784]], grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.056335125118494034 
model_pd.l_d.mean(): -1.9572887420654297 
model_pd.lagr.mean(): -1.9009536504745483 
model_pd.lambdas: dict_items([('pout', tensor([1.2063])), ('power', tensor([0.0622]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4860])), ('power', tensor([-2.6059]))])
epoch：1005	 i:0 	 global-step:20100	 l-p:0.056335125118494034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[25.5439, 25.5473, 25.5440],
        [25.5439, 32.4933, 36.2499],
        [25.5439, 32.9585, 37.2559],
        [25.5439, 25.5439, 25.5439]], grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.056328557431697845 
model_pd.l_d.mean(): -1.9547454118728638 
model_pd.lagr.mean(): -1.8984168767929077 
model_pd.lambdas: dict_items([('pout', tensor([1.2048])), ('power', tensor([0.0621]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4873])), ('power', tensor([-2.5816]))])
epoch：1006	 i:0 	 global-step:20120	 l-p:0.056328557431697845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[25.5680, 28.2617, 28.2467],
        [25.5680, 25.5681, 25.5680],
        [25.5680, 28.3480, 28.3794],
        [25.5680, 26.9734, 26.5295]], grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.0563221275806427 
model_pd.l_d.mean(): -1.9522050619125366 
model_pd.lagr.mean(): -1.8958829641342163 
model_pd.lambdas: dict_items([('pout', tensor([1.2033])), ('power', tensor([0.0620]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4885])), ('power', tensor([-2.5577]))])
epoch：1007	 i:0 	 global-step:20140	 l-p:0.0563221275806427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[25.5917, 25.5917, 25.5917],
        [25.5917, 30.9221, 32.9554],
        [25.5917, 25.5939, 25.5917],
        [25.5917, 28.6551, 28.8497]], grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.0563158318400383 
model_pd.l_d.mean(): -1.9496662616729736 
model_pd.lagr.mean(): -1.8933504819869995 
model_pd.lambdas: dict_items([('pout', tensor([1.2019])), ('power', tensor([0.0618]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4897])), ('power', tensor([-2.5343]))])
epoch：1008	 i:0 	 global-step:20160	 l-p:0.0563158318400383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[25.6148, 28.6813, 28.8762],
        [25.6148, 27.2846, 26.8799],
        [25.6148, 25.6149, 25.6148],
        [25.6148, 29.7256, 30.6672]], grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.056309666484594345 
model_pd.l_d.mean(): -1.947129726409912 
model_pd.lagr.mean(): -1.890820026397705 
model_pd.lambdas: dict_items([('pout', tensor([1.2004])), ('power', tensor([0.0617]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4909])), ('power', tensor([-2.5113]))])
epoch：1009	 i:0 	 global-step:20180	 l-p:0.056309666484594345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[25.6376, 26.5023, 26.0747],
        [25.6376, 25.6398, 25.6376],
        [25.6376, 28.1611, 28.0561],
        [25.6376, 26.5573, 26.1209]], grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.056303638964891434 
model_pd.l_d.mean(): -1.9445953369140625 
model_pd.lagr.mean(): -1.8882917165756226 
model_pd.lambdas: dict_items([('pout', tensor([1.1989])), ('power', tensor([0.0616]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4921])), ('power', tensor([-2.4888]))])
epoch：1010	 i:0 	 global-step:20200	 l-p:0.056303638964891434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[25.6599, 32.6433, 36.4187],
        [25.6599, 25.6615, 25.6599],
        [25.6599, 28.5418, 28.6243],
        [25.6599, 25.9945, 25.7525]], grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.05629773065447807 
model_pd.l_d.mean(): -1.9420630931854248 
model_pd.lagr.mean(): -1.8857653141021729 
model_pd.lambdas: dict_items([('pout', tensor([1.1974])), ('power', tensor([0.0615]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4932])), ('power', tensor([-2.4667]))])
epoch：1011	 i:0 	 global-step:20220	 l-p:0.05629773065447807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[25.6817, 26.4322, 26.0282],
        [25.6817, 25.6912, 25.6821],
        [25.6817, 26.0136, 25.7731],
        [25.6817, 25.8788, 25.7208]], grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.056291960179805756 
model_pd.l_d.mean(): -1.9395325183868408 
model_pd.lagr.mean(): -1.8832405805587769 
model_pd.lambdas: dict_items([('pout', tensor([1.1959])), ('power', tensor([0.0613]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4943])), ('power', tensor([-2.4450]))])
epoch：1012	 i:0 	 global-step:20240	 l-p:0.056291960179805756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[25.7031, 25.7127, 25.7035],
        [25.7031, 30.5973, 32.2050],
        [25.7031, 31.0648, 33.1138],
        [25.7031, 25.7031, 25.7031]], grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.056286316365003586 
model_pd.l_d.mean(): -1.9370038509368896 
model_pd.lagr.mean(): -1.8807175159454346 
model_pd.lambdas: dict_items([('pout', tensor([1.1944])), ('power', tensor([0.0612]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4954])), ('power', tensor([-2.4238]))])
epoch：1013	 i:0 	 global-step:20260	 l-p:0.056286316365003586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[25.7241, 26.8173, 26.3625],
        [25.7241, 25.7619, 25.7269],
        [25.7241, 26.4760, 26.0713],
        [25.7241, 27.4018, 26.9953]], grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.05628078803420067 
model_pd.l_d.mean(): -1.9344768524169922 
model_pd.lagr.mean(): -1.878196120262146 
model_pd.lambdas: dict_items([('pout', tensor([1.1929])), ('power', tensor([0.0611]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4965])), ('power', tensor([-2.4030]))])
epoch：1014	 i:0 	 global-step:20280	 l-p:0.05628078803420067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[25.7447, 25.7939, 25.7489],
        [25.7447, 31.1158, 33.1686],
        [25.7447, 28.8284, 29.0246],
        [25.7447, 31.1098, 33.1569]], grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.056275371462106705 
model_pd.l_d.mean(): -1.931951642036438 
model_pd.lagr.mean(): -1.8756762742996216 
model_pd.lambdas: dict_items([('pout', tensor([1.1914])), ('power', tensor([0.0610]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4975])), ('power', tensor([-2.3826]))])
epoch：1015	 i:0 	 global-step:20300	 l-p:0.056275371462106705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[25.7649, 25.7649, 25.7649],
        [25.7649, 25.8677, 25.7785],
        [25.7649, 25.7649, 25.7649],
        [25.7649, 28.5686, 28.6006]], grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.056270089000463486 
model_pd.l_d.mean(): -1.929428219795227 
model_pd.lagr.mean(): -1.8731580972671509 
model_pd.lambdas: dict_items([('pout', tensor([1.1899])), ('power', tensor([0.0609]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4985])), ('power', tensor([-2.3626]))])
epoch：1016	 i:0 	 global-step:20320	 l-p:0.056270089000463486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[25.7846, 25.7942, 25.7850],
        [25.7846, 32.8048, 36.6005],
        [25.7846, 29.6422, 30.3692],
        [25.7846, 30.6958, 32.3093]], grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.056264907121658325 
model_pd.l_d.mean(): -1.926905632019043 
model_pd.lagr.mean(): -1.870640754699707 
model_pd.lambdas: dict_items([('pout', tensor([1.1884])), ('power', tensor([0.0607]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4996])), ('power', tensor([-2.3430]))])
epoch：1017	 i:0 	 global-step:20340	 l-p:0.056264907121658325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[25.8040, 25.8314, 25.8056],
        [25.8040, 32.9705, 36.9309],
        [25.8040, 25.8040, 25.8040],
        [25.8040, 25.8062, 25.8040]], grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.05625985190272331 
model_pd.l_d.mean(): -1.9243848323822021 
model_pd.lagr.mean(): -1.8681249618530273 
model_pd.lambdas: dict_items([('pout', tensor([1.1869])), ('power', tensor([0.0606]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5005])), ('power', tensor([-2.3239]))])
epoch：1018	 i:0 	 global-step:20360	 l-p:0.05625985190272331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[25.8229, 32.9952, 36.9588],
        [25.8229, 26.1570, 25.9149],
        [25.8229, 27.5840, 27.1927],
        [25.8229, 32.8544, 36.6563]], grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.05625489726662636 
model_pd.l_d.mean(): -1.9218655824661255 
model_pd.lagr.mean(): -1.8656107187271118 
model_pd.lambdas: dict_items([('pout', tensor([1.1854])), ('power', tensor([0.0605]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5015])), ('power', tensor([-2.3051]))])
epoch：1019	 i:0 	 global-step:20380	 l-p:0.05625489726662636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[25.8415, 29.7085, 30.4374],
        [25.8415, 31.2346, 33.2962],
        [25.8415, 25.9447, 25.8552],
        [25.8415, 26.5973, 26.1905]], grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.056250058114528656 
model_pd.l_d.mean(): -1.9193475246429443 
model_pd.lagr.mean(): -1.8630974292755127 
model_pd.lambdas: dict_items([('pout', tensor([1.1839])), ('power', tensor([0.0604]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5024])), ('power', tensor([-2.2867]))])
epoch：1020	 i:0 	 global-step:20400	 l-p:0.056250058114528656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[25.8597, 26.7329, 26.3012],
        [25.8597, 26.1973, 25.9532],
        [25.8597, 25.9091, 25.8639],
        [25.8597, 34.9300, 41.2312]], grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.056245315819978714 
model_pd.l_d.mean(): -1.9168310165405273 
model_pd.lagr.mean(): -1.8605856895446777 
model_pd.lambdas: dict_items([('pout', tensor([1.1824])), ('power', tensor([0.0603]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5034])), ('power', tensor([-2.2686]))])
epoch：1021	 i:0 	 global-step:20420	 l-p:0.056245315819978714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[25.8775, 25.8775, 25.8775],
        [25.8775, 25.9270, 25.8817],
        [25.8775, 26.9780, 26.5203],
        [25.8775, 26.5130, 26.1402]], grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.05624069273471832 
model_pd.l_d.mean(): -1.9143153429031372 
model_pd.lagr.mean(): -1.85807466506958 
model_pd.lambdas: dict_items([('pout', tensor([1.1809])), ('power', tensor([0.0602]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5043])), ('power', tensor([-2.2510]))])
epoch：1022	 i:0 	 global-step:20440	 l-p:0.05624069273471832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[25.8949, 31.2560, 33.2795],
        [25.8949, 27.1331, 26.6723],
        [25.8949, 28.6269, 28.6122],
        [25.8949, 26.2348, 25.9894]], grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.05623617023229599 
model_pd.l_d.mean(): -1.9118009805679321 
model_pd.lagr.mean(): -1.855564832687378 
model_pd.lambdas: dict_items([('pout', tensor([1.1794])), ('power', tensor([0.0601]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5051])), ('power', tensor([-2.2337]))])
epoch：1023	 i:0 	 global-step:20460	 l-p:0.05623617023229599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[25.9120, 26.5485, 26.1751],
        [25.9120, 26.2402, 26.0011],
        [25.9120, 27.1511, 26.6900],
        [25.9120, 32.9698, 36.7863]], grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.05623173713684082 
model_pd.l_d.mean(): -1.909287691116333 
model_pd.lagr.mean(): -1.8530559539794922 
model_pd.lambdas: dict_items([('pout', tensor([1.1779])), ('power', tensor([0.0600]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5060])), ('power', tensor([-2.2168]))])
epoch：1024	 i:0 	 global-step:20480	 l-p:0.05623173713684082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[25.9287, 26.2673, 26.0225],
        [25.9287, 28.8443, 28.9282],
        [25.9287, 28.4842, 28.3783],
        [25.9287, 25.9783, 25.9330]], grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.056227415800094604 
model_pd.l_d.mean(): -1.9067753553390503 
model_pd.lagr.mean(): -1.8505479097366333 
model_pd.lambdas: dict_items([('pout', tensor([1.1764])), ('power', tensor([0.0598]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5068])), ('power', tensor([-2.2002]))])
epoch：1025	 i:0 	 global-step:20500	 l-p:0.056227415800094604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[25.9451, 27.3738, 26.9228],
        [25.9451, 25.9473, 25.9451],
        [25.9451, 26.0488, 25.9588],
        [25.9451, 28.8627, 28.9468]], grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.05622319132089615 
model_pd.l_d.mean(): -1.9042644500732422 
model_pd.lagr.mean(): -1.848041296005249 
model_pd.lambdas: dict_items([('pout', tensor([1.1749])), ('power', tensor([0.0597]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5077])), ('power', tensor([-2.1840]))])
epoch：1026	 i:0 	 global-step:20520	 l-p:0.05622319132089615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[25.9611, 28.5201, 28.4142],
        [25.9611, 27.7327, 27.3392],
        [25.9611, 25.9625, 25.9611],
        [25.9611, 34.4238, 39.9208]], grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.05621906369924545 
model_pd.l_d.mean(): -1.9017542600631714 
model_pd.lagr.mean(): -1.845535159111023 
model_pd.lambdas: dict_items([('pout', tensor([1.1733])), ('power', tensor([0.0596]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5085])), ('power', tensor([-2.1682]))])
epoch：1027	 i:0 	 global-step:20540	 l-p:0.05621906369924545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[25.9767, 25.9790, 25.9768],
        [25.9767, 30.9281, 32.5554],
        [25.9767, 26.0264, 25.9810],
        [25.9767, 25.9767, 25.9767]], grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.05621501803398132 
model_pd.l_d.mean(): -1.8992451429367065 
model_pd.lagr.mean(): -1.8430300951004028 
model_pd.lambdas: dict_items([('pout', tensor([1.1718])), ('power', tensor([0.0595]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5093])), ('power', tensor([-2.1526]))])
epoch：1028	 i:0 	 global-step:20560	 l-p:0.05621501803398132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[25.9921, 25.9921, 25.9921],
        [25.9921, 28.3064, 28.0892],
        [25.9921, 35.1124, 41.4490],
        [25.9921, 25.9938, 25.9921]], grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.05621108040213585 
model_pd.l_d.mean(): -1.896736741065979 
model_pd.lagr.mean(): -1.8405256271362305 
model_pd.lambdas: dict_items([('pout', tensor([1.1703])), ('power', tensor([0.0594]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5100])), ('power', tensor([-2.1374]))])
epoch：1029	 i:0 	 global-step:20580	 l-p:0.05621108040213585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.0071, 28.8403, 28.8730],
        [26.0071, 26.3438, 26.0998],
        [26.0071, 27.7056, 27.2944],
        [26.0071, 28.3229, 28.1056]], grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.056207217276096344 
model_pd.l_d.mean(): -1.8942291736602783 
model_pd.lagr.mean(): -1.838021993637085 
model_pd.lambdas: dict_items([('pout', tensor([1.1688])), ('power', tensor([0.0593]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5108])), ('power', tensor([-2.1226]))])
epoch：1030	 i:0 	 global-step:20600	 l-p:0.056207217276096344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[26.0217, 34.5059, 40.0171],
        [26.0217, 33.1121, 36.9466],
        [26.0217, 27.1293, 26.6687],
        [26.0217, 28.3390, 28.1217]], grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.0562034510076046 
model_pd.l_d.mean(): -1.891722559928894 
model_pd.lagr.mean(): -1.8355190753936768 
model_pd.lambdas: dict_items([('pout', tensor([1.1673])), ('power', tensor([0.0592]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5115])), ('power', tensor([-2.1080]))])
epoch：1031	 i:0 	 global-step:20620	 l-p:0.0562034510076046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.0361, 35.1732, 41.5215],
        [26.0361, 26.0361, 26.0361],
        [26.0361, 30.4608, 31.6182],
        [26.0361, 27.7368, 27.3251]], grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.05619977042078972 
model_pd.l_d.mean(): -1.889216423034668 
model_pd.lagr.mean(): -1.8330166339874268 
model_pd.lambdas: dict_items([('pout', tensor([1.1658])), ('power', tensor([0.0591]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5123])), ('power', tensor([-2.0938]))])
epoch：1032	 i:0 	 global-step:20640	 l-p:0.05619977042078972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.0501, 26.0598, 26.0505],
        [26.0501, 26.9307, 26.4954],
        [26.0501, 27.7519, 27.3399],
        [26.0501, 31.4851, 33.5598]], grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.056196168065071106 
model_pd.l_d.mean(): -1.8867113590240479 
model_pd.lagr.mean(): -1.8305151462554932 
model_pd.lambdas: dict_items([('pout', tensor([1.1643])), ('power', tensor([0.0590]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5130])), ('power', tensor([-2.0799]))])
epoch：1033	 i:0 	 global-step:20660	 l-p:0.056196168065071106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.0639, 26.0638, 26.0639],
        [26.0639, 30.4939, 31.6527],
        [26.0639, 26.4045, 26.1582],
        [26.0639, 26.8271, 26.4164]], grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.056192655116319656 
model_pd.l_d.mean(): -1.884206771850586 
model_pd.lagr.mean(): -1.8280141353607178 
model_pd.lambdas: dict_items([('pout', tensor([1.1628])), ('power', tensor([0.0589]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5137])), ('power', tensor([-2.0663]))])
epoch：1034	 i:0 	 global-step:20680	 l-p:0.056192655116319656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.0773, 26.4151, 26.1703],
        [26.0773, 28.4002, 28.1824],
        [26.0773, 34.5811, 40.1054],
        [26.0773, 29.0116, 29.0965]], grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.05618922784924507 
model_pd.l_d.mean(): -1.8817031383514404 
model_pd.lagr.mean(): -1.8255139589309692 
model_pd.lambdas: dict_items([('pout', tensor([1.1613])), ('power', tensor([0.0588]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5143])), ('power', tensor([-2.0529]))])
epoch：1035	 i:0 	 global-step:20700	 l-p:0.05618922784924507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.0904, 26.0909, 26.0904],
        [26.0904, 27.9601, 27.5866],
        [26.0904, 26.0919, 26.0904],
        [26.0904, 27.5283, 27.0746]], grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.05618587136268616 
model_pd.l_d.mean(): -1.8792002201080322 
model_pd.lagr.mean(): -1.8230143785476685 
model_pd.lambdas: dict_items([('pout', tensor([1.1597])), ('power', tensor([0.0587]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5150])), ('power', tensor([-2.0399]))])
epoch：1036	 i:0 	 global-step:20720	 l-p:0.05618587136268616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1032, 31.0814, 32.7179],
        [26.1032, 30.3007, 31.2635],
        [26.1032, 28.9483, 28.9815],
        [26.1032, 26.1037, 26.1033]], grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.05618259310722351 
model_pd.l_d.mean(): -1.8766975402832031 
model_pd.lagr.mean(): -1.8205149173736572 
model_pd.lambdas: dict_items([('pout', tensor([1.1582])), ('power', tensor([0.0586]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5156])), ('power', tensor([-2.0272]))])
epoch：1037	 i:0 	 global-step:20740	 l-p:0.05618259310722351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.1158, 26.4573, 26.2104],
        [26.1158, 29.2492, 29.4494],
        [26.1158, 31.5274, 33.5707],
        [26.1158, 26.1658, 26.1201]], grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.05617940053343773 
model_pd.l_d.mean(): -1.8741956949234009 
model_pd.lagr.mean(): -1.8180162906646729 
model_pd.lambdas: dict_items([('pout', tensor([1.1567])), ('power', tensor([0.0585]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5163])), ('power', tensor([-2.0148]))])
epoch：1038	 i:0 	 global-step:20760	 l-p:0.05617940053343773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.1281, 26.1281, 26.1281],
        [26.1281, 27.2409, 26.7782],
        [26.1281, 26.1281, 26.1281],
        [26.1281, 26.7707, 26.3938]], grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.056176282465457916 
model_pd.l_d.mean(): -1.871694803237915 
model_pd.lagr.mean(): -1.8155184984207153 
model_pd.lambdas: dict_items([('pout', tensor([1.1552])), ('power', tensor([0.0584]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5169])), ('power', tensor([-2.0026]))])
epoch：1039	 i:0 	 global-step:20780	 l-p:0.056176282465457916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1401, 26.2447, 26.1540],
        [26.1401, 30.3441, 31.3086],
        [26.1401, 26.1401, 26.1401],
        [26.1401, 27.0804, 26.6344]], grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.05617322027683258 
model_pd.l_d.mean(): -1.8691941499710083 
model_pd.lagr.mean(): -1.813020944595337 
model_pd.lambdas: dict_items([('pout', tensor([1.1537])), ('power', tensor([0.0583]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5175])), ('power', tensor([-1.9907]))])
epoch：1040	 i:0 	 global-step:20800	 l-p:0.05617322027683258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.1518, 26.1519, 26.1518],
        [26.1518, 28.4823, 28.2639],
        [26.1518, 26.3530, 26.1917],
        [26.1518, 26.1540, 26.1518]], grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.05617024749517441 
model_pd.l_d.mean(): -1.8666936159133911 
model_pd.lagr.mean(): -1.8105233907699585 
model_pd.lambdas: dict_items([('pout', tensor([1.1522])), ('power', tensor([0.0582]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5181])), ('power', tensor([-1.9791]))])
epoch：1041	 i:0 	 global-step:20820	 l-p:0.05617024749517441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.1632, 26.1632, 26.1632],
        [26.1632, 26.3422, 26.1962],
        [26.1632, 28.7449, 28.6385],
        [26.1632, 26.1634, 26.1632]], grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.056167345494031906 
model_pd.l_d.mean(): -1.8641941547393799 
model_pd.lagr.mean(): -1.8080267906188965 
model_pd.lambdas: dict_items([('pout', tensor([1.1506])), ('power', tensor([0.0581]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5186])), ('power', tensor([-1.9677]))])
epoch：1042	 i:0 	 global-step:20840	 l-p:0.056167345494031906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.1744, 30.6255, 31.7902],
        [26.1744, 26.1744, 26.1744],
        [26.1744, 30.0971, 30.8375],
        [26.1744, 27.0599, 26.6223]], grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.05616450309753418 
model_pd.l_d.mean(): -1.8616948127746582 
model_pd.lagr.mean(): -1.805530309677124 
model_pd.lambdas: dict_items([('pout', tensor([1.1491])), ('power', tensor([0.0580]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5192])), ('power', tensor([-1.9566]))])
epoch：1043	 i:0 	 global-step:20860	 l-p:0.05616450309753418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.1854, 28.5193, 28.3007],
        [26.1854, 31.6130, 33.6627],
        [26.1854, 26.1854, 26.1854],
        [26.1854, 26.3869, 26.2254]], grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.056161731481552124 
model_pd.l_d.mean(): -1.859196424484253 
model_pd.lagr.mean(): -1.8030346632003784 
model_pd.lambdas: dict_items([('pout', tensor([1.1476])), ('power', tensor([0.0579]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5198])), ('power', tensor([-1.9458]))])
epoch：1044	 i:0 	 global-step:20880	 l-p:0.056161731481552124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.1960, 26.5388, 26.2910],
        [26.1960, 28.5311, 28.3124],
        [26.1960, 26.5406, 26.2918],
        [26.1960, 26.1965, 26.1960]], grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.05615903064608574 
model_pd.l_d.mean(): -1.8566980361938477 
model_pd.lagr.mean(): -1.8005390167236328 
model_pd.lambdas: dict_items([('pout', tensor([1.1461])), ('power', tensor([0.0578]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5203])), ('power', tensor([-1.9352]))])
epoch：1045	 i:0 	 global-step:20900	 l-p:0.05615903064608574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.2065, 31.6778, 33.7670],
        [26.2065, 26.2087, 26.2065],
        [26.2065, 30.1346, 30.8761],
        [26.2065, 26.5391, 26.2969]], grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.05615639314055443 
model_pd.l_d.mean(): -1.8541998863220215 
model_pd.lagr.mean(): -1.7980434894561768 
model_pd.lambdas: dict_items([('pout', tensor([1.1446])), ('power', tensor([0.0577]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5208])), ('power', tensor([-1.9248]))])
epoch：1046	 i:0 	 global-step:20920	 l-p:0.05615639314055443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.2167, 26.2168, 26.2167],
        [26.2167, 28.0969, 27.7215],
        [26.2167, 26.3961, 26.2498],
        [26.2167, 26.2264, 26.2170]], grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.056153807789087296 
model_pd.l_d.mean(): -1.8517025709152222 
model_pd.lagr.mean(): -1.7955487966537476 
model_pd.lambdas: dict_items([('pout', tensor([1.1430])), ('power', tensor([0.0576]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5213])), ('power', tensor([-1.9147]))])
epoch：1047	 i:0 	 global-step:20940	 l-p:0.056153807789087296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.2266, 26.2653, 26.2295],
        [26.2266, 29.1802, 29.2661],
        [26.2266, 26.4286, 26.2667],
        [26.2266, 26.2268, 26.2266]], grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.056151293218135834 
model_pd.l_d.mean(): -1.8492052555084229 
model_pd.lagr.mean(): -1.7930539846420288 
model_pd.lambdas: dict_items([('pout', tensor([1.1415])), ('power', tensor([0.0575]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5218])), ('power', tensor([-1.9049]))])
epoch：1048	 i:0 	 global-step:20960	 l-p:0.056151293218135834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[26.2364, 29.1912, 29.2771],
        [26.2364, 27.3546, 26.8897],
        [26.2364, 28.5756, 28.3566],
        [26.2364, 31.6759, 33.7303]], grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.056148841977119446 
model_pd.l_d.mean(): -1.8467086553573608 
model_pd.lagr.mean(): -1.7905597686767578 
model_pd.lambdas: dict_items([('pout', tensor([1.1400])), ('power', tensor([0.0574]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5223])), ('power', tensor([-1.8952]))])
epoch：1049	 i:0 	 global-step:20980	 l-p:0.056148841977119446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.2459, 26.2962, 26.2502],
        [26.2459, 31.7264, 33.8194],
        [26.2459, 29.1088, 29.1425],
        [26.2459, 30.4693, 31.4386]], grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.05614645406603813 
model_pd.l_d.mean(): -1.8442124128341675 
model_pd.lagr.mean(): -1.7880659103393555 
model_pd.lambdas: dict_items([('pout', tensor([1.1385])), ('power', tensor([0.0573]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5228])), ('power', tensor([-1.8858]))])
epoch：1050	 i:0 	 global-step:21000	 l-p:0.05614645406603813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[26.2551, 29.0304, 29.0163],
        [26.2551, 27.0251, 26.6108],
        [26.2551, 27.7037, 27.2468],
        [26.2551, 30.1916, 30.9349]], grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.0561441145837307 
model_pd.l_d.mean(): -1.8417164087295532 
model_pd.lagr.mean(): -1.7855722904205322 
model_pd.lambdas: dict_items([('pout', tensor([1.1369])), ('power', tensor([0.0572]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5232])), ('power', tensor([-1.8766]))])
epoch：1051	 i:0 	 global-step:21020	 l-p:0.0561441145837307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.2642, 26.2642, 26.2642],
        [26.2642, 31.7491, 33.8438],
        [26.2642, 26.2659, 26.2642],
        [26.2642, 26.2642, 26.2642]], grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.05614183098077774 
model_pd.l_d.mean(): -1.839220404624939 
model_pd.lagr.mean(): -1.7830785512924194 
model_pd.lambdas: dict_items([('pout', tensor([1.1354])), ('power', tensor([0.0571]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5237])), ('power', tensor([-1.8676]))])
epoch：1052	 i:0 	 global-step:21040	 l-p:0.05614183098077774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.2730, 30.7432, 31.9133],
        [26.2730, 26.2753, 26.2731],
        [26.2730, 26.2730, 26.2730],
        [26.2730, 28.6161, 28.3969]], grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.05613960698246956 
model_pd.l_d.mean(): -1.8367252349853516 
model_pd.lagr.mean(): -1.7805856466293335 
model_pd.lambdas: dict_items([('pout', tensor([1.1339])), ('power', tensor([0.0570]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5241])), ('power', tensor([-1.8589]))])
epoch：1053	 i:0 	 global-step:21060	 l-p:0.05613960698246956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.2816, 28.6257, 28.4064],
        [26.2816, 26.2817, 26.2817],
        [26.2816, 26.4841, 26.3219],
        [26.2816, 26.2816, 26.2817]], grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.05613744631409645 
model_pd.l_d.mean(): -1.8342301845550537 
model_pd.lagr.mean(): -1.7780927419662476 
model_pd.lambdas: dict_items([('pout', tensor([1.1324])), ('power', tensor([0.0569]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5246])), ('power', tensor([-1.8503]))])
epoch：1054	 i:0 	 global-step:21080	 l-p:0.05613744631409645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.2901, 27.5502, 27.0816],
        [26.2901, 26.2923, 26.2901],
        [26.2901, 26.2901, 26.2901],
        [26.2901, 30.5217, 31.4931]], grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.056135326623916626 
model_pd.l_d.mean(): -1.8317354917526245 
model_pd.lagr.mean(): -1.7756001949310303 
model_pd.lambdas: dict_items([('pout', tensor([1.1308])), ('power', tensor([0.0568]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5250])), ('power', tensor([-1.8419]))])
epoch：1055	 i:0 	 global-step:21100	 l-p:0.056135326623916626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.2983, 26.2983, 26.2983],
        [26.2983, 26.3370, 26.3011],
        [26.2983, 26.2988, 26.2983],
        [26.2983, 33.6155, 37.6615]], grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.05613326653838158 
model_pd.l_d.mean(): -1.8292407989501953 
model_pd.lagr.mean(): -1.7731075286865234 
model_pd.lambdas: dict_items([('pout', tensor([1.1293])), ('power', tensor([0.0568]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5254])), ('power', tensor([-1.8338]))])
epoch：1056	 i:0 	 global-step:21120	 l-p:0.05613326653838158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.3063, 29.1770, 29.2110],
        [26.3063, 30.7831, 31.9551],
        [26.3063, 26.3063, 26.3063],
        [26.3063, 35.5471, 41.9693]], grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.05613124743103981 
model_pd.l_d.mean(): -1.8267465829849243 
model_pd.lagr.mean(): -1.7706153392791748 
model_pd.lambdas: dict_items([('pout', tensor([1.1278])), ('power', tensor([0.0567]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5258])), ('power', tensor([-1.8258]))])
epoch：1057	 i:0 	 global-step:21140	 l-p:0.05613124743103981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.3141, 27.4364, 26.9699],
        [26.3141, 34.9029, 40.4839],
        [26.3141, 27.2054, 26.7650],
        [26.3141, 26.3529, 26.3170]], grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.05612928792834282 
model_pd.l_d.mean(): -1.8242526054382324 
model_pd.lagr.mean(): -1.7681232690811157 
model_pd.lambdas: dict_items([('pout', tensor([1.1263])), ('power', tensor([0.0566]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5262])), ('power', tensor([-1.8181]))])
epoch：1058	 i:0 	 global-step:21160	 l-p:0.05612928792834282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.3218, 28.1221, 27.7228],
        [26.3218, 26.4273, 26.3358],
        [26.3218, 34.9133, 40.4962],
        [26.3218, 27.5838, 27.1146]], grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.05612737312912941 
model_pd.l_d.mean(): -1.82175874710083 
model_pd.lagr.mean(): -1.7656313180923462 
model_pd.lambdas: dict_items([('pout', tensor([1.1247])), ('power', tensor([0.0565]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5266])), ('power', tensor([-1.8105]))])
epoch：1059	 i:0 	 global-step:21180	 l-p:0.05612737312912941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.3292, 28.6783, 28.4587],
        [26.3292, 30.2786, 31.0248],
        [26.3292, 26.3292, 26.3292],
        [26.3292, 26.5096, 26.3625]], grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.05612551048398018 
model_pd.l_d.mean(): -1.8192652463912964 
model_pd.lagr.mean(): -1.7631397247314453 
model_pd.lambdas: dict_items([('pout', tensor([1.1232])), ('power', tensor([0.0564]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5269])), ('power', tensor([-1.8031]))])
epoch：1060	 i:0 	 global-step:21200	 l-p:0.05612551048398018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.3365, 28.1381, 27.7385],
        [26.3365, 29.3046, 29.3913],
        [26.3365, 26.3366, 26.3365],
        [26.3365, 31.8387, 33.9405]], grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.05612369626760483 
model_pd.l_d.mean(): -1.8167718648910522 
model_pd.lagr.mean(): -1.7606481313705444 
model_pd.lambdas: dict_items([('pout', tensor([1.1217])), ('power', tensor([0.0563]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5273])), ('power', tensor([-1.7959]))])
epoch：1061	 i:0 	 global-step:21220	 l-p:0.05612369626760483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.3436, 26.3450, 26.3436],
        [26.3436, 26.3435, 26.3436],
        [26.3436, 26.3436, 26.3436],
        [26.3436, 26.5466, 26.3839]], grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.05612192302942276 
model_pd.l_d.mean(): -1.8142787218093872 
model_pd.lagr.mean(): -1.7581567764282227 
model_pd.lambdas: dict_items([('pout', tensor([1.1202])), ('power', tensor([0.0562]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5277])), ('power', tensor([-1.7889]))])
epoch：1062	 i:0 	 global-step:21240	 l-p:0.05612192302942276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.3505, 31.3822, 33.0376],
        [26.3505, 29.2270, 29.2612],
        [26.3505, 33.6841, 37.7396],
        [26.3505, 26.6926, 26.4447]], grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.056120194494724274 
model_pd.l_d.mean(): -1.8117859363555908 
model_pd.lagr.mean(): -1.7556657791137695 
model_pd.lambdas: dict_items([('pout', tensor([1.1186])), ('power', tensor([0.0561]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5280])), ('power', tensor([-1.7820]))])
epoch：1063	 i:0 	 global-step:21260	 l-p:0.056120194494724274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.3572, 26.3961, 26.3601],
        [26.3572, 29.3282, 29.4150],
        [26.3572, 27.6214, 27.1514],
        [26.3572, 34.0298, 38.4807]], grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.05611851066350937 
model_pd.l_d.mean(): -1.8092933893203735 
model_pd.lagr.mean(): -1.753174901008606 
model_pd.lambdas: dict_items([('pout', tensor([1.1171])), ('power', tensor([0.0560]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5283])), ('power', tensor([-1.7753]))])
epoch：1064	 i:0 	 global-step:21280	 l-p:0.05611851066350937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.3638, 26.4144, 26.3681],
        [26.3638, 26.3919, 26.3655],
        [26.3638, 28.2567, 27.8791],
        [26.3638, 26.7062, 26.4581]], grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.05611686781048775 
model_pd.l_d.mean(): -1.8068006038665771 
model_pd.lagr.mean(): -1.7506837844848633 
model_pd.lambdas: dict_items([('pout', tensor([1.1156])), ('power', tensor([0.0559]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5287])), ('power', tensor([-1.7688]))])
epoch：1065	 i:0 	 global-step:21300	 l-p:0.05611686781048775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.3702, 26.4091, 26.3730],
        [26.3702, 26.3983, 26.3719],
        [26.3702, 26.3702, 26.3702],
        [26.3702, 27.3206, 26.8700]], grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.056115273386240005 
model_pd.l_d.mean(): -1.8043081760406494 
model_pd.lagr.mean(): -1.7481929063796997 
model_pd.lambdas: dict_items([('pout', tensor([1.1140])), ('power', tensor([0.0559]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5290])), ('power', tensor([-1.7625]))])
epoch：1066	 i:0 	 global-step:21320	 l-p:0.056115273386240005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.3764, 26.4153, 26.3793],
        [26.3764, 28.2705, 27.8927],
        [26.3764, 26.3764, 26.3764],
        [26.3764, 26.4822, 26.3905]], grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.05611371994018555 
model_pd.l_d.mean(): -1.8018161058425903 
model_pd.lagr.mean(): -1.7457023859024048 
model_pd.lambdas: dict_items([('pout', tensor([1.1125])), ('power', tensor([0.0558]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5293])), ('power', tensor([-1.7563]))])
epoch：1067	 i:0 	 global-step:21340	 l-p:0.05611371994018555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.3825, 31.4215, 33.0795],
        [26.3825, 26.3825, 26.3825],
        [26.3825, 26.3825, 26.3825],
        [26.3825, 34.9965, 40.5945]], grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.05611220374703407 
model_pd.l_d.mean(): -1.7993241548538208 
model_pd.lagr.mean(): -1.7432119846343994 
model_pd.lambdas: dict_items([('pout', tensor([1.1110])), ('power', tensor([0.0557]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5296])), ('power', tensor([-1.7502]))])
epoch：1068	 i:0 	 global-step:21360	 l-p:0.05611220374703407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[26.3884, 29.1807, 29.1670],
        [26.3884, 29.3638, 29.4509],
        [26.3884, 30.8817, 32.0586],
        [26.3884, 28.1945, 27.7941]], grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.056110724806785583 
model_pd.l_d.mean(): -1.7968323230743408 
model_pd.lagr.mean(): -1.740721583366394 
model_pd.lambdas: dict_items([('pout', tensor([1.1095])), ('power', tensor([0.0556]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5299])), ('power', tensor([-1.7444]))])
epoch：1069	 i:0 	 global-step:21380	 l-p:0.056110724806785583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.3942, 34.0792, 38.5377],
        [26.3942, 26.3942, 26.3942],
        [26.3942, 29.1872, 29.1736],
        [26.3942, 30.8887, 32.0659]], grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.05610927939414978 
model_pd.l_d.mean(): -1.7943404912948608 
model_pd.lagr.mean(): -1.7382311820983887 
model_pd.lambdas: dict_items([('pout', tensor([1.1079])), ('power', tensor([0.0555]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5302])), ('power', tensor([-1.7386]))])
epoch：1070	 i:0 	 global-step:21400	 l-p:0.05610927939414978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.3998, 35.0203, 40.6228],
        [26.3998, 26.3998, 26.3998],
        [26.3998, 26.3998, 26.3998],
        [26.3998, 35.6776, 42.1266]], grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.05610787868499756 
model_pd.l_d.mean(): -1.7918490171432495 
model_pd.lagr.mean(): -1.735741138458252 
model_pd.lambdas: dict_items([('pout', tensor([1.1064])), ('power', tensor([0.0554]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5305])), ('power', tensor([-1.7330]))])
epoch：1071	 i:0 	 global-step:21420	 l-p:0.05610787868499756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.4053, 26.4443, 26.4082],
        [26.4053, 27.1808, 26.7637],
        [26.4053, 28.3020, 27.9238],
        [26.4053, 29.1998, 29.1862]], grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.05610651895403862 
model_pd.l_d.mean(): -1.7893575429916382 
model_pd.lagr.mean(): -1.7332509756088257 
model_pd.lambdas: dict_items([('pout', tensor([1.1049])), ('power', tensor([0.0553]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5307])), ('power', tensor([-1.7276]))])
epoch：1072	 i:0 	 global-step:21440	 l-p:0.05610651895403862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.4106, 29.0215, 28.9147],
        [26.4106, 31.9373, 34.0526],
        [26.4106, 28.7688, 28.5486],
        [26.4106, 26.5166, 26.4247]], grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.05610518530011177 
model_pd.l_d.mean(): -1.786866307258606 
model_pd.lagr.mean(): -1.730761170387268 
model_pd.lambdas: dict_items([('pout', tensor([1.1033])), ('power', tensor([0.0553]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5310])), ('power', tensor([-1.7223]))])
epoch：1073	 i:0 	 global-step:21460	 l-p:0.05610518530011177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.4158, 34.1082, 38.5712],
        [26.4158, 33.7706, 37.8385],
        [26.4158, 30.6716, 31.6494],
        [26.4158, 26.4194, 26.4159]], grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.0561038963496685 
model_pd.l_d.mean(): -1.7843749523162842 
model_pd.lagr.mean(): -1.7282710075378418 
model_pd.lambdas: dict_items([('pout', tensor([1.1018])), ('power', tensor([0.0552]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5313])), ('power', tensor([-1.7172]))])
epoch：1074	 i:0 	 global-step:21480	 l-p:0.0561038963496685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.4209, 26.4491, 26.4226],
        [26.4209, 35.7072, 42.1624],
        [26.4209, 26.4716, 26.4252],
        [26.4209, 26.4307, 26.4212]], grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.05610263720154762 
model_pd.l_d.mean(): -1.7818840742111206 
model_pd.lagr.mean(): -1.7257814407348633 
model_pd.lambdas: dict_items([('pout', tensor([1.1003])), ('power', tensor([0.0551]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5315])), ('power', tensor([-1.7121]))])
epoch：1075	 i:0 	 global-step:21500	 l-p:0.05610263720154762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.4258, 31.9503, 34.0612],
        [26.4258, 27.3789, 26.9271],
        [26.4258, 26.4281, 26.4258],
        [26.4258, 26.4258, 26.4258]], grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.056101419031620026 
model_pd.l_d.mean(): -1.7793934345245361 
model_pd.lagr.mean(): -1.7232919931411743 
model_pd.lambdas: dict_items([('pout', tensor([1.0987])), ('power', tensor([0.0550]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5318])), ('power', tensor([-1.7072]))])
epoch：1076	 i:0 	 global-step:21520	 l-p:0.056101419031620026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.4306, 26.4311, 26.4306],
        [26.4306, 31.9563, 34.0678],
        [26.4306, 26.4329, 26.4306],
        [26.4306, 28.1623, 27.7438]], grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.05610022693872452 
model_pd.l_d.mean(): -1.7769025564193726 
model_pd.lagr.mean(): -1.7208023071289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0972])), ('power', tensor([0.0549]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5320])), ('power', tensor([-1.7025]))])
epoch：1077	 i:0 	 global-step:21540	 l-p:0.05610022693872452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.4353, 26.7717, 26.5267],
        [26.4353, 34.1344, 38.6015],
        [26.4353, 27.0874, 26.7050],
        [26.4353, 26.4388, 26.4353]], grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.056099068373441696 
model_pd.l_d.mean(): -1.7744117975234985 
model_pd.lagr.mean(): -1.7183127403259277 
model_pd.lambdas: dict_items([('pout', tensor([1.0957])), ('power', tensor([0.0548]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5322])), ('power', tensor([-1.6979]))])
epoch：1078	 i:0 	 global-step:21560	 l-p:0.056099068373441696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[26.4398, 27.5691, 27.0998],
        [26.4398, 27.3366, 26.8936],
        [26.4398, 31.9679, 34.0804],
        [26.4398, 31.9288, 34.0033]], grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.05609793961048126 
model_pd.l_d.mean(): -1.7719215154647827 
model_pd.lagr.mean(): -1.7158235311508179 
model_pd.lambdas: dict_items([('pout', tensor([1.0941])), ('power', tensor([0.0547]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5325])), ('power', tensor([-1.6934]))])
epoch：1079	 i:0 	 global-step:21580	 l-p:0.05609793961048126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.4442, 27.3982, 26.9460],
        [26.4442, 26.4442, 26.4442],
        [26.4442, 29.0595, 28.9527],
        [26.4442, 31.4977, 33.1610]], grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.05609685555100441 
model_pd.l_d.mean(): -1.7694313526153564 
model_pd.lagr.mean(): -1.7133344411849976 
model_pd.lambdas: dict_items([('pout', tensor([1.0926])), ('power', tensor([0.0547]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5327])), ('power', tensor([-1.6890]))])
epoch：1080	 i:0 	 global-step:21600	 l-p:0.05609685555100441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.4485, 26.6299, 26.4820],
        [26.4485, 33.8142, 37.8885],
        [26.4485, 28.2600, 27.8586],
        [26.4485, 35.7462, 42.2098]], grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.05609579011797905 
model_pd.l_d.mean(): -1.7669408321380615 
model_pd.lagr.mean(): -1.7108449935913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0911])), ('power', tensor([0.0546]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5329])), ('power', tensor([-1.6847]))])
epoch：1081	 i:0 	 global-step:21620	 l-p:0.05609579011797905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.4526, 26.4809, 26.4543],
        [26.4526, 26.7997, 26.5488],
        [26.4526, 29.4373, 29.5251],
        [26.4526, 31.9841, 34.0981]], grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.05609475448727608 
model_pd.l_d.mean(): -1.7644506692886353 
model_pd.lagr.mean(): -1.7083559036254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0895])), ('power', tensor([0.0545]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5331])), ('power', tensor([-1.6806]))])
epoch：1082	 i:0 	 global-step:21640	 l-p:0.05609475448727608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.4567, 26.4665, 26.4570],
        [26.4567, 26.4567, 26.4567],
        [26.4567, 26.4568, 26.4567],
        [26.4567, 26.4958, 26.4595]], grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.05609375983476639 
model_pd.l_d.mean(): -1.761960506439209 
model_pd.lagr.mean(): -1.7058666944503784 
model_pd.lambdas: dict_items([('pout', tensor([1.0880])), ('power', tensor([0.0544]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5333])), ('power', tensor([-1.6766]))])
epoch：1083	 i:0 	 global-step:21660	 l-p:0.05609375983476639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.4606, 26.4606, 26.4606],
        [26.4606, 26.8079, 26.5568],
        [26.4606, 26.4642, 26.4607],
        [26.4606, 26.8097, 26.5576]], grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.05609278753399849 
model_pd.l_d.mean(): -1.7594705820083618 
model_pd.lagr.mean(): -1.7033778429031372 
model_pd.lambdas: dict_items([('pout', tensor([1.0865])), ('power', tensor([0.0543]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5335])), ('power', tensor([-1.6727]))])
epoch：1084	 i:0 	 global-step:21680	 l-p:0.05609278753399849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[26.4644, 33.6912, 37.6027],
        [26.4644, 30.7303, 31.7108],
        [26.4644, 27.5952, 27.1254],
        [26.4644, 27.7355, 27.2631]], grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.05609184131026268 
model_pd.l_d.mean(): -1.7569806575775146 
model_pd.lagr.mean(): -1.7008888721466064 
model_pd.lambdas: dict_items([('pout', tensor([1.0849])), ('power', tensor([0.0542]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5337])), ('power', tensor([-1.6689]))])
epoch：1085	 i:0 	 global-step:21700	 l-p:0.05609184131026268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.4681, 28.3710, 27.9918],
        [26.4681, 27.9318, 27.4705],
        [26.4681, 26.8051, 26.5597],
        [26.4681, 26.4681, 26.4681]], grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.05609092861413956 
model_pd.l_d.mean(): -1.754490613937378 
model_pd.lagr.mean(): -1.6983996629714966 
model_pd.lambdas: dict_items([('pout', tensor([1.0834])), ('power', tensor([0.0542]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5339])), ('power', tensor([-1.6653]))])
epoch：1086	 i:0 	 global-step:21720	 l-p:0.05609092861413956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.4717, 26.4721, 26.4717],
        [26.4717, 27.4272, 26.9743],
        [26.4717, 33.7010, 37.6140],
        [26.4717, 26.4717, 26.4717]], grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.05609004199504852 
model_pd.l_d.mean(): -1.7520008087158203 
model_pd.lagr.mean(): -1.6959108114242554 
model_pd.lambdas: dict_items([('pout', tensor([1.0819])), ('power', tensor([0.0541]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5341])), ('power', tensor([-1.6617]))])
epoch：1087	 i:0 	 global-step:21740	 l-p:0.05609004199504852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[26.4751, 33.8502, 37.9302],
        [26.4751, 26.4751, 26.4751],
        [26.4751, 26.4787, 26.4752],
        [26.4751, 33.7057, 37.6194]], grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.05608917772769928 
model_pd.l_d.mean(): -1.7495112419128418 
model_pd.lagr.mean(): -1.6934220790863037 
model_pd.lambdas: dict_items([('pout', tensor([1.0803])), ('power', tensor([0.0540]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5342])), ('power', tensor([-1.6582]))])
epoch：1088	 i:0 	 global-step:21760	 l-p:0.05608917772769928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.4785, 26.4785, 26.4785],
        [26.4785, 27.9431, 27.4816],
        [26.4785, 29.6646, 29.8698],
        [26.4785, 26.4786, 26.4785]], grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.05608834698796272 
model_pd.l_d.mean(): -1.7470217943191528 
model_pd.lagr.mean(): -1.6909334659576416 
model_pd.lambdas: dict_items([('pout', tensor([1.0788])), ('power', tensor([0.0539]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5344])), ('power', tensor([-1.6549]))])
epoch：1089	 i:0 	 global-step:21780	 l-p:0.05608834698796272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[26.4817, 33.7146, 37.6298],
        [26.4817, 28.8487, 28.6281],
        [26.4817, 29.1023, 28.9956],
        [26.4817, 35.7938, 42.2682]], grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.056087542325258255 
model_pd.l_d.mean(): -1.7445319890975952 
model_pd.lagr.mean(): -1.6884444952011108 
model_pd.lambdas: dict_items([('pout', tensor([1.0773])), ('power', tensor([0.0538]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5346])), ('power', tensor([-1.6517]))])
epoch：1090	 i:0 	 global-step:21800	 l-p:0.056087542325258255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.4849, 26.4850, 26.4849],
        [26.4849, 26.4854, 26.4849],
        [26.4849, 26.4864, 26.4849],
        [26.4849, 27.9500, 27.4884]], grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.056086767464876175 
model_pd.l_d.mean(): -1.7420426607131958 
model_pd.lagr.mean(): -1.6859558820724487 
model_pd.lambdas: dict_items([('pout', tensor([1.0757])), ('power', tensor([0.0537]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5347])), ('power', tensor([-1.6485]))])
epoch：1091	 i:0 	 global-step:21820	 l-p:0.056086767464876175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.4879, 26.4884, 26.4879],
        [26.4879, 26.8376, 26.5852],
        [26.4879, 27.7608, 27.2879],
        [26.4879, 32.0354, 34.1596]], grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.05608600750565529 
model_pd.l_d.mean(): -1.7395535707473755 
model_pd.lagr.mean(): -1.6834675073623657 
model_pd.lambdas: dict_items([('pout', tensor([1.0742])), ('power', tensor([0.0537]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5349])), ('power', tensor([-1.6455]))])
epoch：1092	 i:0 	 global-step:21840	 l-p:0.05608600750565529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.4909, 26.5008, 26.4912],
        [26.4909, 26.8388, 26.5873],
        [26.4909, 34.2104, 38.6904],
        [26.4909, 35.8071, 42.2845]], grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.05608528479933739 
model_pd.l_d.mean(): -1.7370644807815552 
model_pd.lagr.mean(): -1.6809792518615723 
model_pd.lambdas: dict_items([('pout', tensor([1.0727])), ('power', tensor([0.0536]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5350])), ('power', tensor([-1.6426]))])
epoch：1093	 i:0 	 global-step:21860	 l-p:0.05608528479933739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.4937, 26.8313, 26.5855],
        [26.4937, 29.4851, 29.5735],
        [26.4937, 27.4505, 26.9971],
        [26.4937, 29.6825, 29.8881]], grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.056084584444761276 
model_pd.l_d.mean(): -1.7345751523971558 
model_pd.lagr.mean(): -1.6784905195236206 
model_pd.lambdas: dict_items([('pout', tensor([1.0711])), ('power', tensor([0.0535]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5352])), ('power', tensor([-1.6398]))])
epoch：1094	 i:0 	 global-step:21880	 l-p:0.056084584444761276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.4964, 32.0403, 34.1598],
        [26.4964, 26.5063, 26.4968],
        [26.4964, 26.4982, 26.4965],
        [26.4964, 26.4966, 26.4964]], grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.05608390271663666 
model_pd.l_d.mean(): -1.732085943222046 
model_pd.lagr.mean(): -1.676002025604248 
model_pd.lambdas: dict_items([('pout', tensor([1.0696])), ('power', tensor([0.0534]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5353])), ('power', tensor([-1.6371]))])
epoch：1095	 i:0 	 global-step:21900	 l-p:0.05608390271663666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228]])
 pt:tensor([[26.4991, 32.0499, 34.1757],
        [26.4991, 27.6322, 27.1616],
        [26.4991, 34.2219, 38.7040],
        [26.4991, 27.1537, 26.7699]], grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.056083258241415024 
model_pd.l_d.mean(): -1.7295974493026733 
model_pd.lagr.mean(): -1.6735142469406128 
model_pd.lambdas: dict_items([('pout', tensor([1.0681])), ('power', tensor([0.0533]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5354])), ('power', tensor([-1.6345]))])
epoch：1096	 i:0 	 global-step:21920	 l-p:0.056083258241415024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.5016, 26.5016, 26.5016],
        [26.5016, 30.4844, 31.2384],
        [26.5016, 29.3103, 29.2975],
        [26.5016, 27.4589, 27.0052]], grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.05608263984322548 
model_pd.l_d.mean(): -1.7271082401275635 
model_pd.lagr.mean(): -1.6710256338119507 
model_pd.lambdas: dict_items([('pout', tensor([1.0665])), ('power', tensor([0.0532]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5356])), ('power', tensor([-1.6319]))])
epoch：1097	 i:0 	 global-step:21940	 l-p:0.05608263984322548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.5040, 26.5323, 26.5057],
        [26.5040, 27.9709, 27.5088],
        [26.5040, 26.6106, 26.5182],
        [26.5040, 26.6861, 26.5376]], grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.05608203262090683 
model_pd.l_d.mean(): -1.7246193885803223 
model_pd.lagr.mean(): -1.6685373783111572 
model_pd.lambdas: dict_items([('pout', tensor([1.0650])), ('power', tensor([0.0532]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5357])), ('power', tensor([-1.6295]))])
epoch：1098	 i:0 	 global-step:21960	 l-p:0.05608203262090683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.5063, 34.2322, 38.7163],
        [26.5063, 32.0141, 34.0969],
        [26.5063, 27.4068, 26.9621],
        [26.5063, 26.5065, 26.5063]], grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.05608145892620087 
model_pd.l_d.mean(): -1.7221307754516602 
model_pd.lagr.mean(): -1.6660493612289429 
model_pd.lambdas: dict_items([('pout', tensor([1.0635])), ('power', tensor([0.0531]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5358])), ('power', tensor([-1.6272]))])
epoch：1099	 i:0 	 global-step:21980	 l-p:0.05608145892620087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.5086, 29.1334, 29.0268],
        [26.5086, 26.8569, 26.6051],
        [26.5086, 26.5122, 26.5086],
        [26.5086, 26.5087, 26.5086]], grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): 0.056080903857946396 
model_pd.l_d.mean(): -1.7196422815322876 
model_pd.lagr.mean(): -1.6635613441467285 
model_pd.lambdas: dict_items([('pout', tensor([1.0619])), ('power', tensor([0.0530]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5359])), ('power', tensor([-1.6250]))])
epoch：1100	 i:0 	 global-step:22000	 l-p:0.056080903857946396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.5107, 26.5107, 26.5107],
        [26.5107, 32.0592, 34.1808],
        [26.5107, 26.5143, 26.5108],
        [26.5107, 26.5390, 26.5124]], grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.05608037859201431 
model_pd.l_d.mean(): -1.7171536684036255 
model_pd.lagr.mean(): -1.6610733270645142 
model_pd.lambdas: dict_items([('pout', tensor([1.0604])), ('power', tensor([0.0529]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5360])), ('power', tensor([-1.6229]))])
epoch：1101	 i:0 	 global-step:22020	 l-p:0.05608037859201431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.5127, 29.7054, 29.9116],
        [26.5127, 32.0619, 34.1838],
        [26.5127, 26.8581, 26.6079],
        [26.5127, 27.2932, 26.8735]], grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.05607988312840462 
model_pd.l_d.mean(): -1.714665174484253 
model_pd.lagr.mean(): -1.6585853099822998 
model_pd.lambdas: dict_items([('pout', tensor([1.0588])), ('power', tensor([0.0528]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5361])), ('power', tensor([-1.6208]))])
epoch：1102	 i:0 	 global-step:22040	 l-p:0.05607988312840462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.5147, 29.1406, 29.0341],
        [26.5147, 27.4157, 26.9707],
        [26.5147, 28.8866, 28.6659],
        [26.5147, 26.5183, 26.5147]], grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.05607939139008522 
model_pd.l_d.mean(): -1.7121765613555908 
model_pd.lagr.mean(): -1.656097173690796 
model_pd.lambdas: dict_items([('pout', tensor([1.0573])), ('power', tensor([0.0528]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5362])), ('power', tensor([-1.6189]))])
epoch：1103	 i:0 	 global-step:22060	 l-p:0.05607939139008522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.5165, 26.5201, 26.5166],
        [26.5165, 27.6512, 27.1799],
        [26.5165, 35.1849, 40.8212],
        [26.5165, 28.3356, 27.9329]], grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.056078940629959106 
model_pd.l_d.mean(): -1.7096880674362183 
model_pd.lagr.mean(): -1.6536091566085815 
model_pd.lambdas: dict_items([('pout', tensor([1.0558])), ('power', tensor([0.0527]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5363])), ('power', tensor([-1.6171]))])
epoch：1104	 i:0 	 global-step:22080	 l-p:0.056078940629959106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.5182, 29.5145, 29.6035],
        [26.5182, 27.2991, 26.8792],
        [26.5182, 26.5182, 26.5182],
        [26.5182, 30.5054, 31.2606]], grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.056078508496284485 
model_pd.l_d.mean(): -1.7071995735168457 
model_pd.lagr.mean(): -1.6511210203170776 
model_pd.lambdas: dict_items([('pout', tensor([1.0542])), ('power', tensor([0.0526]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5364])), ('power', tensor([-1.6153]))])
epoch：1105	 i:0 	 global-step:22100	 l-p:0.056078508496284485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.5199, 30.7995, 31.7844],
        [26.5199, 33.7680, 37.6926],
        [26.5199, 33.9128, 38.0040],
        [26.5199, 26.8686, 26.6165]], grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.056078098714351654 
model_pd.l_d.mean(): -1.7047114372253418 
model_pd.lagr.mean(): -1.648633360862732 
model_pd.lambdas: dict_items([('pout', tensor([1.0527])), ('power', tensor([0.0525]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5365])), ('power', tensor([-1.6137]))])
epoch：1106	 i:0 	 global-step:22120	 l-p:0.056078098714351654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.5214, 31.5964, 33.2683],
        [26.5214, 26.5215, 26.5214],
        [26.5214, 26.8702, 26.6181],
        [26.5214, 26.5232, 26.5215]], grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.05607771873474121 
model_pd.l_d.mean(): -1.7022229433059692 
model_pd.lagr.mean(): -1.646145224571228 
model_pd.lambdas: dict_items([('pout', tensor([1.0512])), ('power', tensor([0.0524]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5366])), ('power', tensor([-1.6121]))])
epoch：1107	 i:0 	 global-step:22140	 l-p:0.05607771873474121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[26.5229, 28.8964, 28.6758],
        [26.5229, 32.0820, 34.2118],
        [26.5229, 27.6582, 27.1868],
        [26.5229, 29.3360, 29.3235]], grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.05607734993100166 
model_pd.l_d.mean(): -1.6997348070144653 
model_pd.lagr.mean(): -1.6436574459075928 
model_pd.lambdas: dict_items([('pout', tensor([1.0496])), ('power', tensor([0.0524]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5367])), ('power', tensor([-1.6107]))])
epoch：1108	 i:0 	 global-step:22160	 l-p:0.05607734993100166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.5243, 26.6311, 26.5385],
        [26.5243, 28.4343, 28.0542],
        [26.5243, 26.5243, 26.5243],
        [26.5243, 34.2585, 38.7485]], grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.0560770146548748 
model_pd.l_d.mean(): -1.697246789932251 
model_pd.lagr.mean(): -1.641169786453247 
model_pd.lambdas: dict_items([('pout', tensor([1.0481])), ('power', tensor([0.0523]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5367])), ('power', tensor([-1.6093]))])
epoch：1109	 i:0 	 global-step:22180	 l-p:0.0560770146548748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.5256, 28.8997, 28.6790],
        [26.5256, 28.3461, 27.9433],
        [26.5256, 27.8021, 27.3280],
        [26.5256, 26.8763, 26.6231]], grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.056076694279909134 
model_pd.l_d.mean(): -1.694758415222168 
model_pd.lagr.mean(): -1.6386817693710327 
model_pd.lambdas: dict_items([('pout', tensor([1.0466])), ('power', tensor([0.0522]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5368])), ('power', tensor([-1.6080]))])
epoch：1110	 i:0 	 global-step:22200	 l-p:0.056076694279909134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.5268, 34.2624, 38.7533],
        [26.5268, 26.5269, 26.5268],
        [26.5268, 27.4288, 26.9835],
        [26.5268, 26.8653, 26.6188]], grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.05607638880610466 
model_pd.l_d.mean(): -1.6922701597213745 
model_pd.lagr.mean(): -1.6361937522888184 
model_pd.lambdas: dict_items([('pout', tensor([1.0450])), ('power', tensor([0.0521]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5369])), ('power', tensor([-1.6068]))])
epoch：1111	 i:0 	 global-step:22220	 l-p:0.05607638880610466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.5279, 26.5297, 26.5280],
        [26.5279, 32.0438, 34.1306],
        [26.5279, 26.8770, 26.6247],
        [26.5279, 28.4387, 28.0585]], grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.05607610195875168 
model_pd.l_d.mean(): -1.6897821426391602 
model_pd.lagr.mean(): -1.6337060928344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0435])), ('power', tensor([0.0520]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5369])), ('power', tensor([-1.6056]))])
epoch：1112	 i:0 	 global-step:22240	 l-p:0.05607610195875168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.5290, 26.8750, 26.6244],
        [26.5290, 29.5281, 29.6175],
        [26.5290, 26.5295, 26.5290],
        [26.5290, 35.8651, 42.3586]], grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.056075844913721085 
model_pd.l_d.mean(): -1.6872938871383667 
model_pd.lagr.mean(): -1.6312180757522583 
model_pd.lambdas: dict_items([('pout', tensor([1.0419])), ('power', tensor([0.0520]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5370])), ('power', tensor([-1.6046]))])
epoch：1113	 i:0 	 global-step:22260	 l-p:0.056075844913721085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.5300, 26.8760, 26.6254],
        [26.5300, 26.8810, 26.6276],
        [26.5300, 26.5300, 26.5300],
        [26.5300, 26.5693, 26.5329]], grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.05607559159398079 
model_pd.l_d.mean(): -1.6848056316375732 
model_pd.lagr.mean(): -1.628730058670044 
model_pd.lambdas: dict_items([('pout', tensor([1.0404])), ('power', tensor([0.0519]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5370])), ('power', tensor([-1.6036]))])
epoch：1114	 i:0 	 global-step:22280	 l-p:0.05607559159398079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.5309, 26.5311, 26.5309],
        [26.5309, 26.6378, 26.5452],
        [26.5309, 27.1874, 26.8027],
        [26.5309, 26.5309, 26.5309]], grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.056075356900691986 
model_pd.l_d.mean(): -1.6823173761367798 
model_pd.lagr.mean(): -1.6262420415878296 
model_pd.lambdas: dict_items([('pout', tensor([1.0389])), ('power', tensor([0.0518]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5371])), ('power', tensor([-1.6026]))])
epoch：1115	 i:0 	 global-step:22300	 l-p:0.056075356900691986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.5318, 29.1619, 29.0557],
        [26.5318, 26.5318, 26.5318],
        [26.5318, 26.6387, 26.5460],
        [26.5318, 29.7297, 29.9369]], grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.056075144559144974 
model_pd.l_d.mean(): -1.6798293590545654 
model_pd.lagr.mean(): -1.6237542629241943 
model_pd.lambdas: dict_items([('pout', tensor([1.0373])), ('power', tensor([0.0517]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5371])), ('power', tensor([-1.6017]))])
epoch：1116	 i:0 	 global-step:22320	 l-p:0.056075144559144974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.5327, 29.4385, 29.4750],
        [26.5327, 26.5363, 26.5328],
        [26.5327, 27.4354, 26.9897],
        [26.5327, 28.4445, 28.0642]], grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.056074924767017365 
model_pd.l_d.mean(): -1.6773409843444824 
model_pd.lagr.mean(): -1.6212660074234009 
model_pd.lambdas: dict_items([('pout', tensor([1.0358])), ('power', tensor([0.0516]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5372])), ('power', tensor([-1.6008]))])
epoch：1117	 i:0 	 global-step:22340	 l-p:0.056074924767017365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.5335, 29.3496, 29.3375],
        [26.5335, 28.9096, 28.6891],
        [26.5335, 26.8797, 26.6290],
        [26.5335, 26.8724, 26.6257]], grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.05607471987605095 
model_pd.l_d.mean(): -1.6748524904251099 
model_pd.lagr.mean(): -1.6187777519226074 
model_pd.lambdas: dict_items([('pout', tensor([1.0343])), ('power', tensor([0.0516]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5372])), ('power', tensor([-1.6000]))])
epoch：1118	 i:0 	 global-step:22360	 l-p:0.05607471987605095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.5344, 35.8741, 42.3710],
        [26.5344, 26.5349, 26.5344],
        [26.5344, 26.8855, 26.6320],
        [26.5344, 28.9107, 28.6902]], grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.056074514985084534 
model_pd.l_d.mean(): -1.6723642349243164 
model_pd.lagr.mean(): -1.616289734840393 
model_pd.lambdas: dict_items([('pout', tensor([1.0327])), ('power', tensor([0.0515]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5373])), ('power', tensor([-1.5992]))])
epoch：1119	 i:0 	 global-step:22380	 l-p:0.056074514985084534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.5352, 26.5864, 26.5396],
        [26.5352, 26.8741, 26.6274],
        [26.5352, 27.1921, 26.8071],
        [26.5352, 29.7342, 29.9417]], grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.05607429891824722 
model_pd.l_d.mean(): -1.6698757410049438 
model_pd.lagr.mean(): -1.6138014793395996 
model_pd.lambdas: dict_items([('pout', tensor([1.0312])), ('power', tensor([0.0514]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5373])), ('power', tensor([-1.5983]))])
epoch：1120	 i:0 	 global-step:22400	 l-p:0.05607429891824722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.5361, 30.8219, 31.8090],
        [26.5361, 29.7354, 29.9429],
        [26.5361, 28.2801, 27.8595],
        [26.5361, 26.5460, 26.5364]], grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.056074079126119614 
model_pd.l_d.mean(): -1.6673870086669922 
model_pd.lagr.mean(): -1.611312985420227 
model_pd.lambdas: dict_items([('pout', tensor([1.0296])), ('power', tensor([0.0513]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5373])), ('power', tensor([-1.5974]))])
epoch：1121	 i:0 	 global-step:22420	 l-p:0.056074079126119614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.5370, 26.8883, 26.6347],
        [26.5370, 30.8232, 31.8104],
        [26.5370, 29.5386, 29.6285],
        [26.5370, 26.5387, 26.5370]], grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.05607385188341141 
model_pd.l_d.mean(): -1.66489839553833 
model_pd.lagr.mean(): -1.6088244915008545 
model_pd.lambdas: dict_items([('pout', tensor([1.0281])), ('power', tensor([0.0512]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5374])), ('power', tensor([-1.5965]))])
epoch：1122	 i:0 	 global-step:22440	 l-p:0.05607385188341141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.5381, 26.7209, 26.5718],
        [26.5381, 28.4510, 28.0706],
        [26.5381, 26.5417, 26.5381],
        [26.5381, 30.8246, 31.8120]], grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.056073594838380814 
model_pd.l_d.mean(): -1.6624091863632202 
model_pd.lagr.mean(): -1.6063356399536133 
model_pd.lambdas: dict_items([('pout', tensor([1.0266])), ('power', tensor([0.0512]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5375])), ('power', tensor([-1.5954]))])
epoch：1123	 i:0 	 global-step:22460	 l-p:0.056073594838380814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[26.5396, 27.6770, 27.2049],
        [26.5396, 28.3628, 27.9597],
        [26.5396, 32.0607, 34.1504],
        [26.5396, 32.1000, 34.2279]], grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.05607321858406067 
model_pd.l_d.mean(): -1.6599193811416626 
model_pd.lagr.mean(): -1.6038461923599243 
model_pd.lambdas: dict_items([('pout', tensor([1.0250])), ('power', tensor([0.0511]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5375])), ('power', tensor([-1.5939]))])
epoch：1124	 i:0 	 global-step:22480	 l-p:0.05607321858406067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.5416, 26.8930, 26.6394],
        [26.5416, 32.0633, 34.1532],
        [26.5416, 30.5364, 31.2942],
        [26.5416, 27.5025, 27.0474]], grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.056072723120450974 
model_pd.l_d.mean(): -1.6574293375015259 
model_pd.lagr.mean(): -1.6013566255569458 
model_pd.lambdas: dict_items([('pout', tensor([1.0235])), ('power', tensor([0.0510]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5376])), ('power', tensor([-1.5919]))])
epoch：1125	 i:0 	 global-step:22500	 l-p:0.056072723120450974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[26.5440, 29.1767, 29.0707],
        [26.5440, 27.5051, 27.0499],
        [26.5440, 31.6277, 33.3037],
        [26.5440, 27.8228, 27.3481]], grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.056072115898132324 
model_pd.l_d.mean(): -1.65493905544281 
model_pd.lagr.mean(): -1.5988669395446777 
model_pd.lambdas: dict_items([('pout', tensor([1.0220])), ('power', tensor([0.0509]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5377])), ('power', tensor([-1.5895]))])
epoch：1126	 i:0 	 global-step:22520	 l-p:0.056072115898132324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.5469, 27.4507, 27.0045],
        [26.5469, 33.9529, 38.0531],
        [26.5469, 33.8079, 37.7412],
        [26.5469, 26.5484, 26.5469]], grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.05607141554355621 
model_pd.l_d.mean(): -1.6524488925933838 
model_pd.lagr.mean(): -1.5963774919509888 
model_pd.lambdas: dict_items([('pout', tensor([1.0204])), ('power', tensor([0.0508]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5379])), ('power', tensor([-1.5867]))])
epoch：1127	 i:0 	 global-step:22540	 l-p:0.05607141554355621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.5501, 26.7559, 26.5910],
        [26.5501, 26.8998, 26.6471],
        [26.5501, 26.5501, 26.5501],
        [26.5501, 35.2360, 40.8861]], grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.056070633232593536 
model_pd.l_d.mean(): -1.6499587297439575 
model_pd.lagr.mean(): -1.5938880443572998 
model_pd.lambdas: dict_items([('pout', tensor([1.0189])), ('power', tensor([0.0508]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5381])), ('power', tensor([-1.5835]))])
epoch：1128	 i:0 	 global-step:22560	 l-p:0.056070633232593536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.5536, 30.5507, 31.3091],
        [26.5536, 27.4576, 27.0114],
        [26.5536, 26.7365, 26.5873],
        [26.5536, 26.5536, 26.5536]], grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.05606976896524429 
model_pd.l_d.mean(): -1.6474683284759521 
model_pd.lagr.mean(): -1.5913985967636108 
model_pd.lambdas: dict_items([('pout', tensor([1.0173])), ('power', tensor([0.0507]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5382])), ('power', tensor([-1.5801]))])
epoch：1129	 i:0 	 global-step:22580	 l-p:0.05606976896524429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.5572, 31.0927, 32.2837],
        [26.5572, 29.1916, 29.0856],
        [26.5572, 28.3034, 27.8824],
        [26.5572, 26.9040, 26.6529]], grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.056068867444992065 
model_pd.l_d.mean(): -1.6449782848358154 
model_pd.lagr.mean(): -1.588909387588501 
model_pd.lambdas: dict_items([('pout', tensor([1.0158])), ('power', tensor([0.0506]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5384])), ('power', tensor([-1.5764]))])
epoch：1130	 i:0 	 global-step:22600	 l-p:0.056068867444992065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.5611, 30.8524, 31.8412],
        [26.5611, 29.1959, 29.0899],
        [26.5611, 30.5597, 31.3183],
        [26.5611, 33.9718, 38.0748]], grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.056067924946546555 
model_pd.l_d.mean(): -1.6424877643585205 
model_pd.lagr.mean(): -1.5864198207855225 
model_pd.lambdas: dict_items([('pout', tensor([1.0143])), ('power', tensor([0.0505]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5386])), ('power', tensor([-1.5726]))])
epoch：1131	 i:0 	 global-step:22620	 l-p:0.056067924946546555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.5651, 35.2566, 40.9106],
        [26.5651, 26.5674, 26.5651],
        [26.5651, 33.9770, 38.0808],
        [26.5651, 26.5652, 26.5651]], grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.056066952645778656 
model_pd.l_d.mean(): -1.6399974822998047 
model_pd.lagr.mean(): -1.583930492401123 
model_pd.lambdas: dict_items([('pout', tensor([1.0127])), ('power', tensor([0.0504]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5388])), ('power', tensor([-1.5686]))])
epoch：1132	 i:0 	 global-step:22640	 l-p:0.056066952645778656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.5691, 26.9160, 26.6648],
        [26.5691, 28.3162, 27.8950],
        [26.5691, 26.5976, 26.5708],
        [26.5691, 33.9823, 38.0869]], grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 0.056065961718559265 
model_pd.l_d.mean(): -1.6375070810317993 
model_pd.lagr.mean(): -1.5814411640167236 
model_pd.lambdas: dict_items([('pout', tensor([1.0112])), ('power', tensor([0.0504]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5390])), ('power', tensor([-1.5646]))])
epoch：1133	 i:0 	 global-step:22660	 l-p:0.056065961718559265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.5731, 32.1484, 34.2858],
        [26.5731, 33.8425, 37.7808],
        [26.5731, 29.2094, 29.1035],
        [26.5731, 33.9877, 38.0930]], grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.05606497451663017 
model_pd.l_d.mean(): -1.635016918182373 
model_pd.lagr.mean(): -1.5789519548416138 
model_pd.lambdas: dict_items([('pout', tensor([1.0097])), ('power', tensor([0.0503]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5392])), ('power', tensor([-1.5606]))])
epoch：1134	 i:0 	 global-step:22680	 l-p:0.05606497451663017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.5771, 35.2732, 40.9303],
        [26.5771, 26.5871, 26.5775],
        [26.5771, 26.6166, 26.5800],
        [26.5771, 26.5771, 26.5771]], grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.05606399103999138 
model_pd.l_d.mean(): -1.6325263977050781 
model_pd.lagr.mean(): -1.5764623880386353 
model_pd.lambdas: dict_items([('pout', tensor([1.0081])), ('power', tensor([0.0502]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5394])), ('power', tensor([-1.5567]))])
epoch：1135	 i:0 	 global-step:22700	 l-p:0.05606399103999138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.5810, 27.5439, 27.0879],
        [26.5810, 27.8622, 27.3867],
        [26.5810, 26.5833, 26.5811],
        [26.5810, 26.5910, 26.5814]], grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.056063033640384674 
model_pd.l_d.mean(): -1.6300363540649414 
model_pd.lagr.mean(): -1.573973298072815 
model_pd.lambdas: dict_items([('pout', tensor([1.0066])), ('power', tensor([0.0501]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5396])), ('power', tensor([-1.5528]))])
epoch：1136	 i:0 	 global-step:22720	 l-p:0.056063033640384674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.5848, 29.2226, 29.1166],
        [26.5848, 26.5863, 26.5849],
        [26.5848, 34.0032, 38.1108],
        [26.5848, 28.0599, 27.5958]], grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.05606209859251976 
model_pd.l_d.mean(): -1.627545714378357 
model_pd.lagr.mean(): -1.5714836120605469 
model_pd.lambdas: dict_items([('pout', tensor([1.0050])), ('power', tensor([0.0501]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5398])), ('power', tensor([-1.5490]))])
epoch：1137	 i:0 	 global-step:22740	 l-p:0.05606209859251976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.5885, 26.5885, 26.5885],
        [26.5885, 28.3373, 27.9157],
        [26.5885, 27.3731, 26.9514],
        [26.5885, 26.5885, 26.5885]], grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.056061211973428726 
model_pd.l_d.mean(): -1.6250555515289307 
model_pd.lagr.mean(): -1.5689942836761475 
model_pd.lambdas: dict_items([('pout', tensor([1.0035])), ('power', tensor([0.0500]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5400])), ('power', tensor([-1.5454]))])
epoch：1138	 i:0 	 global-step:22760	 l-p:0.056061211973428726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.5919, 28.3410, 27.9195],
        [26.5919, 28.4199, 28.0159],
        [26.5919, 26.5924, 26.5919],
        [26.5919, 26.7982, 26.6329]], grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.05606035888195038 
model_pd.l_d.mean(): -1.6225651502609253 
model_pd.lagr.mean(): -1.5665048360824585 
model_pd.lambdas: dict_items([('pout', tensor([1.0020])), ('power', tensor([0.0499]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5401])), ('power', tensor([-1.5420]))])
epoch：1139	 i:0 	 global-step:22780	 l-p:0.05606035888195038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.5952, 32.1760, 34.3159],
        [26.5952, 29.4204, 29.4088],
        [26.5952, 26.5975, 26.5952],
        [26.5952, 26.5967, 26.5952]], grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.05605955421924591 
model_pd.l_d.mean(): -1.6200746297836304 
model_pd.lagr.mean(): -1.5640150308609009 
model_pd.lambdas: dict_items([('pout', tensor([1.0004])), ('power', tensor([0.0498]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5403])), ('power', tensor([-1.5387]))])
epoch：1140	 i:0 	 global-step:22800	 l-p:0.05605955421924591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[26.5982, 28.4268, 28.0227],
        [26.5982, 34.0211, 38.1313],
        [26.5982, 28.5169, 28.1357],
        [26.5982, 27.7391, 27.2656]], grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 0.05605881288647652 
model_pd.l_d.mean(): -1.617584228515625 
model_pd.lagr.mean(): -1.5615254640579224 
model_pd.lambdas: dict_items([('pout', tensor([0.9989])), ('power', tensor([0.0497]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5404])), ('power', tensor([-1.5357]))])
epoch：1141	 i:0 	 global-step:22820	 l-p:0.05605881288647652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6010, 27.3862, 26.9642],
        [26.6010, 26.6012, 26.6010],
        [26.6010, 28.4298, 28.0257],
        [26.6010, 27.5648, 27.1084]], grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.0560581237077713 
model_pd.l_d.mean(): -1.6150943040847778 
model_pd.lagr.mean(): -1.559036135673523 
model_pd.lambdas: dict_items([('pout', tensor([0.9973])), ('power', tensor([0.0497]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5406])), ('power', tensor([-1.5330]))])
epoch：1142	 i:0 	 global-step:22840	 l-p:0.0560581237077713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.6035, 35.3098, 40.9738],
        [26.6035, 27.8862, 27.4102],
        [26.6035, 26.6035, 26.6035],
        [26.6035, 29.8136, 30.0223]], grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.056057509034872055 
model_pd.l_d.mean(): -1.612604022026062 
model_pd.lagr.mean(): -1.5565465688705444 
model_pd.lambdas: dict_items([('pout', tensor([0.9958])), ('power', tensor([0.0496]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5407])), ('power', tensor([-1.5304]))])
epoch：1143	 i:0 	 global-step:22860	 l-p:0.056057509034872055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 27.5699, 27.1133],
        [26.6058, 27.3912, 26.9691],
        [26.6058, 29.8162, 30.0250],
        [26.6058, 26.7893, 26.6397]], grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.05605695769190788 
model_pd.l_d.mean(): -1.6101138591766357 
model_pd.lagr.mean(): -1.5540568828582764 
model_pd.lambdas: dict_items([('pout', tensor([0.9942])), ('power', tensor([0.0495]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5408])), ('power', tensor([-1.5282]))])
epoch：1144	 i:0 	 global-step:22880	 l-p:0.05605695769190788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.6078, 29.6200, 29.7107],
        [26.6078, 26.6095, 26.6078],
        [26.6078, 26.9586, 26.7051],
        [26.6078, 26.6078, 26.6078]], grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.05605647712945938 
model_pd.l_d.mean(): -1.607623815536499 
model_pd.lagr.mean(): -1.5515673160552979 
model_pd.lambdas: dict_items([('pout', tensor([0.9927])), ('power', tensor([0.0494]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5262]))])
epoch：1145	 i:0 	 global-step:22900	 l-p:0.05605647712945938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.6095, 26.9622, 26.7076],
        [26.6095, 35.3182, 40.9840],
        [26.6095, 26.6110, 26.6095],
        [26.6095, 27.2691, 26.8826]], grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.056056056171655655 
model_pd.l_d.mean(): -1.6051337718963623 
model_pd.lagr.mean(): -1.5490777492523193 
model_pd.lambdas: dict_items([('pout', tensor([0.9912])), ('power', tensor([0.0494]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5245]))])
epoch：1146	 i:0 	 global-step:22920	 l-p:0.056056056171655655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.6109, 34.3786, 38.8905],
        [26.6109, 31.1579, 32.3525],
        [26.6109, 26.9513, 26.7035],
        [26.6109, 26.6111, 26.6109]], grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.056055698543787 
model_pd.l_d.mean(): -1.6026437282562256 
model_pd.lagr.mean(): -1.5465880632400513 
model_pd.lambdas: dict_items([('pout', tensor([0.9896])), ('power', tensor([0.0493]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5231]))])
epoch：1147	 i:0 	 global-step:22940	 l-p:0.056055698543787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6121, 35.3220, 40.9886],
        [26.6121, 32.1915, 34.3274],
        [26.6121, 27.2718, 26.8853],
        [26.6121, 26.6121, 26.6121]], grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.05605541169643402 
model_pd.l_d.mean(): -1.6001536846160889 
model_pd.lagr.mean(): -1.5440982580184937 
model_pd.lambdas: dict_items([('pout', tensor([0.9881])), ('power', tensor([0.0492]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5219]))])
epoch：1148	 i:0 	 global-step:22960	 l-p:0.05605541169643402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.6130, 32.1533, 34.2510],
        [26.6130, 26.9658, 26.7112],
        [26.6130, 35.3233, 40.9903],
        [26.6130, 26.6644, 26.6174]], grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.05605519190430641 
model_pd.l_d.mean(): -1.5976639986038208 
model_pd.lagr.mean(): -1.5416088104248047 
model_pd.lambdas: dict_items([('pout', tensor([0.9865])), ('power', tensor([0.0491]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5210]))])
epoch：1149	 i:0 	 global-step:22980	 l-p:0.05605519190430641
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6137, 28.4439, 28.0396],
        [26.6137, 29.8256, 30.0347],
        [26.6137, 27.5207, 27.0730],
        [26.6137, 26.6137, 26.6137]], grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.05605503171682358 
model_pd.l_d.mean(): -1.5951743125915527 
model_pd.lagr.mean(): -1.5391192436218262 
model_pd.lambdas: dict_items([('pout', tensor([0.9850])), ('power', tensor([0.0491]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5204]))])
epoch：1150	 i:0 	 global-step:23000	 l-p:0.05605503171682358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.6141, 30.6232, 31.3844],
        [26.6141, 34.0429, 38.1568],
        [26.6141, 26.6164, 26.6141],
        [26.6141, 26.6142, 26.6141]], grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.05605492740869522 
model_pd.l_d.mean(): -1.5926845073699951 
model_pd.lagr.mean(): -1.536629557609558 
model_pd.lambdas: dict_items([('pout', tensor([0.9835])), ('power', tensor([0.0490]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5199]))])
epoch：1151	 i:0 	 global-step:23020	 l-p:0.05605492740869522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.6143, 28.0919, 27.6271],
        [26.6143, 28.5349, 28.1534],
        [26.6143, 26.6179, 26.6143],
        [26.6143, 32.1946, 34.3311]], grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.05605487525463104 
model_pd.l_d.mean(): -1.5901947021484375 
model_pd.lagr.mean(): -1.53413987159729 
model_pd.lambdas: dict_items([('pout', tensor([0.9819])), ('power', tensor([0.0489]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5197]))])
epoch：1152	 i:0 	 global-step:23040	 l-p:0.05605487525463104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.6143, 26.8209, 26.6554],
        [26.6143, 26.7218, 26.6286],
        [26.6143, 26.9653, 26.7116],
        [26.6143, 26.6144, 26.6143]], grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.05605487525463104 
model_pd.l_d.mean(): -1.5877052545547485 
model_pd.lagr.mean(): -1.531650424003601 
model_pd.lambdas: dict_items([('pout', tensor([0.9804])), ('power', tensor([0.0488]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5197]))])
epoch：1153	 i:0 	 global-step:23060	 l-p:0.05605487525463104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.6141, 27.7562, 27.2823],
        [26.6141, 35.3254, 40.9933],
        [26.6141, 26.6241, 26.6144],
        [26.6141, 32.2008, 34.3434]], grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.05605492740869522 
model_pd.l_d.mean(): -1.585215449333191 
model_pd.lagr.mean(): -1.529160499572754 
model_pd.lambdas: dict_items([('pout', tensor([0.9788])), ('power', tensor([0.0488]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5199]))])
epoch：1154	 i:0 	 global-step:23080	 l-p:0.05605492740869522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.6137, 26.6423, 26.6154],
        [26.6137, 28.3653, 27.9433],
        [26.6137, 26.9543, 26.7063],
        [26.6137, 27.3997, 26.9773]], grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.056055013090372086 
model_pd.l_d.mean(): -1.5827258825302124 
model_pd.lagr.mean(): -1.5266708135604858 
model_pd.lambdas: dict_items([('pout', tensor([0.9773])), ('power', tensor([0.0487]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5203]))])
epoch：1155	 i:0 	 global-step:23100	 l-p:0.056055013090372086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.6132, 26.6646, 26.6176],
        [26.6132, 32.1544, 34.2527],
        [26.6132, 26.6168, 26.6133],
        [26.6132, 29.4418, 29.4305]], grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.056055136024951935 
model_pd.l_d.mean(): -1.5802364349365234 
model_pd.lagr.mean(): -1.5241812467575073 
model_pd.lambdas: dict_items([('pout', tensor([0.9758])), ('power', tensor([0.0486]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5208]))])
epoch：1156	 i:0 	 global-step:23120	 l-p:0.056055136024951935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6126, 26.6143, 26.6126],
        [26.6126, 35.9867, 42.5096],
        [26.6126, 26.9605, 26.7085],
        [26.6126, 26.9531, 26.7052]], grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.05605528876185417 
model_pd.l_d.mean(): -1.5777469873428345 
model_pd.lagr.mean(): -1.5216916799545288 
model_pd.lambdas: dict_items([('pout', tensor([0.9742])), ('power', tensor([0.0485]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5214]))])
epoch：1157	 i:0 	 global-step:23140	 l-p:0.05605528876185417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.6118, 26.7193, 26.6261],
        [26.6118, 26.6513, 26.6147],
        [26.6118, 26.8184, 26.6529],
        [26.6118, 29.4404, 29.4292]], grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.056055475026369095 
model_pd.l_d.mean(): -1.5752575397491455 
model_pd.lagr.mean(): -1.5192021131515503 
model_pd.lambdas: dict_items([('pout', tensor([0.9727])), ('power', tensor([0.0485]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5222]))])
epoch：1158	 i:0 	 global-step:23160	 l-p:0.056055475026369095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6109, 26.6114, 26.6109],
        [26.6109, 26.6110, 26.6109],
        [26.6109, 29.5299, 29.5676],
        [26.6109, 26.6124, 26.6110]], grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.05605567991733551 
model_pd.l_d.mean(): -1.5727680921554565 
model_pd.lagr.mean(): -1.5167124271392822 
model_pd.lambdas: dict_items([('pout', tensor([0.9711])), ('power', tensor([0.0484]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5230]))])
epoch：1159	 i:0 	 global-step:23180	 l-p:0.05605567991733551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.6100, 26.6100, 26.6100],
        [26.6100, 30.9128, 31.9051],
        [26.6100, 26.8167, 26.6511],
        [26.6100, 32.1904, 34.3271]], grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.056055907160043716 
model_pd.l_d.mean(): -1.5702787637710571 
model_pd.lagr.mean(): -1.5142228603363037 
model_pd.lambdas: dict_items([('pout', tensor([0.9696])), ('power', tensor([0.0483]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5239]))])
epoch：1160	 i:0 	 global-step:23200	 l-p:0.056055907160043716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.6091, 26.6190, 26.6094],
        [26.6091, 29.5280, 29.5657],
        [26.6091, 26.6091, 26.6091],
        [26.6091, 30.9118, 31.9040]], grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.05605614185333252 
model_pd.l_d.mean(): -1.5677891969680786 
model_pd.lagr.mean(): -1.511733055114746 
model_pd.lambdas: dict_items([('pout', tensor([0.9680])), ('power', tensor([0.0482]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5249]))])
epoch：1161	 i:0 	 global-step:23220	 l-p:0.05605614185333252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6080, 26.6098, 26.6081],
        [26.6080, 26.9591, 26.7054],
        [26.6080, 26.9560, 26.7040],
        [26.6080, 26.7917, 26.6420]], grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.056056395173072815 
model_pd.l_d.mean(): -1.5652999877929688 
model_pd.lagr.mean(): -1.5092436075210571 
model_pd.lambdas: dict_items([('pout', tensor([0.9665])), ('power', tensor([0.0481]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5259]))])
epoch：1162	 i:0 	 global-step:23240	 l-p:0.056056395173072815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.6070, 26.6070, 26.6070],
        [26.6070, 26.9476, 26.6997],
        [26.6070, 29.5259, 29.5636],
        [26.6070, 26.7145, 26.6213]], grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.056056637316942215 
model_pd.l_d.mean(): -1.5628104209899902 
model_pd.lagr.mean(): -1.5067538022994995 
model_pd.lambdas: dict_items([('pout', tensor([0.9650])), ('power', tensor([0.0481]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5269]))])
epoch：1163	 i:0 	 global-step:23260	 l-p:0.056056637316942215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.6060, 28.9928, 28.7721],
        [26.6060, 26.9539, 26.7019],
        [26.6060, 26.6075, 26.6060],
        [26.6060, 30.9085, 31.9008]], grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.05605689436197281 
model_pd.l_d.mean(): -1.5603208541870117 
model_pd.lagr.mean(): -1.504263997077942 
model_pd.lambdas: dict_items([('pout', tensor([0.9634])), ('power', tensor([0.0480]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5279]))])
epoch：1164	 i:0 	 global-step:23280	 l-p:0.05605689436197281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6050, 27.2650, 26.8783],
        [26.6050, 31.7054, 33.3883],
        [26.6050, 26.6445, 26.6079],
        [26.6050, 26.6050, 26.6049]], grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.05605714023113251 
model_pd.l_d.mean(): -1.5578312873840332 
model_pd.lagr.mean(): -1.5017741918563843 
model_pd.lambdas: dict_items([('pout', tensor([0.9619])), ('power', tensor([0.0479]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5408])), ('power', tensor([-1.5289]))])
epoch：1165	 i:0 	 global-step:23300	 l-p:0.05605714023113251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[26.6040, 32.1899, 34.3325],
        [26.6040, 27.2640, 26.8773],
        [26.6040, 28.4345, 28.0303],
        [26.6040, 31.1518, 32.3471]], grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.056057386100292206 
model_pd.l_d.mean(): -1.5553420782089233 
model_pd.lagr.mean(): -1.4992847442626953 
model_pd.lambdas: dict_items([('pout', tensor([0.9603])), ('power', tensor([0.0478]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5408])), ('power', tensor([-1.5299]))])
epoch：1166	 i:0 	 global-step:23320	 l-p:0.056057386100292206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6030, 27.2630, 26.8763],
        [26.6030, 26.6053, 26.6030],
        [26.6030, 31.1508, 32.3461],
        [26.6030, 26.6045, 26.6030]], grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.05605762079358101 
model_pd.l_d.mean(): -1.5528525114059448 
model_pd.lagr.mean(): -1.4967949390411377 
model_pd.lambdas: dict_items([('pout', tensor([0.9588])), ('power', tensor([0.0478]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5407])), ('power', tensor([-1.5308]))])
epoch：1167	 i:0 	 global-step:23340	 l-p:0.05605762079358101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6021, 26.6057, 26.6022],
        [26.6021, 27.3881, 26.9658],
        [26.6021, 26.6022, 26.6021],
        [26.6021, 27.5669, 27.1102]], grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.05605784058570862 
model_pd.l_d.mean(): -1.5503631830215454 
model_pd.lagr.mean(): -1.4943053722381592 
model_pd.lambdas: dict_items([('pout', tensor([0.9573])), ('power', tensor([0.0477]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5407])), ('power', tensor([-1.5317]))])
epoch：1168	 i:0 	 global-step:23360	 l-p:0.05605784058570862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.6012, 32.1807, 34.3173],
        [26.6012, 27.7434, 27.2696],
        [26.6012, 26.6017, 26.6012],
        [26.6012, 26.8079, 26.6423]], grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.05605804920196533 
model_pd.l_d.mean(): -1.5478734970092773 
model_pd.lagr.mean(): -1.491815447807312 
model_pd.lambdas: dict_items([('pout', tensor([0.9557])), ('power', tensor([0.0476]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5406])), ('power', tensor([-1.5326]))])
epoch：1169	 i:0 	 global-step:23380	 l-p:0.05605804920196533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6005, 26.8071, 26.6416],
        [26.6005, 29.5192, 29.5570],
        [26.6005, 27.8842, 27.4080],
        [26.6005, 28.0782, 27.6136]], grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.056058235466480255 
model_pd.l_d.mean(): -1.5453838109970093 
model_pd.lagr.mean(): -1.4893255233764648 
model_pd.lambdas: dict_items([('pout', tensor([0.9542])), ('power', tensor([0.0475]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5406])), ('power', tensor([-1.5333]))])
epoch：1170	 i:0 	 global-step:23400	 l-p:0.056058235466480255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.5997, 28.0775, 27.6129],
        [26.5997, 32.1853, 34.3280],
        [26.5997, 26.9403, 26.6924],
        [26.5997, 30.6086, 31.3703]], grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.056058406829833984 
model_pd.l_d.mean(): -1.5428943634033203 
model_pd.lagr.mean(): -1.4868359565734863 
model_pd.lambdas: dict_items([('pout', tensor([0.9526])), ('power', tensor([0.0475]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5406])), ('power', tensor([-1.5340]))])
epoch：1171	 i:0 	 global-step:23420	 l-p:0.056058406829833984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.5991, 29.8112, 30.0207],
        [26.5991, 30.9013, 31.8937],
        [26.5991, 32.1391, 34.2374],
        [26.5991, 34.0262, 38.1399]], grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.05605857074260712 
model_pd.l_d.mean(): -1.5404046773910522 
model_pd.lagr.mean(): -1.4843461513519287 
model_pd.lambdas: dict_items([('pout', tensor([0.9511])), ('power', tensor([0.0474]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5347]))])
epoch：1172	 i:0 	 global-step:23440	 l-p:0.05605857074260712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.5986, 26.7061, 26.6129],
        [26.5986, 29.2407, 29.1352],
        [26.5986, 31.6987, 33.3816],
        [26.5986, 27.8824, 27.4061]], grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.05605870112776756 
model_pd.l_d.mean(): -1.5379152297973633 
model_pd.lagr.mean(): -1.4818565845489502 
model_pd.lambdas: dict_items([('pout', tensor([0.9496])), ('power', tensor([0.0473]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5352]))])
epoch：1173	 i:0 	 global-step:23460	 l-p:0.05605870112776756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.5981, 27.8819, 27.4057],
        [26.5981, 27.5054, 27.0577],
        [26.5981, 26.9492, 26.6955],
        [26.5981, 28.0759, 27.6113]], grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.056058816611766815 
model_pd.l_d.mean(): -1.5354253053665161 
model_pd.lagr.mean(): -1.4793665409088135 
model_pd.lambdas: dict_items([('pout', tensor([0.9480])), ('power', tensor([0.0472]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5357]))])
epoch：1174	 i:0 	 global-step:23480	 l-p:0.056058816611766815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.5977, 30.8999, 31.8925],
        [26.5977, 32.1377, 34.2362],
        [26.5977, 26.8044, 26.6388],
        [26.5977, 26.9383, 26.6904]], grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.05605890229344368 
model_pd.l_d.mean(): -1.5329359769821167 
model_pd.lagr.mean(): -1.4768770933151245 
model_pd.lambdas: dict_items([('pout', tensor([0.9465])), ('power', tensor([0.0472]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5360]))])
epoch：1175	 i:0 	 global-step:23500	 l-p:0.05605890229344368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.5974, 26.6010, 26.5975],
        [26.5974, 29.5163, 29.5543],
        [26.5974, 34.0245, 38.1384],
        [26.5974, 26.9454, 26.6934]], grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.05605896934866905 
model_pd.l_d.mean(): -1.530446171760559 
model_pd.lagr.mean(): -1.4743871688842773 
model_pd.lambdas: dict_items([('pout', tensor([0.9449])), ('power', tensor([0.0471]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5363]))])
epoch：1176	 i:0 	 global-step:23520	 l-p:0.05605896934866905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.5972, 26.7047, 26.6115],
        [26.5972, 31.1450, 32.3406],
        [26.5972, 27.3834, 26.9610],
        [26.5972, 31.6975, 33.3806]], grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.05605902522802353 
model_pd.l_d.mean(): -1.527956247329712 
model_pd.lagr.mean(): -1.4718972444534302 
model_pd.lambdas: dict_items([('pout', tensor([0.9434])), ('power', tensor([0.0470]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5404])), ('power', tensor([-1.5365]))])
epoch：1177	 i:0 	 global-step:23540	 l-p:0.05605902522802353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.5971, 26.5994, 26.5971],
        [26.5971, 29.6109, 29.7023],
        [26.5971, 26.6257, 26.5988],
        [26.5971, 28.4279, 28.0237]], grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.056059058755636215 
model_pd.l_d.mean(): -1.5254666805267334 
model_pd.lagr.mean(): -1.4694076776504517 
model_pd.lambdas: dict_items([('pout', tensor([0.9419])), ('power', tensor([0.0469]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5404])), ('power', tensor([-1.5366]))])
epoch：1178	 i:0 	 global-step:23560	 l-p:0.056059058755636215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.5970, 26.6007, 26.5971],
        [26.5970, 32.1373, 34.2360],
        [26.5970, 26.6366, 26.5999],
        [26.5970, 28.9841, 28.7636]], grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.05605906620621681 
model_pd.l_d.mean(): -1.5229768753051758 
model_pd.lagr.mean(): -1.4669177532196045 
model_pd.lambdas: dict_items([('pout', tensor([0.9403])), ('power', tensor([0.0468]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5404])), ('power', tensor([-1.5367]))])
epoch：1179	 i:0 	 global-step:23580	 l-p:0.05605906620621681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.5971, 32.1831, 34.3262],
        [26.5971, 29.8096, 30.0194],
        [26.5971, 26.6007, 26.5971],
        [26.5971, 31.6977, 33.3810]], grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.05605905503034592 
model_pd.l_d.mean(): -1.5204869508743286 
model_pd.lagr.mean(): -1.4644279479980469 
model_pd.lambdas: dict_items([('pout', tensor([0.9388])), ('power', tensor([0.0468]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5404])), ('power', tensor([-1.5366]))])
epoch：1180	 i:0 	 global-step:23600	 l-p:0.05605905503034592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.5972, 26.5972, 26.5972],
        [26.5972, 26.5972, 26.5972],
        [26.5972, 28.0753, 27.6106],
        [26.5972, 26.5987, 26.5972]], grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.05605902522802353 
model_pd.l_d.mean(): -1.5179972648620605 
model_pd.lagr.mean(): -1.4619382619857788 
model_pd.lambdas: dict_items([('pout', tensor([0.9372])), ('power', tensor([0.0467]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5365]))])
epoch：1181	 i:0 	 global-step:23620	 l-p:0.05605902522802353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.5973, 28.9846, 28.7642],
        [26.5973, 28.3494, 27.9276],
        [26.5973, 26.9381, 26.6901],
        [26.5973, 26.7811, 26.6313]], grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.056058984249830246 
model_pd.l_d.mean(): -1.5155073404312134 
model_pd.lagr.mean(): -1.4594483375549316 
model_pd.lambdas: dict_items([('pout', tensor([0.9357])), ('power', tensor([0.0466]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5363]))])
epoch：1182	 i:0 	 global-step:23640	 l-p:0.056058984249830246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.5976, 26.6076, 26.5979],
        [26.5976, 32.1779, 34.3153],
        [26.5976, 27.8818, 27.4055],
        [26.5976, 27.5052, 27.0574]], grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.05605892837047577 
model_pd.l_d.mean(): -1.5130174160003662 
model_pd.lagr.mean(): -1.456958532333374 
model_pd.lambdas: dict_items([('pout', tensor([0.9342])), ('power', tensor([0.0465]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5361]))])
epoch：1183	 i:0 	 global-step:23660	 l-p:0.05605892837047577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.5979, 26.5996, 26.5979],
        [26.5979, 26.6079, 26.5982],
        [26.5979, 26.6265, 26.5996],
        [26.5979, 29.5174, 29.5556]], grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.0560588613152504 
model_pd.l_d.mean(): -1.5105276107788086 
model_pd.lagr.mean(): -1.4544687271118164 
model_pd.lambdas: dict_items([('pout', tensor([0.9326])), ('power', tensor([0.0465]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5358]))])
epoch：1184	 i:0 	 global-step:23680	 l-p:0.0560588613152504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.5982, 26.9390, 26.6909],
        [26.5982, 28.4295, 28.0253],
        [26.5982, 29.5179, 29.5561],
        [26.5982, 26.5997, 26.5982]], grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.05605877563357353 
model_pd.l_d.mean(): -1.5080376863479614 
model_pd.lagr.mean(): -1.4519789218902588 
model_pd.lambdas: dict_items([('pout', tensor([0.9311])), ('power', tensor([0.0464]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5355]))])
epoch：1185	 i:0 	 global-step:23700	 l-p:0.05605877563357353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.5986, 26.9518, 26.6969],
        [26.5986, 32.1400, 34.2394],
        [26.5986, 28.4300, 28.0258],
        [26.5986, 27.2590, 26.8721]], grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.05605868250131607 
model_pd.l_d.mean(): -1.5055476427078247 
model_pd.lagr.mean(): -1.4494889974594116 
model_pd.lambdas: dict_items([('pout', tensor([0.9295])), ('power', tensor([0.0463]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5351]))])
epoch：1186	 i:0 	 global-step:23720	 l-p:0.05605868250131607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.5990, 29.4286, 29.4179],
        [26.5990, 26.5990, 26.5990],
        [26.5990, 35.9720, 42.4955],
        [26.5990, 29.5190, 29.5572]], grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.056058574467897415 
model_pd.l_d.mean(): -1.5030573606491089 
model_pd.lagr.mean(): -1.4469988346099854 
model_pd.lambdas: dict_items([('pout', tensor([0.9280])), ('power', tensor([0.0462]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5405])), ('power', tensor([-1.5347]))])
epoch：1187	 i:0 	 global-step:23740	 l-p:0.056058574467897415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.5995, 26.5995, 26.5995],
        [26.5995, 26.7833, 26.6334],
        [26.5995, 30.6100, 31.3724],
        [26.5995, 32.1808, 34.3187]], grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.05605846270918846 
model_pd.l_d.mean(): -1.5005676746368408 
model_pd.lagr.mean(): -1.4445092678070068 
model_pd.lambdas: dict_items([('pout', tensor([0.9265])), ('power', tensor([0.0462]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5406])), ('power', tensor([-1.5342]))])
epoch：1188	 i:0 	 global-step:23760	 l-p:0.05605846270918846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.5999, 27.2605, 26.8735],
        [26.5999, 29.8137, 30.0237],
        [26.5999, 27.5655, 27.1084],
        [26.5999, 26.5999, 26.5999]], grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.05605835095047951 
model_pd.l_d.mean(): -1.4980778694152832 
model_pd.lagr.mean(): -1.4420194625854492 
model_pd.lambdas: dict_items([('pout', tensor([0.9249])), ('power', tensor([0.0461]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5406])), ('power', tensor([-1.5337]))])
epoch：1189	 i:0 	 global-step:23780	 l-p:0.05605835095047951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.6004, 26.6006, 26.6004],
        [26.6004, 28.0792, 27.6144],
        [26.6004, 26.6290, 26.6021],
        [26.6004, 31.1502, 32.3467]], grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.05605822429060936 
model_pd.l_d.mean(): -1.4955878257751465 
model_pd.lagr.mean(): -1.4395296573638916 
model_pd.lambdas: dict_items([('pout', tensor([0.9234])), ('power', tensor([0.0460]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5406])), ('power', tensor([-1.5332]))])
epoch：1190	 i:0 	 global-step:23800	 l-p:0.05605822429060936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.6009, 26.6046, 26.6010],
        [26.6009, 26.6033, 26.6010],
        [26.6009, 35.9752, 42.4997],
        [26.6009, 30.9054, 31.8989]], grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.05605810508131981 
model_pd.l_d.mean(): -1.4930976629257202 
model_pd.lagr.mean(): -1.4370396137237549 
model_pd.lambdas: dict_items([('pout', tensor([0.9218])), ('power', tensor([0.0459]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5407])), ('power', tensor([-1.5327]))])
epoch：1191	 i:0 	 global-step:23820	 l-p:0.05605810508131981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.6014, 26.6032, 26.6015],
        [26.6014, 26.6029, 26.6015],
        [26.6014, 26.9529, 26.6989],
        [26.6014, 27.5095, 27.0615]], grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.05605797842144966 
model_pd.l_d.mean(): -1.4906079769134521 
model_pd.lagr.mean(): -1.4345500469207764 
model_pd.lambdas: dict_items([('pout', tensor([0.9203])), ('power', tensor([0.0458]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5407])), ('power', tensor([-1.5322]))])
epoch：1192	 i:0 	 global-step:23840	 l-p:0.05605797842144966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.6019, 26.6037, 26.6020],
        [26.6019, 26.6043, 26.6020],
        [26.6019, 26.9503, 26.6980],
        [26.6019, 31.1523, 32.3491]], grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.056057848036289215 
model_pd.l_d.mean(): -1.4881176948547363 
model_pd.lagr.mean(): -1.43205988407135 
model_pd.lambdas: dict_items([('pout', tensor([0.9188])), ('power', tensor([0.0458]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5407])), ('power', tensor([-1.5317]))])
epoch：1193	 i:0 	 global-step:23860	 l-p:0.056057848036289215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.6025, 29.4330, 29.4224],
        [26.6025, 26.6024, 26.6025],
        [26.6025, 32.1851, 34.3237],
        [26.6025, 29.8169, 30.0271]], grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.056057725101709366 
model_pd.l_d.mean(): -1.4856276512145996 
model_pd.lagr.mean(): -1.429569959640503 
model_pd.lambdas: dict_items([('pout', tensor([0.9172])), ('power', tensor([0.0457]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5407])), ('power', tensor([-1.5312]))])
epoch：1194	 i:0 	 global-step:23880	 l-p:0.056057725101709366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.6029, 32.1858, 34.3246],
        [26.6029, 26.6029, 26.6030],
        [26.6029, 26.6030, 26.6029],
        [26.6029, 30.9081, 31.9019]], grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.056057605892419815 
model_pd.l_d.mean(): -1.483137607574463 
model_pd.lagr.mean(): -1.4270800352096558 
model_pd.lambdas: dict_items([('pout', tensor([0.9157])), ('power', tensor([0.0456]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5408])), ('power', tensor([-1.5307]))])
epoch：1195	 i:0 	 global-step:23900	 l-p:0.056057605892419815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6034, 26.9444, 26.6962],
        [26.6034, 30.6154, 31.3783],
        [26.6034, 32.1471, 34.2476],
        [26.6034, 26.6034, 26.6034]], grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.056057486683130264 
model_pd.l_d.mean(): -1.480647325515747 
model_pd.lagr.mean(): -1.4245898723602295 
model_pd.lambdas: dict_items([('pout', tensor([0.9141])), ('power', tensor([0.0455]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5408])), ('power', tensor([-1.5302]))])
epoch：1196	 i:0 	 global-step:23920	 l-p:0.056057486683130264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.6039, 26.8108, 26.6450],
        [26.6039, 34.3758, 38.8921],
        [26.6039, 27.5699, 27.1127],
        [26.6039, 29.6201, 29.7120]], grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.05605737119913101 
model_pd.l_d.mean(): -1.4781574010849 
model_pd.lagr.mean(): -1.4221000671386719 
model_pd.lambdas: dict_items([('pout', tensor([0.9126])), ('power', tensor([0.0455]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5408])), ('power', tensor([-1.5298]))])
epoch：1197	 i:0 	 global-step:23940	 l-p:0.05605737119913101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6043, 34.3765, 38.8929],
        [26.6043, 27.5127, 27.0646],
        [26.6043, 35.3180, 40.9892],
        [26.6043, 26.9578, 26.7027]], grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 0.056057266891002655 
model_pd.l_d.mean(): -1.4756675958633423 
model_pd.lagr.mean(): -1.4196103811264038 
model_pd.lambdas: dict_items([('pout', tensor([0.9110])), ('power', tensor([0.0454]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5408])), ('power', tensor([-1.5293]))])
epoch：1198	 i:0 	 global-step:23960	 l-p:0.056057266891002655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6047, 26.6048, 26.6048],
        [26.6047, 32.1490, 34.2499],
        [26.6047, 32.1946, 34.3398],
        [26.6047, 26.9458, 26.6976]], grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.0560571663081646 
model_pd.l_d.mean(): -1.4731773138046265 
model_pd.lagr.mean(): -1.417120099067688 
model_pd.lambdas: dict_items([('pout', tensor([0.9095])), ('power', tensor([0.0453]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5289]))])
epoch：1199	 i:0 	 global-step:23980	 l-p:0.0560571663081646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 27.8905, 27.4139],
        [26.6051, 26.9568, 26.7027],
        [26.6051, 33.8921, 37.8424],
        [26.6051, 26.6338, 26.6069]], grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.05605706572532654 
model_pd.l_d.mean(): -1.4706875085830688 
model_pd.lagr.mean(): -1.41463041305542 
model_pd.lambdas: dict_items([('pout', tensor([0.9080])), ('power', tensor([0.0452]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5285]))])
epoch：1200	 i:0 	 global-step:24000	 l-p:0.05605706572532654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.8125, 26.6467],
        [26.6055, 32.1502, 34.2512],
        [26.6055, 27.8910, 27.4143],
        [26.6055, 30.6183, 31.3815]], grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.056056976318359375 
model_pd.l_d.mean(): -1.4681971073150635 
model_pd.lagr.mean(): -1.412140130996704 
model_pd.lambdas: dict_items([('pout', tensor([0.9064])), ('power', tensor([0.0452]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5282]))])
epoch：1201	 i:0 	 global-step:24020	 l-p:0.056056976318359375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.6058, 26.9543, 26.7020],
        [26.6058, 26.6058, 26.6058],
        [26.6058, 28.0856, 27.6205],
        [26.6058, 26.6058, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.056056901812553406 
model_pd.l_d.mean(): -1.4657071828842163 
model_pd.lagr.mean(): -1.4096503257751465 
model_pd.lambdas: dict_items([('pout', tensor([0.9049])), ('power', tensor([0.0451]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5278]))])
epoch：1202	 i:0 	 global-step:24040	 l-p:0.056056901812553406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6061, 27.3933, 26.9704],
        [26.6061, 32.1512, 34.2524],
        [26.6061, 26.6347, 26.6078],
        [26.6061, 26.9596, 26.7045]], grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.05605682358145714 
model_pd.l_d.mean(): -1.4632171392440796 
model_pd.lagr.mean(): -1.4071602821350098 
model_pd.lambdas: dict_items([('pout', tensor([0.9033])), ('power', tensor([0.0450]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5275]))])
epoch：1203	 i:0 	 global-step:24060	 l-p:0.05605682358145714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6064, 30.6196, 31.3830],
        [26.6064, 27.5727, 27.1154],
        [26.6064, 26.6100, 26.6065],
        [26.6064, 26.9475, 26.6992]], grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.056056756526231766 
model_pd.l_d.mean(): -1.4607272148132324 
model_pd.lagr.mean(): -1.4046704769134521 
model_pd.lambdas: dict_items([('pout', tensor([0.9018])), ('power', tensor([0.0449]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5273]))])
epoch：1204	 i:0 	 global-step:24080	 l-p:0.056056756526231766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[26.6066, 30.9135, 31.9079],
        [26.6066, 27.3939, 26.9710],
        [26.6066, 32.1520, 34.2534],
        [26.6066, 27.5153, 27.0671]], grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.05605670064687729 
model_pd.l_d.mean(): -1.4582370519638062 
model_pd.lagr.mean(): -1.4021803140640259 
model_pd.lambdas: dict_items([('pout', tensor([0.9003])), ('power', tensor([0.0449]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1205	 i:0 	 global-step:24100	 l-p:0.05605670064687729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.6068, 26.9604, 26.7052],
        [26.6068, 26.6104, 26.6069],
        [26.6068, 26.8138, 26.6480],
        [26.6068, 34.3808, 38.8985]], grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.05605665221810341 
model_pd.l_d.mean(): -1.4557468891143799 
model_pd.lagr.mean(): -1.3996902704238892 
model_pd.lambdas: dict_items([('pout', tensor([0.8987])), ('power', tensor([0.0448]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5268]))])
epoch：1206	 i:0 	 global-step:24120	 l-p:0.05605665221810341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.6070, 29.5295, 29.5682],
        [26.6070, 26.6466, 26.6099],
        [26.6070, 27.2681, 26.8809],
        [26.6070, 32.1527, 34.2543]], grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.056056614965200424 
model_pd.l_d.mean(): -1.4532570838928223 
model_pd.lagr.mean(): -1.3972004652023315 
model_pd.lambdas: dict_items([('pout', tensor([0.8972])), ('power', tensor([0.0447]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5267]))])
epoch：1207	 i:0 	 global-step:24140	 l-p:0.056056614965200424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6071, 27.5159, 27.0676],
        [26.6071, 26.9607, 26.7055],
        [26.6071, 26.6076, 26.6071],
        [26.6071, 28.0872, 27.6221]], grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.05605657026171684 
model_pd.l_d.mean(): -1.4507670402526855 
model_pd.lagr.mean(): -1.3947104215621948 
model_pd.lambdas: dict_items([('pout', tensor([0.8956])), ('power', tensor([0.0446]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5265]))])
epoch：1208	 i:0 	 global-step:24160	 l-p:0.05605657026171684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.6072, 26.9608, 26.7057],
        [26.6072, 34.0413, 38.1603],
        [26.6072, 26.9559, 26.7034],
        [26.6072, 30.6211, 31.3847]], grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.05605654790997505 
model_pd.l_d.mean(): -1.4482769966125488 
model_pd.lagr.mean(): -1.3922204971313477 
model_pd.lambdas: dict_items([('pout', tensor([0.8941])), ('power', tensor([0.0445]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5264]))])
epoch：1209	 i:0 	 global-step:24180	 l-p:0.05605654790997505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.6073, 26.6359, 26.6090],
        [26.6073, 26.6469, 26.6102],
        [26.6073, 26.6588, 26.6117],
        [26.6073, 27.2685, 26.8812]], grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.05605653300881386 
model_pd.l_d.mean(): -1.4457870721817017 
model_pd.lagr.mean(): -1.3897305727005005 
model_pd.lambdas: dict_items([('pout', tensor([0.8926])), ('power', tensor([0.0445]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5263]))])
epoch：1210	 i:0 	 global-step:24200	 l-p:0.05605653300881386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.6073, 26.6073, 26.6073],
        [26.6073, 30.6214, 31.3851],
        [26.6073, 26.9591, 26.7049],
        [26.6073, 29.2534, 29.1484]], grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.056056518107652664 
model_pd.l_d.mean(): -1.4432967901229858 
model_pd.lagr.mean(): -1.3872402906417847 
model_pd.lambdas: dict_items([('pout', tensor([0.8910])), ('power', tensor([0.0444]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5263]))])
epoch：1211	 i:0 	 global-step:24220	 l-p:0.056056518107652664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.6073, 27.5740, 27.1165],
        [26.6073, 26.6174, 26.6077],
        [26.6073, 26.6075, 26.6074],
        [26.6073, 30.6215, 31.3853]], grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.056056514382362366 
model_pd.l_d.mean(): -1.4408066272735596 
model_pd.lagr.mean(): -1.3847501277923584 
model_pd.lambdas: dict_items([('pout', tensor([0.8895])), ('power', tensor([0.0443]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5263]))])
epoch：1212	 i:0 	 global-step:24240	 l-p:0.056056514382362366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.6073, 26.6078, 26.6073],
        [26.6073, 29.4399, 29.4297],
        [26.6073, 26.6589, 26.6118],
        [26.6073, 28.3619, 27.9398]], grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.056056514382362366 
model_pd.l_d.mean(): -1.438316822052002 
model_pd.lagr.mean(): -1.3822603225708008 
model_pd.lambdas: dict_items([('pout', tensor([0.8879])), ('power', tensor([0.0442]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5263]))])
epoch：1213	 i:0 	 global-step:24260	 l-p:0.056056514382362366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.6073, 26.6075, 26.6073],
        [26.6073, 26.9592, 26.7049],
        [26.6073, 31.1608, 32.3590],
        [26.6073, 26.6073, 26.6073]], grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.056056518107652664 
model_pd.l_d.mean(): -1.4358266592025757 
model_pd.lagr.mean(): -1.3797701597213745 
model_pd.lambdas: dict_items([('pout', tensor([0.8864])), ('power', tensor([0.0442]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5263]))])
epoch：1214	 i:0 	 global-step:24280	 l-p:0.056056518107652664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.6073, 31.7138, 33.4001],
        [26.6073, 30.6217, 31.3856],
        [26.6073, 26.6073, 26.6073],
        [26.6073, 34.3825, 38.9013]], grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.05605653300881386 
model_pd.l_d.mean(): -1.4333364963531494 
model_pd.lagr.mean(): -1.3772799968719482 
model_pd.lambdas: dict_items([('pout', tensor([0.8848])), ('power', tensor([0.0441]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5263]))])
epoch：1215	 i:0 	 global-step:24300	 l-p:0.05605653300881386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.6072, 33.8967, 37.8490],
        [26.6072, 32.1996, 34.3462],
        [26.6072, 26.7913, 26.6412],
        [26.6072, 26.9559, 26.7034]], grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.05605654790997505 
model_pd.l_d.mean(): -1.4308468103408813 
model_pd.lagr.mean(): -1.3747903108596802 
model_pd.lambdas: dict_items([('pout', tensor([0.8833])), ('power', tensor([0.0440]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5264]))])
epoch：1216	 i:0 	 global-step:24320	 l-p:0.05605654790997505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6071, 28.9979, 28.7776],
        [26.6071, 33.8968, 37.8491],
        [26.6071, 29.4400, 29.4299],
        [26.6071, 26.9485, 26.7000]], grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.056056562811136246 
model_pd.l_d.mean(): -1.428356409072876 
model_pd.lagr.mean(): -1.3722997903823853 
model_pd.lambdas: dict_items([('pout', tensor([0.8818])), ('power', tensor([0.0439]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5265]))])
epoch：1217	 i:0 	 global-step:24340	 l-p:0.056056562811136246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.6070, 31.7138, 33.4003],
        [26.6070, 26.6085, 26.6070],
        [26.6070, 26.6070, 26.6070],
        [26.6070, 35.9869, 42.5166]], grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.056056585162878036 
model_pd.l_d.mean(): -1.4258663654327393 
model_pd.lagr.mean(): -1.3698097467422485 
model_pd.lambdas: dict_items([('pout', tensor([0.8802])), ('power', tensor([0.0439]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5266]))])
epoch：1218	 i:0 	 global-step:24360	 l-p:0.056056585162878036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.6069, 26.6169, 26.6073],
        [26.6069, 27.5738, 27.1163],
        [26.6069, 26.6069, 26.6069],
        [26.6069, 32.1996, 34.3464]], grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.056056614965200424 
model_pd.l_d.mean(): -1.423376202583313 
model_pd.lagr.mean(): -1.3673195838928223 
model_pd.lambdas: dict_items([('pout', tensor([0.8787])), ('power', tensor([0.0438]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5266]))])
epoch：1219	 i:0 	 global-step:24380	 l-p:0.056056614965200424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6068, 26.6068, 26.6068],
        [26.6068, 26.6068, 26.6068],
        [26.6068, 27.7514, 27.2768],
        [26.6068, 28.0875, 27.6223]], grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.056056637316942215 
model_pd.l_d.mean(): -1.4208862781524658 
model_pd.lagr.mean(): -1.364829659461975 
model_pd.lambdas: dict_items([('pout', tensor([0.8771])), ('power', tensor([0.0437]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5268]))])
epoch：1220	 i:0 	 global-step:24400	 l-p:0.056056637316942215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.6067, 26.6103, 26.6068],
        [26.6067, 35.9868, 42.5168],
        [26.6067, 26.9605, 26.7052],
        [26.6067, 27.8931, 27.4162]], grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): 0.0560566708445549 
model_pd.l_d.mean(): -1.418396234512329 
model_pd.lagr.mean(): -1.3623396158218384 
model_pd.lambdas: dict_items([('pout', tensor([0.8756])), ('power', tensor([0.0436]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5269]))])
epoch：1221	 i:0 	 global-step:24420	 l-p:0.0560566708445549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.6066, 31.1607, 32.3593],
        [26.6066, 30.6216, 31.3857],
        [26.6066, 28.4407, 28.0363],
        [26.6066, 26.6066, 26.6066]], grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.05605669319629669 
model_pd.l_d.mean(): -1.4159064292907715 
model_pd.lagr.mean(): -1.3598496913909912 
model_pd.lambdas: dict_items([('pout', tensor([0.8741])), ('power', tensor([0.0436]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1222	 i:0 	 global-step:24440	 l-p:0.05605669319629669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.6064, 26.6066, 26.6064],
        [26.6064, 27.5157, 27.0672],
        [26.6064, 29.8239, 30.0349],
        [26.6064, 26.6064, 26.6064]], grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.05605672672390938 
model_pd.l_d.mean(): -1.4134161472320557 
model_pd.lagr.mean(): -1.3573594093322754 
model_pd.lambdas: dict_items([('pout', tensor([0.8725])), ('power', tensor([0.0435]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1223	 i:0 	 global-step:24460	 l-p:0.05605672672390938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 26.6063, 26.6063],
        [26.6063, 28.9975, 28.7773],
        [26.6063, 26.9478, 26.6992],
        [26.6063, 27.3942, 26.9710]], grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.056056756526231766 
model_pd.l_d.mean(): -1.410926103591919 
model_pd.lagr.mean(): -1.3548693656921387 
model_pd.lambdas: dict_items([('pout', tensor([0.8710])), ('power', tensor([0.0434]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1224	 i:0 	 global-step:24480	 l-p:0.056056756526231766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.6062, 27.5155, 27.0670],
        [26.6062, 26.6578, 26.6106],
        [26.6062, 35.9866, 42.5169],
        [26.6062, 28.5308, 28.1494]], grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.4084359407424927 
model_pd.lagr.mean(): -1.3523792028427124 
model_pd.lambdas: dict_items([('pout', tensor([0.8694])), ('power', tensor([0.0433]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5274]))])
epoch：1225	 i:0 	 global-step:24500	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6060, 28.4403, 28.0359],
        [26.6060, 26.6078, 26.6061],
        [26.6060, 28.5307, 28.1493],
        [26.6060, 26.9599, 26.7046]], grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.05605681985616684 
model_pd.l_d.mean(): -1.405945897102356 
model_pd.lagr.mean(): -1.3498890399932861 
model_pd.lambdas: dict_items([('pout', tensor([0.8679])), ('power', tensor([0.0432]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5275]))])
epoch：1226	 i:0 	 global-step:24520	 l-p:0.05605681985616684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.6059, 34.0418, 38.1624],
        [26.6059, 34.3822, 38.9020],
        [26.6059, 26.9548, 26.7022],
        [26.6059, 32.1931, 34.3344]], grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -1.4034557342529297 
model_pd.lagr.mean(): -1.3473988771438599 
model_pd.lambdas: dict_items([('pout', tensor([0.8664])), ('power', tensor([0.0432]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5276]))])
epoch：1227	 i:0 	 global-step:24540	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 27.3938, 26.9706],
        [26.6058, 26.6063, 26.6058],
        [26.6058, 26.6058, 26.6058],
        [26.6058, 29.5299, 29.5690]], grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -1.4009654521942139 
model_pd.lagr.mean(): -1.344908595085144 
model_pd.lambdas: dict_items([('pout', tensor([0.8648])), ('power', tensor([0.0431]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5277]))])
epoch：1228	 i:0 	 global-step:24560	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 26.6062, 26.6057],
        [26.6057, 26.6573, 26.6101],
        [26.6057, 34.3821, 38.9021],
        [26.6057, 29.6248, 29.7173]], grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.056056901812553406 
model_pd.l_d.mean(): -1.3984755277633667 
model_pd.lagr.mean(): -1.3424186706542969 
model_pd.lambdas: dict_items([('pout', tensor([0.8633])), ('power', tensor([0.0430]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5278]))])
epoch：1229	 i:0 	 global-step:24580	 l-p:0.056056901812553406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.6056, 35.9864, 42.5173],
        [26.6056, 29.4393, 29.4294],
        [26.6056, 26.6056, 26.6056],
        [26.6056, 28.5304, 28.1490]], grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.0560569241642952 
model_pd.l_d.mean(): -1.3959853649139404 
model_pd.lagr.mean(): -1.339928388595581 
model_pd.lambdas: dict_items([('pout', tensor([0.8617])), ('power', tensor([0.0429]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5279]))])
epoch：1230	 i:0 	 global-step:24600	 l-p:0.0560569241642952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.7133, 26.6199],
        [26.6055, 33.8963, 37.8498],
        [26.6055, 27.7504, 27.2758],
        [26.6055, 26.6342, 26.6072]], grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.05605694279074669 
model_pd.l_d.mean(): -1.3934952020645142 
model_pd.lagr.mean(): -1.3374382257461548 
model_pd.lambdas: dict_items([('pout', tensor([0.8602])), ('power', tensor([0.0429]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5280]))])
epoch：1231	 i:0 	 global-step:24620	 l-p:0.05605694279074669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 26.6091, 26.6055],
        [26.6054, 26.9544, 26.7017],
        [26.6054, 26.6154, 26.6058],
        [26.6054, 29.4393, 29.4294]], grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.05605697259306908 
model_pd.l_d.mean(): -1.3910049200057983 
model_pd.lagr.mean(): -1.334947943687439 
model_pd.lambdas: dict_items([('pout', tensor([0.8587])), ('power', tensor([0.0428]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5281]))])
epoch：1232	 i:0 	 global-step:24640	 l-p:0.05605697259306908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 32.1992, 34.3469],
        [26.6053, 26.6340, 26.6071],
        [26.6053, 35.3237, 40.9993],
        [26.6053, 26.9543, 26.7016]], grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.05605698749423027 
model_pd.l_d.mean(): -1.3885146379470825 
model_pd.lagr.mean(): -1.3324576616287231 
model_pd.lambdas: dict_items([('pout', tensor([0.8571])), ('power', tensor([0.0427]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5281]))])
epoch：1233	 i:0 	 global-step:24660	 l-p:0.05605698749423027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6053, 26.6053],
        [26.6053, 28.4400, 28.0356],
        [26.6053, 29.8236, 30.0348],
        [26.6053, 26.7131, 26.6196]], grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.056056998670101166 
model_pd.l_d.mean(): -1.3860245943069458 
model_pd.lagr.mean(): -1.3299676179885864 
model_pd.lambdas: dict_items([('pout', tensor([0.8556])), ('power', tensor([0.0426]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5282]))])
epoch：1234	 i:0 	 global-step:24680	 l-p:0.056056998670101166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6052, 28.3609, 27.9387],
        [26.6052, 27.3934, 26.9701],
        [26.6052, 30.6213, 31.3859],
        [26.6052, 26.6052, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.05605701357126236 
model_pd.l_d.mean(): -1.38353431224823 
model_pd.lagr.mean(): -1.3274773359298706 
model_pd.lambdas: dict_items([('pout', tensor([0.8540])), ('power', tensor([0.0426]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5282]))])
epoch：1235	 i:0 	 global-step:24700	 l-p:0.05605701357126236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.9542, 26.7015],
        [26.6052, 34.3823, 38.9029],
        [26.6052, 29.4393, 29.4295],
        [26.6052, 26.7894, 26.6392]], grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.056057024747133255 
model_pd.l_d.mean(): -1.3810442686080933 
model_pd.lagr.mean(): -1.3249872922897339 
model_pd.lambdas: dict_items([('pout', tensor([0.8525])), ('power', tensor([0.0425]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1236	 i:0 	 global-step:24720	 l-p:0.056057024747133255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[26.6051, 27.5726, 27.1149],
        [26.6051, 28.3609, 27.9387],
        [26.6051, 27.3933, 26.9700],
        [26.6051, 29.6248, 29.7175]], grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.05605702847242355 
model_pd.l_d.mean(): -1.3785539865493774 
model_pd.lagr.mean(): -1.322497010231018 
model_pd.lambdas: dict_items([('pout', tensor([0.8509])), ('power', tensor([0.0424]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1237	 i:0 	 global-step:24740	 l-p:0.05605702847242355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 29.6248, 29.7176],
        [26.6051, 29.8237, 30.0350],
        [26.6051, 31.1607, 32.3600],
        [26.6051, 26.6088, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.05605703219771385 
model_pd.l_d.mean(): -1.376063585281372 
model_pd.lagr.mean(): -1.3200066089630127 
model_pd.lambdas: dict_items([('pout', tensor([0.8494])), ('power', tensor([0.0423]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1238	 i:0 	 global-step:24760	 l-p:0.05605703219771385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.6066, 26.6051],
        [26.6051, 26.9591, 26.7037],
        [26.6051, 34.3825, 38.9035],
        [26.6051, 27.8922, 27.4152]], grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.05605703964829445 
model_pd.l_d.mean(): -1.3735735416412354 
model_pd.lagr.mean(): -1.3175164461135864 
model_pd.lambdas: dict_items([('pout', tensor([0.8479])), ('power', tensor([0.0423]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1239	 i:0 	 global-step:24780	 l-p:0.05605703964829445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 28.0867, 27.6214],
        [26.6051, 26.8124, 26.6463],
        [26.6051, 29.2531, 29.1485],
        [26.6051, 28.5305, 28.1491]], grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.05605704337358475 
model_pd.l_d.mean(): -1.371083378791809 
model_pd.lagr.mean(): -1.3150262832641602 
model_pd.lambdas: dict_items([('pout', tensor([0.8463])), ('power', tensor([0.0422]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1240	 i:0 	 global-step:24800	 l-p:0.05605704337358475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.6051, 26.6051],
        [26.6051, 30.6217, 31.3866],
        [26.6051, 28.4402, 28.0358],
        [26.6051, 26.7894, 26.6391]], grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.05605703964829445 
model_pd.l_d.mean(): -1.3685930967330933 
model_pd.lagr.mean(): -1.3125360012054443 
model_pd.lambdas: dict_items([('pout', tensor([0.8448])), ('power', tensor([0.0421]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1241	 i:0 	 global-step:24820	 l-p:0.05605703964829445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.9468, 26.6981],
        [26.6051, 29.4397, 29.4300],
        [26.6051, 27.5727, 27.1149],
        [26.6051, 27.2671, 26.8794]], grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.05605703219771385 
model_pd.l_d.mean(): -1.366102933883667 
model_pd.lagr.mean(): -1.3100459575653076 
model_pd.lambdas: dict_items([('pout', tensor([0.8432])), ('power', tensor([0.0420]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1242	 i:0 	 global-step:24840	 l-p:0.05605703219771385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[26.6051, 28.4403, 28.0359],
        [26.6051, 28.5307, 28.1493],
        [26.6051, 27.8924, 27.4154],
        [26.6051, 29.5303, 29.5697]], grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.05605702847242355 
model_pd.l_d.mean(): -1.3636126518249512 
model_pd.lagr.mean(): -1.3075556755065918 
model_pd.lambdas: dict_items([('pout', tensor([0.8417])), ('power', tensor([0.0419]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1243	 i:0 	 global-step:24860	 l-p:0.05605702847242355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.8125, 26.6464],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 35.3248, 41.0017],
        [26.6052, 27.7506, 27.2758]], grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.05605702102184296 
model_pd.l_d.mean(): -1.3611222505569458 
model_pd.lagr.mean(): -1.3050652742385864 
model_pd.lambdas: dict_items([('pout', tensor([0.8402])), ('power', tensor([0.0419]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5283]))])
epoch：1244	 i:0 	 global-step:24880	 l-p:0.05605702102184296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6053, 26.6052],
        [26.6052, 28.9977, 28.7777],
        [26.6052, 26.9543, 26.7015],
        [26.6052, 26.6339, 26.6069]], grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.05605701357126236 
model_pd.l_d.mean(): -1.3586320877075195 
model_pd.lagr.mean(): -1.3025751113891602 
model_pd.lambdas: dict_items([('pout', tensor([0.8386])), ('power', tensor([0.0418]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5282]))])
epoch：1245	 i:0 	 global-step:24900	 l-p:0.05605701357126236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6070, 26.6053],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 26.9594, 26.7038],
        [26.6052, 26.6057, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.056056998670101166 
model_pd.l_d.mean(): -1.3561416864395142 
model_pd.lagr.mean(): -1.3000847101211548 
model_pd.lambdas: dict_items([('pout', tensor([0.8371])), ('power', tensor([0.0417]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5282]))])
epoch：1246	 i:0 	 global-step:24920	 l-p:0.056056998670101166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6053, 26.9594, 26.7039],
        [26.6053, 33.8979, 37.8529],
        [26.6053, 26.6076, 26.6053],
        [26.6053, 26.6053, 26.6053]], grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.05605698749423027 
model_pd.l_d.mean(): -1.353651523590088 
model_pd.lagr.mean(): -1.2975945472717285 
model_pd.lambdas: dict_items([('pout', tensor([0.8355])), ('power', tensor([0.0416]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5409])), ('power', tensor([-1.5281]))])
epoch：1247	 i:0 	 global-step:24940	 l-p:0.05605698749423027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 29.6259, 29.7189],
        [26.6053, 26.6340, 26.6071],
        [26.6053, 27.7509, 27.2761],
        [26.6053, 26.6570, 26.6098]], grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.05605697259306908 
model_pd.l_d.mean(): -1.3511611223220825 
model_pd.lagr.mean(): -1.2951041460037231 
model_pd.lambdas: dict_items([('pout', tensor([0.8340])), ('power', tensor([0.0416]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5281]))])
epoch：1248	 i:0 	 global-step:24960	 l-p:0.05605697259306908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 27.8930, 27.4158],
        [26.6054, 33.8983, 37.8535],
        [26.6054, 32.1554, 34.2598],
        [26.6054, 26.7897, 26.6394]], grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.05605696141719818 
model_pd.l_d.mean(): -1.3486707210540771 
model_pd.lagr.mean(): -1.2926137447357178 
model_pd.lambdas: dict_items([('pout', tensor([0.8325])), ('power', tensor([0.0415]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5280]))])
epoch：1249	 i:0 	 global-step:24980	 l-p:0.05605696141719818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 26.9578, 26.7032],
        [26.6054, 26.6072, 26.6055],
        [26.6054, 29.5312, 29.5707],
        [26.6054, 27.2676, 26.8799]], grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.056056953966617584 
model_pd.l_d.mean(): -1.3461804389953613 
model_pd.lagr.mean(): -1.290123462677002 
model_pd.lambdas: dict_items([('pout', tensor([0.8309])), ('power', tensor([0.0414]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5280]))])
epoch：1250	 i:0 	 global-step:25000	 l-p:0.056056953966617584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.6571, 26.6099],
        [26.6055, 26.6342, 26.6072],
        [26.6055, 32.1557, 34.2603],
        [26.6055, 26.6060, 26.6055]], grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.056056927889585495 
model_pd.l_d.mean(): -1.3436901569366455 
model_pd.lagr.mean(): -1.2876331806182861 
model_pd.lambdas: dict_items([('pout', tensor([0.8294])), ('power', tensor([0.0413]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5279]))])
epoch：1251	 i:0 	 global-step:25020	 l-p:0.056056927889585495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.6056, 26.6055],
        [26.6055, 29.6264, 29.7195],
        [26.6055, 26.6055, 26.6055],
        [26.6055, 26.6156, 26.6059]], grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.0560569204390049 
model_pd.l_d.mean(): -1.3411997556686401 
model_pd.lagr.mean(): -1.2851427793502808 
model_pd.lambdas: dict_items([('pout', tensor([0.8278])), ('power', tensor([0.0413]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5278]))])
epoch：1252	 i:0 	 global-step:25040	 l-p:0.0560569204390049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.6056, 31.1627, 32.3629],
        [26.6056, 28.4414, 28.0370],
        [26.6056, 26.6056, 26.6056],
        [26.6056, 26.6056, 26.6056]], grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.056056905537843704 
model_pd.l_d.mean(): -1.3387095928192139 
model_pd.lagr.mean(): -1.282652735710144 
model_pd.lambdas: dict_items([('pout', tensor([0.8263])), ('power', tensor([0.0412]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5278]))])
epoch：1253	 i:0 	 global-step:25060	 l-p:0.056056905537843704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 26.6157, 26.6060],
        [26.6057, 26.9599, 26.7043],
        [26.6057, 26.6454, 26.6086],
        [26.6057, 29.4413, 29.4318]], grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.05605689063668251 
model_pd.l_d.mean(): -1.3362191915512085 
model_pd.lagr.mean(): -1.2801623344421387 
model_pd.lambdas: dict_items([('pout', tensor([0.8248])), ('power', tensor([0.0411]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5277]))])
epoch：1254	 i:0 	 global-step:25080	 l-p:0.05605689063668251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 29.5319, 29.5715],
        [26.6057, 32.1564, 34.2613],
        [26.6057, 27.5738, 27.1158],
        [26.6057, 26.9476, 26.6988]], grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -1.3337287902832031 
model_pd.lagr.mean(): -1.2776719331741333 
model_pd.lambdas: dict_items([('pout', tensor([0.8232])), ('power', tensor([0.0410]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5276]))])
epoch：1255	 i:0 	 global-step:25100	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.9551, 26.7022],
        [26.6058, 26.6455, 26.6087],
        [26.6058, 32.2022, 34.3516],
        [26.6058, 30.9175, 31.9144]], grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -1.3312386274337769 
model_pd.lagr.mean(): -1.275181770324707 
model_pd.lambdas: dict_items([('pout', tensor([0.8217])), ('power', tensor([0.0410]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5276]))])
epoch：1256	 i:0 	 global-step:25120	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 27.8938, 27.4166],
        [26.6058, 26.9477, 26.6989],
        [26.6058, 26.8133, 26.6471],
        [26.6058, 27.7518, 27.2769]], grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -1.328748345375061 
model_pd.lagr.mean(): -1.2726914882659912 
model_pd.lambdas: dict_items([('pout', tensor([0.8201])), ('power', tensor([0.0409]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5275]))])
epoch：1257	 i:0 	 global-step:25140	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[26.6059, 29.8262, 30.0381],
        [26.6059, 28.5324, 28.1510],
        [26.6059, 27.2683, 26.8804],
        [26.6059, 33.9000, 37.8561]], grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.056056827306747437 
model_pd.l_d.mean(): -1.3262580633163452 
model_pd.lagr.mean(): -1.2702012062072754 
model_pd.lambdas: dict_items([('pout', tensor([0.8186])), ('power', tensor([0.0408]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5275]))])
epoch：1258	 i:0 	 global-step:25160	 l-p:0.056056827306747437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.6059, 33.9002, 37.8564],
        [26.6059, 34.3860, 38.9092],
        [26.6059, 26.6059, 26.6059],
        [26.6059, 28.4421, 28.0376]], grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.05605681240558624 
model_pd.l_d.mean(): -1.3237674236297607 
model_pd.lagr.mean(): -1.267710566520691 
model_pd.lambdas: dict_items([('pout', tensor([0.8170])), ('power', tensor([0.0407]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5274]))])
epoch：1259	 i:0 	 global-step:25180	 l-p:0.05605681240558624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[26.6060, 29.4421, 29.4327],
        [26.6060, 27.5165, 27.0675],
        [26.6060, 28.0886, 27.6231],
        [26.6060, 35.3278, 41.0067]], grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.3212769031524658 
model_pd.lagr.mean(): -1.265220046043396 
model_pd.lambdas: dict_items([('pout', tensor([0.8155])), ('power', tensor([0.0407]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5273]))])
epoch：1260	 i:0 	 global-step:25200	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.6060, 26.9554, 26.7025],
        [26.6060, 26.6060, 26.6060],
        [26.6060, 31.1640, 32.3646],
        [26.6060, 26.6084, 26.6061]], grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.3187865018844604 
model_pd.lagr.mean(): -1.2627297639846802 
model_pd.lambdas: dict_items([('pout', tensor([0.8140])), ('power', tensor([0.0406]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5273]))])
epoch：1261	 i:0 	 global-step:25220	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[26.6061, 32.1576, 34.2630],
        [26.6061, 29.8268, 30.0387],
        [26.6061, 30.9184, 31.9156],
        [26.6061, 35.3282, 41.0073]], grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.056056782603263855 
model_pd.l_d.mean(): -1.3162962198257446 
model_pd.lagr.mean(): -1.2602394819259644 
model_pd.lambdas: dict_items([('pout', tensor([0.8124])), ('power', tensor([0.0405]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5273]))])
epoch：1262	 i:0 	 global-step:25240	 l-p:0.056056782603263855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.6061, 27.3951, 26.9714],
        [26.6061, 26.7141, 26.6205],
        [26.6061, 29.5330, 29.5727],
        [26.6061, 28.4425, 28.0381]], grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.05605677142739296 
model_pd.l_d.mean(): -1.3138059377670288 
model_pd.lagr.mean(): -1.2577491998672485 
model_pd.lambdas: dict_items([('pout', tensor([0.8109])), ('power', tensor([0.0404]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1263	 i:0 	 global-step:25260	 l-p:0.05605677142739296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[26.6062, 29.8270, 30.0390],
        [26.6062, 28.3635, 27.9412],
        [26.6062, 29.6281, 29.7214],
        [26.6062, 34.3868, 38.9106]], grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.056056756526231766 
model_pd.l_d.mean(): -1.3113154172897339 
model_pd.lagr.mean(): -1.2552586793899536 
model_pd.lambdas: dict_items([('pout', tensor([0.8093])), ('power', tensor([0.0403]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1264	 i:0 	 global-step:25280	 l-p:0.056056756526231766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.6062, 32.1975, 34.3416],
        [26.6062, 28.0890, 27.6235],
        [26.6062, 35.9915, 42.5268],
        [26.6062, 26.8137, 26.6475]], grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.05605674907565117 
model_pd.l_d.mean(): -1.308825135231018 
model_pd.lagr.mean(): -1.2527683973312378 
model_pd.lambdas: dict_items([('pout', tensor([0.8078])), ('power', tensor([0.0403]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1265	 i:0 	 global-step:25300	 l-p:0.05605674907565117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6062, 26.6349, 26.6080],
        [26.6062, 26.9482, 26.6993],
        [26.6062, 26.6459, 26.6091],
        [26.6062, 29.5333, 29.5731]], grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.05605673789978027 
model_pd.l_d.mean(): -1.3063344955444336 
model_pd.lagr.mean(): -1.2502777576446533 
model_pd.lambdas: dict_items([('pout', tensor([0.8063])), ('power', tensor([0.0402]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1266	 i:0 	 global-step:25320	 l-p:0.05605673789978027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.6062, 35.9918, 42.5274],
        [26.6062, 26.6063, 26.6062],
        [26.6062, 26.8138, 26.6475],
        [26.6062, 27.7526, 27.2776]], grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.05605673789978027 
model_pd.l_d.mean(): -1.3038444519042969 
model_pd.lagr.mean(): -1.2477877140045166 
model_pd.lambdas: dict_items([('pout', tensor([0.8047])), ('power', tensor([0.0401]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1267	 i:0 	 global-step:25340	 l-p:0.05605673789978027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 26.6080, 26.6063],
        [26.6063, 29.0004, 28.7806],
        [26.6063, 31.1650, 32.3660],
        [26.6063, 26.6460, 26.6092]], grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.056056734174489975 
model_pd.l_d.mean(): -1.301353931427002 
model_pd.lagr.mean(): -1.2452971935272217 
model_pd.lambdas: dict_items([('pout', tensor([0.8032])), ('power', tensor([0.0400]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1268	 i:0 	 global-step:25360	 l-p:0.056056734174489975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 26.6099, 26.6063],
        [26.6063, 29.6286, 29.7220],
        [26.6063, 26.6063, 26.6063],
        [26.6063, 26.7143, 26.6206]], grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.05605672672390938 
model_pd.l_d.mean(): -1.298863410949707 
model_pd.lagr.mean(): -1.2428066730499268 
model_pd.lambdas: dict_items([('pout', tensor([0.8016])), ('power', tensor([0.0400]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1269	 i:0 	 global-step:25380	 l-p:0.05605672672390938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 26.9589, 26.7042],
        [26.6063, 26.6063, 26.6063],
        [26.6063, 27.5171, 27.0680],
        [26.6063, 26.9558, 26.7027]], grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.05605672299861908 
model_pd.l_d.mean(): -1.296372890472412 
model_pd.lagr.mean(): -1.2403161525726318 
model_pd.lambdas: dict_items([('pout', tensor([0.8001])), ('power', tensor([0.0399]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1270	 i:0 	 global-step:25400	 l-p:0.05605672299861908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[26.6063, 27.2690, 26.8810],
        [26.6063, 28.0894, 27.6239],
        [26.6063, 29.2566, 29.1525],
        [26.6063, 29.4432, 29.4340]], grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.05605671927332878 
model_pd.l_d.mean(): -1.2938826084136963 
model_pd.lagr.mean(): -1.237825870513916 
model_pd.lambdas: dict_items([('pout', tensor([0.7986])), ('power', tensor([0.0398]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1271	 i:0 	 global-step:25420	 l-p:0.05605671927332878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 31.7186, 33.4084],
        [26.6063, 26.7908, 26.6404],
        [26.6063, 26.9608, 26.7050],
        [26.6063, 26.6081, 26.6063]], grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.05605671554803848 
model_pd.l_d.mean(): -1.291392207145691 
model_pd.lagr.mean(): -1.2353354692459106 
model_pd.lambdas: dict_items([('pout', tensor([0.7970])), ('power', tensor([0.0397]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1272	 i:0 	 global-step:25440	 l-p:0.05605671554803848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 26.8139, 26.6476],
        [26.6063, 27.5172, 27.0681],
        [26.6063, 26.6063, 26.6063],
        [26.6063, 27.8949, 27.4176]], grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.05605671554803848 
model_pd.l_d.mean(): -1.288901925086975 
model_pd.lagr.mean(): -1.2328451871871948 
model_pd.lambdas: dict_items([('pout', tensor([0.7955])), ('power', tensor([0.0397]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1273	 i:0 	 global-step:25460	 l-p:0.05605671554803848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6063, 26.6350, 26.6080],
        [26.6063, 26.6063, 26.6063],
        [26.6063, 26.6078, 26.6063],
        [26.6063, 26.6063, 26.6063]], grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.05605671927332878 
model_pd.l_d.mean(): -1.2864112854003906 
model_pd.lagr.mean(): -1.2303545475006104 
model_pd.lambdas: dict_items([('pout', tensor([0.7939])), ('power', tensor([0.0396]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1274	 i:0 	 global-step:25480	 l-p:0.05605671927332878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 31.7189, 33.4089],
        [26.6063, 26.9608, 26.7050],
        [26.6063, 34.3883, 38.9132],
        [26.6063, 32.1592, 34.2656]], grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.05605671927332878 
model_pd.l_d.mean(): -1.2839208841323853 
model_pd.lagr.mean(): -1.227864146232605 
model_pd.lambdas: dict_items([('pout', tensor([0.7924])), ('power', tensor([0.0395]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1275	 i:0 	 global-step:25500	 l-p:0.05605671927332878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 27.3956, 26.9718],
        [26.6063, 27.7530, 27.2779],
        [26.6063, 26.6163, 26.6066],
        [26.6063, 35.9929, 42.5296]], grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.05605671554803848 
model_pd.l_d.mean(): -1.2814304828643799 
model_pd.lagr.mean(): -1.2253737449645996 
model_pd.lambdas: dict_items([('pout', tensor([0.7909])), ('power', tensor([0.0394]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1276	 i:0 	 global-step:25520	 l-p:0.05605671554803848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[26.6063, 31.1659, 32.3673],
        [26.6063, 35.3302, 41.0111],
        [26.6063, 28.3643, 27.9420],
        [26.6063, 29.0011, 28.7813]], grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.05605671927332878 
model_pd.l_d.mean(): -1.2789400815963745 
model_pd.lagr.mean(): -1.2228833436965942 
model_pd.lambdas: dict_items([('pout', tensor([0.7893])), ('power', tensor([0.0394]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1277	 i:0 	 global-step:25540	 l-p:0.05605671927332878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 27.5751, 27.1169],
        [26.6063, 28.4435, 28.0390],
        [26.6063, 26.9484, 26.6994],
        [26.6063, 26.9590, 26.7042]], grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.05605672299861908 
model_pd.l_d.mean(): -1.2764495611190796 
model_pd.lagr.mean(): -1.2203928232192993 
model_pd.lambdas: dict_items([('pout', tensor([0.7878])), ('power', tensor([0.0393]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1278	 i:0 	 global-step:25560	 l-p:0.05605672299861908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.6062, 26.6062, 26.6062],
        [26.6062, 27.2692, 26.8810],
        [26.6062, 28.0898, 27.6241],
        [26.6062, 29.2572, 29.1531]], grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.05605672299861908 
model_pd.l_d.mean(): -1.2739590406417847 
model_pd.lagr.mean(): -1.2179023027420044 
model_pd.lambdas: dict_items([('pout', tensor([0.7862])), ('power', tensor([0.0392]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1279	 i:0 	 global-step:25580	 l-p:0.05605672299861908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6062, 27.8952, 27.4177],
        [26.6062, 32.2053, 34.3564],
        [26.6062, 29.0012, 28.7815],
        [26.6062, 26.6062, 26.6062]], grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.05605672672390938 
model_pd.l_d.mean(): -1.2714687585830688 
model_pd.lagr.mean(): -1.2154120206832886 
model_pd.lambdas: dict_items([('pout', tensor([0.7847])), ('power', tensor([0.0391]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1280	 i:0 	 global-step:25600	 l-p:0.05605672672390938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6062, 31.7194, 33.4097],
        [26.6062, 29.6294, 29.7231],
        [26.6062, 30.6267, 31.3932],
        [26.6062, 26.9484, 26.6994]], grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.056056734174489975 
model_pd.l_d.mean(): -1.2689781188964844 
model_pd.lagr.mean(): -1.212921380996704 
model_pd.lambdas: dict_items([('pout', tensor([0.7831])), ('power', tensor([0.0390]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1281	 i:0 	 global-step:25620	 l-p:0.056056734174489975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.6062, 28.5341, 28.1527],
        [26.6062, 26.6162, 26.6065],
        [26.6062, 28.4436, 28.0391],
        [26.6062, 32.2055, 34.3567]], grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.056056734174489975 
model_pd.l_d.mean(): -1.266487717628479 
model_pd.lagr.mean(): -1.2104309797286987 
model_pd.lambdas: dict_items([('pout', tensor([0.7816])), ('power', tensor([0.0390]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5270]))])
epoch：1282	 i:0 	 global-step:25640	 l-p:0.056056734174489975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6062, 34.0486, 38.1748],
        [26.6062, 32.1994, 34.3446],
        [26.6062, 26.6162, 26.6065],
        [26.6062, 26.6459, 26.6091]], grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.05605674907565117 
model_pd.l_d.mean(): -1.2639970779418945 
model_pd.lagr.mean(): -1.2079403400421143 
model_pd.lambdas: dict_items([('pout', tensor([0.7801])), ('power', tensor([0.0389]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1283	 i:0 	 global-step:25660	 l-p:0.05605674907565117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6061, 35.3308, 41.0125],
        [26.6061, 26.6459, 26.6091],
        [26.6061, 27.5174, 27.0682],
        [26.6061, 29.5346, 29.5747]], grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.05605674907565117 
model_pd.l_d.mean(): -1.2615066766738892 
model_pd.lagr.mean(): -1.2054499387741089 
model_pd.lambdas: dict_items([('pout', tensor([0.7785])), ('power', tensor([0.0388]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1284	 i:0 	 global-step:25680	 l-p:0.05605674907565117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.6061, 28.5342, 28.1528],
        [26.6061, 34.0488, 38.1751],
        [26.6061, 29.5347, 29.5748],
        [26.6061, 26.6063, 26.6061]], grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.05605675280094147 
model_pd.l_d.mean(): -1.2590162754058838 
model_pd.lagr.mean(): -1.2029595375061035 
model_pd.lambdas: dict_items([('pout', tensor([0.7770])), ('power', tensor([0.0387]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1285	 i:0 	 global-step:25700	 l-p:0.05605675280094147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6061, 26.6578, 26.6105],
        [26.6061, 26.6161, 26.6064],
        [26.6061, 29.4441, 29.4352],
        [26.6061, 26.9608, 26.7049]], grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.056056760251522064 
model_pd.l_d.mean(): -1.2565258741378784 
model_pd.lagr.mean(): -1.2004691362380981 
model_pd.lambdas: dict_items([('pout', tensor([0.7754])), ('power', tensor([0.0387]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1286	 i:0 	 global-step:25720	 l-p:0.056056760251522064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6061, 26.6078, 26.6061],
        [26.6061, 27.5752, 27.1169],
        [26.6061, 29.2575, 29.1535],
        [26.6061, 26.6061, 26.6061]], grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.056056760251522064 
model_pd.l_d.mean(): -1.254035472869873 
model_pd.lagr.mean(): -1.1979787349700928 
model_pd.lambdas: dict_items([('pout', tensor([0.7739])), ('power', tensor([0.0386]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1287	 i:0 	 global-step:25740	 l-p:0.056056760251522064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.6060, 26.8138, 26.6474],
        [26.6060, 27.5174, 27.0681],
        [26.6060, 34.3894, 38.9156],
        [26.6060, 27.7532, 27.2779]], grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.056056760251522064 
model_pd.l_d.mean(): -1.2515448331832886 
model_pd.lagr.mean(): -1.1954880952835083 
model_pd.lambdas: dict_items([('pout', tensor([0.7724])), ('power', tensor([0.0385]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5271]))])
epoch：1288	 i:0 	 global-step:25760	 l-p:0.056056760251522064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.6060, 31.7200, 33.4108],
        [26.6060, 34.0491, 38.1758],
        [26.6060, 32.2060, 34.3577],
        [26.6060, 26.6065, 26.6060]], grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.05605677142739296 
model_pd.l_d.mean(): -1.2490543127059937 
model_pd.lagr.mean(): -1.1929975748062134 
model_pd.lambdas: dict_items([('pout', tensor([0.7708])), ('power', tensor([0.0384]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1289	 i:0 	 global-step:25780	 l-p:0.05605677142739296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.6060, 26.6060, 26.6060],
        [26.6060, 26.9483, 26.6992],
        [26.6060, 26.7141, 26.6204],
        [26.6060, 34.3896, 38.9159]], grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.056056782603263855 
model_pd.l_d.mean(): -1.2465639114379883 
model_pd.lagr.mean(): -1.190507173538208 
model_pd.lambdas: dict_items([('pout', tensor([0.7693])), ('power', tensor([0.0384]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1290	 i:0 	 global-step:25800	 l-p:0.056056782603263855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6060, 30.9211, 31.9195],
        [26.6060, 34.3897, 38.9161],
        [26.6060, 31.1669, 32.3690],
        [26.6060, 26.9607, 26.7048]], grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.056056782603263855 
model_pd.l_d.mean(): -1.2440732717514038 
model_pd.lagr.mean(): -1.1880165338516235 
model_pd.lambdas: dict_items([('pout', tensor([0.7677])), ('power', tensor([0.0383]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1291	 i:0 	 global-step:25820	 l-p:0.056056782603263855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.6060, 26.6060, 26.6060],
        [26.6060, 31.7202, 33.4112],
        [26.6060, 26.6060, 26.6060],
        [26.6060, 28.4439, 28.0394]], grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.056056782603263855 
model_pd.l_d.mean(): -1.2415826320648193 
model_pd.lagr.mean(): -1.185525894165039 
model_pd.lambdas: dict_items([('pout', tensor([0.7662])), ('power', tensor([0.0382]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1292	 i:0 	 global-step:25840	 l-p:0.056056782603263855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.6059, 29.0018, 28.7821],
        [26.6059, 26.6064, 26.6059],
        [26.6059, 27.5753, 27.1169],
        [26.6059, 26.6077, 26.6060]], grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.2390921115875244 
model_pd.lagr.mean(): -1.1830353736877441 
model_pd.lambdas: dict_items([('pout', tensor([0.7647])), ('power', tensor([0.0381]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1293	 i:0 	 global-step:25860	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6059, 34.0495, 38.1767],
        [26.6059, 29.5351, 29.5754],
        [26.6059, 31.1671, 32.3693],
        [26.6059, 26.6074, 26.6060]], grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.236601710319519 
model_pd.lagr.mean(): -1.1805449724197388 
model_pd.lambdas: dict_items([('pout', tensor([0.7631])), ('power', tensor([0.0381]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1294	 i:0 	 global-step:25880	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[26.6059, 32.2003, 34.3463],
        [26.6059, 27.5175, 27.0681],
        [26.6059, 28.3648, 27.9425],
        [26.6059, 35.3317, 41.0146]], grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.2341111898422241 
model_pd.lagr.mean(): -1.1780543327331543 
model_pd.lambdas: dict_items([('pout', tensor([0.7616])), ('power', tensor([0.0380]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5272]))])
epoch：1295	 i:0 	 global-step:25900	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.6059, 29.8293, 30.0421],
        [26.6059, 26.7906, 26.6400],
        [26.6059, 31.1672, 32.3696],
        [26.6059, 32.2004, 34.3465]], grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.05605679377913475 
model_pd.l_d.mean(): -1.2316205501556396 
model_pd.lagr.mean(): -1.1755638122558594 
model_pd.lambdas: dict_items([('pout', tensor([0.7600])), ('power', tensor([0.0379]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5273]))])
epoch：1296	 i:0 	 global-step:25920	 l-p:0.05605679377913475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.6059, 27.2692, 26.8809],
        [26.6059, 26.8137, 26.6472],
        [26.6059, 32.1610, 34.2688],
        [26.6059, 27.8955, 27.4179]], grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.2291301488876343 
model_pd.lagr.mean(): -1.1730732917785645 
model_pd.lambdas: dict_items([('pout', tensor([0.7585])), ('power', tensor([0.0378]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5273]))])
epoch：1297	 i:0 	 global-step:25940	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[26.6059, 27.5175, 27.0682],
        [26.6059, 31.1674, 32.3698],
        [26.6059, 27.7534, 27.2780],
        [26.6059, 29.6304, 29.7244]], grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.2266392707824707 
model_pd.lagr.mean(): -1.1705824136734009 
model_pd.lambdas: dict_items([('pout', tensor([0.7569])), ('power', tensor([0.0377]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5410])), ('power', tensor([-1.5273]))])
epoch：1298	 i:0 	 global-step:25960	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6059, 26.6059, 26.6059],
        [26.6059, 27.7534, 27.2780],
        [26.6059, 27.5175, 27.0682],
        [26.6059, 26.6074, 26.6059]], grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.2241486310958862 
model_pd.lagr.mean(): -1.1680917739868164 
model_pd.lambdas: dict_items([('pout', tensor([0.7554])), ('power', tensor([0.0377]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1299	 i:0 	 global-step:25980	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6073, 26.6059],
        [26.6058, 26.6059, 26.6058],
        [26.6058, 26.6076, 26.6059],
        [26.6058, 30.9217, 31.9206]], grad_fn=<SliceBackward0>)

training epoch:1300, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.2216581106185913 
model_pd.lagr.mean(): -1.1656012535095215 
model_pd.lambdas: dict_items([('pout', tensor([0.7539])), ('power', tensor([0.0376]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1300	 i:0 	 global-step:26000	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1301
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[26.6058, 28.0903, 27.6246],
        [26.6058, 29.0022, 28.7826],
        [26.6058, 29.8296, 30.0425],
        [26.6058, 27.7535, 27.2781]], grad_fn=<SliceBackward0>)

training epoch:1301, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.2191672325134277 
model_pd.lagr.mean(): -1.163110375404358 
model_pd.lambdas: dict_items([('pout', tensor([0.7523])), ('power', tensor([0.0375]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1301	 i:0 	 global-step:26020	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6576, 26.6103],
        [26.6058, 30.9219, 31.9208],
        [26.6058, 26.9483, 26.6991],
        [26.6058, 26.6076, 26.6059]], grad_fn=<SliceBackward0>)

training epoch:1302, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.2166765928268433 
model_pd.lagr.mean(): -1.1606197357177734 
model_pd.lambdas: dict_items([('pout', tensor([0.7508])), ('power', tensor([0.0374]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1302	 i:0 	 global-step:26040	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 32.1615, 34.2697],
        [26.6058, 29.8298, 30.0427],
        [26.6058, 29.2584, 29.1546],
        [26.6058, 26.6159, 26.6062]], grad_fn=<SliceBackward0>)

training epoch:1303, step:0 
model_pd.l_p.mean(): 0.056056808680295944 
model_pd.l_d.mean(): -1.2141860723495483 
model_pd.lagr.mean(): -1.1581292152404785 
model_pd.lambdas: dict_items([('pout', tensor([0.7492])), ('power', tensor([0.0374]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1303	 i:0 	 global-step:26060	 l-p:0.056056808680295944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6082, 26.6059],
        [26.6058, 26.7906, 26.6400],
        [26.6058, 28.3652, 27.9428],
        [26.6058, 26.6058, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1304, step:0 
model_pd.l_p.mean(): 0.056056808680295944 
model_pd.l_d.mean(): -1.2116953134536743 
model_pd.lagr.mean(): -1.1556384563446045 
model_pd.lambdas: dict_items([('pout', tensor([0.7477])), ('power', tensor([0.0373]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1304	 i:0 	 global-step:26080	 l-p:0.056056808680295944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 35.9955, 42.5356],
        [26.6058, 26.6346, 26.6075],
        [26.6058, 29.0024, 28.7829],
        [26.6058, 30.6283, 31.3958]], grad_fn=<SliceBackward0>)

training epoch:1305, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.2092046737670898 
model_pd.lagr.mean(): -1.15314781665802 
model_pd.lambdas: dict_items([('pout', tensor([0.7462])), ('power', tensor([0.0372]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1305	 i:0 	 global-step:26100	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.9484, 26.6991],
        [26.6058, 26.6346, 26.6075],
        [26.6058, 33.9053, 37.8659],
        [26.6058, 28.5350, 28.1536]], grad_fn=<SliceBackward0>)

training epoch:1306, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.2067139148712158 
model_pd.lagr.mean(): -1.150657057762146 
model_pd.lambdas: dict_items([('pout', tensor([0.7446])), ('power', tensor([0.0371]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1306	 i:0 	 global-step:26120	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6058, 26.9608, 26.7047],
        [26.6058, 27.5177, 27.0683],
        [26.6058, 26.9589, 26.7038],
        [26.6058, 26.6058, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1307, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.2042232751846313 
model_pd.lagr.mean(): -1.1481664180755615 
model_pd.lambdas: dict_items([('pout', tensor([0.7431])), ('power', tensor([0.0371]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1307	 i:0 	 global-step:26140	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 27.5178, 27.0683],
        [26.6058, 29.0026, 28.7831],
        [26.6058, 26.8137, 26.6472],
        [26.6058, 32.2077, 34.3606]], grad_fn=<SliceBackward0>)

training epoch:1308, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.2017326354980469 
model_pd.lagr.mean(): -1.145675778388977 
model_pd.lambdas: dict_items([('pout', tensor([0.7415])), ('power', tensor([0.0370]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1308	 i:0 	 global-step:26160	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6058, 26.6058],
        [26.6058, 31.7217, 33.4136],
        [26.6058, 26.6058, 26.6058],
        [26.6058, 26.6346, 26.6075]], grad_fn=<SliceBackward0>)

training epoch:1309, step:0 
model_pd.l_p.mean(): 0.056056808680295944 
model_pd.l_d.mean(): -1.1992418766021729 
model_pd.lagr.mean(): -1.143185019493103 
model_pd.lambdas: dict_items([('pout', tensor([0.7400])), ('power', tensor([0.0369]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1309	 i:0 	 global-step:26180	 l-p:0.056056808680295944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.9590, 26.7039],
        [26.6058, 28.0907, 27.6249],
        [26.6058, 30.9226, 31.9219],
        [26.6058, 26.9608, 26.7047]], grad_fn=<SliceBackward0>)

training epoch:1310, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.1967511177062988 
model_pd.lagr.mean(): -1.140694260597229 
model_pd.lambdas: dict_items([('pout', tensor([0.7385])), ('power', tensor([0.0368]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1310	 i:0 	 global-step:26200	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 30.9227, 31.9220],
        [26.6058, 29.5362, 29.5768],
        [26.6058, 27.3961, 26.9719],
        [26.6058, 26.6576, 26.6103]], grad_fn=<SliceBackward0>)

training epoch:1311, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.1942603588104248 
model_pd.lagr.mean(): -1.138203501701355 
model_pd.lambdas: dict_items([('pout', tensor([0.7369])), ('power', tensor([0.0368]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1311	 i:0 	 global-step:26220	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 28.4448, 28.0403],
        [26.6058, 31.1686, 32.3717],
        [26.6058, 26.6058, 26.6058],
        [26.6058, 29.2590, 29.1553]], grad_fn=<SliceBackward0>)

training epoch:1312, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.1917698383331299 
model_pd.lagr.mean(): -1.13571298122406 
model_pd.lambdas: dict_items([('pout', tensor([0.7354])), ('power', tensor([0.0367]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1312	 i:0 	 global-step:26240	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6058, 26.6058],
        [26.6058, 27.2695, 26.8810],
        [26.6058, 27.5758, 27.1172],
        [26.6058, 26.7906, 26.6400]], grad_fn=<SliceBackward0>)

training epoch:1313, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.1892789602279663 
model_pd.lagr.mean(): -1.1332221031188965 
model_pd.lambdas: dict_items([('pout', tensor([0.7338])), ('power', tensor([0.0366]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1313	 i:0 	 global-step:26260	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6073, 26.6058],
        [26.6058, 27.5180, 27.0684],
        [26.6058, 26.6058, 26.6058],
        [26.6058, 32.2083, 34.3616]], grad_fn=<SliceBackward0>)

training epoch:1314, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.1867882013320923 
model_pd.lagr.mean(): -1.1307313442230225 
model_pd.lambdas: dict_items([('pout', tensor([0.7323])), ('power', tensor([0.0365]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1314	 i:0 	 global-step:26280	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 28.5355, 28.1541],
        [26.6058, 26.6082, 26.6059],
        [26.6058, 26.7140, 26.6202],
        [26.6058, 26.6073, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1315, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.1842975616455078 
model_pd.lagr.mean(): -1.128240704536438 
model_pd.lambdas: dict_items([('pout', tensor([0.7307])), ('power', tensor([0.0365]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1315	 i:0 	 global-step:26300	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.7906, 26.6400],
        [26.6058, 26.6060, 26.6058],
        [26.6058, 28.5356, 28.1541],
        [26.6058, 28.0910, 27.6251]], grad_fn=<SliceBackward0>)

training epoch:1316, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.1818069219589233 
model_pd.lagr.mean(): -1.1257500648498535 
model_pd.lambdas: dict_items([('pout', tensor([0.7292])), ('power', tensor([0.0364]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1316	 i:0 	 global-step:26320	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 29.2593, 29.1557],
        [26.6058, 30.6294, 31.3973],
        [26.6058, 27.7540, 27.2785],
        [26.6058, 26.7906, 26.6400]], grad_fn=<SliceBackward0>)

training epoch:1317, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.1793161630630493 
model_pd.lagr.mean(): -1.1232593059539795 
model_pd.lambdas: dict_items([('pout', tensor([0.7277])), ('power', tensor([0.0363]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1317	 i:0 	 global-step:26340	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.9559, 26.7025],
        [26.6058, 26.6060, 26.6058],
        [26.6058, 26.9609, 26.7048],
        [26.6058, 27.5760, 27.1173]], grad_fn=<SliceBackward0>)

training epoch:1318, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.1768252849578857 
model_pd.lagr.mean(): -1.120768427848816 
model_pd.lambdas: dict_items([('pout', tensor([0.7261])), ('power', tensor([0.0362]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1318	 i:0 	 global-step:26360	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6346, 26.6076],
        [26.6058, 27.7541, 27.2785],
        [26.6058, 26.9610, 26.7048],
        [26.6058, 35.9971, 42.5388]], grad_fn=<SliceBackward0>)

training epoch:1319, step:0 
model_pd.l_p.mean(): 0.05605679377913475 
model_pd.l_d.mean(): -1.1743344068527222 
model_pd.lagr.mean(): -1.118277668952942 
model_pd.lambdas: dict_items([('pout', tensor([0.7246])), ('power', tensor([0.0361]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1319	 i:0 	 global-step:26380	 l-p:0.05605679377913475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 27.2697, 26.8811],
        [26.6058, 26.6076, 26.6059],
        [26.6058, 34.3927, 38.9219],
        [26.6058, 26.9486, 26.6992]], grad_fn=<SliceBackward0>)

training epoch:1320, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.1718438863754272 
model_pd.lagr.mean(): -1.115787148475647 
model_pd.lambdas: dict_items([('pout', tensor([0.7230])), ('power', tensor([0.0361]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1320	 i:0 	 global-step:26400	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6058, 26.6059],
        [26.6058, 34.3928, 38.9221],
        [26.6058, 29.2596, 29.1560],
        [26.6058, 29.8311, 30.0445]], grad_fn=<SliceBackward0>)

training epoch:1321, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.1693532466888428 
model_pd.lagr.mean(): -1.1132965087890625 
model_pd.lambdas: dict_items([('pout', tensor([0.7215])), ('power', tensor([0.0360]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1321	 i:0 	 global-step:26420	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.8138, 26.6472],
        [26.6058, 29.4464, 29.4380],
        [26.6058, 29.8312, 30.0446],
        [26.6058, 32.2091, 34.3629]], grad_fn=<SliceBackward0>)

training epoch:1322, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.1668622493743896 
model_pd.lagr.mean(): -1.1108055114746094 
model_pd.lambdas: dict_items([('pout', tensor([0.7200])), ('power', tensor([0.0359]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1322	 i:0 	 global-step:26440	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6059, 26.6058],
        [26.6058, 26.6063, 26.6058],
        [26.6058, 34.3930, 38.9225],
        [26.6058, 26.7141, 26.6202]], grad_fn=<SliceBackward0>)

training epoch:1323, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.1643717288970947 
model_pd.lagr.mean(): -1.1083149909973145 
model_pd.lambdas: dict_items([('pout', tensor([0.7184])), ('power', tensor([0.0358]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1323	 i:0 	 global-step:26460	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 30.9239, 31.9238],
        [26.6058, 26.9592, 26.7039],
        [26.6058, 30.6300, 31.3982],
        [26.6058, 35.3348, 41.0208]], grad_fn=<SliceBackward0>)

training epoch:1324, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.1618808507919312 
model_pd.lagr.mean(): -1.1058241128921509 
model_pd.lambdas: dict_items([('pout', tensor([0.7169])), ('power', tensor([0.0358]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1324	 i:0 	 global-step:26480	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 27.5183, 27.0686],
        [26.6058, 31.7233, 33.4161],
        [26.6058, 26.6058, 26.6058],
        [26.6058, 26.9486, 26.6992]], grad_fn=<SliceBackward0>)

training epoch:1325, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.1593900918960571 
model_pd.lagr.mean(): -1.1033333539962769 
model_pd.lambdas: dict_items([('pout', tensor([0.7153])), ('power', tensor([0.0357]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1325	 i:0 	 global-step:26500	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6457, 26.6088],
        [26.6058, 28.0914, 27.6255],
        [26.6058, 28.4456, 28.0410],
        [26.6058, 29.4467, 29.4384]], grad_fn=<SliceBackward0>)

training epoch:1326, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.156899094581604 
model_pd.lagr.mean(): -1.1008423566818237 
model_pd.lambdas: dict_items([('pout', tensor([0.7138])), ('power', tensor([0.0356]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1326	 i:0 	 global-step:26520	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6063, 26.6058],
        [26.6058, 26.6058, 26.6059],
        [26.6058, 32.1639, 34.2736],
        [26.6058, 26.6095, 26.6059]], grad_fn=<SliceBackward0>)

training epoch:1327, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.15440833568573 
model_pd.lagr.mean(): -1.0983515977859497 
model_pd.lambdas: dict_items([('pout', tensor([0.7123])), ('power', tensor([0.0355]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1327	 i:0 	 global-step:26540	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 34.3935, 38.9235],
        [26.6058, 26.7907, 26.6400],
        [26.6058, 29.8317, 30.0452],
        [26.6058, 27.5763, 27.1175]], grad_fn=<SliceBackward0>)

training epoch:1328, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.151917815208435 
model_pd.lagr.mean(): -1.0958610773086548 
model_pd.lambdas: dict_items([('pout', tensor([0.7107])), ('power', tensor([0.0355]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1328	 i:0 	 global-step:26560	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6076, 26.6059],
        [26.6058, 29.8317, 30.0453],
        [26.6058, 27.2699, 26.8812],
        [26.6058, 29.6326, 29.7272]], grad_fn=<SliceBackward0>)

training epoch:1329, step:0 
model_pd.l_p.mean(): 0.056056782603263855 
model_pd.l_d.mean(): -1.149427056312561 
model_pd.lagr.mean(): -1.0933703184127808 
model_pd.lambdas: dict_items([('pout', tensor([0.7092])), ('power', tensor([0.0354]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1329	 i:0 	 global-step:26580	 l-p:0.056056782603263855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6058, 34.0533, 38.1837],
        [26.6058, 28.4458, 28.0412],
        [26.6058, 26.6577, 26.6103],
        [26.6058, 26.6058, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1330, step:0 
model_pd.l_p.mean(): 0.056056782603263855 
model_pd.l_d.mean(): -1.146936297416687 
model_pd.lagr.mean(): -1.0908795595169067 
model_pd.lambdas: dict_items([('pout', tensor([0.7076])), ('power', tensor([0.0353]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1330	 i:0 	 global-step:26600	 l-p:0.056056782603263855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 31.7238, 33.4170],
        [26.6058, 26.9593, 26.7040],
        [26.6058, 29.8319, 30.0455],
        [26.6058, 26.6060, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1331, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.1444454193115234 
model_pd.lagr.mean(): -1.0883886814117432 
model_pd.lambdas: dict_items([('pout', tensor([0.7061])), ('power', tensor([0.0352]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1331	 i:0 	 global-step:26620	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1332
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.6058, 35.3357, 41.0225],
        [26.6058, 29.2603, 29.1569],
        [26.6058, 35.9985, 42.5416],
        [26.6058, 34.0535, 38.1841]], grad_fn=<SliceBackward0>)

training epoch:1332, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.1419545412063599 
model_pd.lagr.mean(): -1.0858978033065796 
model_pd.lambdas: dict_items([('pout', tensor([0.7046])), ('power', tensor([0.0352]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1332	 i:0 	 global-step:26640	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6058, 26.6058],
        [26.6058, 34.0536, 38.1843],
        [26.6058, 32.1645, 34.2746],
        [26.6058, 26.6060, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1333, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.1394637823104858 
model_pd.lagr.mean(): -1.0834070444107056 
model_pd.lambdas: dict_items([('pout', tensor([0.7030])), ('power', tensor([0.0351]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1333	 i:0 	 global-step:26660	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 29.2604, 29.1570],
        [26.6058, 26.7908, 26.6400],
        [26.6058, 31.7241, 33.4175],
        [26.6058, 27.3967, 26.9722]], grad_fn=<SliceBackward0>)

training epoch:1334, step:0 
model_pd.l_p.mean(): 0.056056782603263855 
model_pd.l_d.mean(): -1.1369730234146118 
model_pd.lagr.mean(): -1.0809162855148315 
model_pd.lambdas: dict_items([('pout', tensor([0.7015])), ('power', tensor([0.0350]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1334	 i:0 	 global-step:26680	 l-p:0.056056782603263855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.6058, 27.5186, 27.0688],
        [26.6058, 29.4473, 29.4391],
        [26.6058, 26.9487, 26.6992],
        [26.6058, 26.6058, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1335, step:0 
model_pd.l_p.mean(): 0.056056782603263855 
model_pd.l_d.mean(): -1.1344823837280273 
model_pd.lagr.mean(): -1.078425645828247 
model_pd.lambdas: dict_items([('pout', tensor([0.6999])), ('power', tensor([0.0349]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1335	 i:0 	 global-step:26700	 l-p:0.056056782603263855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 27.5765, 27.1176],
        [26.6058, 26.9487, 26.6992],
        [26.6058, 32.1648, 34.2750],
        [26.6058, 26.8139, 26.6472]], grad_fn=<SliceBackward0>)

training epoch:1336, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.1319912672042847 
model_pd.lagr.mean(): -1.0759345293045044 
model_pd.lambdas: dict_items([('pout', tensor([0.6984])), ('power', tensor([0.0348]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1336	 i:0 	 global-step:26720	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 27.3968, 26.9722],
        [26.6058, 28.4461, 28.0415],
        [26.6058, 26.6058, 26.6058],
        [26.6058, 26.6073, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1337, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.129500389099121 
model_pd.lagr.mean(): -1.0734436511993408 
model_pd.lambdas: dict_items([('pout', tensor([0.6968])), ('power', tensor([0.0348]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1337	 i:0 	 global-step:26740	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6058, 26.6058],
        [26.6058, 26.6058, 26.6058],
        [26.6058, 26.7141, 26.6202],
        [26.6058, 26.6081, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1338, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.127009630203247 
model_pd.lagr.mean(): -1.0709528923034668 
model_pd.lambdas: dict_items([('pout', tensor([0.6953])), ('power', tensor([0.0347]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1338	 i:0 	 global-step:26760	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.9612, 26.7048],
        [26.6058, 27.3968, 26.9723],
        [26.6058, 26.9562, 26.7025],
        [26.6058, 29.5382, 29.5793]], grad_fn=<SliceBackward0>)

training epoch:1339, step:0 
model_pd.l_p.mean(): 0.05605678632855415 
model_pd.l_d.mean(): -1.124518871307373 
model_pd.lagr.mean(): -1.0684621334075928 
model_pd.lambdas: dict_items([('pout', tensor([0.6938])), ('power', tensor([0.0346]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1339	 i:0 	 global-step:26780	 l-p:0.05605678632855415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.9562, 26.7025],
        [26.6058, 31.7246, 33.4183],
        [26.6058, 26.6158, 26.6061],
        [26.6058, 26.6073, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1340, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.122028112411499 
model_pd.lagr.mean(): -1.0659713745117188 
model_pd.lambdas: dict_items([('pout', tensor([0.6922])), ('power', tensor([0.0345]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1340	 i:0 	 global-step:26800	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6094, 26.6058],
        [26.6058, 26.9488, 26.6992],
        [26.6058, 27.8971, 27.4191],
        [26.6058, 32.1652, 34.2758]], grad_fn=<SliceBackward0>)

training epoch:1341, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.1195372343063354 
model_pd.lagr.mean(): -1.0634804964065552 
model_pd.lambdas: dict_items([('pout', tensor([0.6907])), ('power', tensor([0.0345]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5271]))])
epoch：1341	 i:0 	 global-step:26820	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.6058, 26.6058, 26.6058],
        [26.6058, 26.8139, 26.6472],
        [26.6058, 28.0921, 27.6260],
        [26.6058, 26.6058, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1342, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.1170463562011719 
model_pd.lagr.mean(): -1.0609896183013916 
model_pd.lambdas: dict_items([('pout', tensor([0.6891])), ('power', tensor([0.0344]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1342	 i:0 	 global-step:26840	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6058, 26.8139, 26.6472],
        [26.6058, 30.6315, 31.4004],
        [26.6058, 29.6335, 29.7284],
        [26.6058, 28.0921, 27.6260]], grad_fn=<SliceBackward0>)

training epoch:1343, step:0 
model_pd.l_p.mean(): 0.05605679005384445 
model_pd.l_d.mean(): -1.1145554780960083 
model_pd.lagr.mean(): -1.058498740196228 
model_pd.lambdas: dict_items([('pout', tensor([0.6876])), ('power', tensor([0.0343]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1343	 i:0 	 global-step:26860	 l-p:0.05605679005384445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 26.9562, 26.7025],
        [26.6057, 31.1714, 32.3760],
        [26.6057, 27.5767, 27.1177],
        [26.6057, 29.5385, 29.5797]], grad_fn=<SliceBackward0>)

training epoch:1344, step:0 
model_pd.l_p.mean(): 0.05605679377913475 
model_pd.l_d.mean(): -1.1120647192001343 
model_pd.lagr.mean(): -1.056007981300354 
model_pd.lambdas: dict_items([('pout', tensor([0.6861])), ('power', tensor([0.0342]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1344	 i:0 	 global-step:26880	 l-p:0.05605679377913475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 26.6456, 26.6087],
        [26.6057, 27.5767, 27.1177],
        [26.6057, 32.2112, 34.3665],
        [26.6057, 26.8139, 26.6472]], grad_fn=<SliceBackward0>)

training epoch:1345, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.1095739603042603 
model_pd.lagr.mean(): -1.0535171031951904 
model_pd.lambdas: dict_items([('pout', tensor([0.6845])), ('power', tensor([0.0342]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1345	 i:0 	 global-step:26900	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1346
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[26.6057, 27.2702, 26.8813],
        [26.6057, 28.4465, 28.0419],
        [26.6057, 28.5371, 28.1556],
        [26.6057, 35.9999, 42.5445]], grad_fn=<SliceBackward0>)

training epoch:1346, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.1070829629898071 
model_pd.lagr.mean(): -1.0510261058807373 
model_pd.lambdas: dict_items([('pout', tensor([0.6830])), ('power', tensor([0.0341]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1346	 i:0 	 global-step:26920	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 26.7908, 26.6399],
        [26.6057, 26.6094, 26.6058],
        [26.6057, 34.0549, 38.1868],
        [26.6057, 27.5768, 27.1177]], grad_fn=<SliceBackward0>)

training epoch:1347, step:0 
model_pd.l_p.mean(): 0.05605679377913475 
model_pd.l_d.mean(): -1.1045920848846436 
model_pd.lagr.mean(): -1.0485353469848633 
model_pd.lambdas: dict_items([('pout', tensor([0.6814])), ('power', tensor([0.0340]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1347	 i:0 	 global-step:26940	 l-p:0.05605679377913475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 29.6338, 29.7288],
        [26.6057, 35.3372, 41.0257],
        [26.6057, 26.6075, 26.6057],
        [26.6057, 26.6072, 26.6057]], grad_fn=<SliceBackward0>)

training epoch:1348, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.10210120677948 
model_pd.lagr.mean(): -1.0460443496704102 
model_pd.lambdas: dict_items([('pout', tensor([0.6799])), ('power', tensor([0.0339]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1348	 i:0 	 global-step:26960	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.6057, 28.5372, 28.1557],
        [26.6057, 26.9594, 26.7039],
        [26.6057, 26.6057, 26.6057],
        [26.6057, 26.6057, 26.6057]], grad_fn=<SliceBackward0>)

training epoch:1349, step:0 
model_pd.l_p.mean(): 0.05605680122971535 
model_pd.l_d.mean(): -1.0996100902557373 
model_pd.lagr.mean(): -1.0435532331466675 
model_pd.lambdas: dict_items([('pout', tensor([0.6784])), ('power', tensor([0.0339]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1349	 i:0 	 global-step:26980	 l-p:0.05605680122971535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 29.6339, 29.7289],
        [26.6057, 26.6080, 26.6057],
        [26.6057, 28.3674, 27.9448],
        [26.6057, 29.8331, 30.0471]], grad_fn=<SliceBackward0>)

training epoch:1350, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.0971192121505737 
model_pd.lagr.mean(): -1.041062355041504 
model_pd.lambdas: dict_items([('pout', tensor([0.6768])), ('power', tensor([0.0338]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1350	 i:0 	 global-step:27000	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.6057, 26.6057, 26.6057],
        [26.6057, 26.9594, 26.7039],
        [26.6057, 26.6062, 26.6057],
        [26.6057, 26.6057, 26.6057]], grad_fn=<SliceBackward0>)

training epoch:1351, step:0 
model_pd.l_p.mean(): 0.056056808680295944 
model_pd.l_d.mean(): -1.0946282148361206 
model_pd.lagr.mean(): -1.0385713577270508 
model_pd.lambdas: dict_items([('pout', tensor([0.6753])), ('power', tensor([0.0337]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1351	 i:0 	 global-step:27020	 l-p:0.056056808680295944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.6057, 26.6057, 26.6057],
        [26.6057, 28.5373, 28.1558],
        [26.6057, 26.6157, 26.6060],
        [26.6057, 26.9594, 26.7039]], grad_fn=<SliceBackward0>)

training epoch:1352, step:0 
model_pd.l_p.mean(): 0.056056804955005646 
model_pd.l_d.mean(): -1.092137336730957 
model_pd.lagr.mean(): -1.0360804796218872 
model_pd.lambdas: dict_items([('pout', tensor([0.6737])), ('power', tensor([0.0336]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1352	 i:0 	 global-step:27040	 l-p:0.056056804955005646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.6056, 27.5769, 27.1178],
        [26.6056, 31.7256, 33.4200],
        [26.6056, 26.6575, 26.6101],
        [26.6056, 34.3959, 38.9281]], grad_fn=<SliceBackward0>)

training epoch:1353, step:0 
model_pd.l_p.mean(): 0.056056808680295944 
model_pd.l_d.mean(): -1.0896464586257935 
model_pd.lagr.mean(): -1.0335896015167236 
model_pd.lambdas: dict_items([('pout', tensor([0.6722])), ('power', tensor([0.0335]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1353	 i:0 	 global-step:27060	 l-p:0.056056808680295944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1354
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[26.6056, 34.0555, 38.1880],
        [26.6056, 32.2119, 34.3678],
        [26.6056, 31.1722, 32.3772],
        [26.6056, 31.7257, 33.4202]], grad_fn=<SliceBackward0>)

training epoch:1354, step:0 
model_pd.l_p.mean(): 0.05605681240558624 
model_pd.l_d.mean(): -1.0871553421020508 
model_pd.lagr.mean(): -1.031098484992981 
model_pd.lambdas: dict_items([('pout', tensor([0.6706])), ('power', tensor([0.0335]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1354	 i:0 	 global-step:27080	 l-p:0.05605681240558624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6056, 26.6093, 26.6057],
        [26.6056, 28.5374, 28.1560],
        [26.6056, 26.6056, 26.6056],
        [26.6056, 28.0925, 27.6263]], grad_fn=<SliceBackward0>)

training epoch:1355, step:0 
model_pd.l_p.mean(): 0.05605681240558624 
model_pd.l_d.mean(): -1.084664225578308 
model_pd.lagr.mean(): -1.0286073684692383 
model_pd.lambdas: dict_items([('pout', tensor([0.6691])), ('power', tensor([0.0334]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1355	 i:0 	 global-step:27100	 l-p:0.05605681240558624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1356
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[26.6056, 35.3379, 41.0272],
        [26.6056, 27.5769, 27.1178],
        [26.6056, 29.8335, 30.0476],
        [26.6056, 29.5391, 29.5805]], grad_fn=<SliceBackward0>)

training epoch:1356, step:0 
model_pd.l_p.mean(): 0.05605681240558624 
model_pd.l_d.mean(): -1.0821733474731445 
model_pd.lagr.mean(): -1.0261164903640747 
model_pd.lambdas: dict_items([('pout', tensor([0.6676])), ('power', tensor([0.0333]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5272]))])
epoch：1356	 i:0 	 global-step:27120	 l-p:0.05605681240558624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.6056, 26.6056, 26.6056],
        [26.6056, 29.2617, 29.1585],
        [26.6056, 26.6455, 26.6085],
        [26.6056, 35.3380, 41.0273]], grad_fn=<SliceBackward0>)

training epoch:1357, step:0 
model_pd.l_p.mean(): 0.05605681985616684 
model_pd.l_d.mean(): -1.079682469367981 
model_pd.lagr.mean(): -1.0236256122589111 
model_pd.lambdas: dict_items([('pout', tensor([0.6660])), ('power', tensor([0.0332]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1357	 i:0 	 global-step:27140	 l-p:0.05605681985616684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.6056, 29.0054, 28.7863],
        [26.6056, 26.9612, 26.7047],
        [26.6056, 33.9104, 37.8755],
        [26.6056, 29.6343, 29.7295]], grad_fn=<SliceBackward0>)

training epoch:1358, step:0 
model_pd.l_p.mean(): 0.05605681985616684 
model_pd.l_d.mean(): -1.0771914720535278 
model_pd.lagr.mean(): -1.021134614944458 
model_pd.lambdas: dict_items([('pout', tensor([0.6645])), ('power', tensor([0.0332]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1358	 i:0 	 global-step:27160	 l-p:0.05605681985616684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.6056, 26.6575, 26.6100],
        [26.6056, 29.2618, 29.1587],
        [26.6056, 26.6056, 26.6056],
        [26.6056, 26.9562, 26.7024]], grad_fn=<SliceBackward0>)

training epoch:1359, step:0 
model_pd.l_p.mean(): 0.05605682358145714 
model_pd.l_d.mean(): -1.0747003555297852 
model_pd.lagr.mean(): -1.0186434984207153 
model_pd.lambdas: dict_items([('pout', tensor([0.6629])), ('power', tensor([0.0331]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1359	 i:0 	 global-step:27180	 l-p:0.05605682358145714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.6056, 26.6071, 26.6056],
        [26.6056, 27.8976, 27.4195],
        [26.6056, 32.2124, 34.3686],
        [26.6056, 26.9562, 26.7024]], grad_fn=<SliceBackward0>)

training epoch:1360, step:0 
model_pd.l_p.mean(): 0.05605682358145714 
model_pd.l_d.mean(): -1.072209358215332 
model_pd.lagr.mean(): -1.0161525011062622 
model_pd.lambdas: dict_items([('pout', tensor([0.6614])), ('power', tensor([0.0330]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1360	 i:0 	 global-step:27200	 l-p:0.05605682358145714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.6056, 26.6056],
        [26.6055, 31.7262, 33.4211],
        [26.6055, 30.6327, 31.4023],
        [26.6055, 26.9612, 26.7047]], grad_fn=<SliceBackward0>)

training epoch:1361, step:0 
model_pd.l_p.mean(): 0.05605683475732803 
model_pd.l_d.mean(): -1.0697182416915894 
model_pd.lagr.mean(): -1.0136613845825195 
model_pd.lambdas: dict_items([('pout', tensor([0.6599])), ('power', tensor([0.0329]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1361	 i:0 	 global-step:27220	 l-p:0.05605683475732803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.9593, 26.7038],
        [26.6055, 26.6055, 26.6055],
        [26.6055, 26.6344, 26.6073],
        [26.6055, 26.6060, 26.6055]], grad_fn=<SliceBackward0>)

training epoch:1362, step:0 
model_pd.l_p.mean(): 0.056056827306747437 
model_pd.l_d.mean(): -1.0672271251678467 
model_pd.lagr.mean(): -1.0111702680587769 
model_pd.lambdas: dict_items([('pout', tensor([0.6583])), ('power', tensor([0.0329]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1362	 i:0 	 global-step:27240	 l-p:0.056056827306747437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.6055, 34.3967, 38.9299],
        [26.6055, 28.5377, 28.1562],
        [26.6055, 34.0563, 38.1895],
        [26.6055, 26.6055, 26.6055]], grad_fn=<SliceBackward0>)

training epoch:1363, step:0 
model_pd.l_p.mean(): 0.05605683475732803 
model_pd.l_d.mean(): -1.064736247062683 
model_pd.lagr.mean(): -1.0086793899536133 
model_pd.lambdas: dict_items([('pout', tensor([0.6568])), ('power', tensor([0.0328]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1363	 i:0 	 global-step:27260	 l-p:0.05605683475732803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.6055, 29.4489, 29.4411],
        [26.6055, 32.1670, 34.2790],
        [26.6055, 34.0564, 38.1897],
        [26.6055, 26.6055, 26.6055]], grad_fn=<SliceBackward0>)

training epoch:1364, step:0 
model_pd.l_p.mean(): 0.05605683848261833 
model_pd.l_d.mean(): -1.0622453689575195 
model_pd.lagr.mean(): -1.0061885118484497 
model_pd.lambdas: dict_items([('pout', tensor([0.6552])), ('power', tensor([0.0327]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1364	 i:0 	 global-step:27280	 l-p:0.05605683848261833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.6078, 26.6055],
        [26.6055, 34.0564, 38.1899],
        [26.6055, 30.6330, 31.4027],
        [26.6055, 28.0927, 27.6266]], grad_fn=<SliceBackward0>)

training epoch:1365, step:0 
model_pd.l_p.mean(): 0.05605683848261833 
model_pd.l_d.mean(): -1.0597541332244873 
model_pd.lagr.mean(): -1.0036972761154175 
model_pd.lambdas: dict_items([('pout', tensor([0.6537])), ('power', tensor([0.0326]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1365	 i:0 	 global-step:27300	 l-p:0.05605683848261833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.6070, 26.6055],
        [26.6055, 28.3679, 27.9453],
        [26.6055, 31.7266, 33.4217],
        [26.6055, 26.6454, 26.6084]], grad_fn=<SliceBackward0>)

training epoch:1366, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -1.0572632551193237 
model_pd.lagr.mean(): -1.001206398010254 
model_pd.lambdas: dict_items([('pout', tensor([0.6522])), ('power', tensor([0.0326]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1366	 i:0 	 global-step:27320	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.6055, 33.9112, 37.8770],
        [26.6055, 32.1673, 34.2794],
        [26.6055, 26.7906, 26.6397],
        [26.6055, 26.6055, 26.6055]], grad_fn=<SliceBackward0>)

training epoch:1367, step:0 
model_pd.l_p.mean(): 0.05605683475732803 
model_pd.l_d.mean(): -1.054771900177002 
model_pd.lagr.mean(): -0.9987150430679321 
model_pd.lambdas: dict_items([('pout', tensor([0.6506])), ('power', tensor([0.0325]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1367	 i:0 	 global-step:27340	 l-p:0.05605683475732803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 26.7906, 26.6397],
        [26.6055, 36.0018, 42.5487],
        [26.6055, 29.0058, 28.7868],
        [26.6055, 26.6155, 26.6058]], grad_fn=<SliceBackward0>)

training epoch:1368, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -1.0522810220718384 
model_pd.lagr.mean(): -0.9962241649627686 
model_pd.lambdas: dict_items([('pout', tensor([0.6491])), ('power', tensor([0.0324]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1368	 i:0 	 global-step:27360	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.6055, 33.9113, 37.8773],
        [26.6055, 36.0019, 42.5489],
        [26.6055, 26.6072, 26.6055],
        [26.6055, 27.2704, 26.8813]], grad_fn=<SliceBackward0>)

training epoch:1369, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -1.0497900247573853 
model_pd.lagr.mean(): -0.9937331676483154 
model_pd.lambdas: dict_items([('pout', tensor([0.6475])), ('power', tensor([0.0323]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1369	 i:0 	 global-step:27380	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 26.9594, 26.7037],
        [26.6054, 32.2070, 34.3577],
        [26.6054, 26.7906, 26.6397],
        [26.6054, 34.0569, 38.1907]], grad_fn=<SliceBackward0>)

training epoch:1370, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -1.0472989082336426 
model_pd.lagr.mean(): -0.9912420511245728 
model_pd.lambdas: dict_items([('pout', tensor([0.6460])), ('power', tensor([0.0323]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5273]))])
epoch：1370	 i:0 	 global-step:27400	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 29.4493, 29.4416],
        [26.6054, 26.9488, 26.6990],
        [26.6054, 27.3972, 26.9723],
        [26.6054, 36.0021, 42.5493]], grad_fn=<SliceBackward0>)

training epoch:1371, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -1.0448079109191895 
model_pd.lagr.mean(): -0.9887510538101196 
model_pd.lambdas: dict_items([('pout', tensor([0.6444])), ('power', tensor([0.0322]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1371	 i:0 	 global-step:27420	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 26.6054, 26.6054],
        [26.6054, 26.6054, 26.6054],
        [26.6054, 26.6078, 26.6055],
        [26.6054, 27.3972, 26.9723]], grad_fn=<SliceBackward0>)

training epoch:1372, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -1.0423167943954468 
model_pd.lagr.mean(): -0.986259937286377 
model_pd.lambdas: dict_items([('pout', tensor([0.6429])), ('power', tensor([0.0321]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1372	 i:0 	 global-step:27440	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 26.6091, 26.6055],
        [26.6054, 26.7906, 26.6397],
        [26.6054, 26.6072, 26.6055],
        [26.6054, 26.9594, 26.7037]], grad_fn=<SliceBackward0>)

training epoch:1373, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -1.039825677871704 
model_pd.lagr.mean(): -0.9837688207626343 
model_pd.lambdas: dict_items([('pout', tensor([0.6414])), ('power', tensor([0.0320]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1373	 i:0 	 global-step:27460	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 26.9562, 26.7023],
        [26.6054, 28.3682, 27.9456],
        [26.6054, 26.9612, 26.7046],
        [26.6054, 27.5772, 27.1179]], grad_fn=<SliceBackward0>)

training epoch:1374, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -1.0373345613479614 
model_pd.lagr.mean(): -0.9812777042388916 
model_pd.lambdas: dict_items([('pout', tensor([0.6398])), ('power', tensor([0.0319]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1374	 i:0 	 global-step:27480	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 29.4495, 29.4419],
        [26.6054, 30.9277, 31.9297],
        [26.6054, 27.3973, 26.9724],
        [26.6054, 26.6069, 26.6054]], grad_fn=<SliceBackward0>)

training epoch:1375, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -1.0348435640335083 
model_pd.lagr.mean(): -0.9787867069244385 
model_pd.lambdas: dict_items([('pout', tensor([0.6383])), ('power', tensor([0.0319]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1375	 i:0 	 global-step:27500	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 28.3683, 27.9456],
        [26.6054, 26.6453, 26.6083],
        [26.6054, 34.0574, 38.1917],
        [26.6054, 26.6077, 26.6054]], grad_fn=<SliceBackward0>)

training epoch:1376, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -1.0323524475097656 
model_pd.lagr.mean(): -0.9762955904006958 
model_pd.lambdas: dict_items([('pout', tensor([0.6367])), ('power', tensor([0.0318]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1376	 i:0 	 global-step:27520	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1377
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[26.6054, 32.1681, 34.2808],
        [26.6054, 27.5773, 27.1180],
        [26.6054, 27.8981, 27.4198],
        [26.6054, 30.6338, 31.4039]], grad_fn=<SliceBackward0>)

training epoch:1377, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -1.0298612117767334 
model_pd.lagr.mean(): -0.9738043546676636 
model_pd.lambdas: dict_items([('pout', tensor([0.6352])), ('power', tensor([0.0317]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1377	 i:0 	 global-step:27540	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 26.6077, 26.6054],
        [26.6054, 27.5773, 27.1180],
        [26.6054, 29.2627, 29.1598],
        [26.6054, 26.9594, 26.7037]], grad_fn=<SliceBackward0>)

training epoch:1378, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -1.0273702144622803 
model_pd.lagr.mean(): -0.9713133573532104 
model_pd.lambdas: dict_items([('pout', tensor([0.6337])), ('power', tensor([0.0316]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1378	 i:0 	 global-step:27560	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 35.3399, 41.0314],
        [26.6054, 27.2705, 26.8813],
        [26.6054, 29.0064, 28.7874],
        [26.6054, 26.6059, 26.6054]], grad_fn=<SliceBackward0>)

training epoch:1379, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -1.024878978729248 
model_pd.lagr.mean(): -0.9688221216201782 
model_pd.lambdas: dict_items([('pout', tensor([0.6321])), ('power', tensor([0.0316]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1379	 i:0 	 global-step:27580	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.6054, 26.6071, 26.6054],
        [26.6054, 28.5384, 28.1569],
        [26.6054, 34.0578, 38.1924],
        [26.6054, 29.2628, 29.1600]], grad_fn=<SliceBackward0>)

training epoch:1380, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -1.022387981414795 
model_pd.lagr.mean(): -0.9663311243057251 
model_pd.lambdas: dict_items([('pout', tensor([0.6306])), ('power', tensor([0.0315]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1380	 i:0 	 global-step:27600	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6053, 26.6054],
        [26.6053, 26.6055, 26.6054],
        [26.6053, 35.3401, 41.0317],
        [26.6053, 28.0932, 27.6270]], grad_fn=<SliceBackward0>)

training epoch:1381, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -1.0198969841003418 
model_pd.lagr.mean(): -0.963840126991272 
model_pd.lambdas: dict_items([('pout', tensor([0.6290])), ('power', tensor([0.0314]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1381	 i:0 	 global-step:27620	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6054, 26.6054],
        [26.6053, 26.6573, 26.6098],
        [26.6053, 34.0579, 38.1927],
        [26.6053, 26.6071, 26.6054]], grad_fn=<SliceBackward0>)

training epoch:1382, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -1.0174057483673096 
model_pd.lagr.mean(): -0.9613488912582397 
model_pd.lambdas: dict_items([('pout', tensor([0.6275])), ('power', tensor([0.0313]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1382	 i:0 	 global-step:27640	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 34.0580, 38.1929],
        [26.6053, 29.8351, 30.0498],
        [26.6053, 28.5385, 28.1570],
        [26.6053, 26.6154, 26.6057]], grad_fn=<SliceBackward0>)

training epoch:1383, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -1.014914631843567 
model_pd.lagr.mean(): -0.9588577747344971 
model_pd.lambdas: dict_items([('pout', tensor([0.6260])), ('power', tensor([0.0313]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1383	 i:0 	 global-step:27660	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6573, 26.6098],
        [26.6053, 26.9488, 26.6989],
        [26.6053, 26.6342, 26.6071],
        [26.6053, 32.1686, 34.2817]], grad_fn=<SliceBackward0>)

training epoch:1384, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -1.0124236345291138 
model_pd.lagr.mean(): -0.956366777420044 
model_pd.lambdas: dict_items([('pout', tensor([0.6244])), ('power', tensor([0.0312]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1384	 i:0 	 global-step:27680	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1385
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[26.6053, 29.8352, 30.0499],
        [26.6053, 28.0934, 27.6271],
        [26.6053, 27.3975, 26.9724],
        [26.6053, 30.9284, 31.9308]], grad_fn=<SliceBackward0>)

training epoch:1385, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -1.009932518005371 
model_pd.lagr.mean(): -0.9538756608963013 
model_pd.lambdas: dict_items([('pout', tensor([0.6229])), ('power', tensor([0.0311]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1385	 i:0 	 global-step:27700	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6071, 26.6054],
        [26.6053, 35.3406, 41.0327],
        [26.6053, 34.3988, 38.9338],
        [26.6053, 30.6344, 31.4049]], grad_fn=<SliceBackward0>)

training epoch:1386, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -1.0074412822723389 
model_pd.lagr.mean(): -0.951384425163269 
model_pd.lambdas: dict_items([('pout', tensor([0.6213])), ('power', tensor([0.0310]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1386	 i:0 	 global-step:27720	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6071, 26.6054],
        [26.6053, 26.6342, 26.6071],
        [26.6053, 29.5409, 29.5827],
        [26.6053, 26.6053, 26.6053]], grad_fn=<SliceBackward0>)

training epoch:1387, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -1.0049500465393066 
model_pd.lagr.mean(): -0.9488931894302368 
model_pd.lambdas: dict_items([('pout', tensor([0.6198])), ('power', tensor([0.0310]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1387	 i:0 	 global-step:27740	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6053, 36.0036, 42.5526],
        [26.6053, 26.9488, 26.6989],
        [26.6053, 29.5409, 29.5828],
        [26.6053, 26.6053, 26.6053]], grad_fn=<SliceBackward0>)

training epoch:1388, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -1.0024590492248535 
model_pd.lagr.mean(): -0.9464021921157837 
model_pd.lambdas: dict_items([('pout', tensor([0.6182])), ('power', tensor([0.0309]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1388	 i:0 	 global-step:27760	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 33.9131, 37.8807],
        [26.6053, 31.1747, 32.3813],
        [26.6053, 26.7138, 26.6198],
        [26.6053, 27.8984, 27.4200]], grad_fn=<SliceBackward0>)

training epoch:1389, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.9999676942825317 
model_pd.lagr.mean(): -0.9439108371734619 
model_pd.lambdas: dict_items([('pout', tensor([0.6167])), ('power', tensor([0.0308]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1389	 i:0 	 global-step:27780	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 30.9288, 31.9313],
        [26.6053, 26.9595, 26.7037],
        [26.6053, 27.8984, 27.4201],
        [26.6053, 29.8355, 30.0503]], grad_fn=<SliceBackward0>)

training epoch:1390, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -0.9974766373634338 
model_pd.lagr.mean(): -0.941419780254364 
model_pd.lambdas: dict_items([('pout', tensor([0.6152])), ('power', tensor([0.0307]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1390	 i:0 	 global-step:27800	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 29.4504, 29.4430],
        [26.6053, 26.7906, 26.6396],
        [26.6053, 30.9288, 31.9314],
        [26.6053, 26.6154, 26.6057]], grad_fn=<SliceBackward0>)

training epoch:1391, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.9949855804443359 
model_pd.lagr.mean(): -0.9389287233352661 
model_pd.lambdas: dict_items([('pout', tensor([0.6136])), ('power', tensor([0.0306]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1391	 i:0 	 global-step:27820	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6452, 26.6082],
        [26.6053, 26.6342, 26.6070],
        [26.6053, 26.8138, 26.6468],
        [26.6053, 31.7286, 33.4251]], grad_fn=<SliceBackward0>)

training epoch:1392, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.9924944043159485 
model_pd.lagr.mean(): -0.9364375472068787 
model_pd.lambdas: dict_items([('pout', tensor([0.6121])), ('power', tensor([0.0306]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1392	 i:0 	 global-step:27840	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 27.2707, 26.8813],
        [26.6053, 29.2635, 29.1608],
        [26.6053, 27.8985, 27.4201],
        [26.6053, 26.6572, 26.6098]], grad_fn=<SliceBackward0>)

training epoch:1393, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.9900031685829163 
model_pd.lagr.mean(): -0.9339463114738464 
model_pd.lambdas: dict_items([('pout', tensor([0.6105])), ('power', tensor([0.0305]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1393	 i:0 	 global-step:27860	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6053, 26.6053],
        [26.6053, 30.6350, 31.4057],
        [26.6053, 30.9291, 31.9318],
        [26.6053, 29.5413, 29.5833]], grad_fn=<SliceBackward0>)

training epoch:1394, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.9875119924545288 
model_pd.lagr.mean(): -0.931455135345459 
model_pd.lambdas: dict_items([('pout', tensor([0.6090])), ('power', tensor([0.0304]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1394	 i:0 	 global-step:27880	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6068, 26.6053],
        [26.6053, 28.5390, 28.1576],
        [26.6053, 27.7561, 27.2798],
        [26.6053, 26.6572, 26.6097]], grad_fn=<SliceBackward0>)

training epoch:1395, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.9850208759307861 
model_pd.lagr.mean(): -0.9289640188217163 
model_pd.lambdas: dict_items([('pout', tensor([0.6075])), ('power', tensor([0.0303]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1395	 i:0 	 global-step:27900	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6076, 26.6053],
        [26.6053, 26.7906, 26.6396],
        [26.6053, 34.0592, 38.1951],
        [26.6053, 27.2708, 26.8814]], grad_fn=<SliceBackward0>)

training epoch:1396, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.9825295805931091 
model_pd.lagr.mean(): -0.9264727234840393 
model_pd.lambdas: dict_items([('pout', tensor([0.6059])), ('power', tensor([0.0303]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1396	 i:0 	 global-step:27920	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 29.6366, 29.7324],
        [26.6053, 26.6053, 26.6053],
        [26.6053, 32.2092, 34.3615],
        [26.6053, 26.9614, 26.7045]], grad_fn=<SliceBackward0>)

training epoch:1397, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9800386428833008 
model_pd.lagr.mean(): -0.923981785774231 
model_pd.lambdas: dict_items([('pout', tensor([0.6044])), ('power', tensor([0.0302]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1397	 i:0 	 global-step:27940	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 27.7562, 27.2799],
        [26.6053, 26.6076, 26.6053],
        [26.6053, 29.2638, 29.1612],
        [26.6053, 26.6070, 26.6053]], grad_fn=<SliceBackward0>)

training epoch:1398, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9775472283363342 
model_pd.lagr.mean(): -0.9214903712272644 
model_pd.lambdas: dict_items([('pout', tensor([0.6028])), ('power', tensor([0.0301]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1398	 i:0 	 global-step:27960	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 29.0074, 28.7885],
        [26.6053, 26.6341, 26.6070],
        [26.6053, 26.7138, 26.6197],
        [26.6053, 27.3977, 26.9725]], grad_fn=<SliceBackward0>)

training epoch:1399, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.975055992603302 
model_pd.lagr.mean(): -0.9189991354942322 
model_pd.lambdas: dict_items([('pout', tensor([0.6013])), ('power', tensor([0.0300]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5411])), ('power', tensor([-1.5274]))])
epoch：1399	 i:0 	 global-step:27980	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 27.5198, 27.0693],
        [26.6053, 26.6053, 26.6053],
        [26.6053, 26.6572, 26.6097],
        [26.6053, 26.6452, 26.6082]], grad_fn=<SliceBackward0>)

training epoch:1400, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9725648164749146 
model_pd.lagr.mean(): -0.9165079593658447 
model_pd.lambdas: dict_items([('pout', tensor([0.5998])), ('power', tensor([0.0300]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1400	 i:0 	 global-step:28000	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6058, 26.6053],
        [26.6053, 29.5417, 29.5838],
        [26.6053, 33.9142, 37.8827],
        [26.6053, 35.3419, 41.0354]], grad_fn=<SliceBackward0>)

training epoch:1401, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9700736403465271 
model_pd.lagr.mean(): -0.9140167832374573 
model_pd.lambdas: dict_items([('pout', tensor([0.5982])), ('power', tensor([0.0299]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1401	 i:0 	 global-step:28020	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6053, 26.6053],
        [26.6053, 30.9296, 31.9327],
        [26.6053, 29.4510, 29.4438],
        [26.6053, 26.6058, 26.6053]], grad_fn=<SliceBackward0>)

training epoch:1402, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9675825238227844 
model_pd.lagr.mean(): -0.9115256667137146 
model_pd.lambdas: dict_items([('pout', tensor([0.5967])), ('power', tensor([0.0298]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1402	 i:0 	 global-step:28040	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.6053, 26.6053, 26.6053],
        [26.6053, 27.8988, 27.4203],
        [26.6053, 28.0940, 27.6276],
        [26.6053, 36.0050, 42.5554]], grad_fn=<SliceBackward0>)

training epoch:1403, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9650912284851074 
model_pd.lagr.mean(): -0.9090343713760376 
model_pd.lambdas: dict_items([('pout', tensor([0.5951])), ('power', tensor([0.0297]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1403	 i:0 	 global-step:28060	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 29.6370, 29.7330],
        [26.6052, 26.9614, 26.7045],
        [26.6052, 30.9298, 31.9329],
        [26.6052, 26.6572, 26.6097]], grad_fn=<SliceBackward0>)

training epoch:1404, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9626001119613647 
model_pd.lagr.mean(): -0.9065432548522949 
model_pd.lambdas: dict_items([('pout', tensor([0.5936])), ('power', tensor([0.0297]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1404	 i:0 	 global-step:28080	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 34.0600, 38.1966],
        [26.6052, 27.5779, 27.1183],
        [26.6052, 29.0077, 28.7889],
        [26.6052, 26.6452, 26.6082]], grad_fn=<SliceBackward0>)

training epoch:1405, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9601088762283325 
model_pd.lagr.mean(): -0.9040520191192627 
model_pd.lambdas: dict_items([('pout', tensor([0.5920])), ('power', tensor([0.0296]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1405	 i:0 	 global-step:28100	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 31.1760, 32.3833],
        [26.6052, 26.6089, 26.6053],
        [26.6052, 29.4512, 29.4441],
        [26.6052, 26.9596, 26.7037]], grad_fn=<SliceBackward0>)

training epoch:1406, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9576175808906555 
model_pd.lagr.mean(): -0.9015607237815857 
model_pd.lambdas: dict_items([('pout', tensor([0.5905])), ('power', tensor([0.0295]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1406	 i:0 	 global-step:28120	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1407
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[26.6052, 29.0078, 28.7890],
        [26.6052, 32.2100, 34.3629],
        [26.6052, 34.0601, 38.1969],
        [26.6052, 27.8989, 27.4204]], grad_fn=<SliceBackward0>)

training epoch:1407, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9551264047622681 
model_pd.lagr.mean(): -0.8990695476531982 
model_pd.lambdas: dict_items([('pout', tensor([0.5890])), ('power', tensor([0.0294]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1407	 i:0 	 global-step:28140	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6089, 26.6053],
        [26.6052, 27.5200, 27.0694],
        [26.6052, 30.9301, 31.9333],
        [26.6052, 27.5780, 27.1184]], grad_fn=<SliceBackward0>)

training epoch:1408, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9526351690292358 
model_pd.lagr.mean(): -0.896578311920166 
model_pd.lambdas: dict_items([('pout', tensor([0.5874])), ('power', tensor([0.0293]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1408	 i:0 	 global-step:28160	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6070, 26.6053],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 28.4489, 28.0442],
        [26.6052, 34.0603, 38.1972]], grad_fn=<SliceBackward0>)

training epoch:1409, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9501440525054932 
model_pd.lagr.mean(): -0.8940871953964233 
model_pd.lambdas: dict_items([('pout', tensor([0.5859])), ('power', tensor([0.0293]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1409	 i:0 	 global-step:28180	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 29.4515, 29.4444],
        [26.6052, 27.3979, 26.9726],
        [26.6052, 26.6089, 26.6053],
        [26.6052, 26.7907, 26.6395]], grad_fn=<SliceBackward0>)

training epoch:1410, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9476527571678162 
model_pd.lagr.mean(): -0.8915959000587463 
model_pd.lambdas: dict_items([('pout', tensor([0.5843])), ('power', tensor([0.0292]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1410	 i:0 	 global-step:28200	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1411
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[26.6052, 36.0057, 42.5569],
        [26.6052, 34.0605, 38.1976],
        [26.6052, 29.5422, 29.5845],
        [26.6052, 28.0943, 27.6278]], grad_fn=<SliceBackward0>)

training epoch:1411, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9451615214347839 
model_pd.lagr.mean(): -0.8891046643257141 
model_pd.lambdas: dict_items([('pout', tensor([0.5828])), ('power', tensor([0.0291]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1411	 i:0 	 global-step:28220	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6076, 26.6052],
        [26.6052, 26.6452, 26.6082],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 27.5201, 27.0695]], grad_fn=<SliceBackward0>)

training epoch:1412, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9426702260971069 
model_pd.lagr.mean(): -0.8866133689880371 
model_pd.lambdas: dict_items([('pout', tensor([0.5813])), ('power', tensor([0.0290]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1412	 i:0 	 global-step:28240	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6052, 26.6052],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 26.6054, 26.6052],
        [26.6052, 32.2105, 34.3637]], grad_fn=<SliceBackward0>)

training epoch:1413, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9401791095733643 
model_pd.lagr.mean(): -0.8841222524642944 
model_pd.lambdas: dict_items([('pout', tensor([0.5797])), ('power', tensor([0.0290]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1413	 i:0 	 global-step:28260	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 29.5424, 29.5847],
        [26.6052, 26.6153, 26.6055],
        [26.6052, 27.3980, 26.9727],
        [26.6052, 28.4491, 28.0444]], grad_fn=<SliceBackward0>)

training epoch:1414, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9376878142356873 
model_pd.lagr.mean(): -0.8816309571266174 
model_pd.lambdas: dict_items([('pout', tensor([0.5782])), ('power', tensor([0.0289]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1414	 i:0 	 global-step:28280	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.8139, 26.6468],
        [26.6052, 26.9596, 26.7037],
        [26.6052, 28.4492, 28.0444],
        [26.6052, 31.1766, 32.3843]], grad_fn=<SliceBackward0>)

training epoch:1415, step:0 
model_pd.l_p.mean(): 0.05605688691139221 
model_pd.l_d.mean(): -0.9351965188980103 
model_pd.lagr.mean(): -0.8791396617889404 
model_pd.lambdas: dict_items([('pout', tensor([0.5766])), ('power', tensor([0.0288]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1415	 i:0 	 global-step:28300	 l-p:0.05605688691139221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1416
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[26.6052, 32.2169, 34.3763],
        [26.6052, 27.5201, 27.0695],
        [26.6052, 27.7567, 27.2802],
        [26.6052, 27.5782, 27.1185]], grad_fn=<SliceBackward0>)

training epoch:1416, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.932705283164978 
model_pd.lagr.mean(): -0.8766484260559082 
model_pd.lambdas: dict_items([('pout', tensor([0.5751])), ('power', tensor([0.0287]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1416	 i:0 	 global-step:28320	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6052, 26.6052],
        [26.6052, 32.1713, 34.2863],
        [26.6052, 27.5782, 27.1185],
        [26.6052, 31.1768, 32.3845]], grad_fn=<SliceBackward0>)

training epoch:1417, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9302140474319458 
model_pd.lagr.mean(): -0.874157190322876 
model_pd.lambdas: dict_items([('pout', tensor([0.5736])), ('power', tensor([0.0287]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1417	 i:0 	 global-step:28340	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 28.4493, 28.0446],
        [26.6052, 26.6341, 26.6069],
        [26.6052, 26.6057, 26.6052],
        [26.6052, 26.6452, 26.6081]], grad_fn=<SliceBackward0>)

training epoch:1418, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9277226328849792 
model_pd.lagr.mean(): -0.8716657757759094 
model_pd.lambdas: dict_items([('pout', tensor([0.5720])), ('power', tensor([0.0286]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1418	 i:0 	 global-step:28360	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6057, 26.6052],
        [26.6052, 27.7568, 27.2803],
        [26.6052, 30.6367, 31.4083],
        [26.6052, 26.6052, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1419, step:0 
model_pd.l_p.mean(): 0.056056879460811615 
model_pd.l_d.mean(): -0.925231397151947 
model_pd.lagr.mean(): -0.8691745400428772 
model_pd.lambdas: dict_items([('pout', tensor([0.5705])), ('power', tensor([0.0285]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1419	 i:0 	 global-step:28380	 l-p:0.056056879460811615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 29.8373, 30.0528],
        [26.6052, 26.9565, 26.7022],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 28.5401, 28.1586]], grad_fn=<SliceBackward0>)

training epoch:1420, step:0 
model_pd.l_p.mean(): 0.056056879460811615 
model_pd.l_d.mean(): -0.9227401614189148 
model_pd.lagr.mean(): -0.866683304309845 
model_pd.lambdas: dict_items([('pout', tensor([0.5689])), ('power', tensor([0.0284]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1420	 i:0 	 global-step:28400	 l-p:0.056056879460811615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.9565, 26.7022],
        [26.6052, 28.5401, 28.1586],
        [26.6052, 27.8994, 27.4207],
        [26.6052, 31.1771, 32.3850]], grad_fn=<SliceBackward0>)

training epoch:1421, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9202487468719482 
model_pd.lagr.mean(): -0.8641918897628784 
model_pd.lambdas: dict_items([('pout', tensor([0.5674])), ('power', tensor([0.0284]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1421	 i:0 	 global-step:28420	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6153, 26.6055],
        [26.6052, 29.8375, 30.0529],
        [26.6052, 29.4521, 29.4452],
        [26.6052, 26.7907, 26.6395]], grad_fn=<SliceBackward0>)

training epoch:1422, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9177573919296265 
model_pd.lagr.mean(): -0.8617005348205566 
model_pd.lambdas: dict_items([('pout', tensor([0.5658])), ('power', tensor([0.0283]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1422	 i:0 	 global-step:28440	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 36.0067, 42.5591],
        [26.6052, 30.9311, 31.9350],
        [26.6052, 26.9565, 26.7022],
        [26.6052, 26.6053, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1423, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9152659773826599 
model_pd.lagr.mean(): -0.8592091202735901 
model_pd.lambdas: dict_items([('pout', tensor([0.5643])), ('power', tensor([0.0282]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1423	 i:0 	 global-step:28460	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6153, 26.6055],
        [26.6052, 27.7569, 27.2804],
        [26.6052, 26.6088, 26.6052],
        [26.6052, 30.9312, 31.9351]], grad_fn=<SliceBackward0>)

training epoch:1424, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9127745032310486 
model_pd.lagr.mean(): -0.8567176461219788 
model_pd.lambdas: dict_items([('pout', tensor([0.5628])), ('power', tensor([0.0281]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1424	 i:0 	 global-step:28480	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.9616, 26.7045],
        [26.6052, 26.8139, 26.6467],
        [26.6052, 26.6075, 26.6052],
        [26.6052, 28.5403, 28.1588]], grad_fn=<SliceBackward0>)

training epoch:1425, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9102831482887268 
model_pd.lagr.mean(): -0.854226291179657 
model_pd.lambdas: dict_items([('pout', tensor([0.5612])), ('power', tensor([0.0281]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1425	 i:0 	 global-step:28500	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.9616, 26.7045],
        [26.6052, 26.6075, 26.6052],
        [26.6052, 34.4023, 38.9406],
        [26.6052, 30.6372, 31.4090]], grad_fn=<SliceBackward0>)

training epoch:1426, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9077918529510498 
model_pd.lagr.mean(): -0.85173499584198 
model_pd.lambdas: dict_items([('pout', tensor([0.5597])), ('power', tensor([0.0280]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1426	 i:0 	 global-step:28520	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.9566, 26.7023],
        [26.6052, 27.3983, 26.9728],
        [26.6052, 27.5204, 27.0697],
        [26.6052, 26.6067, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1427, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.905300498008728 
model_pd.lagr.mean(): -0.8492436408996582 
model_pd.lambdas: dict_items([('pout', tensor([0.5581])), ('power', tensor([0.0279]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1427	 i:0 	 global-step:28540	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 30.9315, 31.9355],
        [26.6052, 26.7907, 26.6395],
        [26.6052, 31.1776, 32.3858],
        [26.6052, 27.3983, 26.9728]], grad_fn=<SliceBackward0>)

training epoch:1428, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.9028090238571167 
model_pd.lagr.mean(): -0.8467521667480469 
model_pd.lambdas: dict_items([('pout', tensor([0.5566])), ('power', tensor([0.0278]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1428	 i:0 	 global-step:28560	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.8139, 26.6468],
        [26.6052, 28.0949, 27.6283],
        [26.6052, 26.6067, 26.6052],
        [26.6052, 28.5405, 28.1590]], grad_fn=<SliceBackward0>)

training epoch:1429, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.9003177285194397 
model_pd.lagr.mean(): -0.8442608714103699 
model_pd.lambdas: dict_items([('pout', tensor([0.5551])), ('power', tensor([0.0277]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1429	 i:0 	 global-step:28580	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6052, 26.6052],
        [26.6052, 26.6057, 26.6052],
        [26.6052, 26.6452, 26.6081],
        [26.6052, 26.6075, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1430, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.8978263735771179 
model_pd.lagr.mean(): -0.8417695164680481 
model_pd.lambdas: dict_items([('pout', tensor([0.5535])), ('power', tensor([0.0277]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1430	 i:0 	 global-step:28600	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6053, 26.6052],
        [26.6052, 26.7138, 26.6196],
        [26.6052, 27.5785, 27.1187],
        [26.6052, 29.5434, 29.5860]], grad_fn=<SliceBackward0>)

training epoch:1431, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.8953348398208618 
model_pd.lagr.mean(): -0.839277982711792 
model_pd.lambdas: dict_items([('pout', tensor([0.5520])), ('power', tensor([0.0276]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1431	 i:0 	 global-step:28620	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 36.0075, 42.5608],
        [26.6052, 26.9616, 26.7045],
        [26.6052, 29.2657, 29.1633],
        [26.6052, 32.1725, 34.2883]], grad_fn=<SliceBackward0>)

training epoch:1432, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.89284348487854 
model_pd.lagr.mean(): -0.8367866277694702 
model_pd.lambdas: dict_items([('pout', tensor([0.5504])), ('power', tensor([0.0275]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5274]))])
epoch：1432	 i:0 	 global-step:28640	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 29.0091, 28.7905],
        [26.6052, 26.9616, 26.7045],
        [26.6052, 28.0950, 27.6284],
        [26.6052, 26.9598, 26.7037]], grad_fn=<SliceBackward0>)

training epoch:1433, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.8903520107269287 
model_pd.lagr.mean(): -0.8342951536178589 
model_pd.lambdas: dict_items([('pout', tensor([0.5489])), ('power', tensor([0.0274]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1433	 i:0 	 global-step:28660	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.9617, 26.7045],
        [26.6052, 30.6378, 31.4098],
        [26.6052, 27.8998, 27.4211],
        [26.6052, 34.0625, 38.2013]], grad_fn=<SliceBackward0>)

training epoch:1434, step:0 
model_pd.l_p.mean(): 0.056056875735521317 
model_pd.l_d.mean(): -0.8878606557846069 
model_pd.lagr.mean(): -0.8318037986755371 
model_pd.lambdas: dict_items([('pout', tensor([0.5474])), ('power', tensor([0.0274]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1434	 i:0 	 global-step:28680	 l-p:0.056056875735521317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 36.0078, 42.5613],
        [26.6052, 26.7908, 26.6395],
        [26.6052, 31.1781, 32.3866],
        [26.6052, 32.1728, 34.2887]], grad_fn=<SliceBackward0>)

training epoch:1435, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -0.8853692412376404 
model_pd.lagr.mean(): -0.8293123841285706 
model_pd.lambdas: dict_items([('pout', tensor([0.5458])), ('power', tensor([0.0273]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1435	 i:0 	 global-step:28700	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6053, 26.6052],
        [26.6052, 28.4501, 28.0453],
        [26.6052, 28.5408, 28.1593],
        [26.6052, 26.6052, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1436, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.882877767086029 
model_pd.lagr.mean(): -0.8268209099769592 
model_pd.lambdas: dict_items([('pout', tensor([0.5443])), ('power', tensor([0.0272]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1436	 i:0 	 global-step:28720	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 30.9322, 31.9365],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 29.2659, 29.1637],
        [26.6052, 29.6390, 29.7355]], grad_fn=<SliceBackward0>)

training epoch:1437, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.8803863525390625 
model_pd.lagr.mean(): -0.8243294954299927 
model_pd.lambdas: dict_items([('pout', tensor([0.5427])), ('power', tensor([0.0271]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1437	 i:0 	 global-step:28740	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6052, 26.6052],
        [26.6052, 29.5438, 29.5865],
        [26.6052, 35.3452, 41.0420],
        [26.6052, 26.6075, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1438, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.877894937992096 
model_pd.lagr.mean(): -0.8218380808830261 
model_pd.lambdas: dict_items([('pout', tensor([0.5412])), ('power', tensor([0.0271]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1438	 i:0 	 global-step:28760	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1439
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[26.6052, 28.4502, 28.0454],
        [26.6052, 35.3453, 41.0422],
        [26.6052, 30.9323, 31.9367],
        [26.6052, 29.4531, 29.4464]], grad_fn=<SliceBackward0>)

training epoch:1439, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -0.8754035234451294 
model_pd.lagr.mean(): -0.8193466663360596 
model_pd.lambdas: dict_items([('pout', tensor([0.5396])), ('power', tensor([0.0270]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1439	 i:0 	 global-step:28780	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6452, 26.6081],
        [26.6052, 29.6391, 29.7357],
        [26.6052, 28.5410, 28.1595],
        [26.6052, 29.2661, 29.1639]], grad_fn=<SliceBackward0>)

training epoch:1440, step:0 
model_pd.l_p.mean(): 0.05605687201023102 
model_pd.l_d.mean(): -0.8729121088981628 
model_pd.lagr.mean(): -0.816855251789093 
model_pd.lambdas: dict_items([('pout', tensor([0.5381])), ('power', tensor([0.0269]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1440	 i:0 	 global-step:28800	 l-p:0.05605687201023102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 34.0631, 38.2025],
        [26.6052, 31.7325, 33.4313],
        [26.6052, 29.5440, 29.5867],
        [26.6052, 26.8140, 26.6468]], grad_fn=<SliceBackward0>)

training epoch:1441, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -0.8704206943511963 
model_pd.lagr.mean(): -0.8143638372421265 
model_pd.lambdas: dict_items([('pout', tensor([0.5366])), ('power', tensor([0.0268]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1441	 i:0 	 global-step:28820	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 27.7574, 27.2807],
        [26.6052, 29.4533, 29.4466],
        [26.6052, 28.0954, 27.6287],
        [26.6052, 26.6075, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1442, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -0.8679291605949402 
model_pd.lagr.mean(): -0.8118723034858704 
model_pd.lambdas: dict_items([('pout', tensor([0.5350])), ('power', tensor([0.0268]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1442	 i:0 	 global-step:28840	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 32.2129, 34.3677],
        [26.6052, 29.5441, 29.5868],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 29.2663, 29.1641]], grad_fn=<SliceBackward0>)

training epoch:1443, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.8654377460479736 
model_pd.lagr.mean(): -0.8093808889389038 
model_pd.lambdas: dict_items([('pout', tensor([0.5335])), ('power', tensor([0.0267]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1443	 i:0 	 global-step:28860	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6052, 34.0634, 38.2030],
        [26.6052, 29.6394, 29.7360],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 26.6052, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1444, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.8629462718963623 
model_pd.lagr.mean(): -0.8068894147872925 
model_pd.lambdas: dict_items([('pout', tensor([0.5319])), ('power', tensor([0.0266]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1444	 i:0 	 global-step:28880	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 27.5208, 27.0699],
        [26.6052, 26.6088, 26.6052],
        [26.6052, 29.0097, 28.7912],
        [26.6052, 28.4504, 28.0457]], grad_fn=<SliceBackward0>)

training epoch:1445, step:0 
model_pd.l_p.mean(): 0.05605686083436012 
model_pd.l_d.mean(): -0.8604548573493958 
model_pd.lagr.mean(): -0.8043980002403259 
model_pd.lambdas: dict_items([('pout', tensor([0.5304])), ('power', tensor([0.0265]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1445	 i:0 	 global-step:28900	 l-p:0.05605686083436012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 27.5209, 27.0700],
        [26.6052, 30.9328, 31.9374],
        [26.6052, 26.6572, 26.6096],
        [26.6052, 26.6452, 26.6081]], grad_fn=<SliceBackward0>)

training epoch:1446, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8579633831977844 
model_pd.lagr.mean(): -0.8019065260887146 
model_pd.lambdas: dict_items([('pout', tensor([0.5289])), ('power', tensor([0.0264]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1446	 i:0 	 global-step:28920	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 29.5443, 29.5871],
        [26.6052, 30.9329, 31.9375],
        [26.6052, 26.9599, 26.7037],
        [26.6052, 30.6387, 31.4111]], grad_fn=<SliceBackward0>)

training epoch:1447, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8554717898368835 
model_pd.lagr.mean(): -0.7994149327278137 
model_pd.lambdas: dict_items([('pout', tensor([0.5273])), ('power', tensor([0.0264]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1447	 i:0 	 global-step:28940	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 31.1791, 32.3881],
        [26.6052, 26.6075, 26.6052],
        [26.6052, 26.9618, 26.7046],
        [26.6052, 29.8391, 30.0550]], grad_fn=<SliceBackward0>)

training epoch:1448, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.8529804348945618 
model_pd.lagr.mean(): -0.7969235777854919 
model_pd.lambdas: dict_items([('pout', tensor([0.5258])), ('power', tensor([0.0263]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1448	 i:0 	 global-step:28960	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6153, 26.6055],
        [26.6052, 26.6069, 26.6052],
        [26.6052, 28.5414, 28.1599],
        [26.6052, 34.0638, 38.2038]], grad_fn=<SliceBackward0>)

training epoch:1449, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.8504889607429504 
model_pd.lagr.mean(): -0.7944321036338806 
model_pd.lambdas: dict_items([('pout', tensor([0.5242])), ('power', tensor([0.0262]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1449	 i:0 	 global-step:28980	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1450
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[26.6052, 34.0639, 38.2039],
        [26.6052, 31.7332, 33.4324],
        [26.6052, 28.3712, 27.9484],
        [26.6052, 30.9331, 31.9379]], grad_fn=<SliceBackward0>)

training epoch:1450, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.8479973673820496 
model_pd.lagr.mean(): -0.7919405102729797 
model_pd.lambdas: dict_items([('pout', tensor([0.5227])), ('power', tensor([0.0261]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1450	 i:0 	 global-step:29000	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1451
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[26.6052, 27.7577, 27.2809],
        [26.6052, 31.7332, 33.4325],
        [26.6052, 32.2197, 34.3810],
        [26.6052, 29.2667, 29.1646]], grad_fn=<SliceBackward0>)

training epoch:1451, step:0 
model_pd.l_p.mean(): 0.05605686828494072 
model_pd.l_d.mean(): -0.8455061316490173 
model_pd.lagr.mean(): -0.7894492745399475 
model_pd.lambdas: dict_items([('pout', tensor([0.5212])), ('power', tensor([0.0261]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1451	 i:0 	 global-step:29020	 l-p:0.05605686828494072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6052, 27.3988, 26.9731],
        [26.6052, 29.5446, 29.5875],
        [26.6052, 26.9568, 26.7023],
        [26.6052, 26.6052, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1452, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.8430145978927612 
model_pd.lagr.mean(): -0.7869577407836914 
model_pd.lambdas: dict_items([('pout', tensor([0.5196])), ('power', tensor([0.0260]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1452	 i:0 	 global-step:29040	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6057, 26.6052],
        [26.6052, 26.9568, 26.7023],
        [26.6052, 26.6153, 26.6055],
        [26.6052, 27.7578, 27.2810]], grad_fn=<SliceBackward0>)

training epoch:1453, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8405231833457947 
model_pd.lagr.mean(): -0.7844663262367249 
model_pd.lambdas: dict_items([('pout', tensor([0.5181])), ('power', tensor([0.0259]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1453	 i:0 	 global-step:29060	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1454
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[26.6052, 27.9004, 27.4216],
        [26.6052, 27.5791, 27.1191],
        [26.6052, 28.3714, 27.9486],
        [26.6052, 28.5416, 28.1601]], grad_fn=<SliceBackward0>)

training epoch:1454, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.8380315899848938 
model_pd.lagr.mean(): -0.781974732875824 
model_pd.lambdas: dict_items([('pout', tensor([0.5165])), ('power', tensor([0.0258]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1454	 i:0 	 global-step:29080	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 31.7335, 33.4330],
        [26.6052, 26.6057, 26.6052],
        [26.6052, 30.9334, 31.9384],
        [26.6052, 33.9188, 37.8913]], grad_fn=<SliceBackward0>)

training epoch:1455, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.835540235042572 
model_pd.lagr.mean(): -0.7794833779335022 
model_pd.lambdas: dict_items([('pout', tensor([0.5150])), ('power', tensor([0.0258]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1455	 i:0 	 global-step:29100	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6053, 26.6052],
        [26.6052, 29.2670, 29.1649],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 26.6052, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1456, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.8330486416816711 
model_pd.lagr.mean(): -0.7769917845726013 
model_pd.lambdas: dict_items([('pout', tensor([0.5134])), ('power', tensor([0.0257]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5273]))])
epoch：1456	 i:0 	 global-step:29120	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 31.1797, 32.3891],
        [26.6052, 26.9569, 26.7024],
        [26.6052, 33.9190, 37.8916],
        [26.6052, 29.4541, 29.4476]], grad_fn=<SliceBackward0>)

training epoch:1457, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8305570483207703 
model_pd.lagr.mean(): -0.7745001912117004 
model_pd.lambdas: dict_items([('pout', tensor([0.5119])), ('power', tensor([0.0256]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1457	 i:0 	 global-step:29140	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6088, 26.6052],
        [26.6052, 29.2671, 29.1650],
        [26.6052, 26.9494, 26.6990],
        [26.6052, 29.5450, 29.5879]], grad_fn=<SliceBackward0>)

training epoch:1458, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8280656933784485 
model_pd.lagr.mean(): -0.7720088362693787 
model_pd.lambdas: dict_items([('pout', tensor([0.5104])), ('power', tensor([0.0255]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1458	 i:0 	 global-step:29160	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 34.0646, 38.2053],
        [26.6052, 29.0104, 28.7919],
        [26.6052, 26.6057, 26.6052],
        [26.6052, 26.6452, 26.6081]], grad_fn=<SliceBackward0>)

training epoch:1459, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8255742192268372 
model_pd.lagr.mean(): -0.7695173621177673 
model_pd.lambdas: dict_items([('pout', tensor([0.5088])), ('power', tensor([0.0255]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1459	 i:0 	 global-step:29180	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6052, 26.6052],
        [26.6052, 34.4053, 38.9462],
        [26.6052, 28.4511, 28.0463],
        [26.6052, 26.9619, 26.7046]], grad_fn=<SliceBackward0>)

training epoch:1460, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.823082685470581 
model_pd.lagr.mean(): -0.7670258283615112 
model_pd.lambdas: dict_items([('pout', tensor([0.5073])), ('power', tensor([0.0254]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1460	 i:0 	 global-step:29200	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6067, 26.6052],
        [26.6052, 30.6396, 31.4125],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 27.5793, 27.1192]], grad_fn=<SliceBackward0>)

training epoch:1461, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.820591151714325 
model_pd.lagr.mean(): -0.7645342946052551 
model_pd.lambdas: dict_items([('pout', tensor([0.5057])), ('power', tensor([0.0253]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1461	 i:0 	 global-step:29220	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 29.5452, 29.5882],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 29.2673, 29.1653],
        [26.6052, 27.5212, 27.0702]], grad_fn=<SliceBackward0>)

training epoch:1462, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.8180996179580688 
model_pd.lagr.mean(): -0.762042760848999 
model_pd.lambdas: dict_items([('pout', tensor([0.5042])), ('power', tensor([0.0252]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1462	 i:0 	 global-step:29240	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.9569, 26.7024],
        [26.6052, 26.6067, 26.6052],
        [26.6052, 26.6341, 26.6069],
        [26.6052, 27.3990, 26.9732]], grad_fn=<SliceBackward0>)

training epoch:1463, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8156081438064575 
model_pd.lagr.mean(): -0.7595512866973877 
model_pd.lambdas: dict_items([('pout', tensor([0.5027])), ('power', tensor([0.0251]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1463	 i:0 	 global-step:29260	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6052, 26.6052],
        [26.6052, 28.0961, 27.6294],
        [26.6052, 35.3475, 41.0465],
        [26.6052, 28.5420, 28.1605]], grad_fn=<SliceBackward0>)

training epoch:1464, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.8131166696548462 
model_pd.lagr.mean(): -0.7570598125457764 
model_pd.lambdas: dict_items([('pout', tensor([0.5011])), ('power', tensor([0.0251]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1464	 i:0 	 global-step:29280	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 30.6399, 31.4129],
        [26.6052, 32.2208, 34.3828],
        [26.6052, 26.7909, 26.6395],
        [26.6052, 32.2146, 34.3706]], grad_fn=<SliceBackward0>)

training epoch:1465, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.8106251955032349 
model_pd.lagr.mean(): -0.754568338394165 
model_pd.lambdas: dict_items([('pout', tensor([0.4996])), ('power', tensor([0.0250]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1465	 i:0 	 global-step:29300	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6572, 26.6096],
        [26.6052, 33.9197, 37.8930],
        [26.6052, 32.2209, 34.3829],
        [26.6052, 26.6452, 26.6081]], grad_fn=<SliceBackward0>)

training epoch:1466, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.808133602142334 
model_pd.lagr.mean(): -0.7520767450332642 
model_pd.lambdas: dict_items([('pout', tensor([0.4980])), ('power', tensor([0.0249]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1466	 i:0 	 global-step:29320	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.8142, 26.6468],
        [26.6052, 32.1753, 34.2928],
        [26.6052, 29.0108, 28.7924],
        [26.6052, 29.8402, 30.0565]], grad_fn=<SliceBackward0>)

training epoch:1467, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8056420087814331 
model_pd.lagr.mean(): -0.7495851516723633 
model_pd.lambdas: dict_items([('pout', tensor([0.4965])), ('power', tensor([0.0248]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1467	 i:0 	 global-step:29340	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 27.3991, 26.9733],
        [26.6052, 26.6153, 26.6055],
        [26.6052, 35.3478, 41.0471],
        [26.6052, 26.6088, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1468, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.803150475025177 
model_pd.lagr.mean(): -0.7470936179161072 
model_pd.lambdas: dict_items([('pout', tensor([0.4950])), ('power', tensor([0.0248]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1468	 i:0 	 global-step:29360	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.7909, 26.6395],
        [26.6052, 29.8403, 30.0567],
        [26.6052, 26.6067, 26.6052],
        [26.6052, 27.2720, 26.8819]], grad_fn=<SliceBackward0>)

training epoch:1469, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.8006587624549866 
model_pd.lagr.mean(): -0.7446019053459167 
model_pd.lambdas: dict_items([('pout', tensor([0.4934])), ('power', tensor([0.0247]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1469	 i:0 	 global-step:29380	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 31.7346, 33.4347],
        [26.6052, 29.5456, 29.5887],
        [26.6052, 26.6053, 26.6052],
        [26.6052, 27.3992, 26.9733]], grad_fn=<SliceBackward0>)

training epoch:1470, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -0.7981672286987305 
model_pd.lagr.mean(): -0.7421103715896606 
model_pd.lambdas: dict_items([('pout', tensor([0.4919])), ('power', tensor([0.0246]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1470	 i:0 	 global-step:29400	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6088, 26.6052],
        [26.6052, 33.9201, 37.8937],
        [26.6052, 26.6053, 26.6052],
        [26.6052, 28.3720, 27.9492]], grad_fn=<SliceBackward0>)

training epoch:1471, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7956756949424744 
model_pd.lagr.mean(): -0.7396188378334045 
model_pd.lambdas: dict_items([('pout', tensor([0.4903])), ('power', tensor([0.0245]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1471	 i:0 	 global-step:29420	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 27.2721, 26.8819],
        [26.6052, 26.9496, 26.6990],
        [26.6052, 29.5457, 29.5888],
        [26.6052, 26.6452, 26.6081]], grad_fn=<SliceBackward0>)

training epoch:1472, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -0.7931840419769287 
model_pd.lagr.mean(): -0.7371271848678589 
model_pd.lambdas: dict_items([('pout', tensor([0.4888])), ('power', tensor([0.0245]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1472	 i:0 	 global-step:29440	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.7140, 26.6197],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 29.6410, 29.7381],
        [26.6052, 27.5215, 27.0703]], grad_fn=<SliceBackward0>)

training epoch:1473, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.7906924486160278 
model_pd.lagr.mean(): -0.734635591506958 
model_pd.lambdas: dict_items([('pout', tensor([0.4872])), ('power', tensor([0.0244]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1473	 i:0 	 global-step:29460	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6052, 26.6052],
        [26.6052, 34.0659, 38.2076],
        [26.6052, 26.9570, 26.7024],
        [26.6052, 26.6053, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1474, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -0.788200855255127 
model_pd.lagr.mean(): -0.7321439981460571 
model_pd.lambdas: dict_items([('pout', tensor([0.4857])), ('power', tensor([0.0243]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1474	 i:0 	 global-step:29480	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 30.6405, 31.4138],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 32.2215, 34.3840],
        [26.6052, 26.7140, 26.6197]], grad_fn=<SliceBackward0>)

training epoch:1475, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7857093214988708 
model_pd.lagr.mean(): -0.729652464389801 
model_pd.lambdas: dict_items([('pout', tensor([0.4842])), ('power', tensor([0.0242]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1475	 i:0 	 global-step:29500	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 29.8407, 30.0572],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 26.6052, 26.6052],
        [26.6052, 26.6057, 26.6052]], grad_fn=<SliceBackward0>)

training epoch:1476, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7832176089286804 
model_pd.lagr.mean(): -0.7271607518196106 
model_pd.lambdas: dict_items([('pout', tensor([0.4826])), ('power', tensor([0.0242]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1476	 i:0 	 global-step:29520	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 31.7351, 33.4355],
        [26.6052, 26.9570, 26.7024],
        [26.6052, 26.6057, 26.6052],
        [26.6052, 29.5460, 29.5892]], grad_fn=<SliceBackward0>)

training epoch:1477, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7807260155677795 
model_pd.lagr.mean(): -0.7246691584587097 
model_pd.lambdas: dict_items([('pout', tensor([0.4811])), ('power', tensor([0.0241]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1477	 i:0 	 global-step:29540	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.6052, 26.6052],
        [26.6052, 31.7352, 33.4356],
        [26.6052, 26.6572, 26.6096],
        [26.6052, 28.3723, 27.9494]], grad_fn=<SliceBackward0>)

training epoch:1478, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -0.7782344818115234 
model_pd.lagr.mean(): -0.7221776247024536 
model_pd.lambdas: dict_items([('pout', tensor([0.4795])), ('power', tensor([0.0240]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1478	 i:0 	 global-step:29560	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.6052, 26.7910, 26.6395],
        [26.6052, 27.2722, 26.8820],
        [26.6052, 28.0966, 27.6298],
        [26.6052, 31.1812, 32.3913]], grad_fn=<SliceBackward0>)

training epoch:1479, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7757428884506226 
model_pd.lagr.mean(): -0.7196860313415527 
model_pd.lambdas: dict_items([('pout', tensor([0.4780])), ('power', tensor([0.0239]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1479	 i:0 	 global-step:29580	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.7140, 26.6196],
        [26.6051, 26.6051, 26.6052],
        [26.6051, 26.6067, 26.6052],
        [26.6051, 29.4553, 29.4491]], grad_fn=<SliceBackward0>)

training epoch:1480, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7732512354850769 
model_pd.lagr.mean(): -0.7171943783760071 
model_pd.lambdas: dict_items([('pout', tensor([0.4765])), ('power', tensor([0.0239]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1480	 i:0 	 global-step:29600	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 32.2220, 34.3847],
        [26.6051, 28.4519, 28.0470],
        [26.6051, 26.6067, 26.6052],
        [26.6051, 33.9209, 37.8951]], grad_fn=<SliceBackward0>)

training epoch:1481, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.770759642124176 
model_pd.lagr.mean(): -0.7147027850151062 
model_pd.lambdas: dict_items([('pout', tensor([0.4749])), ('power', tensor([0.0238]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1481	 i:0 	 global-step:29620	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.6051, 26.9496, 26.6990],
        [26.6051, 31.7355, 33.4361],
        [26.6051, 34.0665, 38.2087],
        [26.6051, 26.6052, 26.6051]], grad_fn=<SliceBackward0>)

training epoch:1482, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7682679891586304 
model_pd.lagr.mean(): -0.7122111320495605 
model_pd.lambdas: dict_items([('pout', tensor([0.4734])), ('power', tensor([0.0237]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1482	 i:0 	 global-step:29640	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 36.0119, 42.5697],
        [26.6051, 30.6410, 31.4145],
        [26.6051, 26.6075, 26.6052],
        [26.6051, 33.9211, 37.8954]], grad_fn=<SliceBackward0>)

training epoch:1483, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7657763957977295 
model_pd.lagr.mean(): -0.7097195386886597 
model_pd.lambdas: dict_items([('pout', tensor([0.4718])), ('power', tensor([0.0236]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1483	 i:0 	 global-step:29660	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 29.4555, 29.4494],
        [26.6051, 26.9571, 26.7024],
        [26.6051, 26.6052, 26.6052],
        [26.6051, 29.2683, 29.1665]], grad_fn=<SliceBackward0>)

training epoch:1484, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7632847428321838 
model_pd.lagr.mean(): -0.707227885723114 
model_pd.lambdas: dict_items([('pout', tensor([0.4703])), ('power', tensor([0.0235]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1484	 i:0 	 global-step:29680	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1485
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[26.6051, 29.0116, 28.7933],
        [26.6051, 31.1816, 32.3919],
        [26.6051, 29.8412, 30.0578],
        [26.6051, 29.6416, 29.7388]], grad_fn=<SliceBackward0>)

training epoch:1485, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.760793149471283 
model_pd.lagr.mean(): -0.7047362923622131 
model_pd.lambdas: dict_items([('pout', tensor([0.4688])), ('power', tensor([0.0235]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1485	 i:0 	 global-step:29700	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1486
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[26.6051, 27.3994, 26.9734],
        [26.6051, 27.7586, 27.2816],
        [26.6051, 29.8412, 30.0579],
        [26.6051, 29.2684, 29.1666]], grad_fn=<SliceBackward0>)

training epoch:1486, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7583014965057373 
model_pd.lagr.mean(): -0.7022446393966675 
model_pd.lambdas: dict_items([('pout', tensor([0.4672])), ('power', tensor([0.0234]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1486	 i:0 	 global-step:29720	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.6051, 26.6051],
        [26.6051, 34.4074, 38.9502],
        [26.6051, 28.3726, 27.9497],
        [26.6051, 27.9014, 27.4223]], grad_fn=<SliceBackward0>)

training epoch:1487, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -0.7558098435401917 
model_pd.lagr.mean(): -0.6997529864311218 
model_pd.lambdas: dict_items([('pout', tensor([0.4657])), ('power', tensor([0.0233]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1487	 i:0 	 global-step:29740	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.6066, 26.6051],
        [26.6051, 26.9603, 26.7038],
        [26.6051, 35.3494, 41.0503],
        [26.6051, 27.9014, 27.4223]], grad_fn=<SliceBackward0>)

training epoch:1488, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.7533181309700012 
model_pd.lagr.mean(): -0.6972612738609314 
model_pd.lambdas: dict_items([('pout', tensor([0.4641])), ('power', tensor([0.0232]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1488	 i:0 	 global-step:29760	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.8143, 26.6468],
        [26.6051, 26.6051, 26.6051],
        [26.6051, 26.6051, 26.6051],
        [26.6051, 31.1819, 32.3923]], grad_fn=<SliceBackward0>)

training epoch:1489, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.7508266568183899 
model_pd.lagr.mean(): -0.6947697997093201 
model_pd.lambdas: dict_items([('pout', tensor([0.4626])), ('power', tensor([0.0232]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1489	 i:0 	 global-step:29780	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 30.6414, 31.4151],
        [26.6051, 36.0124, 42.5708],
        [26.6051, 26.6051, 26.6051],
        [26.6051, 32.2164, 34.3735]], grad_fn=<SliceBackward0>)

training epoch:1490, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.7483349442481995 
model_pd.lagr.mean(): -0.6922780871391296 
model_pd.lambdas: dict_items([('pout', tensor([0.4610])), ('power', tensor([0.0231]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1490	 i:0 	 global-step:29800	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 34.4077, 38.9508],
        [26.6051, 30.9358, 31.9419],
        [26.6051, 26.6056, 26.6051],
        [26.6051, 35.3496, 41.0507]], grad_fn=<SliceBackward0>)

training epoch:1491, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7458432912826538 
model_pd.lagr.mean(): -0.689786434173584 
model_pd.lambdas: dict_items([('pout', tensor([0.4595])), ('power', tensor([0.0230]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1491	 i:0 	 global-step:29820	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 30.6415, 31.4153],
        [26.6051, 26.6075, 26.6051],
        [26.6051, 26.6341, 26.6069],
        [26.6051, 32.2227, 34.3860]], grad_fn=<SliceBackward0>)

training epoch:1492, step:0 
model_pd.l_p.mean(): 0.056056857109069824 
model_pd.l_d.mean(): -0.7433516979217529 
model_pd.lagr.mean(): -0.6872948408126831 
model_pd.lambdas: dict_items([('pout', tensor([0.4580])), ('power', tensor([0.0229]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1492	 i:0 	 global-step:29840	 l-p:0.056056857109069824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 27.2724, 26.8821],
        [26.6051, 26.6152, 26.6055],
        [26.6051, 26.9497, 26.6990],
        [26.6051, 27.7588, 27.2817]], grad_fn=<SliceBackward0>)

training epoch:1493, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.7408599853515625 
model_pd.lagr.mean(): -0.6848031282424927 
model_pd.lambdas: dict_items([('pout', tensor([0.4564])), ('power', tensor([0.0229]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1493	 i:0 	 global-step:29860	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.6051, 30.9360, 31.9422],
        [26.6051, 26.9571, 26.7024],
        [26.6051, 31.1822, 32.3928],
        [26.6051, 26.6051, 26.6051]], grad_fn=<SliceBackward0>)

training epoch:1494, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.7383683919906616 
model_pd.lagr.mean(): -0.6823115348815918 
model_pd.lambdas: dict_items([('pout', tensor([0.4549])), ('power', tensor([0.0228]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1494	 i:0 	 global-step:29880	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.6051, 26.9497, 26.6990],
        [26.6051, 27.2724, 26.8821],
        [26.6051, 32.2167, 34.3741],
        [26.6051, 26.6051, 26.6051]], grad_fn=<SliceBackward0>)

training epoch:1495, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7358765602111816 
model_pd.lagr.mean(): -0.6798197031021118 
model_pd.lambdas: dict_items([('pout', tensor([0.4533])), ('power', tensor([0.0227]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1495	 i:0 	 global-step:29900	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.9497, 26.6990],
        [26.6051, 26.6056, 26.6051],
        [26.6051, 28.5432, 28.1617],
        [26.6051, 29.5469, 29.5903]], grad_fn=<SliceBackward0>)

training epoch:1496, step:0 
model_pd.l_p.mean(): 0.05605684220790863 
model_pd.l_d.mean(): -0.7333848476409912 
model_pd.lagr.mean(): -0.6773279905319214 
model_pd.lambdas: dict_items([('pout', tensor([0.4518])), ('power', tensor([0.0226]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1496	 i:0 	 global-step:29920	 l-p:0.05605684220790863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.6066, 26.6051],
        [26.6051, 26.9603, 26.7038],
        [26.6051, 35.3500, 41.0516],
        [26.6051, 33.9221, 37.8973]], grad_fn=<SliceBackward0>)

training epoch:1497, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.730893075466156 
model_pd.lagr.mean(): -0.6748362183570862 
model_pd.lambdas: dict_items([('pout', tensor([0.4503])), ('power', tensor([0.0226]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1497	 i:0 	 global-step:29940	 l-p:0.056056853383779526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 27.5801, 27.1196],
        [26.6051, 26.6452, 26.6080],
        [26.6051, 27.9016, 27.4225],
        [26.6051, 34.0676, 38.2109]], grad_fn=<SliceBackward0>)

training epoch:1498, step:0 
model_pd.l_p.mean(): 0.05605684593319893 
model_pd.l_d.mean(): -0.7284013032913208 
model_pd.lagr.mean(): -0.672344446182251 
model_pd.lambdas: dict_items([('pout', tensor([0.4487])), ('power', tensor([0.0225]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1498	 i:0 	 global-step:29960	 l-p:0.05605684593319893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6051, 26.9572, 26.7024],
        [26.6051, 26.6340, 26.6068],
        [26.6051, 26.6051, 26.6051],
        [26.6051, 28.0972, 27.6303]], grad_fn=<SliceBackward0>)

training epoch:1499, step:0 
model_pd.l_p.mean(): 0.056056853383779526 
model_pd.l_d.mean(): -0.7259096503257751 
model_pd.lagr.mean(): -0.6698527932167053 
model_pd.lambdas: dict_items([('pout', tensor([0.4472])), ('power', tensor([0.0224]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5412])), ('power', tensor([-1.5272]))])
epoch：1499	 i:0 	 global-step:29980	 l-p:0.056056853383779526
