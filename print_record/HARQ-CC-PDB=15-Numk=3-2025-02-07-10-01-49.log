
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.22045108675956726 
model_pd.l_d.mean(): -22.75084114074707 
model_pd.lagr.mean(): -22.5303897857666 
model_pd.lambdas: dict_items([('pout', tensor([1.0006], device='cuda:0')), ('power', tensor([0.9988], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6202], device='cuda:0')), ('power', tensor([-23.3711], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.22045108675956726
epoch£º0	 i:1 	 global-step:1	 l-p:0.16397282481193542
epoch£º0	 i:2 	 global-step:2	 l-p:0.20958468317985535
epoch£º0	 i:3 	 global-step:3	 l-p:0.1854775995016098
epoch£º0	 i:4 	 global-step:4	 l-p:0.1604577749967575
epoch£º0	 i:5 	 global-step:5	 l-p:0.13323140144348145
epoch£º0	 i:6 	 global-step:6	 l-p:0.07205275446176529
epoch£º0	 i:7 	 global-step:7	 l-p:0.15227170288562775
epoch£º0	 i:8 	 global-step:8	 l-p:0.12441851943731308
epoch£º0	 i:9 	 global-step:9	 l-p:0.15233907103538513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5306, 3.5310, 3.5306],
        [3.5306, 3.7427, 3.6997],
        [3.5306, 3.5912, 3.5525],
        [3.5306, 4.5243, 5.2126]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.03253820911049843 
model_pd.l_d.mean(): -21.972126007080078 
model_pd.lagr.mean(): -21.93958854675293 
model_pd.lambdas: dict_items([('pout', tensor([1.0040], device='cuda:0')), ('power', tensor([0.9874], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1465], device='cuda:0')), ('power', tensor([-22.3759], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.03253820911049843
epoch£º1	 i:1 	 global-step:21	 l-p:0.09445901960134506
epoch£º1	 i:2 	 global-step:22	 l-p:0.12930788099765778
epoch£º1	 i:3 	 global-step:23	 l-p:0.12599505484104156
epoch£º1	 i:4 	 global-step:24	 l-p:0.0813751369714737
epoch£º1	 i:5 	 global-step:25	 l-p:0.10950823873281479
epoch£º1	 i:6 	 global-step:26	 l-p:0.1423972100019455
epoch£º1	 i:7 	 global-step:27	 l-p:0.13017643988132477
epoch£º1	 i:8 	 global-step:28	 l-p:0.11144404113292694
epoch£º1	 i:9 	 global-step:29	 l-p:0.09816776216030121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0521, 4.9088, 5.3316],
        [4.0521, 4.1309, 4.0826],
        [4.0521, 4.7012, 4.9121],
        [4.0521, 4.1182, 4.0749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.1304812729358673 
model_pd.l_d.mean(): -22.362926483154297 
model_pd.lagr.mean(): -22.232444763183594 
model_pd.lambdas: dict_items([('pout', tensor([1.0037], device='cuda:0')), ('power', tensor([0.9761], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0801], device='cuda:0')), ('power', tensor([-22.8020], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.1304812729358673
epoch£º2	 i:1 	 global-step:41	 l-p:0.07918518781661987
epoch£º2	 i:2 	 global-step:42	 l-p:0.12901386618614197
epoch£º2	 i:3 	 global-step:43	 l-p:0.11164464801549911
epoch£º2	 i:4 	 global-step:44	 l-p:0.12377621978521347
epoch£º2	 i:5 	 global-step:45	 l-p:0.10214482992887497
epoch£º2	 i:6 	 global-step:46	 l-p:0.10770920664072037
epoch£º2	 i:7 	 global-step:47	 l-p:0.11148755997419357
epoch£º2	 i:8 	 global-step:48	 l-p:0.09680107235908508
epoch£º2	 i:9 	 global-step:49	 l-p:0.11993006616830826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6766, 4.3726, 4.6791],
        [3.6766, 3.7098, 3.6845],
        [3.6766, 3.7455, 3.7029],
        [3.6766, 4.6756, 5.3405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.16278617084026337 
model_pd.l_d.mean(): -21.15666961669922 
model_pd.lagr.mean(): -20.99388313293457 
model_pd.lambdas: dict_items([('pout', tensor([1.0038], device='cuda:0')), ('power', tensor([0.9648], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1459], device='cuda:0')), ('power', tensor([-22.0551], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.16278617084026337
epoch£º3	 i:1 	 global-step:61	 l-p:0.1634552925825119
epoch£º3	 i:2 	 global-step:62	 l-p:-0.05362088233232498
epoch£º3	 i:3 	 global-step:63	 l-p:0.12227378785610199
epoch£º3	 i:4 	 global-step:64	 l-p:0.11334805935621262
epoch£º3	 i:5 	 global-step:65	 l-p:0.11686652153730392
epoch£º3	 i:6 	 global-step:66	 l-p:0.13335935771465302
epoch£º3	 i:7 	 global-step:67	 l-p:0.1449870765209198
epoch£º3	 i:8 	 global-step:68	 l-p:0.1051197499036789
epoch£º3	 i:9 	 global-step:69	 l-p:0.11009463667869568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5896, 4.0820, 4.2054],
        [3.5896, 4.1897, 4.4116],
        [3.5896, 4.6293, 5.3689],
        [3.5896, 3.6504, 3.6115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.1487928181886673 
model_pd.l_d.mean(): -21.1348876953125 
model_pd.lagr.mean(): -20.986095428466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0045], device='cuda:0')), ('power', tensor([0.9534], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1367], device='cuda:0')), ('power', tensor([-22.2856], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.1487928181886673
epoch£º4	 i:1 	 global-step:81	 l-p:0.12207380682229996
epoch£º4	 i:2 	 global-step:82	 l-p:0.20485933125019073
epoch£º4	 i:3 	 global-step:83	 l-p:0.11841531842947006
epoch£º4	 i:4 	 global-step:84	 l-p:0.11888457089662552
epoch£º4	 i:5 	 global-step:85	 l-p:0.140040785074234
epoch£º4	 i:6 	 global-step:86	 l-p:0.1169748529791832
epoch£º4	 i:7 	 global-step:87	 l-p:0.21662375330924988
epoch£º4	 i:8 	 global-step:88	 l-p:0.13303878903388977
epoch£º4	 i:9 	 global-step:89	 l-p:0.0904504731297493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8242, 3.8243, 3.8242],
        [3.8242, 3.8290, 3.8245],
        [3.8242, 3.8983, 3.8532],
        [3.8242, 3.9305, 3.8766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.11847678571939468 
model_pd.l_d.mean(): -21.529455184936523 
model_pd.lagr.mean(): -21.410978317260742 
model_pd.lambdas: dict_items([('pout', tensor([1.0050], device='cuda:0')), ('power', tensor([0.9420], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0205], device='cuda:0')), ('power', tensor([-22.8489], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.11847678571939468
epoch£º5	 i:1 	 global-step:101	 l-p:0.28767502307891846
epoch£º5	 i:2 	 global-step:102	 l-p:0.11154605448246002
epoch£º5	 i:3 	 global-step:103	 l-p:0.1266310214996338
epoch£º5	 i:4 	 global-step:104	 l-p:0.07307795435190201
epoch£º5	 i:5 	 global-step:105	 l-p:0.12575283646583557
epoch£º5	 i:6 	 global-step:106	 l-p:0.21331320703029633
epoch£º5	 i:7 	 global-step:107	 l-p:0.11264251172542572
epoch£º5	 i:8 	 global-step:108	 l-p:0.11957772821187973
epoch£º5	 i:9 	 global-step:109	 l-p:0.10877031832933426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9048, 3.9263, 3.9086],
        [3.9048, 3.9048, 3.9048],
        [3.9048, 4.9845, 5.7080],
        [3.9048, 3.9085, 3.9050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.13312260806560516 
model_pd.l_d.mean(): -21.590721130371094 
model_pd.lagr.mean(): -21.457597732543945 
model_pd.lambdas: dict_items([('pout', tensor([1.0048], device='cuda:0')), ('power', tensor([0.9307], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0632], device='cuda:0')), ('power', tensor([-23.1017], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.13312260806560516
epoch£º6	 i:1 	 global-step:121	 l-p:0.11334838718175888
epoch£º6	 i:2 	 global-step:122	 l-p:0.10139812529087067
epoch£º6	 i:3 	 global-step:123	 l-p:0.12308090180158615
epoch£º6	 i:4 	 global-step:124	 l-p:0.1032840684056282
epoch£º6	 i:5 	 global-step:125	 l-p:0.19276905059814453
epoch£º6	 i:6 	 global-step:126	 l-p:0.12480784952640533
epoch£º6	 i:7 	 global-step:127	 l-p:0.14407995343208313
epoch£º6	 i:8 	 global-step:128	 l-p:0.08105284720659256
epoch£º6	 i:9 	 global-step:129	 l-p:0.09758483618497849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7541, 4.2297, 4.3261],
        [3.7541, 3.8001, 3.7676],
        [3.7541, 3.7542, 3.7541],
        [3.7541, 3.7697, 3.7565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.14706964790821075 
model_pd.l_d.mean(): -21.27377700805664 
model_pd.lagr.mean(): -21.126707077026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0051], device='cuda:0')), ('power', tensor([0.9193], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0038], device='cuda:0')), ('power', tensor([-23.1153], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.14706964790821075
epoch£º7	 i:1 	 global-step:141	 l-p:0.14558470249176025
epoch£º7	 i:2 	 global-step:142	 l-p:-0.03407792001962662
epoch£º7	 i:3 	 global-step:143	 l-p:0.12229583412408829
epoch£º7	 i:4 	 global-step:144	 l-p:0.12265127152204514
epoch£º7	 i:5 	 global-step:145	 l-p:0.09169786423444748
epoch£º7	 i:6 	 global-step:146	 l-p:0.10129571706056595
epoch£º7	 i:7 	 global-step:147	 l-p:0.095199353992939
epoch£º7	 i:8 	 global-step:148	 l-p:0.10386688262224197
epoch£º7	 i:9 	 global-step:149	 l-p:0.10675731301307678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9301, 3.9301, 3.9301],
        [3.9301, 3.9302, 3.9301],
        [3.9301, 4.5733, 4.7983],
        [3.9301, 4.0456, 3.9894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.09998912364244461 
model_pd.l_d.mean(): -19.972166061401367 
model_pd.lagr.mean(): -19.872177124023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0051], device='cuda:0')), ('power', tensor([0.9081], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0270], device='cuda:0')), ('power', tensor([-21.9972], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.09998912364244461
epoch£º8	 i:1 	 global-step:161	 l-p:0.11443908512592316
epoch£º8	 i:2 	 global-step:162	 l-p:0.21370656788349152
epoch£º8	 i:3 	 global-step:163	 l-p:0.1221722960472107
epoch£º8	 i:4 	 global-step:164	 l-p:0.09891385585069656
epoch£º8	 i:5 	 global-step:165	 l-p:0.11762481182813644
epoch£º8	 i:6 	 global-step:166	 l-p:0.12087522447109222
epoch£º8	 i:7 	 global-step:167	 l-p:0.10898908227682114
epoch£º8	 i:8 	 global-step:168	 l-p:0.2358168214559555
epoch£º8	 i:9 	 global-step:169	 l-p:0.12623439729213715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8633, 3.8647, 3.8634],
        [3.8633, 3.8648, 3.8634],
        [3.8633, 3.9760, 3.9212],
        [3.8633, 3.8792, 3.8657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.08299285918474197 
model_pd.l_d.mean(): -19.079017639160156 
model_pd.lagr.mean(): -18.99602508544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0050], device='cuda:0')), ('power', tensor([0.8968], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0797], device='cuda:0')), ('power', tensor([-21.3386], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.08299285918474197
epoch£º9	 i:1 	 global-step:181	 l-p:0.10745460540056229
epoch£º9	 i:2 	 global-step:182	 l-p:0.11004354059696198
epoch£º9	 i:3 	 global-step:183	 l-p:-0.04443414509296417
epoch£º9	 i:4 	 global-step:184	 l-p:0.1173194870352745
epoch£º9	 i:5 	 global-step:185	 l-p:0.11227182298898697
epoch£º9	 i:6 	 global-step:186	 l-p:0.1148642972111702
epoch£º9	 i:7 	 global-step:187	 l-p:0.12429779767990112
epoch£º9	 i:8 	 global-step:188	 l-p:-1.7135409116744995
epoch£º9	 i:9 	 global-step:189	 l-p:0.12277302145957947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8187, 4.7452, 5.2969],
        [3.8187, 4.4519, 4.6824],
        [3.8187, 3.8226, 3.8190],
        [3.8187, 3.8187, 3.8187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.1602509319782257 
model_pd.l_d.mean(): -20.316648483276367 
model_pd.lagr.mean(): -20.156396865844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0049], device='cuda:0')), ('power', tensor([0.8854], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0073], device='cuda:0')), ('power', tensor([-22.9251], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.1602509319782257
epoch£º10	 i:1 	 global-step:201	 l-p:0.08999935537576675
epoch£º10	 i:2 	 global-step:202	 l-p:0.03264366090297699
epoch£º10	 i:3 	 global-step:203	 l-p:0.12148036807775497
epoch£º10	 i:4 	 global-step:204	 l-p:0.19489417970180511
epoch£º10	 i:5 	 global-step:205	 l-p:0.12131335586309433
epoch£º10	 i:6 	 global-step:206	 l-p:0.09271268546581268
epoch£º10	 i:7 	 global-step:207	 l-p:0.1147804781794548
epoch£º10	 i:8 	 global-step:208	 l-p:0.12035821378231049
epoch£º10	 i:9 	 global-step:209	 l-p:0.12045448273420334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9797, 4.3954, 4.4330],
        [3.9797, 5.0230, 5.6888],
        [3.9797, 4.5483, 4.7026],
        [3.9797, 3.9983, 3.9827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11425920575857162 
model_pd.l_d.mean(): -19.95643424987793 
model_pd.lagr.mean(): -19.842174530029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0047], device='cuda:0')), ('power', tensor([0.8741], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0403], device='cuda:0')), ('power', tensor([-22.7553], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11425920575857162
epoch£º11	 i:1 	 global-step:221	 l-p:0.11392560601234436
epoch£º11	 i:2 	 global-step:222	 l-p:0.09680196642875671
epoch£º11	 i:3 	 global-step:223	 l-p:0.19471444189548492
epoch£º11	 i:4 	 global-step:224	 l-p:0.12557636201381683
epoch£º11	 i:5 	 global-step:225	 l-p:0.11113777756690979
epoch£º11	 i:6 	 global-step:226	 l-p:-0.14344385266304016
epoch£º11	 i:7 	 global-step:227	 l-p:0.11505647748708725
epoch£º11	 i:8 	 global-step:228	 l-p:0.10125432908535004
epoch£º11	 i:9 	 global-step:229	 l-p:0.11490149050951004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8674, 4.1704, 4.1518],
        [3.8674, 3.8952, 3.8733],
        [3.8674, 3.8910, 3.8719],
        [3.8674, 3.9959, 3.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12517650425434113 
model_pd.l_d.mean(): -19.275774002075195 
model_pd.lagr.mean(): -19.150596618652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0046], device='cuda:0')), ('power', tensor([0.8628], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0563], device='cuda:0')), ('power', tensor([-22.3778], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12517650425434113
epoch£º12	 i:1 	 global-step:241	 l-p:0.11517169326543808
epoch£º12	 i:2 	 global-step:242	 l-p:0.1169905811548233
epoch£º12	 i:3 	 global-step:243	 l-p:0.12435577064752579
epoch£º12	 i:4 	 global-step:244	 l-p:0.13702359795570374
epoch£º12	 i:5 	 global-step:245	 l-p:0.05304034426808357
epoch£º12	 i:6 	 global-step:246	 l-p:0.11400915682315826
epoch£º12	 i:7 	 global-step:247	 l-p:0.11016103625297546
epoch£º12	 i:8 	 global-step:248	 l-p:0.1291113942861557
epoch£º12	 i:9 	 global-step:249	 l-p:-0.10131687670946121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8286, 4.6871, 5.1593],
        [3.8286, 3.8286, 3.8286],
        [3.8286, 3.8311, 3.8287],
        [3.8286, 4.1266, 4.1081]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.08171241730451584 
model_pd.l_d.mean(): -18.68384552001953 
model_pd.lagr.mean(): -18.60213279724121 
model_pd.lambdas: dict_items([('pout', tensor([1.0046], device='cuda:0')), ('power', tensor([0.8515], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0593], device='cuda:0')), ('power', tensor([-21.9844], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.08171241730451584
epoch£º13	 i:1 	 global-step:261	 l-p:0.1350431740283966
epoch£º13	 i:2 	 global-step:262	 l-p:0.11291078478097916
epoch£º13	 i:3 	 global-step:263	 l-p:0.11138403415679932
epoch£º13	 i:4 	 global-step:264	 l-p:0.11498633027076721
epoch£º13	 i:5 	 global-step:265	 l-p:0.12665733695030212
epoch£º13	 i:6 	 global-step:266	 l-p:-0.16457246243953705
epoch£º13	 i:7 	 global-step:267	 l-p:0.13434770703315735
epoch£º13	 i:8 	 global-step:268	 l-p:0.11628610640764236
epoch£º13	 i:9 	 global-step:269	 l-p:0.1005854383111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8691, 3.8691, 3.8691],
        [3.8691, 4.5562, 4.8367],
        [3.8691, 3.8954, 3.8745],
        [3.8691, 4.5059, 4.7370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.11099519580602646 
model_pd.l_d.mean(): -18.38884162902832 
model_pd.lagr.mean(): -18.277847290039062 
model_pd.lambdas: dict_items([('pout', tensor([1.0047], device='cuda:0')), ('power', tensor([0.8402], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0338], device='cuda:0')), ('power', tensor([-21.8994], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.11099519580602646
epoch£º14	 i:1 	 global-step:281	 l-p:0.11056767404079437
epoch£º14	 i:2 	 global-step:282	 l-p:0.11359894275665283
epoch£º14	 i:3 	 global-step:283	 l-p:0.09579316526651382
epoch£º14	 i:4 	 global-step:284	 l-p:-0.43465593457221985
epoch£º14	 i:5 	 global-step:285	 l-p:0.15359187126159668
epoch£º14	 i:6 	 global-step:286	 l-p:0.10402306914329529
epoch£º14	 i:7 	 global-step:287	 l-p:0.13300251960754395
epoch£º14	 i:8 	 global-step:288	 l-p:0.10270914435386658
epoch£º14	 i:9 	 global-step:289	 l-p:0.11265131086111069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0695, 4.0954, 4.0746],
        [4.0695, 4.0726, 4.0697],
        [4.0695, 4.2121, 4.1523],
        [4.0695, 4.0783, 4.0704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.11723042279481888 
model_pd.l_d.mean(): -19.156822204589844 
model_pd.lagr.mean(): -19.03959083557129 
model_pd.lambdas: dict_items([('pout', tensor([1.0043], device='cuda:0')), ('power', tensor([0.8288], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1088], device='cuda:0')), ('power', tensor([-22.9506], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.11723042279481888
epoch£º15	 i:1 	 global-step:301	 l-p:0.11109691113233566
epoch£º15	 i:2 	 global-step:302	 l-p:0.12101928144693375
epoch£º15	 i:3 	 global-step:303	 l-p:0.10933681577444077
epoch£º15	 i:4 	 global-step:304	 l-p:0.1140073835849762
epoch£º15	 i:5 	 global-step:305	 l-p:0.11359741538763046
epoch£º15	 i:6 	 global-step:306	 l-p:0.061000797897577286
epoch£º15	 i:7 	 global-step:307	 l-p:0.11073434352874756
epoch£º15	 i:8 	 global-step:308	 l-p:0.09826818108558655
epoch£º15	 i:9 	 global-step:309	 l-p:0.1350817084312439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0396, 4.0412, 4.0397],
        [4.0396, 4.0607, 4.0433],
        [4.0396, 4.0399, 4.0396],
        [4.0396, 4.3269, 4.2953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.10450681298971176 
model_pd.l_d.mean(): -18.122394561767578 
model_pd.lagr.mean(): -18.017887115478516 
model_pd.lambdas: dict_items([('pout', tensor([1.0037], device='cuda:0')), ('power', tensor([0.8176], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0491], device='cuda:0')), ('power', tensor([-22.0765], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.10450681298971176
epoch£º16	 i:1 	 global-step:321	 l-p:0.13397005200386047
epoch£º16	 i:2 	 global-step:322	 l-p:0.10638447850942612
epoch£º16	 i:3 	 global-step:323	 l-p:0.08128150552511215
epoch£º16	 i:4 	 global-step:324	 l-p:0.13129451870918274
epoch£º16	 i:5 	 global-step:325	 l-p:0.14849631488323212
epoch£º16	 i:6 	 global-step:326	 l-p:0.10657228529453278
epoch£º16	 i:7 	 global-step:327	 l-p:0.08301268517971039
epoch£º16	 i:8 	 global-step:328	 l-p:0.11284371465444565
epoch£º16	 i:9 	 global-step:329	 l-p:0.07646262645721436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0377, 4.7737, 5.0837],
        [4.0377, 4.2877, 4.2436],
        [4.0377, 4.0378, 4.0377],
        [4.0377, 4.7641, 5.0643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.09817176312208176 
model_pd.l_d.mean(): -18.45258140563965 
model_pd.lagr.mean(): -18.35441017150879 
model_pd.lambdas: dict_items([('pout', tensor([1.0031], device='cuda:0')), ('power', tensor([0.8062], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0702], device='cuda:0')), ('power', tensor([-22.7678], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.09817176312208176
epoch£º17	 i:1 	 global-step:341	 l-p:0.12997132539749146
epoch£º17	 i:2 	 global-step:342	 l-p:0.10398987680673599
epoch£º17	 i:3 	 global-step:343	 l-p:0.06308753043413162
epoch£º17	 i:4 	 global-step:344	 l-p:0.13932955265045166
epoch£º17	 i:5 	 global-step:345	 l-p:0.11386100202798843
epoch£º17	 i:6 	 global-step:346	 l-p:0.12245816737413406
epoch£º17	 i:7 	 global-step:347	 l-p:0.0966823473572731
epoch£º17	 i:8 	 global-step:348	 l-p:0.10714931041002274
epoch£º17	 i:9 	 global-step:349	 l-p:0.10695268213748932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0342, 4.0740, 4.0446],
        [4.0342, 4.0344, 4.0342],
        [4.0342, 4.1263, 4.0753],
        [4.0342, 4.0444, 4.0354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14352160692214966 
model_pd.l_d.mean(): -18.112546920776367 
model_pd.lagr.mean(): -17.969024658203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0025], device='cuda:0')), ('power', tensor([0.7950], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0502], device='cuda:0')), ('power', tensor([-22.6884], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14352160692214966
epoch£º18	 i:1 	 global-step:361	 l-p:0.10518351942300797
epoch£º18	 i:2 	 global-step:362	 l-p:0.12721988558769226
epoch£º18	 i:3 	 global-step:363	 l-p:0.11700378358364105
epoch£º18	 i:4 	 global-step:364	 l-p:0.08905141800642014
epoch£º18	 i:5 	 global-step:365	 l-p:0.11514469236135483
epoch£º18	 i:6 	 global-step:366	 l-p:0.11753355711698532
epoch£º18	 i:7 	 global-step:367	 l-p:0.09705973416566849
epoch£º18	 i:8 	 global-step:368	 l-p:0.12017323076725006
epoch£º18	 i:9 	 global-step:369	 l-p:0.055514924228191376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8308, 3.9644, 3.9093],
        [3.8308, 3.8537, 3.8352],
        [3.8308, 3.8308, 3.8308],
        [3.8308, 3.9894, 3.9345]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.11151565611362457 
model_pd.l_d.mean(): -18.16288185119629 
model_pd.lagr.mean(): -18.051366806030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0022], device='cuda:0')), ('power', tensor([0.7836], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0370], device='cuda:0')), ('power', tensor([-23.0964], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.11151565611362457
epoch£º19	 i:1 	 global-step:381	 l-p:0.0032503223046660423
epoch£º19	 i:2 	 global-step:382	 l-p:0.12739665806293488
epoch£º19	 i:3 	 global-step:383	 l-p:0.12996651232242584
epoch£º19	 i:4 	 global-step:384	 l-p:0.10885759443044662
epoch£º19	 i:5 	 global-step:385	 l-p:0.11813279986381531
epoch£º19	 i:6 	 global-step:386	 l-p:0.10499812662601471
epoch£º19	 i:7 	 global-step:387	 l-p:0.1263720840215683
epoch£º19	 i:8 	 global-step:388	 l-p:0.1467234194278717
epoch£º19	 i:9 	 global-step:389	 l-p:0.11087929457426071
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0290, 4.0311, 4.0291],
        [4.0290, 4.3632, 4.3551],
        [4.0290, 4.0625, 4.0369],
        [4.0290, 4.0774, 4.0434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.12580111622810364 
model_pd.l_d.mean(): -17.853923797607422 
model_pd.lagr.mean(): -17.72812271118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0020], device='cuda:0')), ('power', tensor([0.7723], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0922], device='cuda:0')), ('power', tensor([-22.9635], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.12580111622810364
epoch£º20	 i:1 	 global-step:401	 l-p:0.10827137529850006
epoch£º20	 i:2 	 global-step:402	 l-p:0.07984980940818787
epoch£º20	 i:3 	 global-step:403	 l-p:0.09242206066846848
epoch£º20	 i:4 	 global-step:404	 l-p:0.10727568715810776
epoch£º20	 i:5 	 global-step:405	 l-p:0.05549963191151619
epoch£º20	 i:6 	 global-step:406	 l-p:0.10867999494075775
epoch£º20	 i:7 	 global-step:407	 l-p:0.12607574462890625
epoch£º20	 i:8 	 global-step:408	 l-p:0.1025838851928711
epoch£º20	 i:9 	 global-step:409	 l-p:0.11255792528390884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1591, 5.3169, 6.1001],
        [4.1591, 4.2099, 4.1743],
        [4.1591, 4.1591, 4.1591],
        [4.1591, 4.2417, 4.1929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): -0.055661410093307495 
model_pd.l_d.mean(): -17.48477554321289 
model_pd.lagr.mean(): -17.540437698364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0012], device='cuda:0')), ('power', tensor([0.7611], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1168], device='cuda:0')), ('power', tensor([-22.7863], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:-0.055661410093307495
epoch£º21	 i:1 	 global-step:421	 l-p:0.12103451788425446
epoch£º21	 i:2 	 global-step:422	 l-p:0.060947541147470474
epoch£º21	 i:3 	 global-step:423	 l-p:0.11687881499528885
epoch£º21	 i:4 	 global-step:424	 l-p:0.10837089270353317
epoch£º21	 i:5 	 global-step:425	 l-p:0.1159134954214096
epoch£º21	 i:6 	 global-step:426	 l-p:0.09572499245405197
epoch£º21	 i:7 	 global-step:427	 l-p:0.04999266937375069
epoch£º21	 i:8 	 global-step:428	 l-p:0.11630088835954666
epoch£º21	 i:9 	 global-step:429	 l-p:0.09982509166002274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0823, 4.1645, 4.1164],
        [4.0823, 4.3214, 4.2741],
        [4.0823, 4.0841, 4.0824],
        [4.0823, 4.1080, 4.0874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.1167428195476532 
model_pd.l_d.mean(): -17.322813034057617 
model_pd.lagr.mean(): -17.206069946289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0003], device='cuda:0')), ('power', tensor([0.7498], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1104], device='cuda:0')), ('power', tensor([-22.9208], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.1167428195476532
epoch£º22	 i:1 	 global-step:441	 l-p:0.06593132019042969
epoch£º22	 i:2 	 global-step:442	 l-p:0.10944020748138428
epoch£º22	 i:3 	 global-step:443	 l-p:0.10610488057136536
epoch£º22	 i:4 	 global-step:444	 l-p:0.10690496116876602
epoch£º22	 i:5 	 global-step:445	 l-p:0.13883401453495026
epoch£º22	 i:6 	 global-step:446	 l-p:0.10964489728212357
epoch£º22	 i:7 	 global-step:447	 l-p:0.1382926106452942
epoch£º22	 i:8 	 global-step:448	 l-p:0.09472312778234482
epoch£º22	 i:9 	 global-step:449	 l-p:0.06909310817718506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1187, 5.2584, 6.0283],
        [4.1187, 4.5622, 4.6149],
        [4.1187, 4.1191, 4.1187],
        [4.1187, 4.8616, 5.1721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): -0.023976584896445274 
model_pd.l_d.mean(): -16.819721221923828 
model_pd.lagr.mean(): -16.843698501586914 
model_pd.lambdas: dict_items([('pout', tensor([0.9997], device='cuda:0')), ('power', tensor([0.7385], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0804], device='cuda:0')), ('power', tensor([-22.6306], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:-0.023976584896445274
epoch£º23	 i:1 	 global-step:461	 l-p:0.11388954520225525
epoch£º23	 i:2 	 global-step:462	 l-p:0.11151334643363953
epoch£º23	 i:3 	 global-step:463	 l-p:0.10853812843561172
epoch£º23	 i:4 	 global-step:464	 l-p:0.0889625996351242
epoch£º23	 i:5 	 global-step:465	 l-p:0.11229021102190018
epoch£º23	 i:6 	 global-step:466	 l-p:0.12448450177907944
epoch£º23	 i:7 	 global-step:467	 l-p:-0.9281135201454163
epoch£º23	 i:8 	 global-step:468	 l-p:0.09399962425231934
epoch£º23	 i:9 	 global-step:469	 l-p:0.101114422082901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2402, 4.6277, 4.6376],
        [4.2402, 4.4859, 4.4356],
        [4.2402, 4.3099, 4.2656],
        [4.2402, 4.2402, 4.2402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.1052040159702301 
model_pd.l_d.mean(): -17.016530990600586 
model_pd.lagr.mean(): -16.911327362060547 
model_pd.lambdas: dict_items([('pout', tensor([0.9984], device='cuda:0')), ('power', tensor([0.7273], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2071], device='cuda:0')), ('power', tensor([-23.0762], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.1052040159702301
epoch£º24	 i:1 	 global-step:481	 l-p:0.10217517614364624
epoch£º24	 i:2 	 global-step:482	 l-p:0.04279516637325287
epoch£º24	 i:3 	 global-step:483	 l-p:0.10449310392141342
epoch£º24	 i:4 	 global-step:484	 l-p:0.8963157534599304
epoch£º24	 i:5 	 global-step:485	 l-p:0.7750027179718018
epoch£º24	 i:6 	 global-step:486	 l-p:0.10998634248971939
epoch£º24	 i:7 	 global-step:487	 l-p:0.10568272322416306
epoch£º24	 i:8 	 global-step:488	 l-p:0.11043670773506165
epoch£º24	 i:9 	 global-step:489	 l-p:0.11223041266202927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1751, 4.1751, 4.1751],
        [4.1751, 4.1751, 4.1751],
        [4.1751, 4.1751, 4.1751],
        [4.1751, 5.1169, 5.6353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 1.4083902835845947 
model_pd.l_d.mean(): -15.532196044921875 
model_pd.lagr.mean(): -14.12380599975586 
model_pd.lambdas: dict_items([('pout', tensor([0.9975], device='cuda:0')), ('power', tensor([0.7161], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0043], device='cuda:0')), ('power', tensor([-21.6624], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:1.4083902835845947
epoch£º25	 i:1 	 global-step:501	 l-p:0.1042880117893219
epoch£º25	 i:2 	 global-step:502	 l-p:0.11097010225057602
epoch£º25	 i:3 	 global-step:503	 l-p:0.1270952820777893
epoch£º25	 i:4 	 global-step:504	 l-p:0.12371298670768738
epoch£º25	 i:5 	 global-step:505	 l-p:0.1161244809627533
epoch£º25	 i:6 	 global-step:506	 l-p:0.10795131325721741
epoch£º25	 i:7 	 global-step:507	 l-p:0.17529556155204773
epoch£º25	 i:8 	 global-step:508	 l-p:0.10541602224111557
epoch£º25	 i:9 	 global-step:509	 l-p:0.09330776333808899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0009, 4.1552, 4.0979],
        [4.0009, 4.4837, 4.5751],
        [4.0009, 4.0490, 4.0154],
        [4.0009, 4.4177, 4.4619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.1179831475019455 
model_pd.l_d.mean(): -16.3399600982666 
model_pd.lagr.mean(): -16.22197723388672 
model_pd.lambdas: dict_items([('pout', tensor([0.9968], device='cuda:0')), ('power', tensor([0.7048], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0859], device='cuda:0')), ('power', tensor([-23.0254], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.1179831475019455
epoch£º26	 i:1 	 global-step:521	 l-p:0.08808495104312897
epoch£º26	 i:2 	 global-step:522	 l-p:0.10549740493297577
epoch£º26	 i:3 	 global-step:523	 l-p:0.12411750853061676
epoch£º26	 i:4 	 global-step:524	 l-p:0.12192397564649582
epoch£º26	 i:5 	 global-step:525	 l-p:0.10282362997531891
epoch£º26	 i:6 	 global-step:526	 l-p:0.130351260304451
epoch£º26	 i:7 	 global-step:527	 l-p:0.06278003007173538
epoch£º26	 i:8 	 global-step:528	 l-p:0.10105372965335846
epoch£º26	 i:9 	 global-step:529	 l-p:0.11645913869142532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2377, 4.3382, 4.2840],
        [4.2377, 4.5231, 4.4858],
        [4.2377, 4.2377, 4.2377],
        [4.2377, 4.2399, 4.2378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.19108407199382782 
model_pd.l_d.mean(): -15.04583740234375 
model_pd.lagr.mean(): -14.854753494262695 
model_pd.lambdas: dict_items([('pout', tensor([0.9961], device='cuda:0')), ('power', tensor([0.6936], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0591], device='cuda:0')), ('power', tensor([-21.5748], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.19108407199382782
epoch£º27	 i:1 	 global-step:541	 l-p:0.10819219052791595
epoch£º27	 i:2 	 global-step:542	 l-p:0.10778322070837021
epoch£º27	 i:3 	 global-step:543	 l-p:0.10554631054401398
epoch£º27	 i:4 	 global-step:544	 l-p:0.09761156141757965
epoch£º27	 i:5 	 global-step:545	 l-p:0.15838195383548737
epoch£º27	 i:6 	 global-step:546	 l-p:0.10776742547750473
epoch£º27	 i:7 	 global-step:547	 l-p:0.1066642478108406
epoch£º27	 i:8 	 global-step:548	 l-p:0.11023461073637009
epoch£º27	 i:9 	 global-step:549	 l-p:0.09584906697273254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2836, 4.4591, 4.3973],
        [4.2836, 4.8061, 4.9041],
        [4.2836, 4.3263, 4.2950],
        [4.2836, 5.4241, 6.1630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.10935933887958527 
model_pd.l_d.mean(): -15.476531028747559 
model_pd.lagr.mean(): -15.367171287536621 
model_pd.lambdas: dict_items([('pout', tensor([0.9946], device='cuda:0')), ('power', tensor([0.6823], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1213], device='cuda:0')), ('power', tensor([-22.4681], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.10935933887958527
epoch£º28	 i:1 	 global-step:561	 l-p:0.1069135069847107
epoch£º28	 i:2 	 global-step:562	 l-p:0.03373687341809273
epoch£º28	 i:3 	 global-step:563	 l-p:-0.1667986661195755
epoch£º28	 i:4 	 global-step:564	 l-p:0.10884663462638855
epoch£º28	 i:5 	 global-step:565	 l-p:0.49737802147865295
epoch£º28	 i:6 	 global-step:566	 l-p:0.10835472494363785
epoch£º28	 i:7 	 global-step:567	 l-p:0.10303740948438644
epoch£º28	 i:8 	 global-step:568	 l-p:0.11608127504587173
epoch£º28	 i:9 	 global-step:569	 l-p:0.10677371919155121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2327, 4.2328, 4.2327],
        [4.2327, 4.4043, 4.3434],
        [4.2327, 4.7724, 4.8885],
        [4.2327, 4.2364, 4.2330]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.10732191056013107 
model_pd.l_d.mean(): -15.152130126953125 
model_pd.lagr.mean(): -15.044808387756348 
model_pd.lambdas: dict_items([('pout', tensor([0.9935], device='cuda:0')), ('power', tensor([0.6711], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1093], device='cuda:0')), ('power', tensor([-22.3789], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.10732191056013107
epoch£º29	 i:1 	 global-step:581	 l-p:0.1382889747619629
epoch£º29	 i:2 	 global-step:582	 l-p:0.10758397728204727
epoch£º29	 i:3 	 global-step:583	 l-p:0.11522294580936432
epoch£º29	 i:4 	 global-step:584	 l-p:0.11464814841747284
epoch£º29	 i:5 	 global-step:585	 l-p:0.10487723350524902
epoch£º29	 i:6 	 global-step:586	 l-p:0.12468686699867249
epoch£º29	 i:7 	 global-step:587	 l-p:0.10327211022377014
epoch£º29	 i:8 	 global-step:588	 l-p:0.031236261129379272
epoch£º29	 i:9 	 global-step:589	 l-p:0.10106170177459717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1516, 4.1530, 4.1517],
        [4.1516, 5.2376, 5.9355],
        [4.1516, 4.3330, 4.2746],
        [4.1516, 5.1386, 5.7160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.11473790556192398 
model_pd.l_d.mean(): -14.557462692260742 
model_pd.lagr.mean(): -14.44272518157959 
model_pd.lambdas: dict_items([('pout', tensor([0.9925], device='cuda:0')), ('power', tensor([0.6599], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0179], device='cuda:0')), ('power', tensor([-21.9973], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.11473790556192398
epoch£º30	 i:1 	 global-step:601	 l-p:0.11911609023809433
epoch£º30	 i:2 	 global-step:602	 l-p:0.05797433853149414
epoch£º30	 i:3 	 global-step:603	 l-p:0.11292002350091934
epoch£º30	 i:4 	 global-step:604	 l-p:0.10517030209302902
epoch£º30	 i:5 	 global-step:605	 l-p:0.11239992082118988
epoch£º30	 i:6 	 global-step:606	 l-p:0.1013999879360199
epoch£º30	 i:7 	 global-step:607	 l-p:0.10936925560235977
epoch£º30	 i:8 	 global-step:608	 l-p:0.28459495306015015
epoch£º30	 i:9 	 global-step:609	 l-p:0.18748246133327484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2502, 4.2502, 4.2502],
        [4.2502, 4.2575, 4.2509],
        [4.2502, 5.4277, 6.2218],
        [4.2502, 4.2994, 4.2647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): -0.32176727056503296 
model_pd.l_d.mean(): -13.973052024841309 
model_pd.lagr.mean(): -14.294818878173828 
model_pd.lambdas: dict_items([('pout', tensor([0.9915], device='cuda:0')), ('power', tensor([0.6487], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0076], device='cuda:0')), ('power', tensor([-21.4940], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:-0.32176727056503296
epoch£º31	 i:1 	 global-step:621	 l-p:0.09969095140695572
epoch£º31	 i:2 	 global-step:622	 l-p:0.09177219122648239
epoch£º31	 i:3 	 global-step:623	 l-p:0.09861152619123459
epoch£º31	 i:4 	 global-step:624	 l-p:0.09783431887626648
epoch£º31	 i:5 	 global-step:625	 l-p:0.10389107465744019
epoch£º31	 i:6 	 global-step:626	 l-p:0.12047576904296875
epoch£º31	 i:7 	 global-step:627	 l-p:0.12899468839168549
epoch£º31	 i:8 	 global-step:628	 l-p:0.10344850271940231
epoch£º31	 i:9 	 global-step:629	 l-p:0.10729144513607025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3262, 5.6043, 6.5131],
        [4.3262, 5.1600, 5.5391],
        [4.3262, 4.3779, 4.3417],
        [4.3262, 4.3276, 4.3263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.10397152602672577 
model_pd.l_d.mean(): -14.50060749053955 
model_pd.lagr.mean(): -14.396636009216309 
model_pd.lambdas: dict_items([('pout', tensor([0.9897], device='cuda:0')), ('power', tensor([0.6374], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1610], device='cuda:0')), ('power', tensor([-22.4590], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.10397152602672577
epoch£º32	 i:1 	 global-step:641	 l-p:0.14194048941135406
epoch£º32	 i:2 	 global-step:642	 l-p:0.1204671636223793
epoch£º32	 i:3 	 global-step:643	 l-p:-0.7667353749275208
epoch£º32	 i:4 	 global-step:644	 l-p:0.10939912497997284
epoch£º32	 i:5 	 global-step:645	 l-p:0.0979132279753685
epoch£º32	 i:6 	 global-step:646	 l-p:0.1015084981918335
epoch£º32	 i:7 	 global-step:647	 l-p:0.11850259453058243
epoch£º32	 i:8 	 global-step:648	 l-p:0.19172215461730957
epoch£º32	 i:9 	 global-step:649	 l-p:0.11037097871303558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2298, 4.2300, 4.2298],
        [4.2298, 4.4413, 4.3852],
        [4.2298, 4.2304, 4.2298],
        [4.2298, 4.4415, 4.3854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.11712303012609482 
model_pd.l_d.mean(): -14.415168762207031 
model_pd.lagr.mean(): -14.298046112060547 
model_pd.lambdas: dict_items([('pout', tensor([0.9885], device='cuda:0')), ('power', tensor([0.6262], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1390], device='cuda:0')), ('power', tensor([-22.7596], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.11712303012609482
epoch£º33	 i:1 	 global-step:661	 l-p:0.10516761243343353
epoch£º33	 i:2 	 global-step:662	 l-p:0.10075470805168152
epoch£º33	 i:3 	 global-step:663	 l-p:7.713129043579102
epoch£º33	 i:4 	 global-step:664	 l-p:0.05754236876964569
epoch£º33	 i:5 	 global-step:665	 l-p:0.10527423024177551
epoch£º33	 i:6 	 global-step:666	 l-p:0.10954621434211731
epoch£º33	 i:7 	 global-step:667	 l-p:0.09572580456733704
epoch£º33	 i:8 	 global-step:668	 l-p:0.1207282766699791
epoch£º33	 i:9 	 global-step:669	 l-p:0.10727587342262268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2628, 4.7844, 4.8851],
        [4.2628, 4.2630, 4.2628],
        [4.2628, 4.5082, 4.4585],
        [4.2628, 4.3119, 4.2772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 2.5885555744171143 
model_pd.l_d.mean(): -14.17198371887207 
model_pd.lagr.mean(): -11.583428382873535 
model_pd.lambdas: dict_items([('pout', tensor([0.9873], device='cuda:0')), ('power', tensor([0.6150], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1524], device='cuda:0')), ('power', tensor([-22.7588], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:2.5885555744171143
epoch£º34	 i:1 	 global-step:681	 l-p:0.09255658835172653
epoch£º34	 i:2 	 global-step:682	 l-p:0.13338787853717804
epoch£º34	 i:3 	 global-step:683	 l-p:0.11917506903409958
epoch£º34	 i:4 	 global-step:684	 l-p:0.12095275521278381
epoch£º34	 i:5 	 global-step:685	 l-p:0.10429725050926208
epoch£º34	 i:6 	 global-step:686	 l-p:0.10683396458625793
epoch£º34	 i:7 	 global-step:687	 l-p:0.08230891078710556
epoch£º34	 i:8 	 global-step:688	 l-p:0.1019367203116417
epoch£º34	 i:9 	 global-step:689	 l-p:0.0970880314707756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4605, 4.8766, 4.8917],
        [4.4605, 4.4605, 4.4605],
        [4.4605, 4.5824, 4.5219],
        [4.4605, 5.6853, 6.4969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.10137549787759781 
model_pd.l_d.mean(): -13.697904586791992 
model_pd.lagr.mean(): -13.596529006958008 
model_pd.lambdas: dict_items([('pout', tensor([0.9856], device='cuda:0')), ('power', tensor([0.6038], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1698], device='cuda:0')), ('power', tensor([-22.3678], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.10137549787759781
epoch£º35	 i:1 	 global-step:701	 l-p:0.10655522346496582
epoch£º35	 i:2 	 global-step:702	 l-p:0.10294294357299805
epoch£º35	 i:3 	 global-step:703	 l-p:0.10452403873205185
epoch£º35	 i:4 	 global-step:704	 l-p:0.10051734000444412
epoch£º35	 i:5 	 global-step:705	 l-p:0.10089470446109772
epoch£º35	 i:6 	 global-step:706	 l-p:0.10277973115444183
epoch£º35	 i:7 	 global-step:707	 l-p:0.11003222316503525
epoch£º35	 i:8 	 global-step:708	 l-p:0.3982672691345215
epoch£º35	 i:9 	 global-step:709	 l-p:0.10328170657157898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2476, 4.2476, 4.2476],
        [4.2476, 4.2476, 4.2476],
        [4.2476, 4.8003, 4.9273],
        [4.2476, 4.2882, 4.2582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.10168920457363129 
model_pd.l_d.mean(): -13.149059295654297 
model_pd.lagr.mean(): -13.047369956970215 
model_pd.lambdas: dict_items([('pout', tensor([0.9841], device='cuda:0')), ('power', tensor([0.5926], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0834], device='cuda:0')), ('power', tensor([-22.0084], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.10168920457363129
epoch£º36	 i:1 	 global-step:721	 l-p:0.10004071891307831
epoch£º36	 i:2 	 global-step:722	 l-p:-0.020764971151947975
epoch£º36	 i:3 	 global-step:723	 l-p:0.10889943689107895
epoch£º36	 i:4 	 global-step:724	 l-p:0.09963434189558029
epoch£º36	 i:5 	 global-step:725	 l-p:0.11342281103134155
epoch£º36	 i:6 	 global-step:726	 l-p:0.14936108887195587
epoch£º36	 i:7 	 global-step:727	 l-p:0.135780468583107
epoch£º36	 i:8 	 global-step:728	 l-p:0.10729669034481049
epoch£º36	 i:9 	 global-step:729	 l-p:0.10195876657962799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4198, 4.4255, 4.4202],
        [4.4198, 5.5989, 6.3622],
        [4.4198, 4.7837, 4.7737],
        [4.4198, 4.8752, 4.9176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.11933407187461853 
model_pd.l_d.mean(): -12.704029083251953 
model_pd.lagr.mean(): -12.584694862365723 
model_pd.lambdas: dict_items([('pout', tensor([0.9827], device='cuda:0')), ('power', tensor([0.5814], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0942], device='cuda:0')), ('power', tensor([-21.6500], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.11933407187461853
epoch£º37	 i:1 	 global-step:741	 l-p:0.09827428311109543
epoch£º37	 i:2 	 global-step:742	 l-p:0.10189197212457657
epoch£º37	 i:3 	 global-step:743	 l-p:0.10331778973340988
epoch£º37	 i:4 	 global-step:744	 l-p:0.10756266117095947
epoch£º37	 i:5 	 global-step:745	 l-p:0.0873536691069603
epoch£º37	 i:6 	 global-step:746	 l-p:0.08974061906337738
epoch£º37	 i:7 	 global-step:747	 l-p:0.12079410254955292
epoch£º37	 i:8 	 global-step:748	 l-p:0.11512541770935059
epoch£º37	 i:9 	 global-step:749	 l-p:0.10626641660928726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4294, 4.5312, 4.4756],
        [4.4294, 4.4900, 4.4492],
        [4.4294, 4.4529, 4.4336],
        [4.4294, 4.4873, 4.4477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.10380581021308899 
model_pd.l_d.mean(): -12.940619468688965 
model_pd.lagr.mean(): -12.836813926696777 
model_pd.lambdas: dict_items([('pout', tensor([0.9808], device='cuda:0')), ('power', tensor([0.5702], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1617], device='cuda:0')), ('power', tensor([-22.3714], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.10380581021308899
epoch£º38	 i:1 	 global-step:761	 l-p:0.1151294857263565
epoch£º38	 i:2 	 global-step:762	 l-p:0.105925053358078
epoch£º38	 i:3 	 global-step:763	 l-p:0.13119518756866455
epoch£º38	 i:4 	 global-step:764	 l-p:0.0933455154299736
epoch£º38	 i:5 	 global-step:765	 l-p:0.11148946732282639
epoch£º38	 i:6 	 global-step:766	 l-p:0.09086602926254272
epoch£º38	 i:7 	 global-step:767	 l-p:0.10616552829742432
epoch£º38	 i:8 	 global-step:768	 l-p:0.11472255736589432
epoch£º38	 i:9 	 global-step:769	 l-p:0.10577437281608582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3932, 4.3932, 4.3932],
        [4.3932, 5.3655, 5.8879],
        [4.3932, 4.3932, 4.3932],
        [4.3932, 4.3951, 4.3932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.10456634312868118 
model_pd.l_d.mean(): -13.031940460205078 
model_pd.lagr.mean(): -12.927373886108398 
model_pd.lambdas: dict_items([('pout', tensor([0.9790], device='cuda:0')), ('power', tensor([0.5590], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2306], device='cuda:0')), ('power', tensor([-22.8608], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.10456634312868118
epoch£º39	 i:1 	 global-step:781	 l-p:0.09714175760746002
epoch£º39	 i:2 	 global-step:782	 l-p:0.10138469189405441
epoch£º39	 i:3 	 global-step:783	 l-p:0.11039784550666809
epoch£º39	 i:4 	 global-step:784	 l-p:0.13838940858840942
epoch£º39	 i:5 	 global-step:785	 l-p:0.1048695519566536
epoch£º39	 i:6 	 global-step:786	 l-p:0.1257409006357193
epoch£º39	 i:7 	 global-step:787	 l-p:0.09973163157701492
epoch£º39	 i:8 	 global-step:788	 l-p:0.15584413707256317
epoch£º39	 i:9 	 global-step:789	 l-p:0.10034200549125671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4428, 4.4430, 4.4428],
        [4.4428, 4.4600, 4.4454],
        [4.4428, 4.5463, 4.4903],
        [4.4428, 4.4497, 4.4434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.10917501896619797 
model_pd.l_d.mean(): -12.706890106201172 
model_pd.lagr.mean(): -12.597715377807617 
model_pd.lambdas: dict_items([('pout', tensor([0.9774], device='cuda:0')), ('power', tensor([0.5478], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2232], device='cuda:0')), ('power', tensor([-22.7486], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.10917501896619797
epoch£º40	 i:1 	 global-step:801	 l-p:0.12314406782388687
epoch£º40	 i:2 	 global-step:802	 l-p:0.09686410427093506
epoch£º40	 i:3 	 global-step:803	 l-p:0.11109352111816406
epoch£º40	 i:4 	 global-step:804	 l-p:0.11244215816259384
epoch£º40	 i:5 	 global-step:805	 l-p:0.10063720494508743
epoch£º40	 i:6 	 global-step:806	 l-p:0.07981877774000168
epoch£º40	 i:7 	 global-step:807	 l-p:0.10059651732444763
epoch£º40	 i:8 	 global-step:808	 l-p:0.10653520375490189
epoch£º40	 i:9 	 global-step:809	 l-p:0.08062860369682312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5528, 4.5553, 4.5529],
        [4.5528, 4.5531, 4.5528],
        [4.5528, 4.7278, 4.6620],
        [4.5528, 4.6624, 4.6039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.08426987379789352 
model_pd.l_d.mean(): -12.407346725463867 
model_pd.lagr.mean(): -12.323077201843262 
model_pd.lambdas: dict_items([('pout', tensor([0.9754], device='cuda:0')), ('power', tensor([0.5367], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2537], device='cuda:0')), ('power', tensor([-22.6085], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.08426987379789352
epoch£º41	 i:1 	 global-step:821	 l-p:0.10941609740257263
epoch£º41	 i:2 	 global-step:822	 l-p:0.11052060127258301
epoch£º41	 i:3 	 global-step:823	 l-p:0.09198632091283798
epoch£º41	 i:4 	 global-step:824	 l-p:0.09542720764875412
epoch£º41	 i:5 	 global-step:825	 l-p:0.1024848073720932
epoch£º41	 i:6 	 global-step:826	 l-p:0.09402257949113846
epoch£º41	 i:7 	 global-step:827	 l-p:0.1286807358264923
epoch£º41	 i:8 	 global-step:828	 l-p:0.09913325309753418
epoch£º41	 i:9 	 global-step:829	 l-p:0.10238973051309586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4657, 4.7318, 4.6821],
        [4.4657, 4.4657, 4.4657],
        [4.4657, 5.7359, 6.6075],
        [4.4657, 4.4809, 4.4678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.09872114658355713 
model_pd.l_d.mean(): -11.664231300354004 
model_pd.lagr.mean(): -11.565509796142578 
model_pd.lambdas: dict_items([('pout', tensor([0.9734], device='cuda:0')), ('power', tensor([0.5256], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1528], device='cuda:0')), ('power', tensor([-21.8632], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.09872114658355713
epoch£º42	 i:1 	 global-step:841	 l-p:0.12936949729919434
epoch£º42	 i:2 	 global-step:842	 l-p:0.09595058113336563
epoch£º42	 i:3 	 global-step:843	 l-p:0.10731968283653259
epoch£º42	 i:4 	 global-step:844	 l-p:0.1055486872792244
epoch£º42	 i:5 	 global-step:845	 l-p:0.1107802540063858
epoch£º42	 i:6 	 global-step:846	 l-p:0.09730202704668045
epoch£º42	 i:7 	 global-step:847	 l-p:0.0996742770075798
epoch£º42	 i:8 	 global-step:848	 l-p:0.10702630877494812
epoch£º42	 i:9 	 global-step:849	 l-p:0.10665397346019745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.4369, 5.5727, 6.2814],
        [4.4369, 4.9554, 5.0416],
        [4.4369, 4.7252, 4.6836],
        [4.4369, 5.5172, 6.1590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.11253053694963455 
model_pd.l_d.mean(): -11.697805404663086 
model_pd.lagr.mean(): -11.585274696350098 
model_pd.lambdas: dict_items([('pout', tensor([0.9716], device='cuda:0')), ('power', tensor([0.5144], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1674], device='cuda:0')), ('power', tensor([-22.3750], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.11253053694963455
epoch£º43	 i:1 	 global-step:861	 l-p:0.1070285215973854
epoch£º43	 i:2 	 global-step:862	 l-p:0.1265140175819397
epoch£º43	 i:3 	 global-step:863	 l-p:0.12212032079696655
epoch£º43	 i:4 	 global-step:864	 l-p:0.09874316304922104
epoch£º43	 i:5 	 global-step:865	 l-p:0.09740366041660309
epoch£º43	 i:6 	 global-step:866	 l-p:0.08184473216533661
epoch£º43	 i:7 	 global-step:867	 l-p:0.09235815703868866
epoch£º43	 i:8 	 global-step:868	 l-p:0.1060517206788063
epoch£º43	 i:9 	 global-step:869	 l-p:0.10249294340610504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5261, 4.6477, 4.5871],
        [4.5261, 4.5329, 4.5267],
        [4.5261, 4.5284, 4.5262],
        [4.5261, 4.5261, 4.5261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.09230069816112518 
model_pd.l_d.mean(): -11.633991241455078 
model_pd.lagr.mean(): -11.541690826416016 
model_pd.lambdas: dict_items([('pout', tensor([0.9696], device='cuda:0')), ('power', tensor([0.5033], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2468], device='cuda:0')), ('power', tensor([-22.5914], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.09230069816112518
epoch£º44	 i:1 	 global-step:881	 l-p:0.10315095633268356
epoch£º44	 i:2 	 global-step:882	 l-p:0.09884539991617203
epoch£º44	 i:3 	 global-step:883	 l-p:0.13265185058116913
epoch£º44	 i:4 	 global-step:884	 l-p:0.11093400418758392
epoch£º44	 i:5 	 global-step:885	 l-p:0.0940200462937355
epoch£º44	 i:6 	 global-step:886	 l-p:0.08196788281202316
epoch£º44	 i:7 	 global-step:887	 l-p:0.09752898663282394
epoch£º44	 i:8 	 global-step:888	 l-p:0.10093934834003448
epoch£º44	 i:9 	 global-step:889	 l-p:0.10915055871009827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5601, 4.5602, 4.5601],
        [4.5601, 4.5612, 4.5602],
        [4.5601, 4.7873, 4.7264],
        [4.5601, 4.5601, 4.5601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.10360150039196014 
model_pd.l_d.mean(): -11.517826080322266 
model_pd.lagr.mean(): -11.414224624633789 
model_pd.lambdas: dict_items([('pout', tensor([0.9675], device='cuda:0')), ('power', tensor([0.4921], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2843], device='cuda:0')), ('power', tensor([-22.7934], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.10360150039196014
epoch£º45	 i:1 	 global-step:901	 l-p:0.10225854814052582
epoch£º45	 i:2 	 global-step:902	 l-p:0.10257048159837723
epoch£º45	 i:3 	 global-step:903	 l-p:0.10016851872205734
epoch£º45	 i:4 	 global-step:904	 l-p:0.10972870886325836
epoch£º45	 i:5 	 global-step:905	 l-p:0.08431752771139145
epoch£º45	 i:6 	 global-step:906	 l-p:0.10502231121063232
epoch£º45	 i:7 	 global-step:907	 l-p:0.1106158047914505
epoch£º45	 i:8 	 global-step:908	 l-p:0.08992145955562592
epoch£º45	 i:9 	 global-step:909	 l-p:0.10482469201087952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.5654, 4.5696, 4.5657],
        [4.5654, 4.5915, 4.5704],
        [4.5654, 4.5654, 4.5654],
        [4.5654, 4.5910, 4.5702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.09196624904870987 
model_pd.l_d.mean(): -10.887587547302246 
model_pd.lagr.mean(): -10.795620918273926 
model_pd.lambdas: dict_items([('pout', tensor([0.9655], device='cuda:0')), ('power', tensor([0.4810], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1993], device='cuda:0')), ('power', tensor([-22.1842], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.09196624904870987
epoch£º46	 i:1 	 global-step:921	 l-p:0.09993516653776169
epoch£º46	 i:2 	 global-step:922	 l-p:0.09986752271652222
epoch£º46	 i:3 	 global-step:923	 l-p:0.1028071939945221
epoch£º46	 i:4 	 global-step:924	 l-p:0.09319078922271729
epoch£º46	 i:5 	 global-step:925	 l-p:0.11784715205430984
epoch£º46	 i:6 	 global-step:926	 l-p:0.09983028471469879
epoch£º46	 i:7 	 global-step:927	 l-p:0.09837623685598373
epoch£º46	 i:8 	 global-step:928	 l-p:0.09510624408721924
epoch£º46	 i:9 	 global-step:929	 l-p:0.08689214289188385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6236, 4.6805, 4.6411],
        [4.6236, 4.7317, 4.6733],
        [4.6236, 4.6255, 4.6237],
        [4.6236, 4.9661, 4.9381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.09805294126272202 
model_pd.l_d.mean(): -10.311356544494629 
model_pd.lagr.mean(): -10.213303565979004 
model_pd.lambdas: dict_items([('pout', tensor([0.9632], device='cuda:0')), ('power', tensor([0.4699], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1909], device='cuda:0')), ('power', tensor([-21.5023], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.09805294126272202
epoch£º47	 i:1 	 global-step:941	 l-p:0.09536571055650711
epoch£º47	 i:2 	 global-step:942	 l-p:0.09906169027090073
epoch£º47	 i:3 	 global-step:943	 l-p:0.10033135116100311
epoch£º47	 i:4 	 global-step:944	 l-p:0.10523638129234314
epoch£º47	 i:5 	 global-step:945	 l-p:0.09304707497358322
epoch£º47	 i:6 	 global-step:946	 l-p:0.1004796028137207
epoch£º47	 i:7 	 global-step:947	 l-p:0.08858910948038101
epoch£º47	 i:8 	 global-step:948	 l-p:0.10939060151576996
epoch£º47	 i:9 	 global-step:949	 l-p:0.09810833632946014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6090, 4.9517, 4.9247],
        [4.6090, 4.7935, 4.7275],
        [4.6090, 4.6090, 4.6090],
        [4.6090, 4.8434, 4.7828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.09946542233228683 
model_pd.l_d.mean(): -10.745861053466797 
model_pd.lagr.mean(): -10.646395683288574 
model_pd.lambdas: dict_items([('pout', tensor([0.9608], device='cuda:0')), ('power', tensor([0.4587], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2959], device='cuda:0')), ('power', tensor([-22.7482], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.09946542233228683
epoch£º48	 i:1 	 global-step:961	 l-p:0.08141343295574188
epoch£º48	 i:2 	 global-step:962	 l-p:0.11352100968360901
epoch£º48	 i:3 	 global-step:963	 l-p:0.10932193696498871
epoch£º48	 i:4 	 global-step:964	 l-p:0.09591194242238998
epoch£º48	 i:5 	 global-step:965	 l-p:0.10071705281734467
epoch£º48	 i:6 	 global-step:966	 l-p:0.10082563012838364
epoch£º48	 i:7 	 global-step:967	 l-p:0.09728511422872543
epoch£º48	 i:8 	 global-step:968	 l-p:0.08083804696798325
epoch£º48	 i:9 	 global-step:969	 l-p:0.0852583646774292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7229, 4.7583, 4.7309],
        [4.7229, 4.7243, 4.7230],
        [4.7229, 4.8841, 4.8166],
        [4.7229, 4.7293, 4.7234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.09829921275377274 
model_pd.l_d.mean(): -10.498632431030273 
model_pd.lagr.mean(): -10.400333404541016 
model_pd.lambdas: dict_items([('pout', tensor([0.9583], device='cuda:0')), ('power', tensor([0.4476], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3337], device='cuda:0')), ('power', tensor([-22.6809], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.09829921275377274
epoch£º49	 i:1 	 global-step:981	 l-p:0.07957146316766739
epoch£º49	 i:2 	 global-step:982	 l-p:0.09846507757902145
epoch£º49	 i:3 	 global-step:983	 l-p:0.0902356505393982
epoch£º49	 i:4 	 global-step:984	 l-p:0.10724944621324539
epoch£º49	 i:5 	 global-step:985	 l-p:0.08646363765001297
epoch£º49	 i:6 	 global-step:986	 l-p:0.047487638890743256
epoch£º49	 i:7 	 global-step:987	 l-p:0.1023593470454216
epoch£º49	 i:8 	 global-step:988	 l-p:0.09578946977853775
epoch£º49	 i:9 	 global-step:989	 l-p:0.09686601161956787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8732, 5.7563, 6.1175],
        [4.8732, 4.8733, 4.8732],
        [4.8732, 5.2168, 5.1785],
        [4.8732, 4.8751, 4.8732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.0676192045211792 
model_pd.l_d.mean(): -9.996763229370117 
model_pd.lagr.mean(): -9.929143905639648 
model_pd.lambdas: dict_items([('pout', tensor([0.9556], device='cuda:0')), ('power', tensor([0.4366], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3026], device='cuda:0')), ('power', tensor([-22.1774], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.0676192045211792
epoch£º50	 i:1 	 global-step:1001	 l-p:0.08584080636501312
epoch£º50	 i:2 	 global-step:1002	 l-p:0.12175420671701431
epoch£º50	 i:3 	 global-step:1003	 l-p:0.09523940086364746
epoch£º50	 i:4 	 global-step:1004	 l-p:0.09279615432024002
epoch£º50	 i:5 	 global-step:1005	 l-p:0.09417262673377991
epoch£º50	 i:6 	 global-step:1006	 l-p:0.09353393316268921
epoch£º50	 i:7 	 global-step:1007	 l-p:0.09332988411188126
epoch£º50	 i:8 	 global-step:1008	 l-p:0.3201887905597687
epoch£º50	 i:9 	 global-step:1009	 l-p:0.08815173804759979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2215, 5.2215, 5.2215],
        [5.2215, 5.9024, 6.0450],
        [5.2215, 6.6885, 7.6550],
        [5.2215, 5.2216, 5.2215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.09475412219762802 
model_pd.l_d.mean(): -9.3916654586792 
model_pd.lagr.mean(): -9.296911239624023 
model_pd.lambdas: dict_items([('pout', tensor([0.9521], device='cuda:0')), ('power', tensor([0.4257], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3377], device='cuda:0')), ('power', tensor([-21.2530], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.09475412219762802
epoch£º51	 i:1 	 global-step:1021	 l-p:0.09805043041706085
epoch£º51	 i:2 	 global-step:1022	 l-p:0.0949944257736206
epoch£º51	 i:3 	 global-step:1023	 l-p:0.0920945554971695
epoch£º51	 i:4 	 global-step:1024	 l-p:0.10444505512714386
epoch£º51	 i:5 	 global-step:1025	 l-p:0.08561384677886963
epoch£º51	 i:6 	 global-step:1026	 l-p:0.102327361702919
epoch£º51	 i:7 	 global-step:1027	 l-p:0.09387170523405075
epoch£º51	 i:8 	 global-step:1028	 l-p:0.0926978662610054
epoch£º51	 i:9 	 global-step:1029	 l-p:0.09056393057107925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1341, 5.8198, 5.9744],
        [5.1341, 5.6310, 5.6540],
        [5.1341, 5.1939, 5.1516],
        [5.1341, 5.1341, 5.1341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.086826853454113 
model_pd.l_d.mean(): -9.637711524963379 
model_pd.lagr.mean(): -9.550884246826172 
model_pd.lambdas: dict_items([('pout', tensor([0.9480], device='cuda:0')), ('power', tensor([0.4148], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4252], device='cuda:0')), ('power', tensor([-22.2044], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.086826853454113
epoch£º52	 i:1 	 global-step:1041	 l-p:0.09450593590736389
epoch£º52	 i:2 	 global-step:1042	 l-p:0.09176848083734512
epoch£º52	 i:3 	 global-step:1043	 l-p:0.09808492660522461
epoch£º52	 i:4 	 global-step:1044	 l-p:0.15573467314243317
epoch£º52	 i:5 	 global-step:1045	 l-p:0.10021480172872543
epoch£º52	 i:6 	 global-step:1046	 l-p:0.10012078285217285
epoch£º52	 i:7 	 global-step:1047	 l-p:0.07098355889320374
epoch£º52	 i:8 	 global-step:1048	 l-p:0.07917974889278412
epoch£º52	 i:9 	 global-step:1049	 l-p:0.08821941167116165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8760, 4.8854, 4.8769],
        [4.8760, 4.8761, 4.8760],
        [4.8760, 4.8767, 4.8760],
        [4.8760, 4.8765, 4.8760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.09622152149677277 
model_pd.l_d.mean(): -9.044743537902832 
model_pd.lagr.mean(): -8.948521614074707 
model_pd.lambdas: dict_items([('pout', tensor([0.9449], device='cuda:0')), ('power', tensor([0.4038], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2828], device='cuda:0')), ('power', tensor([-21.6787], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.09622152149677277
epoch£º53	 i:1 	 global-step:1061	 l-p:0.09489789605140686
epoch£º53	 i:2 	 global-step:1062	 l-p:0.09873495995998383
epoch£º53	 i:3 	 global-step:1063	 l-p:0.033561822026968
epoch£º53	 i:4 	 global-step:1064	 l-p:0.12744498252868652
epoch£º53	 i:5 	 global-step:1065	 l-p:0.09662297368049622
epoch£º53	 i:6 	 global-step:1066	 l-p:0.09298080950975418
epoch£º53	 i:7 	 global-step:1067	 l-p:1.3679081201553345
epoch£º53	 i:8 	 global-step:1068	 l-p:0.07183746993541718
epoch£º53	 i:9 	 global-step:1069	 l-p:0.0541720986366272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3988, 5.3988, 5.3988],
        [5.3988, 6.2286, 6.4773],
        [5.3988, 6.2873, 6.5887],
        [5.3988, 5.5273, 5.4572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.09052242338657379 
model_pd.l_d.mean(): -9.132997512817383 
model_pd.lagr.mean(): -9.042474746704102 
model_pd.lambdas: dict_items([('pout', tensor([0.9412], device='cuda:0')), ('power', tensor([0.3928], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4881], device='cuda:0')), ('power', tensor([-22.0178], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.09052242338657379
epoch£º54	 i:1 	 global-step:1081	 l-p:0.08688268810510635
epoch£º54	 i:2 	 global-step:1082	 l-p:0.13772906363010406
epoch£º54	 i:3 	 global-step:1083	 l-p:0.09412197768688202
epoch£º54	 i:4 	 global-step:1084	 l-p:0.0946379005908966
epoch£º54	 i:5 	 global-step:1085	 l-p:0.10530522465705872
epoch£º54	 i:6 	 global-step:1086	 l-p:0.08898892998695374
epoch£º54	 i:7 	 global-step:1087	 l-p:0.08292184770107269
epoch£º54	 i:8 	 global-step:1088	 l-p:0.0886538177728653
epoch£º54	 i:9 	 global-step:1089	 l-p:0.08528164029121399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[5.7710, 6.7358, 7.0637],
        [5.7710, 7.1166, 7.8368],
        [5.7710, 6.3484, 6.3771],
        [5.7710, 6.7472, 7.0859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.09995389729738235 
model_pd.l_d.mean(): -8.706172943115234 
model_pd.lagr.mean(): -8.606219291687012 
model_pd.lambdas: dict_items([('pout', tensor([0.9362], device='cuda:0')), ('power', tensor([0.3821], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5240], device='cuda:0')), ('power', tensor([-21.4392], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.09995389729738235
epoch£º55	 i:1 	 global-step:1101	 l-p:0.09055071324110031
epoch£º55	 i:2 	 global-step:1102	 l-p:0.09142223000526428
epoch£º55	 i:3 	 global-step:1103	 l-p:0.10915999859571457
epoch£º55	 i:4 	 global-step:1104	 l-p:0.08518467843532562
epoch£º55	 i:5 	 global-step:1105	 l-p:0.08820005506277084
epoch£º55	 i:6 	 global-step:1106	 l-p:0.09077338874340057
epoch£º55	 i:7 	 global-step:1107	 l-p:0.09038019180297852
epoch£º55	 i:8 	 global-step:1108	 l-p:0.08684832602739334
epoch£º55	 i:9 	 global-step:1109	 l-p:0.0991532951593399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3531, 6.8700, 7.8723],
        [5.3531, 5.3531, 5.3531],
        [5.3531, 5.6419, 5.5703],
        [5.3531, 5.4475, 5.3887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.09696074575185776 
model_pd.l_d.mean(): -8.494738578796387 
model_pd.lagr.mean(): -8.397777557373047 
model_pd.lambdas: dict_items([('pout', tensor([0.9315], device='cuda:0')), ('power', tensor([0.3714], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4162], device='cuda:0')), ('power', tensor([-21.7671], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.09696074575185776
epoch£º56	 i:1 	 global-step:1121	 l-p:0.12815675139427185
epoch£º56	 i:2 	 global-step:1122	 l-p:0.06663192808628082
epoch£º56	 i:3 	 global-step:1123	 l-p:0.08967173099517822
epoch£º56	 i:4 	 global-step:1124	 l-p:0.11016737669706345
epoch£º56	 i:5 	 global-step:1125	 l-p:0.06310049444437027
epoch£º56	 i:6 	 global-step:1126	 l-p:0.08868540823459625
epoch£º56	 i:7 	 global-step:1127	 l-p:0.09241422265768051
epoch£º56	 i:8 	 global-step:1128	 l-p:0.08778253197669983
epoch£º56	 i:9 	 global-step:1129	 l-p:0.08985398709774017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2983, 6.4600, 7.0520],
        [5.2983, 6.5917, 7.3330],
        [5.2983, 5.2983, 5.2983],
        [5.2983, 5.3813, 5.3274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.09223682433366776 
model_pd.l_d.mean(): -8.331064224243164 
model_pd.lagr.mean(): -8.2388277053833 
model_pd.lambdas: dict_items([('pout', tensor([0.9273], device='cuda:0')), ('power', tensor([0.3605], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4315], device='cuda:0')), ('power', tensor([-21.9343], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.09223682433366776
epoch£º57	 i:1 	 global-step:1141	 l-p:0.09224142134189606
epoch£º57	 i:2 	 global-step:1142	 l-p:0.08869562298059464
epoch£º57	 i:3 	 global-step:1143	 l-p:0.09005162864923477
epoch£º57	 i:4 	 global-step:1144	 l-p:0.12786193192005157
epoch£º57	 i:5 	 global-step:1145	 l-p:0.09038548916578293
epoch£º57	 i:6 	 global-step:1146	 l-p:0.09243329614400864
epoch£º57	 i:7 	 global-step:1147	 l-p:0.09127650409936905
epoch£º57	 i:8 	 global-step:1148	 l-p:0.07290362566709518
epoch£º57	 i:9 	 global-step:1149	 l-p:0.10717769712209702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2933, 5.6657, 5.6200],
        [5.2933, 5.5560, 5.4816],
        [5.2933, 6.7313, 7.6472],
        [5.2933, 5.2933, 5.2933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.09241939336061478 
model_pd.l_d.mean(): -8.181171417236328 
model_pd.lagr.mean(): -8.088751792907715 
model_pd.lambdas: dict_items([('pout', tensor([0.9233], device='cuda:0')), ('power', tensor([0.3496], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4584], device='cuda:0')), ('power', tensor([-22.1216], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.09241939336061478
epoch£º58	 i:1 	 global-step:1161	 l-p:0.06474637240171432
epoch£º58	 i:2 	 global-step:1162	 l-p:0.11064132302999496
epoch£º58	 i:3 	 global-step:1163	 l-p:0.08471554517745972
epoch£º58	 i:4 	 global-step:1164	 l-p:0.09080400317907333
epoch£º58	 i:5 	 global-step:1165	 l-p:0.09930475056171417
epoch£º58	 i:6 	 global-step:1166	 l-p:0.09313688427209854
epoch£º58	 i:7 	 global-step:1167	 l-p:0.16197730600833893
epoch£º58	 i:8 	 global-step:1168	 l-p:0.08713947981595993
epoch£º58	 i:9 	 global-step:1169	 l-p:0.08761826902627945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6345, 6.0859, 6.0566],
        [5.6345, 5.6375, 5.6346],
        [5.6345, 5.6345, 5.6345],
        [5.6345, 5.8551, 5.7703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.11292846500873566 
model_pd.l_d.mean(): -7.589302062988281 
model_pd.lagr.mean(): -7.476373672485352 
model_pd.lambdas: dict_items([('pout', tensor([0.9188], device='cuda:0')), ('power', tensor([0.3388], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4639], device='cuda:0')), ('power', tensor([-21.0752], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.11292846500873566
epoch£º59	 i:1 	 global-step:1181	 l-p:0.09189680963754654
epoch£º59	 i:2 	 global-step:1182	 l-p:0.08614742010831833
epoch£º59	 i:3 	 global-step:1183	 l-p:0.08910337090492249
epoch£º59	 i:4 	 global-step:1184	 l-p:0.0917109027504921
epoch£º59	 i:5 	 global-step:1185	 l-p:0.08752665668725967
epoch£º59	 i:6 	 global-step:1186	 l-p:0.08953835070133209
epoch£º59	 i:7 	 global-step:1187	 l-p:0.0867781937122345
epoch£º59	 i:8 	 global-step:1188	 l-p:0.08920112252235413
epoch£º59	 i:9 	 global-step:1189	 l-p:0.08772765845060349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7300, 5.8740, 5.7971],
        [5.7300, 5.7300, 5.7300],
        [5.7300, 5.7327, 5.7301],
        [5.7300, 6.1797, 6.1450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.10206438601016998 
model_pd.l_d.mean(): -7.179049015045166 
model_pd.lagr.mean(): -7.076984405517578 
model_pd.lambdas: dict_items([('pout', tensor([0.9136], device='cuda:0')), ('power', tensor([0.3281], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4029], device='cuda:0')), ('power', tensor([-20.6904], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.10206438601016998
epoch£º60	 i:1 	 global-step:1201	 l-p:0.08593447506427765
epoch£º60	 i:2 	 global-step:1202	 l-p:0.08767185360193253
epoch£º60	 i:3 	 global-step:1203	 l-p:0.10546745359897614
epoch£º60	 i:4 	 global-step:1204	 l-p:0.08738028258085251
epoch£º60	 i:5 	 global-step:1205	 l-p:0.08512137830257416
epoch£º60	 i:6 	 global-step:1206	 l-p:0.0872371643781662
epoch£º60	 i:7 	 global-step:1207	 l-p:0.08550779521465302
epoch£º60	 i:8 	 global-step:1208	 l-p:0.09875699877738953
epoch£º60	 i:9 	 global-step:1209	 l-p:0.09235870838165283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5438, 5.5438, 5.5438],
        [5.5438, 6.3413, 6.5471],
        [5.5438, 5.5528, 5.5446],
        [5.5438, 6.4492, 6.7492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.09073562175035477 
model_pd.l_d.mean(): -7.187406063079834 
model_pd.lagr.mean(): -7.096670627593994 
model_pd.lambdas: dict_items([('pout', tensor([0.9086], device='cuda:0')), ('power', tensor([0.3174], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4534], device='cuda:0')), ('power', tensor([-21.2759], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.09073562175035477
epoch£º61	 i:1 	 global-step:1221	 l-p:0.08570342510938644
epoch£º61	 i:2 	 global-step:1222	 l-p:0.08945091813802719
epoch£º61	 i:3 	 global-step:1223	 l-p:0.09007733315229416
epoch£º61	 i:4 	 global-step:1224	 l-p:0.0905914306640625
epoch£º61	 i:5 	 global-step:1225	 l-p:0.009338929317891598
epoch£º61	 i:6 	 global-step:1226	 l-p:0.10288388282060623
epoch£º61	 i:7 	 global-step:1227	 l-p:-5.136287689208984
epoch£º61	 i:8 	 global-step:1228	 l-p:0.09588920325040817
epoch£º61	 i:9 	 global-step:1229	 l-p:0.08484186232089996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5557, 5.5989, 5.5654],
        [5.5557, 6.7339, 7.3041],
        [5.5557, 5.7317, 5.6509],
        [5.5557, 5.8393, 5.7611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.143125981092453 
model_pd.l_d.mean(): -7.0570478439331055 
model_pd.lagr.mean(): -6.91392183303833 
model_pd.lambdas: dict_items([('pout', tensor([0.9040], device='cuda:0')), ('power', tensor([0.3066], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4601], device='cuda:0')), ('power', tensor([-21.5866], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.143125981092453
epoch£º62	 i:1 	 global-step:1241	 l-p:0.09614282101392746
epoch£º62	 i:2 	 global-step:1242	 l-p:0.08480329066514969
epoch£º62	 i:3 	 global-step:1243	 l-p:0.09884408861398697
epoch£º62	 i:4 	 global-step:1244	 l-p:0.08962391316890717
epoch£º62	 i:5 	 global-step:1245	 l-p:0.09092468023300171
epoch£º62	 i:6 	 global-step:1246	 l-p:0.08712390810251236
epoch£º62	 i:7 	 global-step:1247	 l-p:0.08206060528755188
epoch£º62	 i:8 	 global-step:1248	 l-p:0.07861001789569855
epoch£º62	 i:9 	 global-step:1249	 l-p:0.08646804839372635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.9285, 5.9324, 5.9287],
        [5.9285, 7.7081, 8.9241],
        [5.9285, 7.7141, 8.9379],
        [5.9285, 5.9285, 5.9285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.0931752547621727 
model_pd.l_d.mean(): -6.840256690979004 
model_pd.lagr.mean(): -6.747081279754639 
model_pd.lambdas: dict_items([('pout', tensor([0.8987], device='cuda:0')), ('power', tensor([0.2959], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5651], device='cuda:0')), ('power', tensor([-21.3234], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.0931752547621727
epoch£º63	 i:1 	 global-step:1261	 l-p:0.08521775901317596
epoch£º63	 i:2 	 global-step:1262	 l-p:0.08484134823083878
epoch£º63	 i:3 	 global-step:1263	 l-p:0.09360538423061371
epoch£º63	 i:4 	 global-step:1264	 l-p:0.09261482208967209
epoch£º63	 i:5 	 global-step:1265	 l-p:0.0862841010093689
epoch£º63	 i:6 	 global-step:1266	 l-p:0.08619096875190735
epoch£º63	 i:7 	 global-step:1267	 l-p:0.0863223597407341
epoch£º63	 i:8 	 global-step:1268	 l-p:0.08459741622209549
epoch£º63	 i:9 	 global-step:1269	 l-p:0.08205383270978928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7965, 5.7991, 5.7966],
        [5.7965, 5.7965, 5.7965],
        [5.7965, 5.8370, 5.8049],
        [5.7965, 5.7965, 5.7965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.08627820760011673 
model_pd.l_d.mean(): -6.578405857086182 
model_pd.lagr.mean(): -6.492127418518066 
model_pd.lambdas: dict_items([('pout', tensor([0.8932], device='cuda:0')), ('power', tensor([0.2852], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5172], device='cuda:0')), ('power', tensor([-21.3616], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.08627820760011673
epoch£º64	 i:1 	 global-step:1281	 l-p:0.08994177728891373
epoch£º64	 i:2 	 global-step:1282	 l-p:0.10012633353471756
epoch£º64	 i:3 	 global-step:1283	 l-p:0.09518446028232574
epoch£º64	 i:4 	 global-step:1284	 l-p:0.08658882230520248
epoch£º64	 i:5 	 global-step:1285	 l-p:0.08918122202157974
epoch£º64	 i:6 	 global-step:1286	 l-p:0.0851062536239624
epoch£º64	 i:7 	 global-step:1287	 l-p:0.08958874642848969
epoch£º64	 i:8 	 global-step:1288	 l-p:0.0854097455739975
epoch£º64	 i:9 	 global-step:1289	 l-p:0.08631057292222977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7779, 5.7786, 5.7779],
        [5.7779, 5.7779, 5.7779],
        [5.7779, 7.1652, 7.9318],
        [5.7779, 5.9443, 5.8623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.10203535854816437 
model_pd.l_d.mean(): -6.3747239112854 
model_pd.lagr.mean(): -6.272688388824463 
model_pd.lambdas: dict_items([('pout', tensor([0.8879], device='cuda:0')), ('power', tensor([0.2746], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5229], device='cuda:0')), ('power', tensor([-21.4419], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.10203535854816437
epoch£º65	 i:1 	 global-step:1301	 l-p:0.08654504269361496
epoch£º65	 i:2 	 global-step:1302	 l-p:0.08705789595842361
epoch£º65	 i:3 	 global-step:1303	 l-p:0.08393453061580658
epoch£º65	 i:4 	 global-step:1304	 l-p:0.08737453073263168
epoch£º65	 i:5 	 global-step:1305	 l-p:0.09748712927103043
epoch£º65	 i:6 	 global-step:1306	 l-p:0.08921851217746735
epoch£º65	 i:7 	 global-step:1307	 l-p:0.09081929177045822
epoch£º65	 i:8 	 global-step:1308	 l-p:0.08283279836177826
epoch£º65	 i:9 	 global-step:1309	 l-p:0.08815743029117584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8108, 5.8108, 5.8108],
        [5.8108, 7.1896, 7.9411],
        [5.8108, 6.5688, 6.7179],
        [5.8108, 5.8475, 5.8180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.08779899030923843 
model_pd.l_d.mean(): -6.220882415771484 
model_pd.lagr.mean(): -6.133083343505859 
model_pd.lambdas: dict_items([('pout', tensor([0.8826], device='cuda:0')), ('power', tensor([0.2639], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5684], device='cuda:0')), ('power', tensor([-21.5848], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.08779899030923843
epoch£º66	 i:1 	 global-step:1321	 l-p:0.07620063424110413
epoch£º66	 i:2 	 global-step:1322	 l-p:0.09359283000230789
epoch£º66	 i:3 	 global-step:1323	 l-p:0.09786917269229889
epoch£º66	 i:4 	 global-step:1324	 l-p:0.08491798490285873
epoch£º66	 i:5 	 global-step:1325	 l-p:0.09956087172031403
epoch£º66	 i:6 	 global-step:1326	 l-p:0.0830739364027977
epoch£º66	 i:7 	 global-step:1327	 l-p:0.08489574491977692
epoch£º66	 i:8 	 global-step:1328	 l-p:0.08582902699708939
epoch£º66	 i:9 	 global-step:1329	 l-p:0.08632112294435501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.9403, 5.9404, 5.9403],
        [5.9403, 5.9404, 5.9403],
        [5.9403, 7.2732, 7.9520],
        [5.9403, 7.0022, 7.4012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.09398508816957474 
model_pd.l_d.mean(): -5.897542476654053 
model_pd.lagr.mean(): -5.803557395935059 
model_pd.lambdas: dict_items([('pout', tensor([0.8772], device='cuda:0')), ('power', tensor([0.2532], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5596], device='cuda:0')), ('power', tensor([-21.2599], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.09398508816957474
epoch£º67	 i:1 	 global-step:1341	 l-p:0.09425109624862671
epoch£º67	 i:2 	 global-step:1342	 l-p:0.084523044526577
epoch£º67	 i:3 	 global-step:1343	 l-p:0.0801098495721817
epoch£º67	 i:4 	 global-step:1344	 l-p:0.08446463197469711
epoch£º67	 i:5 	 global-step:1345	 l-p:0.07823040336370468
epoch£º67	 i:6 	 global-step:1346	 l-p:0.08694322407245636
epoch£º67	 i:7 	 global-step:1347	 l-p:0.08150098472833633
epoch£º67	 i:8 	 global-step:1348	 l-p:0.08760709315538406
epoch£º67	 i:9 	 global-step:1349	 l-p:0.08652696758508682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.0748, 7.8371, 9.0002],
        [6.0748, 7.9127, 9.1725],
        [6.0748, 6.0902, 6.0765],
        [6.0748, 7.5481, 8.3628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.08603307604789734 
model_pd.l_d.mean(): -5.534620761871338 
model_pd.lagr.mean(): -5.448587894439697 
model_pd.lambdas: dict_items([('pout', tensor([0.8714], device='cuda:0')), ('power', tensor([0.2427], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5286], device='cuda:0')), ('power', tensor([-20.8192], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.08603307604789734
epoch£º68	 i:1 	 global-step:1361	 l-p:0.0859178900718689
epoch£º68	 i:2 	 global-step:1362	 l-p:0.0840330645442009
epoch£º68	 i:3 	 global-step:1363	 l-p:0.0897650346159935
epoch£º68	 i:4 	 global-step:1364	 l-p:0.08011970669031143
epoch£º68	 i:5 	 global-step:1365	 l-p:0.08910724520683289
epoch£º68	 i:6 	 global-step:1366	 l-p:0.08501546084880829
epoch£º68	 i:7 	 global-step:1367	 l-p:0.08288603276014328
epoch£º68	 i:8 	 global-step:1368	 l-p:0.08415798842906952
epoch£º68	 i:9 	 global-step:1369	 l-p:0.0730002224445343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1870, 6.4453, 6.3504],
        [6.1870, 7.7476, 8.6439],
        [6.1870, 6.5326, 6.4473],
        [6.1870, 6.1870, 6.1870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.08618707209825516 
model_pd.l_d.mean(): -5.253427028656006 
model_pd.lagr.mean(): -5.167240142822266 
model_pd.lambdas: dict_items([('pout', tensor([0.8654], device='cuda:0')), ('power', tensor([0.2321], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5389], device='cuda:0')), ('power', tensor([-20.5302], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.08618707209825516
epoch£º69	 i:1 	 global-step:1381	 l-p:0.08629482239484787
epoch£º69	 i:2 	 global-step:1382	 l-p:0.08260001242160797
epoch£º69	 i:3 	 global-step:1383	 l-p:0.07586444914340973
epoch£º69	 i:4 	 global-step:1384	 l-p:0.07410500943660736
epoch£º69	 i:5 	 global-step:1385	 l-p:0.08429504930973053
epoch£º69	 i:6 	 global-step:1386	 l-p:0.08309321105480194
epoch£º69	 i:7 	 global-step:1387	 l-p:0.0859365239739418
epoch£º69	 i:8 	 global-step:1388	 l-p:0.08187509328126907
epoch£º69	 i:9 	 global-step:1389	 l-p:0.08214781433343887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3647, 8.3793, 9.8054],
        [6.3647, 6.4101, 6.3741],
        [6.3647, 6.3647, 6.3646],
        [6.3647, 8.2549, 9.5199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.07899662107229233 
model_pd.l_d.mean(): -4.962093353271484 
model_pd.lagr.mean(): -4.883096694946289 
model_pd.lambdas: dict_items([('pout', tensor([0.8590], device='cuda:0')), ('power', tensor([0.2217], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5426], device='cuda:0')), ('power', tensor([-20.1898], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.07899662107229233
epoch£º70	 i:1 	 global-step:1401	 l-p:0.08283434063196182
epoch£º70	 i:2 	 global-step:1402	 l-p:0.08191456645727158
epoch£º70	 i:3 	 global-step:1403	 l-p:0.06147811934351921
epoch£º70	 i:4 	 global-step:1404	 l-p:0.08143667876720428
epoch£º70	 i:5 	 global-step:1405	 l-p:0.08243907988071442
epoch£º70	 i:6 	 global-step:1406	 l-p:0.08357924968004227
epoch£º70	 i:7 	 global-step:1407	 l-p:0.08112375438213348
epoch£º70	 i:8 	 global-step:1408	 l-p:0.0824834406375885
epoch£º70	 i:9 	 global-step:1409	 l-p:0.07987652719020844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5868, 6.9139, 6.8149],
        [6.5868, 6.5925, 6.5872],
        [6.5868, 6.5868, 6.5868],
        [6.5868, 7.5701, 7.8256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.08130042999982834 
model_pd.l_d.mean(): -5.0416669845581055 
model_pd.lagr.mean(): -4.960366725921631 
model_pd.lambdas: dict_items([('pout', tensor([0.8521], device='cuda:0')), ('power', tensor([0.2112], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7161], device='cuda:0')), ('power', tensor([-20.8746], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.08130042999982834
epoch£º71	 i:1 	 global-step:1421	 l-p:0.08182921260595322
epoch£º71	 i:2 	 global-step:1422	 l-p:-0.0749058648943901
epoch£º71	 i:3 	 global-step:1423	 l-p:0.0797930508852005
epoch£º71	 i:4 	 global-step:1424	 l-p:0.07507677376270294
epoch£º71	 i:5 	 global-step:1425	 l-p:0.08561737090349197
epoch£º71	 i:6 	 global-step:1426	 l-p:0.08107886463403702
epoch£º71	 i:7 	 global-step:1427	 l-p:0.07998162508010864
epoch£º71	 i:8 	 global-step:1428	 l-p:0.07829839736223221
epoch£º71	 i:9 	 global-step:1429	 l-p:0.07892236858606339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8597, 6.8597, 6.8597],
        [6.8597, 6.8597, 6.8597],
        [6.8597, 6.8681, 6.8603],
        [6.8597, 6.8597, 6.8597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.07684395462274551 
model_pd.l_d.mean(): -4.724271297454834 
model_pd.lagr.mean(): -4.647427558898926 
model_pd.lambdas: dict_items([('pout', tensor([0.8449], device='cuda:0')), ('power', tensor([0.2009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7465], device='cuda:0')), ('power', tensor([-20.2664], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.07684395462274551
epoch£º72	 i:1 	 global-step:1441	 l-p:0.0804014727473259
epoch£º72	 i:2 	 global-step:1442	 l-p:0.07754599303007126
epoch£º72	 i:3 	 global-step:1443	 l-p:0.08239223062992096
epoch£º72	 i:4 	 global-step:1444	 l-p:0.09856641292572021
epoch£º72	 i:5 	 global-step:1445	 l-p:0.08132655173540115
epoch£º72	 i:6 	 global-step:1446	 l-p:0.0736389234662056
epoch£º72	 i:7 	 global-step:1447	 l-p:0.07785037159919739
epoch£º72	 i:8 	 global-step:1448	 l-p:0.07650461047887802
epoch£º72	 i:9 	 global-step:1449	 l-p:0.07741067558526993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.0234, 7.0234, 7.0234],
        [7.0234, 7.3128, 7.2021],
        [7.0234, 8.6625, 9.5066],
        [7.0234, 7.4252, 7.3259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.07540010660886765 
model_pd.l_d.mean(): -4.527090072631836 
model_pd.lagr.mean(): -4.451690196990967 
model_pd.lambdas: dict_items([('pout', tensor([0.8372], device='cuda:0')), ('power', tensor([0.1907], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885], device='cuda:0')), ('power', tensor([-20.1635], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.07540010660886765
epoch£º73	 i:1 	 global-step:1461	 l-p:0.07815846055746078
epoch£º73	 i:2 	 global-step:1462	 l-p:0.07878020405769348
epoch£º73	 i:3 	 global-step:1463	 l-p:0.08259990811347961
epoch£º73	 i:4 	 global-step:1464	 l-p:0.07750766724348068
epoch£º73	 i:5 	 global-step:1465	 l-p:0.07967347651720047
epoch£º73	 i:6 	 global-step:1466	 l-p:0.07920579612255096
epoch£º73	 i:7 	 global-step:1467	 l-p:0.07905133068561554
epoch£º73	 i:8 	 global-step:1468	 l-p:0.08234832435846329
epoch£º73	 i:9 	 global-step:1469	 l-p:0.07384894043207169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.1320, 7.1671, 7.1377],
        [7.1320, 7.8005, 7.7958],
        [7.1320, 8.4069, 8.8612],
        [7.1320, 7.5398, 7.4384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.0809236690402031 
model_pd.l_d.mean(): -4.314249038696289 
model_pd.lagr.mean(): -4.233325481414795 
model_pd.lambdas: dict_items([('pout', tensor([0.8293], device='cuda:0')), ('power', tensor([0.1806], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7750], device='cuda:0')), ('power', tensor([-20.2143], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.0809236690402031
epoch£º74	 i:1 	 global-step:1481	 l-p:0.0885246992111206
epoch£º74	 i:2 	 global-step:1482	 l-p:0.07597965002059937
epoch£º74	 i:3 	 global-step:1483	 l-p:0.07426951825618744
epoch£º74	 i:4 	 global-step:1484	 l-p:0.07759946584701538
epoch£º74	 i:5 	 global-step:1485	 l-p:0.08040915429592133
epoch£º74	 i:6 	 global-step:1486	 l-p:0.07701422274112701
epoch£º74	 i:7 	 global-step:1487	 l-p:0.07750362157821655
epoch£º74	 i:8 	 global-step:1488	 l-p:0.06742381304502487
epoch£º74	 i:9 	 global-step:1489	 l-p:0.07735788822174072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.2612, 7.8838, 7.8494],
        [7.2612, 7.6616, 7.5551],
        [7.2612, 7.2612, 7.2612],
        [7.2612, 8.6595, 9.2163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.07892312109470367 
model_pd.l_d.mean(): -4.1514410972595215 
model_pd.lagr.mean(): -4.0725178718566895 
model_pd.lambdas: dict_items([('pout', tensor([0.8211], device='cuda:0')), ('power', tensor([0.1705], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8363], device='cuda:0')), ('power', tensor([-20.1986], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.07892312109470367
epoch£º75	 i:1 	 global-step:1501	 l-p:0.07680706679821014
epoch£º75	 i:2 	 global-step:1502	 l-p:0.08483637869358063
epoch£º75	 i:3 	 global-step:1503	 l-p:0.07764579355716705
epoch£º75	 i:4 	 global-step:1504	 l-p:0.0758722797036171
epoch£º75	 i:5 	 global-step:1505	 l-p:0.0763198584318161
epoch£º75	 i:6 	 global-step:1506	 l-p:0.07654432952404022
epoch£º75	 i:7 	 global-step:1507	 l-p:0.07820765674114227
epoch£º75	 i:8 	 global-step:1508	 l-p:0.07680171728134155
epoch£º75	 i:9 	 global-step:1509	 l-p:0.06467098742723465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.4067, 7.4079, 7.4068],
        [7.4067, 7.4068, 7.4067],
        [7.4067, 8.7794, 9.2927],
        [7.4067, 7.8586, 7.7580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.07079720497131348 
model_pd.l_d.mean(): -3.8689937591552734 
model_pd.lagr.mean(): -3.79819655418396 
model_pd.lambdas: dict_items([('pout', tensor([0.8127], device='cuda:0')), ('power', tensor([0.1605], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8183], device='cuda:0')), ('power', tensor([-19.8397], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.07079720497131348
epoch£º76	 i:1 	 global-step:1521	 l-p:0.07014568150043488
epoch£º76	 i:2 	 global-step:1522	 l-p:0.07759883999824524
epoch£º76	 i:3 	 global-step:1523	 l-p:0.07559269666671753
epoch£º76	 i:4 	 global-step:1524	 l-p:0.08173865079879761
epoch£º76	 i:5 	 global-step:1525	 l-p:0.07708095014095306
epoch£º76	 i:6 	 global-step:1526	 l-p:0.07280005514621735
epoch£º76	 i:7 	 global-step:1527	 l-p:0.07617580145597458
epoch£º76	 i:8 	 global-step:1528	 l-p:0.07607684284448624
epoch£º76	 i:9 	 global-step:1529	 l-p:0.0760168507695198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.6589, 7.6956, 7.6647],
        [7.6589, 7.6597, 7.6589],
        [7.6589, 7.7474, 7.6832],
        [7.6589, 7.8682, 7.7580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.07540301233530045 
model_pd.l_d.mean(): -3.7774131298065186 
model_pd.lagr.mean(): -3.702010154724121 
model_pd.lambdas: dict_items([('pout', tensor([0.8039], device='cuda:0')), ('power', tensor([0.1505], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9283], device='cuda:0')), ('power', tensor([-20.0019], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.07540301233530045
epoch£º77	 i:1 	 global-step:1541	 l-p:0.0790056437253952
epoch£º77	 i:2 	 global-step:1542	 l-p:0.07257954776287079
epoch£º77	 i:3 	 global-step:1543	 l-p:0.07619799673557281
epoch£º77	 i:4 	 global-step:1544	 l-p:0.0743110179901123
epoch£º77	 i:5 	 global-step:1545	 l-p:0.07522628456354141
epoch£º77	 i:6 	 global-step:1546	 l-p:0.07495568692684174
epoch£º77	 i:7 	 global-step:1547	 l-p:0.0639035701751709
epoch£º77	 i:8 	 global-step:1548	 l-p:0.0751967504620552
epoch£º77	 i:9 	 global-step:1549	 l-p:0.07031849026679993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.9270,  9.1518,  9.4726],
        [ 7.9270,  9.6808, 10.5090],
        [ 7.9270,  8.1523,  8.0360],
        [ 7.9270,  8.0434,  7.9640]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.07544999569654465 
model_pd.l_d.mean(): -3.5083327293395996 
model_pd.lagr.mean(): -3.432882785797119 
model_pd.lambdas: dict_items([('pout', tensor([0.7947], device='cuda:0')), ('power', tensor([0.1407], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9303], device='cuda:0')), ('power', tensor([-19.5412], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.07544999569654465
epoch£º78	 i:1 	 global-step:1561	 l-p:0.07466881722211838
epoch£º78	 i:2 	 global-step:1562	 l-p:0.0732952281832695
epoch£º78	 i:3 	 global-step:1563	 l-p:0.07466541975736618
epoch£º78	 i:4 	 global-step:1564	 l-p:0.0655292198061943
epoch£º78	 i:5 	 global-step:1565	 l-p:0.07674919068813324
epoch£º78	 i:6 	 global-step:1566	 l-p:0.06126195192337036
epoch£º78	 i:7 	 global-step:1567	 l-p:0.07335952669382095
epoch£º78	 i:8 	 global-step:1568	 l-p:0.07286940515041351
epoch£º78	 i:9 	 global-step:1569	 l-p:0.07023753225803375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 8.2705,  9.6609, 10.0883],
        [ 8.2705,  8.2705,  8.2705],
        [ 8.2705,  8.2745,  8.2707],
        [ 8.2705,  8.2811,  8.2713]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.0729389414191246 
model_pd.l_d.mean(): -3.3677210807800293 
model_pd.lagr.mean(): -3.2947821617126465 
model_pd.lambdas: dict_items([('pout', tensor([0.7850], device='cuda:0')), ('power', tensor([0.1310], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0224], device='cuda:0')), ('power', tensor([-19.4336], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.0729389414191246
epoch£º79	 i:1 	 global-step:1581	 l-p:0.0693412572145462
epoch£º79	 i:2 	 global-step:1582	 l-p:0.0726713091135025
epoch£º79	 i:3 	 global-step:1583	 l-p:0.0727996975183487
epoch£º79	 i:4 	 global-step:1584	 l-p:0.07254429161548615
epoch£º79	 i:5 	 global-step:1585	 l-p:0.07450978457927704
epoch£º79	 i:6 	 global-step:1586	 l-p:0.06298519670963287
epoch£º79	 i:7 	 global-step:1587	 l-p:-0.059193432331085205
epoch£º79	 i:8 	 global-step:1588	 l-p:0.07172388583421707
epoch£º79	 i:9 	 global-step:1589	 l-p:0.07224354892969131
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 8.7442, 11.6929, 13.8104],
        [ 8.7442,  8.7871,  8.7510],
        [ 8.7442,  8.7442,  8.7442],
        [ 8.7442, 10.8582, 11.9511]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.06824343651533127 
model_pd.l_d.mean(): -3.0886549949645996 
model_pd.lagr.mean(): -3.020411491394043 
model_pd.lambdas: dict_items([('pout', tensor([0.7747], device='cuda:0')), ('power', tensor([0.1215], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.0504], device='cuda:0')), ('power', tensor([-18.5769], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.06824343651533127
epoch£º80	 i:1 	 global-step:1601	 l-p:0.07184923440217972
epoch£º80	 i:2 	 global-step:1602	 l-p:0.07084210216999054
epoch£º80	 i:3 	 global-step:1603	 l-p:0.07147146761417389
epoch£º80	 i:4 	 global-step:1604	 l-p:0.06730903685092926
epoch£º80	 i:5 	 global-step:1605	 l-p:0.07133612781763077
epoch£º80	 i:6 	 global-step:1606	 l-p:0.07060622423887253
epoch£º80	 i:7 	 global-step:1607	 l-p:0.05923847109079361
epoch£º80	 i:8 	 global-step:1608	 l-p:0.07026293873786926
epoch£º80	 i:9 	 global-step:1609	 l-p:0.070553719997406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 9.3475,  9.3641,  9.3490],
        [ 9.3475,  9.3476,  9.3475],
        [ 9.3475, 11.6357, 12.8242],
        [ 9.3475,  9.5376,  9.4207]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.06998897343873978 
model_pd.l_d.mean(): -2.97084903717041 
model_pd.lagr.mean(): -2.900860071182251 
model_pd.lambdas: dict_items([('pout', tensor([0.7635], device='cuda:0')), ('power', tensor([0.1122], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1685], device='cuda:0')), ('power', tensor([-18.3660], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.06998897343873978
epoch£º81	 i:1 	 global-step:1621	 l-p:0.06924209743738174
epoch£º81	 i:2 	 global-step:1622	 l-p:0.06999541074037552
epoch£º81	 i:3 	 global-step:1623	 l-p:0.075440913438797
epoch£º81	 i:4 	 global-step:1624	 l-p:0.12830597162246704
epoch£º81	 i:5 	 global-step:1625	 l-p:0.06494543701410294
epoch£º81	 i:6 	 global-step:1626	 l-p:0.06848320364952087
epoch£º81	 i:7 	 global-step:1627	 l-p:0.06879004091024399
epoch£º81	 i:8 	 global-step:1628	 l-p:0.06138653680682182
epoch£º81	 i:9 	 global-step:1629	 l-p:0.0683574378490448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[10.0455, 10.2520, 10.1250],
        [10.0455, 11.8391, 12.4272],
        [10.0455, 10.6465, 10.4951],
        [10.0455, 12.8139, 14.4349]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.06819586455821991 
model_pd.l_d.mean(): -2.797285556793213 
model_pd.lagr.mean(): -2.7290897369384766 
model_pd.lambdas: dict_items([('pout', tensor([0.7514], device='cuda:0')), ('power', tensor([0.1032], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2675], device='cuda:0')), ('power', tensor([-17.7054], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.06819586455821991
epoch£º82	 i:1 	 global-step:1641	 l-p:0.06832803785800934
epoch£º82	 i:2 	 global-step:1642	 l-p:0.06738416105508804
epoch£º82	 i:3 	 global-step:1643	 l-p:0.06755727529525757
epoch£º82	 i:4 	 global-step:1644	 l-p:0.06779985129833221
epoch£º82	 i:5 	 global-step:1645	 l-p:0.0748501569032669
epoch£º82	 i:6 	 global-step:1646	 l-p:0.05141015723347664
epoch£º82	 i:7 	 global-step:1647	 l-p:0.06273482739925385
epoch£º82	 i:8 	 global-step:1648	 l-p:0.06715597212314606
epoch£º82	 i:9 	 global-step:1649	 l-p:0.07079102098941803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[10.5745, 13.0715, 14.2940],
        [10.5745, 11.2710, 11.1240],
        [10.5745, 10.5766, 10.5746],
        [10.5745, 12.0065, 12.2425]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.07260646671056747 
model_pd.l_d.mean(): -2.570396661758423 
model_pd.lagr.mean(): -2.4977900981903076 
model_pd.lambdas: dict_items([('pout', tensor([0.7386], device='cuda:0')), ('power', tensor([0.0946], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2838], device='cuda:0')), ('power', tensor([-16.9857], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.07260646671056747
epoch£º83	 i:1 	 global-step:1661	 l-p:0.06695397198200226
epoch£º83	 i:2 	 global-step:1662	 l-p:0.06640887260437012
epoch£º83	 i:3 	 global-step:1663	 l-p:0.06119481846690178
epoch£º83	 i:4 	 global-step:1664	 l-p:0.06637824326753616
epoch£º83	 i:5 	 global-step:1665	 l-p:0.06674473732709885
epoch£º83	 i:6 	 global-step:1666	 l-p:0.06656985729932785
epoch£º83	 i:7 	 global-step:1667	 l-p:0.06878197193145752
epoch£º83	 i:8 	 global-step:1668	 l-p:0.06755021214485168
epoch£º83	 i:9 	 global-step:1669	 l-p:0.11930913478136063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[11.0371, 11.0377, 11.0371],
        [11.0371, 11.0663, 11.0402],
        [11.0371, 11.2816, 11.1350],
        [11.0371, 11.7689, 11.6150]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.06619764119386673 
model_pd.l_d.mean(): -2.463921070098877 
model_pd.lagr.mean(): -2.397723436355591 
model_pd.lambdas: dict_items([('pout', tensor([0.7250], device='cuda:0')), ('power', tensor([0.0861], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3818], device='cuda:0')), ('power', tensor([-16.7959], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.06619764119386673
epoch£º84	 i:1 	 global-step:1681	 l-p:0.06283286213874817
epoch£º84	 i:2 	 global-step:1682	 l-p:0.06583606451749802
epoch£º84	 i:3 	 global-step:1683	 l-p:0.0658319815993309
epoch£º84	 i:4 	 global-step:1684	 l-p:0.08079119771718979
epoch£º84	 i:5 	 global-step:1685	 l-p:0.06538086384534836
epoch£º84	 i:6 	 global-step:1686	 l-p:0.06511679291725159
epoch£º84	 i:7 	 global-step:1687	 l-p:0.0659630224108696
epoch£º84	 i:8 	 global-step:1688	 l-p:0.06503000855445862
epoch£º84	 i:9 	 global-step:1689	 l-p:0.0652492344379425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[11.7551, 14.1020, 15.0024],
        [11.7551, 13.3004, 13.5191],
        [11.7551, 14.4109, 15.6243],
        [11.7551, 12.8624, 12.8177]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.06522427499294281 
model_pd.l_d.mean(): -2.293006420135498 
model_pd.lagr.mean(): -2.2277822494506836 
model_pd.lambdas: dict_items([('pout', tensor([0.7108], device='cuda:0')), ('power', tensor([0.0779], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4498], device='cuda:0')), ('power', tensor([-16.0046], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.06522427499294281
epoch£º85	 i:1 	 global-step:1701	 l-p:0.06548593938350677
epoch£º85	 i:2 	 global-step:1702	 l-p:0.06497710198163986
epoch£º85	 i:3 	 global-step:1703	 l-p:0.06469134986400604
epoch£º85	 i:4 	 global-step:1704	 l-p:0.06474174559116364
epoch£º85	 i:5 	 global-step:1705	 l-p:0.0658470019698143
epoch£º85	 i:6 	 global-step:1706	 l-p:0.0645127221941948
epoch£º85	 i:7 	 global-step:1707	 l-p:0.06636630743741989
epoch£º85	 i:8 	 global-step:1708	 l-p:0.06423284113407135
epoch£º85	 i:9 	 global-step:1709	 l-p:-0.09439989924430847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[12.3080, 15.1159, 16.4079],
        [12.3080, 12.3459, 12.3124],
        [12.3080, 12.3081, 12.3080],
        [12.3080, 13.4088, 13.3320]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.07034478336572647 
model_pd.l_d.mean(): -1.9898427724838257 
model_pd.lagr.mean(): -1.9194979667663574 
model_pd.lambdas: dict_items([('pout', tensor([0.6961], device='cuda:0')), ('power', tensor([0.0701], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.3395], device='cuda:0')), ('power', tensor([-14.8992], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.07034478336572647
epoch£º86	 i:1 	 global-step:1721	 l-p:0.06459266692399979
epoch£º86	 i:2 	 global-step:1722	 l-p:0.06389324367046356
epoch£º86	 i:3 	 global-step:1723	 l-p:0.08783713728189468
epoch£º86	 i:4 	 global-step:1724	 l-p:0.06362965703010559
epoch£º86	 i:5 	 global-step:1725	 l-p:0.06383000314235687
epoch£º86	 i:6 	 global-step:1726	 l-p:0.06347117573022842
epoch£º86	 i:7 	 global-step:1727	 l-p:0.06315018236637115
epoch£º86	 i:8 	 global-step:1728	 l-p:0.06291576474905014
epoch£º86	 i:9 	 global-step:1729	 l-p:0.06430760025978088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[13.3270, 13.7160, 13.5101],
        [13.3270, 13.3461, 13.3284],
        [13.3270, 13.3290, 13.3270],
        [13.3270, 15.2580, 15.6252]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.06330670416355133 
model_pd.l_d.mean(): -1.9922783374786377 
model_pd.lagr.mean(): -1.9289716482162476 
model_pd.lambdas: dict_items([('pout', tensor([0.6802], device='cuda:0')), ('power', tensor([0.0626], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5833], device='cuda:0')), ('power', tensor([-14.4146], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.06330670416355133
epoch£º87	 i:1 	 global-step:1741	 l-p:0.06528354436159134
epoch£º87	 i:2 	 global-step:1742	 l-p:0.06341470777988434
epoch£º87	 i:3 	 global-step:1743	 l-p:0.06250278651714325
epoch£º87	 i:4 	 global-step:1744	 l-p:0.06242551654577255
epoch£º87	 i:5 	 global-step:1745	 l-p:0.06360389292240143
epoch£º87	 i:6 	 global-step:1746	 l-p:0.06299193203449249
epoch£º87	 i:7 	 global-step:1747	 l-p:0.06263141334056854
epoch£º87	 i:8 	 global-step:1748	 l-p:0.06672801077365875
epoch£º87	 i:9 	 global-step:1749	 l-p:0.06205029785633087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[14.0837, 14.0837, 14.0837],
        [14.0837, 15.3060, 15.1940],
        [14.0837, 14.5280, 14.3026],
        [14.0837, 14.1085, 14.0857]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.06205624341964722 
model_pd.l_d.mean(): -1.943126916885376 
model_pd.lagr.mean(): -1.881070613861084 
model_pd.lambdas: dict_items([('pout', tensor([0.6635], device='cuda:0')), ('power', tensor([0.0555], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7408], device='cuda:0')), ('power', tensor([-13.9608], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.06205624341964722
epoch£º88	 i:1 	 global-step:1761	 l-p:0.06206715479493141
epoch£º88	 i:2 	 global-step:1762	 l-p:0.0625944584608078
epoch£º88	 i:3 	 global-step:1763	 l-p:0.06207389757037163
epoch£º88	 i:4 	 global-step:1764	 l-p:0.06191646307706833
epoch£º88	 i:5 	 global-step:1765	 l-p:0.0630822479724884
epoch£º88	 i:6 	 global-step:1766	 l-p:0.061620693653821945
epoch£º88	 i:7 	 global-step:1767	 l-p:0.06151605024933815
epoch£º88	 i:8 	 global-step:1768	 l-p:0.06313728541135788
epoch£º88	 i:9 	 global-step:1769	 l-p:0.0650494396686554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[14.7723, 18.7615, 20.9640],
        [14.7723, 14.8939, 14.7978],
        [14.7723, 14.9355, 14.8135],
        [14.7723, 15.4811, 15.2250]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.061955876648426056 
model_pd.l_d.mean(): -1.7361235618591309 
model_pd.lagr.mean(): -1.6741676330566406 
model_pd.lambdas: dict_items([('pout', tensor([0.6463], device='cuda:0')), ('power', tensor([0.0488], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6853], device='cuda:0')), ('power', tensor([-13.0145], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.061955876648426056
epoch£º89	 i:1 	 global-step:1781	 l-p:0.06139601394534111
epoch£º89	 i:2 	 global-step:1782	 l-p:0.061921775341033936
epoch£º89	 i:3 	 global-step:1783	 l-p:0.06154759228229523
epoch£º89	 i:4 	 global-step:1784	 l-p:0.06201125308871269
epoch£º89	 i:5 	 global-step:1785	 l-p:0.064703069627285
epoch£º89	 i:6 	 global-step:1786	 l-p:0.06142671778798103
epoch£º89	 i:7 	 global-step:1787	 l-p:0.06102670356631279
epoch£º89	 i:8 	 global-step:1788	 l-p:0.06100446730852127
epoch£º89	 i:9 	 global-step:1789	 l-p:0.0607076995074749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.5715, 20.1454, 22.8930],
        [15.5715, 15.5715, 15.5715],
        [15.5715, 15.5717, 15.5715],
        [15.5715, 15.7604, 15.6219]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.061919793486595154 
model_pd.l_d.mean(): -1.6711561679840088 
model_pd.lagr.mean(): -1.6092363595962524 
model_pd.lambdas: dict_items([('pout', tensor([0.6283], device='cuda:0')), ('power', tensor([0.0425], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8115], device='cuda:0')), ('power', tensor([-12.2921], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.061919793486595154
epoch£º90	 i:1 	 global-step:1801	 l-p:0.06304486095905304
epoch£º90	 i:2 	 global-step:1802	 l-p:0.061101216822862625
epoch£º90	 i:3 	 global-step:1803	 l-p:0.060763776302337646
epoch£º90	 i:4 	 global-step:1804	 l-p:0.06067627668380737
epoch£º90	 i:5 	 global-step:1805	 l-p:0.06048727035522461
epoch£º90	 i:6 	 global-step:1806	 l-p:0.060381244868040085
epoch£º90	 i:7 	 global-step:1807	 l-p:0.06037424877285957
epoch£º90	 i:8 	 global-step:1808	 l-p:0.06021106615662575
epoch£º90	 i:9 	 global-step:1809	 l-p:0.06095857918262482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.5360, 16.5360, 16.5360],
        [16.5360, 16.5382, 16.5360],
        [16.5360, 16.5711, 16.5392],
        [16.5360, 20.1496, 21.6693]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.06097032502293587 
model_pd.l_d.mean(): -1.5889149904251099 
model_pd.lagr.mean(): -1.5279446840286255 
model_pd.lambdas: dict_items([('pout', tensor([0.6096], device='cuda:0')), ('power', tensor([0.0366], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9068], device='cuda:0')), ('power', tensor([-11.3912], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.06097032502293587
epoch£º91	 i:1 	 global-step:1821	 l-p:0.06003382429480553
epoch£º91	 i:2 	 global-step:1822	 l-p:0.06016019359230995
epoch£º91	 i:3 	 global-step:1823	 l-p:0.06147296354174614
epoch£º91	 i:4 	 global-step:1824	 l-p:0.0597752183675766
epoch£º91	 i:5 	 global-step:1825	 l-p:0.06009633094072342
epoch£º91	 i:6 	 global-step:1826	 l-p:0.05976536497473717
epoch£º91	 i:7 	 global-step:1827	 l-p:0.05963848903775215
epoch£º91	 i:8 	 global-step:1828	 l-p:0.05946856364607811
epoch£º91	 i:9 	 global-step:1829	 l-p:0.06016542389988899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.6964, 17.6964, 17.6964],
        [17.6964, 22.7489, 25.6710],
        [17.6964, 18.5189, 18.2068],
        [17.6964, 18.0424, 17.8206]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.059593550860881805 
model_pd.l_d.mean(): -1.4707863330841064 
model_pd.lagr.mean(): -1.411192774772644 
model_pd.lambdas: dict_items([('pout', tensor([0.5901], device='cuda:0')), ('power', tensor([0.0312], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9387], device='cuda:0')), ('power', tensor([-10.2001], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.059593550860881805
epoch£º92	 i:1 	 global-step:1841	 l-p:0.05919242650270462
epoch£º92	 i:2 	 global-step:1842	 l-p:0.05963126942515373
epoch£º92	 i:3 	 global-step:1843	 l-p:0.05953224003314972
epoch£º92	 i:4 	 global-step:1844	 l-p:0.05912588909268379
epoch£º92	 i:5 	 global-step:1845	 l-p:0.05957280844449997
epoch£º92	 i:6 	 global-step:1846	 l-p:0.058867499232292175
epoch£º92	 i:7 	 global-step:1847	 l-p:0.059816379100084305
epoch£º92	 i:8 	 global-step:1848	 l-p:0.05917046591639519
epoch£º92	 i:9 	 global-step:1849	 l-p:0.0587594099342823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.0676, 19.0676, 19.0676],
        [19.0676, 21.1725, 21.2206],
        [19.0676, 21.3517, 21.5060],
        [19.0676, 20.6787, 20.4905]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.05891840159893036 
model_pd.l_d.mean(): -1.429516315460205 
model_pd.lagr.mean(): -1.3705979585647583 
model_pd.lambdas: dict_items([('pout', tensor([0.5696], device='cuda:0')), ('power', tensor([0.0264], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.0833], device='cuda:0')), ('power', tensor([-8.8926], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.05891840159893036
epoch£º93	 i:1 	 global-step:1861	 l-p:0.05854591727256775
epoch£º93	 i:2 	 global-step:1862	 l-p:0.05877542495727539
epoch£º93	 i:3 	 global-step:1863	 l-p:0.05856014043092728
epoch£º93	 i:4 	 global-step:1864	 l-p:0.059092070907354355
epoch£º93	 i:5 	 global-step:1865	 l-p:0.05823289602994919
epoch£º93	 i:6 	 global-step:1866	 l-p:0.0583229623734951
epoch£º93	 i:7 	 global-step:1867	 l-p:0.05863439291715622
epoch£º93	 i:8 	 global-step:1868	 l-p:0.058166854083538055
epoch£º93	 i:9 	 global-step:1869	 l-p:0.0581144317984581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.6549, 23.9089, 24.6250],
        [20.6549, 20.6549, 20.6549],
        [20.6549, 20.9605, 20.7461],
        [20.6549, 20.7571, 20.6703]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.057869452983140945 
model_pd.l_d.mean(): -1.4100929498672485 
model_pd.lagr.mean(): -1.352223515510559 
model_pd.lambdas: dict_items([('pout', tensor([0.5481], device='cuda:0')), ('power', tensor([0.0223], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2534], device='cuda:0')), ('power', tensor([-7.4925], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.057869452983140945
epoch£º94	 i:1 	 global-step:1881	 l-p:0.058443427085876465
epoch£º94	 i:2 	 global-step:1882	 l-p:0.05789327621459961
epoch£º94	 i:3 	 global-step:1883	 l-p:0.05778208747506142
epoch£º94	 i:4 	 global-step:1884	 l-p:0.05790938809514046
epoch£º94	 i:5 	 global-step:1885	 l-p:0.05761462077498436
epoch£º94	 i:6 	 global-step:1886	 l-p:0.057542599737644196
epoch£º94	 i:7 	 global-step:1887	 l-p:0.057430606335401535
epoch£º94	 i:8 	 global-step:1888	 l-p:0.05789266526699066
epoch£º94	 i:9 	 global-step:1889	 l-p:0.05744369700551033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.4408, 22.4413, 22.4408],
        [22.4408, 22.4408, 22.4408],
        [22.4408, 22.4524, 22.4413],
        [22.4408, 22.5640, 22.4605]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.05748802050948143 
model_pd.l_d.mean(): -1.3012828826904297 
model_pd.lagr.mean(): -1.2437949180603027 
model_pd.lambdas: dict_items([('pout', tensor([0.5256], device='cuda:0')), ('power', tensor([0.0191], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2621], device='cuda:0')), ('power', tensor([-5.5276], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.05748802050948143
epoch£º95	 i:1 	 global-step:1901	 l-p:0.05718633532524109
epoch£º95	 i:2 	 global-step:1902	 l-p:0.05714139714837074
epoch£º95	 i:3 	 global-step:1903	 l-p:0.05736108496785164
epoch£º95	 i:4 	 global-step:1904	 l-p:0.05717797949910164
epoch£º95	 i:5 	 global-step:1905	 l-p:0.056906700134277344
epoch£º95	 i:6 	 global-step:1906	 l-p:0.05688384920358658
epoch£º95	 i:7 	 global-step:1907	 l-p:0.05690774694085121
epoch£º95	 i:8 	 global-step:1908	 l-p:0.05719621852040291
epoch£º95	 i:9 	 global-step:1909	 l-p:0.0566527359187603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.3825, 32.3062, 37.4335],
        [24.3825, 24.3883, 24.3827],
        [24.3825, 29.8304, 32.1211],
        [24.3825, 24.3825, 24.3825]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.05693100765347481 
model_pd.l_d.mean(): -1.2091559171676636 
model_pd.lagr.mean(): -1.1522248983383179 
model_pd.lambdas: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([0.0168], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2803], device='cuda:0')), ('power', tensor([-3.4720], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.05693100765347481
epoch£º96	 i:1 	 global-step:1921	 l-p:0.05665702372789383
epoch£º96	 i:2 	 global-step:1922	 l-p:0.05649422109127045
epoch£º96	 i:3 	 global-step:1923	 l-p:0.05648699775338173
epoch£º96	 i:4 	 global-step:1924	 l-p:0.056377146393060684
epoch£º96	 i:5 	 global-step:1925	 l-p:0.0567195788025856
epoch£º96	 i:6 	 global-step:1926	 l-p:0.05627376586198807
epoch£º96	 i:7 	 global-step:1927	 l-p:0.056361787021160126
epoch£º96	 i:8 	 global-step:1928	 l-p:0.056191958487033844
epoch£º96	 i:9 	 global-step:1929	 l-p:0.056296877562999725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3952, 30.1383, 30.7080],
        [26.3952, 32.7286, 35.6476],
        [26.3952, 26.3954, 26.3952],
        [26.3952, 35.6931, 42.1542]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.05609709769487381 
model_pd.l_d.mean(): -1.2458174228668213 
model_pd.lagr.mean(): -1.1897202730178833 
model_pd.lambdas: dict_items([('pout', tensor([0.4772], device='cuda:0')), ('power', tensor([0.0155], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5392], device='cuda:0')), ('power', tensor([-1.7686], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.05609709769487381
epoch£º97	 i:1 	 global-step:1941	 l-p:0.056047506630420685
epoch£º97	 i:2 	 global-step:1942	 l-p:0.05608396232128143
epoch£º97	 i:3 	 global-step:1943	 l-p:0.05612713843584061
epoch£º97	 i:4 	 global-step:1944	 l-p:0.05597022920846939
epoch£º97	 i:5 	 global-step:1945	 l-p:0.05585019290447235
epoch£º97	 i:6 	 global-step:1946	 l-p:0.05583319440484047
epoch£º97	 i:7 	 global-step:1947	 l-p:0.05576007813215256
epoch£º97	 i:8 	 global-step:1948	 l-p:0.05581602826714516
epoch£º97	 i:9 	 global-step:1949	 l-p:0.05601024627685547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3341, 31.7533, 31.9573],
        [28.3341, 28.3341, 28.3341],
        [28.3341, 28.7598, 28.4608],
        [28.3341, 33.6628, 35.3372]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.05564600229263306 
model_pd.l_d.mean(): -1.1890928745269775 
model_pd.lagr.mean(): -1.1334469318389893 
model_pd.lambdas: dict_items([('pout', tensor([0.4516], device='cuda:0')), ('power', tensor([0.0152], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6249], device='cuda:0')), ('power', tensor([0.2074], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.05564600229263306
epoch£º98	 i:1 	 global-step:1961	 l-p:0.05573428049683571
epoch£º98	 i:2 	 global-step:1962	 l-p:0.055736131966114044
epoch£º98	 i:3 	 global-step:1963	 l-p:0.05575280636548996
epoch£º98	 i:4 	 global-step:1964	 l-p:0.05545956641435623
epoch£º98	 i:5 	 global-step:1965	 l-p:0.05544043332338333
epoch£º98	 i:6 	 global-step:1966	 l-p:0.05550149828195572
epoch£º98	 i:7 	 global-step:1967	 l-p:0.05545160174369812
epoch£º98	 i:8 	 global-step:1968	 l-p:0.05550910532474518
epoch£º98	 i:9 	 global-step:1969	 l-p:0.05531970039010048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[29.9979, 29.9979, 29.9979],
        [29.9979, 29.9992, 29.9979],
        [29.9979, 34.2813, 34.9357],
        [29.9979, 29.9986, 29.9979]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.05547933280467987 
model_pd.l_d.mean(): -1.1233463287353516 
model_pd.lagr.mean(): -1.0678670406341553 
model_pd.lambdas: dict_items([('pout', tensor([0.4251], device='cuda:0')), ('power', tensor([0.0158], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6949], device='cuda:0')), ('power', tensor([1.8781], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.05547933280467987
epoch£º99	 i:1 	 global-step:1981	 l-p:0.05529389902949333
epoch£º99	 i:2 	 global-step:1982	 l-p:0.055286161601543427
epoch£º99	 i:3 	 global-step:1983	 l-p:0.055229902267456055
epoch£º99	 i:4 	 global-step:1984	 l-p:0.055261243134737015
epoch£º99	 i:5 	 global-step:1985	 l-p:0.055282700806856155
epoch£º99	 i:6 	 global-step:1986	 l-p:0.05531241372227669
epoch£º99	 i:7 	 global-step:1987	 l-p:0.05526071786880493
epoch£º99	 i:8 	 global-step:1988	 l-p:0.055159490555524826
epoch£º99	 i:9 	 global-step:1989	 l-p:0.05513404309749603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[31.1392, 31.1468, 31.1394],
        [31.1392, 35.9882, 36.9659],
        [31.1392, 32.1125, 31.5998],
        [31.1392, 31.1392, 31.1392]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.05507752299308777 
model_pd.l_d.mean(): -1.0692641735076904 
model_pd.lagr.mean(): -1.0141866207122803 
model_pd.lambdas: dict_items([('pout', tensor([0.3979], device='cuda:0')), ('power', tensor([0.0170], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7909], device='cuda:0')), ('power', tensor([2.9044], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.05507752299308777
epoch£º100	 i:1 	 global-step:2001	 l-p:0.05508625879883766
epoch£º100	 i:2 	 global-step:2002	 l-p:0.055214330554008484
epoch£º100	 i:3 	 global-step:2003	 l-p:0.05512530729174614
epoch£º100	 i:4 	 global-step:2004	 l-p:0.055064864456653595
epoch£º100	 i:5 	 global-step:2005	 l-p:0.05511881411075592
epoch£º100	 i:6 	 global-step:2006	 l-p:0.05519647151231766
epoch£º100	 i:7 	 global-step:2007	 l-p:0.055030472576618195
epoch£º100	 i:8 	 global-step:2008	 l-p:0.05510856583714485
epoch£º100	 i:9 	 global-step:2009	 l-p:0.05517955124378204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[31.5426, 32.2361, 31.8047],
        [31.5426, 31.5431, 31.5426],
        [31.5426, 31.5685, 31.5439],
        [31.5426, 31.5426, 31.5426]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.05506618320941925 
model_pd.l_d.mean(): -0.9700226783752441 
model_pd.lagr.mean(): -0.9149565100669861 
model_pd.lambdas: dict_items([('pout', tensor([0.3705], device='cuda:0')), ('power', tensor([0.0187], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7658], device='cuda:0')), ('power', tensor([3.3724], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.05506618320941925
epoch£º101	 i:1 	 global-step:2021	 l-p:0.055062759667634964
epoch£º101	 i:2 	 global-step:2022	 l-p:0.05513976886868477
epoch£º101	 i:3 	 global-step:2023	 l-p:0.05503305420279503
epoch£º101	 i:4 	 global-step:2024	 l-p:0.055263739079236984
epoch£º101	 i:5 	 global-step:2025	 l-p:0.055155906826257706
epoch£º101	 i:6 	 global-step:2026	 l-p:0.05509109050035477
epoch£º101	 i:7 	 global-step:2027	 l-p:0.05508996918797493
epoch£º101	 i:8 	 global-step:2028	 l-p:0.055077504366636276
epoch£º101	 i:9 	 global-step:2029	 l-p:0.05517001822590828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[31.1179, 37.6777, 40.1669],
        [31.1179, 37.7172, 40.2448],
        [31.1179, 41.7465, 48.8872],
        [31.1179, 32.8947, 32.3433]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.055130284279584885 
model_pd.l_d.mean(): -0.8826948404312134 
model_pd.lagr.mean(): -0.827564537525177 
model_pd.lambdas: dict_items([('pout', tensor([0.3431], device='cuda:0')), ('power', tensor([0.0203], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.7245], device='cuda:0')), ('power', tensor([2.9600], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.055130284279584885
epoch£º102	 i:1 	 global-step:2041	 l-p:0.05515020713210106
epoch£º102	 i:2 	 global-step:2042	 l-p:0.05535352975130081
epoch£º102	 i:3 	 global-step:2043	 l-p:0.05532081425189972
epoch£º102	 i:4 	 global-step:2044	 l-p:0.05515678599476814
epoch£º102	 i:5 	 global-step:2045	 l-p:0.055190153419971466
epoch£º102	 i:6 	 global-step:2046	 l-p:0.05518181249499321
epoch£º102	 i:7 	 global-step:2047	 l-p:0.05526512861251831
epoch£º102	 i:8 	 global-step:2048	 l-p:0.0554639995098114
epoch£º102	 i:9 	 global-step:2049	 l-p:0.055325593799352646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[29.9629, 31.0135, 30.4983],
        [29.9629, 33.3662, 33.4514],
        [29.9629, 30.9455, 30.4429],
        [29.9629, 30.0881, 29.9796]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.055359646677970886 
model_pd.l_d.mean(): -0.8081294298171997 
model_pd.lagr.mean(): -0.7527697682380676 
model_pd.lambdas: dict_items([('pout', tensor([0.3161], device='cuda:0')), ('power', tensor([0.0215], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6596], device='cuda:0')), ('power', tensor([1.8513], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.055359646677970886
epoch£º103	 i:1 	 global-step:2061	 l-p:0.055332064628601074
epoch£º103	 i:2 	 global-step:2062	 l-p:0.05554618686437607
epoch£º103	 i:3 	 global-step:2063	 l-p:0.05567887797951698
epoch£º103	 i:4 	 global-step:2064	 l-p:0.05555674433708191
epoch£º103	 i:5 	 global-step:2065	 l-p:0.05545991659164429
epoch£º103	 i:6 	 global-step:2066	 l-p:0.055525898933410645
epoch£º103	 i:7 	 global-step:2067	 l-p:0.05553535372018814
epoch£º103	 i:8 	 global-step:2068	 l-p:0.05562710762023926
epoch£º103	 i:9 	 global-step:2069	 l-p:0.05561703443527222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3382, 28.3581, 28.3391],
        [28.3382, 29.7542, 29.2406],
        [28.3382, 28.9277, 28.5540],
        [28.3382, 35.9039, 39.8631]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.05561590939760208 
model_pd.l_d.mean(): -0.7773631811141968 
model_pd.lagr.mean(): -0.7217472791671753 
model_pd.lambdas: dict_items([('pout', tensor([0.2896], device='cuda:0')), ('power', tensor([0.0219], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6663], device='cuda:0')), ('power', tensor([0.0830], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.05561590939760208
epoch£º104	 i:1 	 global-step:2081	 l-p:0.055701617151498795
epoch£º104	 i:2 	 global-step:2082	 l-p:0.055683888494968414
epoch£º104	 i:3 	 global-step:2083	 l-p:0.056172989308834076
epoch£º104	 i:4 	 global-step:2084	 l-p:0.055941566824913025
epoch£º104	 i:5 	 global-step:2085	 l-p:0.05591682344675064
epoch£º104	 i:6 	 global-step:2086	 l-p:0.055861663073301315
epoch£º104	 i:7 	 global-step:2087	 l-p:0.05603807792067528
epoch£º104	 i:8 	 global-step:2088	 l-p:0.055938806384801865
epoch£º104	 i:9 	 global-step:2089	 l-p:0.056148186326026917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.5506, 26.5506, 26.5506],
        [26.5506, 26.5506, 26.5506],
        [26.5506, 28.4122, 28.0096],
        [26.5506, 30.9237, 31.9654]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.05605923384428024 
model_pd.l_d.mean(): -0.7139204740524292 
model_pd.lagr.mean(): -0.6578612327575684 
model_pd.lambdas: dict_items([('pout', tensor([0.2640], device='cuda:0')), ('power', tensor([0.0216], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5474], device='cuda:0')), ('power', tensor([-1.6159], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.05605923384428024
epoch£º105	 i:1 	 global-step:2101	 l-p:0.05606377497315407
epoch£º105	 i:2 	 global-step:2102	 l-p:0.05643608793616295
epoch£º105	 i:3 	 global-step:2103	 l-p:0.0561235211789608
epoch£º105	 i:4 	 global-step:2104	 l-p:0.056300707161426544
epoch£º105	 i:5 	 global-step:2105	 l-p:0.056302063167095184
epoch£º105	 i:6 	 global-step:2106	 l-p:0.05633280798792839
epoch£º105	 i:7 	 global-step:2107	 l-p:0.05654546990990639
epoch£º105	 i:8 	 global-step:2108	 l-p:0.05668952316045761
epoch£º105	 i:9 	 global-step:2109	 l-p:0.056613121181726456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.8546, 24.8828, 24.8563],
        [24.8546, 30.1146, 32.1531],
        [24.8546, 25.0296, 24.8871],
        [24.8546, 31.4680, 34.9381]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.0568360798060894 
model_pd.l_d.mean(): -0.6311771273612976 
model_pd.lagr.mean(): -0.5743410587310791 
model_pd.lambdas: dict_items([('pout', tensor([0.2393], device='cuda:0')), ('power', tensor([0.0203], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3531], device='cuda:0')), ('power', tensor([-3.0510], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.0568360798060894
epoch£º106	 i:1 	 global-step:2121	 l-p:0.056509941816329956
epoch£º106	 i:2 	 global-step:2122	 l-p:0.05654279515147209
epoch£º106	 i:3 	 global-step:2123	 l-p:0.056592267006635666
epoch£º106	 i:4 	 global-step:2124	 l-p:0.05667850002646446
epoch£º106	 i:5 	 global-step:2125	 l-p:0.057208966463804245
epoch£º106	 i:6 	 global-step:2126	 l-p:0.056777436286211014
epoch£º106	 i:7 	 global-step:2127	 l-p:0.056775398552417755
epoch£º106	 i:8 	 global-step:2128	 l-p:0.05693738907575607
epoch£º106	 i:9 	 global-step:2129	 l-p:0.05719552934169769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.4102, 23.4341, 23.4116],
        [23.4102, 23.4574, 23.4144],
        [23.4102, 30.3207, 34.3932],
        [23.4102, 29.2035, 31.9999]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.05713700130581856 
model_pd.l_d.mean(): -0.5927979946136475 
model_pd.lagr.mean(): -0.535660982131958 
model_pd.lambdas: dict_items([('pout', tensor([0.2154], device='cuda:0')), ('power', tensor([0.0183], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3303], device='cuda:0')), ('power', tensor([-4.6177], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.05713700130581856
epoch£º107	 i:1 	 global-step:2141	 l-p:0.057002581655979156
epoch£º107	 i:2 	 global-step:2142	 l-p:0.057104893028736115
epoch£º107	 i:3 	 global-step:2143	 l-p:0.05705079808831215
epoch£º107	 i:4 	 global-step:2144	 l-p:0.057432763278484344
epoch£º107	 i:5 	 global-step:2145	 l-p:0.05707474425435066
epoch£º107	 i:6 	 global-step:2146	 l-p:0.057103827595710754
epoch£º107	 i:7 	 global-step:2147	 l-p:0.05770187824964523
epoch£º107	 i:8 	 global-step:2148	 l-p:0.05748474970459938
epoch£º107	 i:9 	 global-step:2149	 l-p:0.05726087465882301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.2910, 22.3823, 22.3032],
        [22.2910, 22.2920, 22.2910],
        [22.2910, 23.0701, 22.6915],
        [22.2910, 22.2911, 22.2910]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.05779784917831421 
model_pd.l_d.mean(): -0.5279505848884583 
model_pd.lagr.mean(): -0.47015273571014404 
model_pd.lambdas: dict_items([('pout', tensor([0.1921], device='cuda:0')), ('power', tensor([0.0156], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2531], device='cuda:0')), ('power', tensor([-5.6488], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.05779784917831421
epoch£º108	 i:1 	 global-step:2161	 l-p:0.057282812893390656
epoch£º108	 i:2 	 global-step:2162	 l-p:0.05730641260743141
epoch£º108	 i:3 	 global-step:2163	 l-p:0.05781690403819084
epoch£º108	 i:4 	 global-step:2164	 l-p:0.05763202905654907
epoch£º108	 i:5 	 global-step:2165	 l-p:0.05758647620677948
epoch£º108	 i:6 	 global-step:2166	 l-p:0.05755733698606491
epoch£º108	 i:7 	 global-step:2167	 l-p:0.05744808912277222
epoch£º108	 i:8 	 global-step:2168	 l-p:0.0576741099357605
epoch£º108	 i:9 	 global-step:2169	 l-p:0.057782191783189774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.5183, 22.1636, 21.8197],
        [21.5183, 21.5316, 21.5189],
        [21.5183, 29.0653, 34.3319],
        [21.5183, 21.5183, 21.5183]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.05766230449080467 
model_pd.l_d.mean(): -0.4695616364479065 
model_pd.lagr.mean(): -0.4118993282318115 
model_pd.lambdas: dict_items([('pout', tensor([0.1694], device='cuda:0')), ('power', tensor([0.0125], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2452], device='cuda:0')), ('power', tensor([-6.5435], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.05766230449080467
epoch£º109	 i:1 	 global-step:2181	 l-p:0.05828438699245453
epoch£º109	 i:2 	 global-step:2182	 l-p:0.0576351024210453
epoch£º109	 i:3 	 global-step:2183	 l-p:0.05759301409125328
epoch£º109	 i:4 	 global-step:2184	 l-p:0.057736385613679886
epoch£º109	 i:5 	 global-step:2185	 l-p:0.05821476876735687
epoch£º109	 i:6 	 global-step:2186	 l-p:0.05777258798480034
epoch£º109	 i:7 	 global-step:2187	 l-p:0.05797058343887329
epoch£º109	 i:8 	 global-step:2188	 l-p:0.05778573453426361
epoch£º109	 i:9 	 global-step:2189	 l-p:0.057710107415914536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.0884, 21.0923, 21.0885],
        [21.0884, 21.0884, 21.0884],
        [21.0884, 21.0967, 21.0887],
        [21.0884, 21.2535, 21.1214]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.05768200382590294 
model_pd.l_d.mean(): -0.40965035557746887 
model_pd.lagr.mean(): -0.35196834802627563 
model_pd.lambdas: dict_items([('pout', tensor([0.1470], device='cuda:0')), ('power', tensor([0.0091], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2936], device='cuda:0')), ('power', tensor([-7.0842], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.05768200382590294
epoch£º110	 i:1 	 global-step:2201	 l-p:0.05770919844508171
epoch£º110	 i:2 	 global-step:2202	 l-p:0.05831474810838699
epoch£º110	 i:3 	 global-step:2203	 l-p:0.05777621269226074
epoch£º110	 i:4 	 global-step:2204	 l-p:0.057839225977659225
epoch£º110	 i:5 	 global-step:2205	 l-p:0.058137815445661545
epoch£º110	 i:6 	 global-step:2206	 l-p:0.05831339582800865
epoch£º110	 i:7 	 global-step:2207	 l-p:0.05808081850409508
epoch£º110	 i:8 	 global-step:2208	 l-p:0.057813484221696854
epoch£º110	 i:9 	 global-step:2209	 l-p:0.05788970738649368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.9898, 21.0271, 20.9928],
        [20.9898, 25.3479, 27.0099],
        [20.9898, 24.3991, 25.2076],
        [20.9898, 26.3859, 29.1323]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.057853810489177704 
model_pd.l_d.mean(): -0.3236684799194336 
model_pd.lagr.mean(): -0.2658146619796753 
model_pd.lambdas: dict_items([('pout', tensor([0.1249], device='cuda:0')), ('power', tensor([0.0056], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2147], device='cuda:0')), ('power', tensor([-7.0523], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.057853810489177704
epoch£º111	 i:1 	 global-step:2221	 l-p:0.057875704020261765
epoch£º111	 i:2 	 global-step:2222	 l-p:0.058022983372211456
epoch£º111	 i:3 	 global-step:2223	 l-p:0.058029163628816605
epoch£º111	 i:4 	 global-step:2224	 l-p:0.057742927223443985
epoch£º111	 i:5 	 global-step:2225	 l-p:0.05776681751012802
epoch£º111	 i:6 	 global-step:2226	 l-p:0.05819699168205261
epoch£º111	 i:7 	 global-step:2227	 l-p:0.0577046275138855
epoch£º111	 i:8 	 global-step:2228	 l-p:0.05780502036213875
epoch£º111	 i:9 	 global-step:2229	 l-p:0.05834108218550682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.2161, 21.7164, 21.4168],
        [21.2161, 21.7017, 21.4072],
        [21.2161, 21.2163, 21.2161],
        [21.2161, 21.3967, 21.2540]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.057682618498802185 
model_pd.l_d.mean(): -0.25479134917259216 
model_pd.lagr.mean(): -0.19710873067378998 
model_pd.lambdas: dict_items([('pout', tensor([0.1027], device='cuda:0')), ('power', tensor([0.0021], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2639], device='cuda:0')), ('power', tensor([-6.9501], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.057682618498802185
epoch£º112	 i:1 	 global-step:2241	 l-p:0.05816514790058136
epoch£º112	 i:2 	 global-step:2242	 l-p:0.05775051936507225
epoch£º112	 i:3 	 global-step:2243	 l-p:0.05765377730131149
epoch£º112	 i:4 	 global-step:2244	 l-p:0.05784992873668671
epoch£º112	 i:5 	 global-step:2245	 l-p:0.05785452574491501
epoch£º112	 i:6 	 global-step:2246	 l-p:0.05771305039525032
epoch£º112	 i:7 	 global-step:2247	 l-p:0.05810019373893738
epoch£º112	 i:8 	 global-step:2248	 l-p:0.05748337507247925
epoch£º112	 i:9 	 global-step:2249	 l-p:0.05755290389060974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[21.7653, 21.7656, 21.7653],
        [21.7653, 26.3366, 28.1057],
        [21.7653, 22.2796, 21.9716],
        [21.7653, 21.7657, 21.7653]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.057723697274923325 
model_pd.l_d.mean(): -0.18518958985805511 
model_pd.lagr.mean(): -0.1274658888578415 
model_pd.lambdas: dict_items([('pout', tensor([0.0802], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2449], device='cuda:0')), ('power', tensor([-6.2427], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.057723697274923325
epoch£º113	 i:1 	 global-step:2261	 l-p:0.05767826735973358
epoch£º113	 i:2 	 global-step:2262	 l-p:0.05741504579782486
epoch£º113	 i:3 	 global-step:2263	 l-p:0.05739659070968628
epoch£º113	 i:4 	 global-step:2264	 l-p:0.057500917464494705
epoch£º113	 i:5 	 global-step:2265	 l-p:0.057400330901145935
epoch£º113	 i:6 	 global-step:2266	 l-p:0.05736791342496872
epoch£º113	 i:7 	 global-step:2267	 l-p:0.05811893194913864
epoch£º113	 i:8 	 global-step:2268	 l-p:0.05739792808890343
epoch£º113	 i:9 	 global-step:2269	 l-p:0.05719524621963501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.5100, 22.5100, 22.5099],
        [22.5100, 22.5250, 22.5106],
        [22.5100, 22.5180, 22.5102],
        [22.5100, 28.4675, 31.5909]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.057357657700777054 
model_pd.l_d.mean(): -0.13508932292461395 
model_pd.lagr.mean(): -0.0777316689491272 
model_pd.lambdas: dict_items([('pout', tensor([0.0574], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.2643], device='cuda:0')), ('power', tensor([-5.4869], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.057357657700777054
epoch£º114	 i:1 	 global-step:2281	 l-p:0.05717825889587402
epoch£º114	 i:2 	 global-step:2282	 l-p:0.057237476110458374
epoch£º114	 i:3 	 global-step:2283	 l-p:0.057792775332927704
epoch£º114	 i:4 	 global-step:2284	 l-p:0.05709755793213844
epoch£º114	 i:5 	 global-step:2285	 l-p:0.05705471709370613
epoch£º114	 i:6 	 global-step:2286	 l-p:0.05731027573347092
epoch£º114	 i:7 	 global-step:2287	 l-p:0.057069167494773865
epoch£º114	 i:8 	 global-step:2288	 l-p:0.057395175099372864
epoch£º114	 i:9 	 global-step:2289	 l-p:0.05700278654694557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.2052, 23.2052, 23.2052],
        [23.2052, 23.2053, 23.2052],
        [23.2052, 23.2052, 23.2052],
        [23.2052, 24.8159, 24.4656]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.05701424926519394 
model_pd.l_d.mean(): -0.08595463633537292 
model_pd.lagr.mean(): -0.028940387070178986 
model_pd.lambdas: dict_items([('pout', tensor([0.0340], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3615], device='cuda:0')), ('power', tensor([-4.9073], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.05701424926519394
epoch£º115	 i:1 	 global-step:2301	 l-p:0.057043228298425674
epoch£º115	 i:2 	 global-step:2302	 l-p:0.057352446019649506
epoch£º115	 i:3 	 global-step:2303	 l-p:0.05694250762462616
epoch£º115	 i:4 	 global-step:2304	 l-p:0.057284191250801086
epoch£º115	 i:5 	 global-step:2305	 l-p:0.057041507214307785
epoch£º115	 i:6 	 global-step:2306	 l-p:0.056902989745140076
epoch£º115	 i:7 	 global-step:2307	 l-p:0.05685505270957947
epoch£º115	 i:8 	 global-step:2308	 l-p:0.05711546912789345
epoch£º115	 i:9 	 global-step:2309	 l-p:0.056798867881298065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.7319, 24.4469, 24.0656],
        [23.7319, 24.0754, 23.8324],
        [23.7319, 23.7322, 23.7319],
        [23.7319, 25.4198, 25.0707]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.056936558336019516 
model_pd.l_d.mean(): -0.030058441683650017 
model_pd.lagr.mean(): 0.0268781166523695 
model_pd.lambdas: dict_items([('pout', tensor([0.0104], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3513], device='cuda:0')), ('power', tensor([-4.3360], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.056936558336019516
epoch£º116	 i:1 	 global-step:2321	 l-p:0.056794002652168274
epoch£º116	 i:2 	 global-step:2322	 l-p:0.05703984946012497
epoch£º116	 i:3 	 global-step:2323	 l-p:0.05701642483472824
epoch£º116	 i:4 	 global-step:2324	 l-p:0.0567643977701664
epoch£º116	 i:5 	 global-step:2325	 l-p:0.056740064173936844
epoch£º116	 i:6 	 global-step:2326	 l-p:0.05676696076989174
epoch£º116	 i:7 	 global-step:2327	 l-p:0.05675786733627319
epoch£º116	 i:8 	 global-step:2328	 l-p:0.057194821536540985
epoch£º116	 i:9 	 global-step:2329	 l-p:0.056929029524326324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.0395, 24.0491, 24.0398],
        [24.0395, 24.5933, 24.2572],
        [24.0395, 24.0396, 24.0395],
        [24.0395, 24.0395, 24.0395]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.05669105425477028 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05669105425477028 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4528], device='cuda:0')), ('power', tensor([-4.2275], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.05669105425477028
epoch£º117	 i:1 	 global-step:2341	 l-p:0.05675607919692993
epoch£º117	 i:2 	 global-step:2342	 l-p:0.05671998858451843
epoch£º117	 i:3 	 global-step:2343	 l-p:0.056745532900094986
epoch£º117	 i:4 	 global-step:2344	 l-p:0.056816909462213516
epoch£º117	 i:5 	 global-step:2345	 l-p:0.05667499452829361
epoch£º117	 i:6 	 global-step:2346	 l-p:0.05718041583895683
epoch£º117	 i:7 	 global-step:2347	 l-p:0.05703185126185417
epoch£º117	 i:8 	 global-step:2348	 l-p:0.05691966786980629
epoch£º117	 i:9 	 global-step:2349	 l-p:0.05668443441390991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.1922, 32.6788, 38.5733],
        [24.1922, 26.7878, 26.7871],
        [24.1922, 24.4002, 24.2360],
        [24.1922, 24.1922, 24.1922]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.05678553134202957 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05678553134202957 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3732], device='cuda:0')), ('power', tensor([-3.8671], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.05678553134202957
epoch£º118	 i:1 	 global-step:2361	 l-p:0.056697968393564224
epoch£º118	 i:2 	 global-step:2362	 l-p:0.05726361647248268
epoch£º118	 i:3 	 global-step:2363	 l-p:0.05665554478764534
epoch£º118	 i:4 	 global-step:2364	 l-p:0.056668318808078766
epoch£º118	 i:5 	 global-step:2365	 l-p:0.05680180341005325
epoch£º118	 i:6 	 global-step:2366	 l-p:0.05672303959727287
epoch£º118	 i:7 	 global-step:2367	 l-p:0.056711167097091675
epoch£º118	 i:8 	 global-step:2368	 l-p:0.05684520676732063
epoch£º118	 i:9 	 global-step:2369	 l-p:0.056693725287914276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.2877, 25.9813, 25.6145],
        [24.2877, 24.3700, 24.2975],
        [24.2877, 28.4659, 29.5774],
        [24.2877, 29.3743, 31.3174]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.056612346321344376 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056612346321344376 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4628], device='cuda:0')), ('power', tensor([-3.8980], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.056612346321344376
epoch£º119	 i:1 	 global-step:2381	 l-p:0.05669926479458809
epoch£º119	 i:2 	 global-step:2382	 l-p:0.057026538997888565
epoch£º119	 i:3 	 global-step:2383	 l-p:0.05668000131845474
epoch£º119	 i:4 	 global-step:2384	 l-p:0.056646011769771576
epoch£º119	 i:5 	 global-step:2385	 l-p:0.056791190057992935
epoch£º119	 i:6 	 global-step:2386	 l-p:0.05670139566063881
epoch£º119	 i:7 	 global-step:2387	 l-p:0.05659015476703644
epoch£º119	 i:8 	 global-step:2388	 l-p:0.05700371041893959
epoch£º119	 i:9 	 global-step:2389	 l-p:0.05683346465229988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.3624, 24.3633, 24.3624],
        [24.3624, 24.3640, 24.3624],
        [24.3624, 24.3721, 24.3627],
        [24.3624, 27.6557, 28.0746]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.056642211973667145 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056642211973667145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4266], device='cuda:0')), ('power', tensor([-3.7546], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.056642211973667145
epoch£º120	 i:1 	 global-step:2401	 l-p:0.05697259679436684
epoch£º120	 i:2 	 global-step:2402	 l-p:0.05661299452185631
epoch£º120	 i:3 	 global-step:2403	 l-p:0.05663812533020973
epoch£º120	 i:4 	 global-step:2404	 l-p:0.05663112550973892
epoch£º120	 i:5 	 global-step:2405	 l-p:0.05689729377627373
epoch£º120	 i:6 	 global-step:2406	 l-p:0.05667305365204811
epoch£º120	 i:7 	 global-step:2407	 l-p:0.05698038265109062
epoch£º120	 i:8 	 global-step:2408	 l-p:0.056563738733530045
epoch£º120	 i:9 	 global-step:2409	 l-p:0.056756000965833664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.4304, 26.1319, 25.7622],
        [24.4304, 24.7124, 24.5018],
        [24.4304, 24.4304, 24.4304],
        [24.4304, 24.4367, 24.4306]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.056909237056970596 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056909237056970596 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4391], device='cuda:0')), ('power', tensor([-3.6395], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.056909237056970596
epoch£º121	 i:1 	 global-step:2421	 l-p:0.056597258895635605
epoch£º121	 i:2 	 global-step:2422	 l-p:0.05662599578499794
epoch£º121	 i:3 	 global-step:2423	 l-p:0.056729115545749664
epoch£º121	 i:4 	 global-step:2424	 l-p:0.05674660950899124
epoch£º121	 i:5 	 global-step:2425	 l-p:0.056613415479660034
epoch£º121	 i:6 	 global-step:2426	 l-p:0.056826259940862656
epoch£º121	 i:7 	 global-step:2427	 l-p:0.05664537474513054
epoch£º121	 i:8 	 global-step:2428	 l-p:0.0567852146923542
epoch£º121	 i:9 	 global-step:2429	 l-p:0.05668613687157631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.4963, 24.7649, 24.5621],
        [24.4963, 24.5973, 24.5098],
        [24.4963, 24.4963, 24.4963],
        [24.4963, 24.5678, 24.5040]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.05701411888003349 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05701411888003349 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3686], device='cuda:0')), ('power', tensor([-3.4400], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.05701411888003349
epoch£º122	 i:1 	 global-step:2441	 l-p:0.05684885010123253
epoch£º122	 i:2 	 global-step:2442	 l-p:0.056615784764289856
epoch£º122	 i:3 	 global-step:2443	 l-p:0.056583624333143234
epoch£º122	 i:4 	 global-step:2444	 l-p:0.0565376952290535
epoch£º122	 i:5 	 global-step:2445	 l-p:0.056585509330034256
epoch£º122	 i:6 	 global-step:2446	 l-p:0.056523390114307404
epoch£º122	 i:7 	 global-step:2447	 l-p:0.05665900185704231
epoch£º122	 i:8 	 global-step:2448	 l-p:0.05664746090769768
epoch£º122	 i:9 	 global-step:2449	 l-p:0.05695135518908501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.5616, 24.5669, 24.5617],
        [24.5616, 25.1039, 24.7689],
        [24.5616, 24.6275, 24.5684],
        [24.5616, 24.5616, 24.5616]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.05656103044748306 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05656103044748306 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4549], device='cuda:0')), ('power', tensor([-3.5898], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.05656103044748306
epoch£º123	 i:1 	 global-step:2461	 l-p:0.0565493106842041
epoch£º123	 i:2 	 global-step:2462	 l-p:0.056593745946884155
epoch£º123	 i:3 	 global-step:2463	 l-p:0.0565292090177536
epoch£º123	 i:4 	 global-step:2464	 l-p:0.05697184428572655
epoch£º123	 i:5 	 global-step:2465	 l-p:0.05657782405614853
epoch£º123	 i:6 	 global-step:2466	 l-p:0.05693480372428894
epoch£º123	 i:7 	 global-step:2467	 l-p:0.056694161146879196
epoch£º123	 i:8 	 global-step:2468	 l-p:0.05655006691813469
epoch£º123	 i:9 	 global-step:2469	 l-p:0.05680786818265915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.6266, 24.6305, 24.6267],
        [24.6266, 30.3432, 32.8752],
        [24.6266, 24.9909, 24.7345],
        [24.6266, 25.1547, 24.8247]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.056615106761455536 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056615106761455536 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4343], device='cuda:0')), ('power', tensor([-3.5124], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.056615106761455536
epoch£º124	 i:1 	 global-step:2481	 l-p:0.05671195685863495
epoch£º124	 i:2 	 global-step:2482	 l-p:0.05659031867980957
epoch£º124	 i:3 	 global-step:2483	 l-p:0.05688983201980591
epoch£º124	 i:4 	 global-step:2484	 l-p:0.05674316734075546
epoch£º124	 i:5 	 global-step:2485	 l-p:0.05684823542833328
epoch£º124	 i:6 	 global-step:2486	 l-p:0.0565045066177845
epoch£º124	 i:7 	 global-step:2487	 l-p:0.05658993497490883
epoch£º124	 i:8 	 global-step:2488	 l-p:0.05653480067849159
epoch£º124	 i:9 	 global-step:2489	 l-p:0.056548748165369034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.6925, 24.8325, 24.7153],
        [24.6925, 33.7806, 40.3603],
        [24.6925, 24.8682, 24.7254],
        [24.6925, 28.0481, 28.4835]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.056884221732616425 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056884221732616425 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4096], device='cuda:0')), ('power', tensor([-3.3081], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.056884221732616425
epoch£º125	 i:1 	 global-step:2501	 l-p:0.05680287629365921
epoch£º125	 i:2 	 global-step:2502	 l-p:0.056750696152448654
epoch£º125	 i:3 	 global-step:2503	 l-p:0.056522443890571594
epoch£º125	 i:4 	 global-step:2504	 l-p:0.05672767385840416
epoch£º125	 i:5 	 global-step:2505	 l-p:0.05649050697684288
epoch£º125	 i:6 	 global-step:2506	 l-p:0.05660272389650345
epoch£º125	 i:7 	 global-step:2507	 l-p:0.0565849132835865
epoch£º125	 i:8 	 global-step:2508	 l-p:0.05653459578752518
epoch£º125	 i:9 	 global-step:2509	 l-p:0.05648088827729225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.7588, 30.2964, 32.6254],
        [24.7588, 28.4955, 29.2039],
        [24.7588, 24.7589, 24.7588],
        [24.7588, 24.8213, 24.7650]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.05650186538696289 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05650186538696289 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4881], device='cuda:0')), ('power', tensor([-3.4595], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.05650186538696289
epoch£º126	 i:1 	 global-step:2521	 l-p:0.056839533150196075
epoch£º126	 i:2 	 global-step:2522	 l-p:0.056630637496709824
epoch£º126	 i:3 	 global-step:2523	 l-p:0.05663834512233734
epoch£º126	 i:4 	 global-step:2524	 l-p:0.056520335376262665
epoch£º126	 i:5 	 global-step:2525	 l-p:0.056511811912059784
epoch£º126	 i:6 	 global-step:2526	 l-p:0.05671331286430359
epoch£º126	 i:7 	 global-step:2527	 l-p:0.056512754410505295
epoch£º126	 i:8 	 global-step:2528	 l-p:0.056546829640865326
epoch£º126	 i:9 	 global-step:2529	 l-p:0.056769177317619324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[24.8247, 25.9835, 25.5358],
        [24.8247, 25.6465, 25.2312],
        [24.8247, 26.6736, 26.3292],
        [24.8247, 31.1596, 34.3238]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.05664697661995888 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05664697661995888 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4393], device='cuda:0')), ('power', tensor([-3.2370], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.05664697661995888
epoch£º127	 i:1 	 global-step:2541	 l-p:0.05649040266871452
epoch£º127	 i:2 	 global-step:2542	 l-p:0.05656991899013519
epoch£º127	 i:3 	 global-step:2543	 l-p:0.05662182345986366
epoch£º127	 i:4 	 global-step:2544	 l-p:0.05689097195863724
epoch£º127	 i:5 	 global-step:2545	 l-p:0.05655408650636673
epoch£º127	 i:6 	 global-step:2546	 l-p:0.056451138108968735
epoch£º127	 i:7 	 global-step:2547	 l-p:0.056511349976062775
epoch£º127	 i:8 	 global-step:2548	 l-p:0.056512508541345596
epoch£º127	 i:9 	 global-step:2549	 l-p:0.056741274893283844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.8913, 28.5220, 29.1387],
        [24.8913, 26.4750, 26.0653],
        [24.8913, 24.9210, 24.8932],
        [24.8913, 24.8986, 24.8915]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.056579865515232086 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056579865515232086 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4047], device='cuda:0')), ('power', tensor([-3.1761], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.056579865515232086
epoch£º128	 i:1 	 global-step:2561	 l-p:0.05654340609908104
epoch£º128	 i:2 	 global-step:2562	 l-p:0.05649394914507866
epoch£º128	 i:3 	 global-step:2563	 l-p:0.05683000758290291
epoch£º128	 i:4 	 global-step:2564	 l-p:0.05643358454108238
epoch£º128	 i:5 	 global-step:2565	 l-p:0.056455906480550766
epoch£º128	 i:6 	 global-step:2566	 l-p:0.05665358901023865
epoch£º128	 i:7 	 global-step:2567	 l-p:0.05662418156862259
epoch£º128	 i:8 	 global-step:2568	 l-p:0.05674277991056442
epoch£º128	 i:9 	 global-step:2569	 l-p:0.05643858760595322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.9583, 24.9681, 24.9586],
        [24.9583, 25.1561, 24.9978],
        [24.9583, 24.9584, 24.9582],
        [24.9583, 26.2509, 25.8039]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.0565115250647068 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0565115250647068 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4185], device='cuda:0')), ('power', tensor([-3.1095], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.0565115250647068
epoch£º129	 i:1 	 global-step:2581	 l-p:0.05683594569563866
epoch£º129	 i:2 	 global-step:2582	 l-p:0.0565837100148201
epoch£º129	 i:3 	 global-step:2583	 l-p:0.056454531848430634
epoch£º129	 i:4 	 global-step:2584	 l-p:0.056603774428367615
epoch£º129	 i:5 	 global-step:2585	 l-p:0.05645165219902992
epoch£º129	 i:6 	 global-step:2586	 l-p:0.056887730956077576
epoch£º129	 i:7 	 global-step:2587	 l-p:0.05640699341893196
epoch£º129	 i:8 	 global-step:2588	 l-p:0.05643705278635025
epoch£º129	 i:9 	 global-step:2589	 l-p:0.05642913654446602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.0259, 26.3744, 25.9297],
        [25.0259, 25.0262, 25.0259],
        [25.0259, 25.0261, 25.0259],
        [25.0259, 25.0385, 25.0264]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.056818291544914246 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056818291544914246 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3906], device='cuda:0')), ('power', tensor([-2.9722], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.056818291544914246
epoch£º130	 i:1 	 global-step:2601	 l-p:0.056694015860557556
epoch£º130	 i:2 	 global-step:2602	 l-p:0.056432608515024185
epoch£º130	 i:3 	 global-step:2603	 l-p:0.056404635310173035
epoch£º130	 i:4 	 global-step:2604	 l-p:0.056710805743932724
epoch£º130	 i:5 	 global-step:2605	 l-p:0.0565093494951725
epoch£º130	 i:6 	 global-step:2606	 l-p:0.05664651095867157
epoch£º130	 i:7 	 global-step:2607	 l-p:0.0564064122736454
epoch£º130	 i:8 	 global-step:2608	 l-p:0.056389082223176956
epoch£º130	 i:9 	 global-step:2609	 l-p:0.0563957579433918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.0939, 25.0939, 25.0939],
        [25.0939, 25.8680, 25.4599],
        [25.0939, 25.0952, 25.0940],
        [25.0939, 25.0941, 25.0939]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.05652838572859764 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05652838572859764 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4303], device='cuda:0')), ('power', tensor([-2.9649], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.05652838572859764
epoch£º131	 i:1 	 global-step:2621	 l-p:0.056430015712976456
epoch£º131	 i:2 	 global-step:2622	 l-p:0.056441161781549454
epoch£º131	 i:3 	 global-step:2623	 l-p:0.05654033645987511
epoch£º131	 i:4 	 global-step:2624	 l-p:0.056580446660518646
epoch£º131	 i:5 	 global-step:2625	 l-p:0.05674035847187042
epoch£º131	 i:6 	 global-step:2626	 l-p:0.05642671138048172
epoch£º131	 i:7 	 global-step:2627	 l-p:0.05640588328242302
epoch£º131	 i:8 	 global-step:2628	 l-p:0.05666874721646309
epoch£º131	 i:9 	 global-step:2629	 l-p:0.05644906312227249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.1618, 25.1622, 25.1618],
        [25.1618, 33.2653, 38.4569],
        [25.1618, 25.1618, 25.1617],
        [25.1618, 28.7845, 29.3717]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.05661880224943161 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05661880224943161 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3955], device='cuda:0')), ('power', tensor([-2.8597], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.05661880224943161
epoch£º132	 i:1 	 global-step:2641	 l-p:0.056731127202510834
epoch£º132	 i:2 	 global-step:2642	 l-p:0.056388575583696365
epoch£º132	 i:3 	 global-step:2643	 l-p:0.05633510649204254
epoch£º132	 i:4 	 global-step:2644	 l-p:0.056421760469675064
epoch£º132	 i:5 	 global-step:2645	 l-p:0.05667348578572273
epoch£º132	 i:6 	 global-step:2646	 l-p:0.056404247879981995
epoch£º132	 i:7 	 global-step:2647	 l-p:0.05640057846903801
epoch£º132	 i:8 	 global-step:2648	 l-p:0.05661804974079132
epoch£º132	 i:9 	 global-step:2649	 l-p:0.05642612278461456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.2304, 25.2304, 25.2304],
        [25.2304, 25.2304, 25.2304],
        [25.2304, 27.9541, 27.9593],
        [25.2304, 26.3388, 25.8850]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.05675940960645676 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05675940960645676 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4144], device='cuda:0')), ('power', tensor([-2.6852], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.05675940960645676
epoch£º133	 i:1 	 global-step:2661	 l-p:0.05646820738911629
epoch£º133	 i:2 	 global-step:2662	 l-p:0.05635440722107887
epoch£º133	 i:3 	 global-step:2663	 l-p:0.0563458651304245
epoch£º133	 i:4 	 global-step:2664	 l-p:0.056731216609478
epoch£º133	 i:5 	 global-step:2665	 l-p:0.05649852752685547
epoch£º133	 i:6 	 global-step:2666	 l-p:0.05642489343881607
epoch£º133	 i:7 	 global-step:2667	 l-p:0.05640142410993576
epoch£º133	 i:8 	 global-step:2668	 l-p:0.056389130651950836
epoch£º133	 i:9 	 global-step:2669	 l-p:0.05645056441426277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.2994, 27.9292, 27.8808],
        [25.2994, 26.1205, 25.7002],
        [25.2994, 25.4797, 25.3331],
        [25.2994, 25.3296, 25.3013]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.056387677788734436 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056387677788734436 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4806], device='cuda:0')), ('power', tensor([-2.8814], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.056387677788734436
epoch£º134	 i:1 	 global-step:2681	 l-p:0.056945111602544785
epoch£º134	 i:2 	 global-step:2682	 l-p:0.056389033794403076
epoch£º134	 i:3 	 global-step:2683	 l-p:0.0565374456346035
epoch£º134	 i:4 	 global-step:2684	 l-p:0.056331925094127655
epoch£º134	 i:5 	 global-step:2685	 l-p:0.05636104941368103
epoch£º134	 i:6 	 global-step:2686	 l-p:0.05662123113870621
epoch£º134	 i:7 	 global-step:2687	 l-p:0.05633266642689705
epoch£º134	 i:8 	 global-step:2688	 l-p:0.05637620761990547
epoch£º134	 i:9 	 global-step:2689	 l-p:0.056347090750932693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.3689, 25.3689, 25.3689],
        [25.3689, 33.7273, 39.1975],
        [25.3689, 26.6280, 26.1708],
        [25.3689, 32.4886, 36.4498]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.05664163455367088 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05664163455367088 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.3888], device='cuda:0')), ('power', tensor([-2.5617], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.05664163455367088
epoch£º135	 i:1 	 global-step:2701	 l-p:0.056433651596307755
epoch£º135	 i:2 	 global-step:2702	 l-p:0.05638166889548302
epoch£º135	 i:3 	 global-step:2703	 l-p:0.05630543455481529
epoch£º135	 i:4 	 global-step:2704	 l-p:0.05640588700771332
epoch£º135	 i:5 	 global-step:2705	 l-p:0.05635271593928337
epoch£º135	 i:6 	 global-step:2706	 l-p:0.05632864683866501
epoch£º135	 i:7 	 global-step:2707	 l-p:0.056506041437387466
epoch£º135	 i:8 	 global-step:2708	 l-p:0.056484393775463104
epoch£º135	 i:9 	 global-step:2709	 l-p:0.05659355968236923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.4382, 25.4393, 25.4382],
        [25.4382, 26.8806, 26.4349],
        [25.4382, 25.4474, 25.4385],
        [25.4382, 27.1381, 26.7348]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.056275662034749985 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056275662034749985 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5473], device='cuda:0')), ('power', tensor([-2.8224], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.056275662034749985
epoch£º136	 i:1 	 global-step:2721	 l-p:0.05635262280702591
epoch£º136	 i:2 	 global-step:2722	 l-p:0.05652199313044548
epoch£º136	 i:3 	 global-step:2723	 l-p:0.05639783665537834
epoch£º136	 i:4 	 global-step:2724	 l-p:0.0563204288482666
epoch£º136	 i:5 	 global-step:2725	 l-p:0.05665389820933342
epoch£º136	 i:6 	 global-step:2726	 l-p:0.056319043040275574
epoch£º136	 i:7 	 global-step:2727	 l-p:0.05660474672913551
epoch£º136	 i:8 	 global-step:2728	 l-p:0.05626875162124634
epoch£º136	 i:9 	 global-step:2729	 l-p:0.056525297462940216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.5081, 25.5081, 25.5081],
        [25.5081, 25.7051, 25.5467],
        [25.5081, 25.5081, 25.5081],
        [25.5081, 33.2374, 37.8975]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.056286055594682693 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056286055594682693 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5158], device='cuda:0')), ('power', tensor([-2.6947], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.056286055594682693
epoch£º137	 i:1 	 global-step:2741	 l-p:0.05631227418780327
epoch£º137	 i:2 	 global-step:2742	 l-p:0.056334417313337326
epoch£º137	 i:3 	 global-step:2743	 l-p:0.05662805959582329
epoch£º137	 i:4 	 global-step:2744	 l-p:0.05660663917660713
epoch£º137	 i:5 	 global-step:2745	 l-p:0.05645396560430527
epoch£º137	 i:6 	 global-step:2746	 l-p:0.05633996054530144
epoch£º137	 i:7 	 global-step:2747	 l-p:0.056429069489240646
epoch£º137	 i:8 	 global-step:2748	 l-p:0.05634809285402298
epoch£º137	 i:9 	 global-step:2749	 l-p:0.05630876496434212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.5787, 26.3533, 25.9403],
        [25.5787, 25.5787, 25.5787],
        [25.5787, 25.5787, 25.5787],
        [25.5787, 25.5787, 25.5787]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.05626955255866051 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05626955255866051 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5271], device='cuda:0')), ('power', tensor([-2.6207], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.05626955255866051
epoch£º138	 i:1 	 global-step:2761	 l-p:0.056348156183958054
epoch£º138	 i:2 	 global-step:2762	 l-p:0.05665713548660278
epoch£º138	 i:3 	 global-step:2763	 l-p:0.05629846453666687
epoch£º138	 i:4 	 global-step:2764	 l-p:0.05658191442489624
epoch£º138	 i:5 	 global-step:2765	 l-p:0.05652959644794464
epoch£º138	 i:6 	 global-step:2766	 l-p:0.05623718723654747
epoch£º138	 i:7 	 global-step:2767	 l-p:0.05633697286248207
epoch£º138	 i:8 	 global-step:2768	 l-p:0.05626661330461502
epoch£º138	 i:9 	 global-step:2769	 l-p:0.05632792413234711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.6498, 33.4242, 38.1116],
        [25.6498, 34.6736, 40.9436],
        [25.6498, 25.6498, 25.6498],
        [25.6498, 25.6779, 25.6515]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.05662396177649498 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05662396177649498 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4225], device='cuda:0')), ('power', tensor([-2.3120], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.05662396177649498
epoch£º139	 i:1 	 global-step:2781	 l-p:0.0562201626598835
epoch£º139	 i:2 	 global-step:2782	 l-p:0.05624900385737419
epoch£º139	 i:3 	 global-step:2783	 l-p:0.05637694150209427
epoch£º139	 i:4 	 global-step:2784	 l-p:0.05631592497229576
epoch£º139	 i:5 	 global-step:2785	 l-p:0.056345388293266296
epoch£º139	 i:6 	 global-step:2786	 l-p:0.05631478130817413
epoch£º139	 i:7 	 global-step:2787	 l-p:0.056520845741033554
epoch£º139	 i:8 	 global-step:2788	 l-p:0.05641503259539604
epoch£º139	 i:9 	 global-step:2789	 l-p:0.05627705529332161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.7209, 33.9079, 39.0890],
        [25.7209, 26.1335, 25.8494],
        [25.7209, 30.9024, 32.7538],
        [25.7209, 34.0373, 39.3806]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.05629391968250275 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05629391968250275 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4803], device='cuda:0')), ('power', tensor([-2.3641], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.05629391968250275
epoch£º140	 i:1 	 global-step:2801	 l-p:0.056262221187353134
epoch£º140	 i:2 	 global-step:2802	 l-p:0.056471988558769226
epoch£º140	 i:3 	 global-step:2803	 l-p:0.056238193064928055
epoch£º140	 i:4 	 global-step:2804	 l-p:0.056210391223430634
epoch£º140	 i:5 	 global-step:2805	 l-p:0.056349847465753555
epoch£º140	 i:6 	 global-step:2806	 l-p:0.056403130292892456
epoch£º140	 i:7 	 global-step:2807	 l-p:0.05654161423444748
epoch£º140	 i:8 	 global-step:2808	 l-p:0.05627606436610222
epoch£º140	 i:9 	 global-step:2809	 l-p:0.056417711079120636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.7923, 25.7940, 25.7923],
        [25.7923, 25.7939, 25.7923],
        [25.7923, 25.7923, 25.7923],
        [25.7923, 27.1260, 26.6630]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.0562734454870224 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0562734454870224 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4930], device='cuda:0')), ('power', tensor([-2.3405], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.0562734454870224
epoch£º141	 i:1 	 global-step:2821	 l-p:0.05627508834004402
epoch£º141	 i:2 	 global-step:2822	 l-p:0.05620967596769333
epoch£º141	 i:3 	 global-step:2823	 l-p:0.05626564472913742
epoch£º141	 i:4 	 global-step:2824	 l-p:0.05620865523815155
epoch£º141	 i:5 	 global-step:2825	 l-p:0.056334156543016434
epoch£º141	 i:6 	 global-step:2826	 l-p:0.05630936473608017
epoch£º141	 i:7 	 global-step:2827	 l-p:0.05629415437579155
epoch£º141	 i:8 	 global-step:2828	 l-p:0.056189171969890594
epoch£º141	 i:9 	 global-step:2829	 l-p:0.05691169574856758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.8638, 27.0016, 26.5358],
        [25.8638, 27.6799, 27.2895],
        [25.8638, 27.5150, 27.0888],
        [25.8638, 25.8714, 25.8640]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.056246910244226456 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056246910244226456 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5001], device='cuda:0')), ('power', tensor([-2.2768], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.056246910244226456
epoch£º142	 i:1 	 global-step:2841	 l-p:0.05640855059027672
epoch£º142	 i:2 	 global-step:2842	 l-p:0.05622217059135437
epoch£º142	 i:3 	 global-step:2843	 l-p:0.05620391666889191
epoch£º142	 i:4 	 global-step:2844	 l-p:0.05644241347908974
epoch£º142	 i:5 	 global-step:2845	 l-p:0.0562325082719326
epoch£º142	 i:6 	 global-step:2846	 l-p:0.05656161159276962
epoch£º142	 i:7 	 global-step:2847	 l-p:0.05634741485118866
epoch£º142	 i:8 	 global-step:2848	 l-p:0.05623101443052292
epoch£º142	 i:9 	 global-step:2849	 l-p:0.0561838373541832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.9367, 25.9367, 25.9367],
        [25.9367, 25.9367, 25.9367],
        [25.9367, 25.9369, 25.9367],
        [25.9367, 31.9835, 34.6687]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.056466758251190186 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056466758251190186 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4435], device='cuda:0')), ('power', tensor([-2.0796], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.056466758251190186
epoch£º143	 i:1 	 global-step:2861	 l-p:0.05622616782784462
epoch£º143	 i:2 	 global-step:2862	 l-p:0.056185007095336914
epoch£º143	 i:3 	 global-step:2863	 l-p:0.056154925376176834
epoch£º143	 i:4 	 global-step:2864	 l-p:0.05628514662384987
epoch£º143	 i:5 	 global-step:2865	 l-p:0.05627380311489105
epoch£º143	 i:6 	 global-step:2866	 l-p:0.05618918314576149
epoch£º143	 i:7 	 global-step:2867	 l-p:0.05613480135798454
epoch£º143	 i:8 	 global-step:2868	 l-p:0.05652506649494171
epoch£º143	 i:9 	 global-step:2869	 l-p:0.05644483491778374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0094, 35.5747, 42.4823],
        [26.0094, 31.1289, 32.8864],
        [26.0094, 27.1198, 26.6529],
        [26.0094, 26.0900, 26.0184]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.05638071522116661 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05638071522116661 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4918], device='cuda:0')), ('power', tensor([-2.1183], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.05638071522116661
epoch£º144	 i:1 	 global-step:2881	 l-p:0.05614662915468216
epoch£º144	 i:2 	 global-step:2882	 l-p:0.05648024380207062
epoch£º144	 i:3 	 global-step:2883	 l-p:0.0561828650534153
epoch£º144	 i:4 	 global-step:2884	 l-p:0.05616550147533417
epoch£º144	 i:5 	 global-step:2885	 l-p:0.0561441108584404
epoch£º144	 i:6 	 global-step:2886	 l-p:0.0561925433576107
epoch£º144	 i:7 	 global-step:2887	 l-p:0.05655631050467491
epoch£º144	 i:8 	 global-step:2888	 l-p:0.05619749799370766
epoch£º144	 i:9 	 global-step:2889	 l-p:0.05624730885028839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0828, 32.0070, 34.5445],
        [26.0828, 32.6616, 35.8927],
        [26.0828, 26.1988, 26.0990],
        [26.0828, 34.4979, 39.8903]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.05615045130252838 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05615045130252838 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5318], device='cuda:0')), ('power', tensor([-2.0767], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.05615045130252838
epoch£º145	 i:1 	 global-step:2901	 l-p:0.05648180469870567
epoch£º145	 i:2 	 global-step:2902	 l-p:0.05626792833209038
epoch£º145	 i:3 	 global-step:2903	 l-p:0.056401073932647705
epoch£º145	 i:4 	 global-step:2904	 l-p:0.05618821829557419
epoch£º145	 i:5 	 global-step:2905	 l-p:0.056169524788856506
epoch£º145	 i:6 	 global-step:2906	 l-p:0.05620231479406357
epoch£º145	 i:7 	 global-step:2907	 l-p:0.056413497775793076
epoch£º145	 i:8 	 global-step:2908	 l-p:0.0561264231801033
epoch£º145	 i:9 	 global-step:2909	 l-p:0.056099824607372284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1567, 26.1567, 26.1567],
        [26.1567, 26.5674, 26.2828],
        [26.1567, 28.7392, 28.6190],
        [26.1567, 26.8882, 26.4811]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.05627577751874924 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05627577751874924 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4924], device='cuda:0')), ('power', tensor([-1.9297], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.05627577751874924
epoch£º146	 i:1 	 global-step:2921	 l-p:0.05617024376988411
epoch£º146	 i:2 	 global-step:2922	 l-p:0.056204985827207565
epoch£º146	 i:3 	 global-step:2923	 l-p:0.056149233132600784
epoch£º146	 i:4 	 global-step:2924	 l-p:0.05633734166622162
epoch£º146	 i:5 	 global-step:2925	 l-p:0.0564936064183712
epoch£º146	 i:6 	 global-step:2926	 l-p:0.056096401065588
epoch£º146	 i:7 	 global-step:2927	 l-p:0.05629543587565422
epoch£º146	 i:8 	 global-step:2928	 l-p:0.05612365901470184
epoch£º146	 i:9 	 global-step:2929	 l-p:0.056160878390073776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.2306, 28.1331, 27.7523],
        [26.2306, 26.2319, 26.2306],
        [26.2306, 26.8124, 26.4531],
        [26.2306, 28.8734, 28.7784]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.05609319731593132 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05609319731593132 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5673], device='cuda:0')), ('power', tensor([-1.9994], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.05609319731593132
epoch£º147	 i:1 	 global-step:2941	 l-p:0.056456468999385834
epoch£º147	 i:2 	 global-step:2942	 l-p:0.05615338683128357
epoch£º147	 i:3 	 global-step:2943	 l-p:0.056118205189704895
epoch£º147	 i:4 	 global-step:2944	 l-p:0.056200627237558365
epoch£º147	 i:5 	 global-step:2945	 l-p:0.05619996413588524
epoch£º147	 i:6 	 global-step:2946	 l-p:0.05619852989912033
epoch£º147	 i:7 	 global-step:2947	 l-p:0.056090228259563446
epoch£º147	 i:8 	 global-step:2948	 l-p:0.056097157299518585
epoch£º147	 i:9 	 global-step:2949	 l-p:0.056506939232349396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3047, 26.5794, 26.3698],
        [26.3047, 26.4373, 26.3247],
        [26.3047, 34.9867, 40.6699],
        [26.3047, 26.3048, 26.3047]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.05610077083110809 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05610077083110809 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5482], device='cuda:0')), ('power', tensor([-1.8943], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.05610077083110809
epoch£º148	 i:1 	 global-step:2961	 l-p:0.056131020188331604
epoch£º148	 i:2 	 global-step:2962	 l-p:0.05609489232301712
epoch£º148	 i:3 	 global-step:2963	 l-p:0.056192636489868164
epoch£º148	 i:4 	 global-step:2964	 l-p:0.05609140172600746
epoch£º148	 i:5 	 global-step:2965	 l-p:0.05623617395758629
epoch£º148	 i:6 	 global-step:2966	 l-p:0.056342534720897675
epoch£º148	 i:7 	 global-step:2967	 l-p:0.05633066967129707
epoch£º148	 i:8 	 global-step:2968	 l-p:0.05608654022216797
epoch£º148	 i:9 	 global-step:2969	 l-p:0.056315794587135315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3792, 27.1725, 26.7475],
        [26.3792, 26.7854, 26.5023],
        [26.3792, 34.9863, 40.5589],
        [26.3792, 30.7251, 31.7616]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.05632511526346207 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05632511526346207 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4381], device='cuda:0')), ('power', tensor([-1.5715], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.05632511526346207
epoch£º149	 i:1 	 global-step:2981	 l-p:0.0560600571334362
epoch£º149	 i:2 	 global-step:2982	 l-p:0.056054312735795975
epoch£º149	 i:3 	 global-step:2983	 l-p:0.056075867265462875
epoch£º149	 i:4 	 global-step:2984	 l-p:0.05616595223546028
epoch£º149	 i:5 	 global-step:2985	 l-p:0.05618925020098686
epoch£º149	 i:6 	 global-step:2986	 l-p:0.05609382688999176
epoch£º149	 i:7 	 global-step:2987	 l-p:0.05613209307193756
epoch£º149	 i:8 	 global-step:2988	 l-p:0.056087005883455276
epoch£º149	 i:9 	 global-step:2989	 l-p:0.056547317653894424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.4542, 29.4931, 29.5973],
        [26.4542, 30.5393, 31.3602],
        [26.4542, 29.4387, 29.5117],
        [26.4542, 26.4848, 26.4561]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.05640273913741112 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05640273913741112 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4651], device='cuda:0')), ('power', tensor([-1.5139], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.05640273913741112
epoch£º150	 i:1 	 global-step:3001	 l-p:0.05621156096458435
epoch£º150	 i:2 	 global-step:3002	 l-p:0.05603618174791336
epoch£º150	 i:3 	 global-step:3003	 l-p:0.05626790225505829
epoch£º150	 i:4 	 global-step:3004	 l-p:0.056048717349767685
epoch£º150	 i:5 	 global-step:3005	 l-p:0.05618632584810257
epoch£º150	 i:6 	 global-step:3006	 l-p:0.05612862482666969
epoch£º150	 i:7 	 global-step:3007	 l-p:0.05611672252416611
epoch£º150	 i:8 	 global-step:3008	 l-p:0.05605858564376831
epoch£º150	 i:9 	 global-step:3009	 l-p:0.05608353391289711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.5304, 30.0288, 30.4163],
        [26.5304, 26.5304, 26.5304],
        [26.5304, 33.2278, 36.5176],
        [26.5304, 30.9866, 32.0991]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.056018151342868805 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056018151342868805 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5811], device='cuda:0')), ('power', tensor([-1.6777], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.056018151342868805
epoch£º151	 i:1 	 global-step:3021	 l-p:0.05603034794330597
epoch£º151	 i:2 	 global-step:3022	 l-p:0.05601797625422478
epoch£º151	 i:3 	 global-step:3023	 l-p:0.05612315237522125
epoch£º151	 i:4 	 global-step:3024	 l-p:0.05604538694024086
epoch£º151	 i:5 	 global-step:3025	 l-p:0.056199152022600174
epoch£º151	 i:6 	 global-step:3026	 l-p:0.0563555508852005
epoch£º151	 i:7 	 global-step:3027	 l-p:0.05618264526128769
epoch£º151	 i:8 	 global-step:3028	 l-p:0.056350648403167725
epoch£º151	 i:9 	 global-step:3029	 l-p:0.05602363497018814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[26.6059, 28.1707, 27.7099],
        [26.6059, 34.5114, 39.1744],
        [26.6059, 28.5599, 28.1797],
        [26.6059, 28.5117, 28.1185]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.0560901053249836 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0560901053249836 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4886], device='cuda:0')), ('power', tensor([-1.4652], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.0560901053249836
epoch£º152	 i:1 	 global-step:3041	 l-p:0.05603671073913574
epoch£º152	 i:2 	 global-step:3042	 l-p:0.0560624785721302
epoch£º152	 i:3 	 global-step:3043	 l-p:0.05632619559764862
epoch£º152	 i:4 	 global-step:3044	 l-p:0.05600179731845856
epoch£º152	 i:5 	 global-step:3045	 l-p:0.056332096457481384
epoch£º152	 i:6 	 global-step:3046	 l-p:0.055989742279052734
epoch£º152	 i:7 	 global-step:3047	 l-p:0.05610483139753342
epoch£º152	 i:8 	 global-step:3048	 l-p:0.05623115599155426
epoch£º152	 i:9 	 global-step:3049	 l-p:0.05598166212439537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.6824, 26.7414, 26.6878],
        [26.6824, 26.6839, 26.6824],
        [26.6824, 26.8344, 26.7071],
        [26.6824, 26.6824, 26.6824]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.05625709891319275 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05625709891319275 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5342], device='cuda:0')), ('power', tensor([-1.3887], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.05625709891319275
epoch£º153	 i:1 	 global-step:3061	 l-p:0.05619323253631592
epoch£º153	 i:2 	 global-step:3062	 l-p:0.05600340664386749
epoch£º153	 i:3 	 global-step:3063	 l-p:0.05602634325623512
epoch£º153	 i:4 	 global-step:3064	 l-p:0.056004591286182404
epoch£º153	 i:5 	 global-step:3065	 l-p:0.05602700263261795
epoch£º153	 i:6 	 global-step:3066	 l-p:0.056028153747320175
epoch£º153	 i:7 	 global-step:3067	 l-p:0.05618591979146004
epoch£º153	 i:8 	 global-step:3068	 l-p:0.05614675581455231
epoch£º153	 i:9 	 global-step:3069	 l-p:0.05609287694096565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.7592, 32.3926, 34.5469],
        [26.7592, 28.4734, 28.0321],
        [26.7592, 27.1031, 26.8520],
        [26.7592, 26.8297, 26.7664]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.05625619739294052 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05625619739294052 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4790], device='cuda:0')), ('power', tensor([-1.2011], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.05625619739294052
epoch£º154	 i:1 	 global-step:3081	 l-p:0.056069593876600266
epoch£º154	 i:2 	 global-step:3082	 l-p:0.05601955205202103
epoch£º154	 i:3 	 global-step:3083	 l-p:0.05623656138777733
epoch£º154	 i:4 	 global-step:3084	 l-p:0.05621408298611641
epoch£º154	 i:5 	 global-step:3085	 l-p:0.05599347874522209
epoch£º154	 i:6 	 global-step:3086	 l-p:0.056030888110399246
epoch£º154	 i:7 	 global-step:3087	 l-p:0.05601153150200844
epoch£º154	 i:8 	 global-step:3088	 l-p:0.05597902089357376
epoch£º154	 i:9 	 global-step:3089	 l-p:0.05596406012773514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.8367, 32.5294, 34.7315],
        [26.8367, 31.8750, 33.4598],
        [26.8367, 27.3930, 27.0403],
        [26.8367, 30.0660, 30.2579]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.056042153388261795 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056042153388261795 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5321], device='cuda:0')), ('power', tensor([-1.2998], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.056042153388261795
epoch£º155	 i:1 	 global-step:3101	 l-p:0.05600711703300476
epoch£º155	 i:2 	 global-step:3102	 l-p:0.05607081949710846
epoch£º155	 i:3 	 global-step:3103	 l-p:0.055944737046957016
epoch£º155	 i:4 	 global-step:3104	 l-p:0.056141164153814316
epoch£º155	 i:5 	 global-step:3105	 l-p:0.055933184921741486
epoch£º155	 i:6 	 global-step:3106	 l-p:0.05644802749156952
epoch£º155	 i:7 	 global-step:3107	 l-p:0.0559452623128891
epoch£º155	 i:8 	 global-step:3108	 l-p:0.05597041919827461
epoch£º155	 i:9 	 global-step:3109	 l-p:0.05607973784208298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[26.9139, 36.1759, 42.4730],
        [26.9139, 36.3372, 42.8452],
        [26.9139, 28.7107, 28.2813],
        [26.9139, 36.1528, 42.4198]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.05601578950881958 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05601578950881958 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5245], device='cuda:0')), ('power', tensor([-1.2257], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.05601578950881958
epoch£º156	 i:1 	 global-step:3121	 l-p:0.05622190982103348
epoch£º156	 i:2 	 global-step:3122	 l-p:0.05638285353779793
epoch£º156	 i:3 	 global-step:3123	 l-p:0.05595818907022476
epoch£º156	 i:4 	 global-step:3124	 l-p:0.05596347153186798
epoch£º156	 i:5 	 global-step:3125	 l-p:0.05602164939045906
epoch£º156	 i:6 	 global-step:3126	 l-p:0.05605955794453621
epoch£º156	 i:7 	 global-step:3127	 l-p:0.05589689314365387
epoch£º156	 i:8 	 global-step:3128	 l-p:0.05593317747116089
epoch£º156	 i:9 	 global-step:3129	 l-p:0.05594034120440483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[26.9922, 27.8287, 27.3878],
        [26.9922, 31.7511, 33.0716],
        [26.9922, 34.6640, 38.9789],
        [26.9922, 29.5382, 29.3571]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.05596033111214638 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05596033111214638 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5462], device='cuda:0')), ('power', tensor([-1.0984], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.05596033111214638
epoch£º157	 i:1 	 global-step:3141	 l-p:0.05593142285943031
epoch£º157	 i:2 	 global-step:3142	 l-p:0.05606154724955559
epoch£º157	 i:3 	 global-step:3143	 l-p:0.05621660500764847
epoch£º157	 i:4 	 global-step:3144	 l-p:0.05589289590716362
epoch£º157	 i:5 	 global-step:3145	 l-p:0.05609319731593132
epoch£º157	 i:6 	 global-step:3146	 l-p:0.0559903047978878
epoch£º157	 i:7 	 global-step:3147	 l-p:0.05597549304366112
epoch£º157	 i:8 	 global-step:3148	 l-p:0.05617116764187813
epoch£º157	 i:9 	 global-step:3149	 l-p:0.05590885132551193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.0702, 27.0707, 27.0701],
        [27.0702, 27.2023, 27.0896],
        [27.0702, 27.0705, 27.0702],
        [27.0702, 31.1761, 31.9562]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.05608965829014778 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05608965829014778 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5508], device='cuda:0')), ('power', tensor([-1.0248], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.05608965829014778
epoch£º158	 i:1 	 global-step:3161	 l-p:0.05589655414223671
epoch£º158	 i:2 	 global-step:3162	 l-p:0.05598155036568642
epoch£º158	 i:3 	 global-step:3163	 l-p:0.05611954256892204
epoch£º158	 i:4 	 global-step:3164	 l-p:0.05592498555779457
epoch£º158	 i:5 	 global-step:3165	 l-p:0.05602054297924042
epoch£º158	 i:6 	 global-step:3166	 l-p:0.05624908208847046
epoch£º158	 i:7 	 global-step:3167	 l-p:0.05587522312998772
epoch£º158	 i:8 	 global-step:3168	 l-p:0.055860985070466995
epoch£º158	 i:9 	 global-step:3169	 l-p:0.055994536727666855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.1489, 28.8895, 28.4416],
        [27.1489, 28.0054, 27.5585],
        [27.1489, 28.5124, 28.0217],
        [27.1489, 27.6544, 27.3216]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.055899590253829956 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055899590253829956 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5755], device='cuda:0')), ('power', tensor([-1.0270], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.055899590253829956
epoch£º159	 i:1 	 global-step:3181	 l-p:0.05588877946138382
epoch£º159	 i:2 	 global-step:3182	 l-p:0.05596540868282318
epoch£º159	 i:3 	 global-step:3183	 l-p:0.05590879172086716
epoch£º159	 i:4 	 global-step:3184	 l-p:0.056184399873018265
epoch£º159	 i:5 	 global-step:3185	 l-p:0.055857010185718536
epoch£º159	 i:6 	 global-step:3186	 l-p:0.05600963532924652
epoch£º159	 i:7 	 global-step:3187	 l-p:0.056061752140522
epoch£º159	 i:8 	 global-step:3188	 l-p:0.05591589957475662
epoch£º159	 i:9 	 global-step:3189	 l-p:0.05613059550523758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2276, 28.0341, 27.5981],
        [27.2276, 33.7718, 36.7890],
        [27.2276, 27.2276, 27.2276],
        [27.2276, 27.2382, 27.2280]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.05584482103586197 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05584482103586197 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6279], device='cuda:0')), ('power', tensor([-1.0239], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.05584482103586197
epoch£º160	 i:1 	 global-step:3201	 l-p:0.05587877333164215
epoch£º160	 i:2 	 global-step:3202	 l-p:0.056151848286390305
epoch£º160	 i:3 	 global-step:3203	 l-p:0.05606571212410927
epoch£º160	 i:4 	 global-step:3204	 l-p:0.05587347224354744
epoch£º160	 i:5 	 global-step:3205	 l-p:0.05614054575562477
epoch£º160	 i:6 	 global-step:3206	 l-p:0.05594149976968765
epoch£º160	 i:7 	 global-step:3207	 l-p:0.05588061362504959
epoch£º160	 i:8 	 global-step:3208	 l-p:0.055966224521398544
epoch£º160	 i:9 	 global-step:3209	 l-p:0.055890291929244995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[27.3074, 29.0006, 28.5397],
        [27.3074, 28.2061, 27.7487],
        [27.3074, 32.8951, 34.9346],
        [27.3074, 33.4659, 36.0702]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.05582907423377037 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05582907423377037 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6254], device='cuda:0')), ('power', tensor([-0.9694], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.05582907423377037
epoch£º161	 i:1 	 global-step:3221	 l-p:0.05589967966079712
epoch£º161	 i:2 	 global-step:3222	 l-p:0.05630706995725632
epoch£º161	 i:3 	 global-step:3223	 l-p:0.05583355948328972
epoch£º161	 i:4 	 global-step:3224	 l-p:0.05597139149904251
epoch£º161	 i:5 	 global-step:3225	 l-p:0.055908314883708954
epoch£º161	 i:6 	 global-step:3226	 l-p:0.055871155112981796
epoch£º161	 i:7 	 global-step:3227	 l-p:0.05584351345896721
epoch£º161	 i:8 	 global-step:3228	 l-p:0.05613406002521515
epoch£º161	 i:9 	 global-step:3229	 l-p:0.05584634840488434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3873, 32.5553, 34.1937],
        [27.3873, 27.3983, 27.3877],
        [27.3873, 28.5964, 28.1016],
        [27.3873, 33.5407, 36.1286]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.055862657725811005 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055862657725811005 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5708], device='cuda:0')), ('power', tensor([-0.7571], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.055862657725811005
epoch£º162	 i:1 	 global-step:3241	 l-p:0.05621664971113205
epoch£º162	 i:2 	 global-step:3242	 l-p:0.05582410842180252
epoch£º162	 i:3 	 global-step:3243	 l-p:0.05579927936196327
epoch£º162	 i:4 	 global-step:3244	 l-p:0.0558469332754612
epoch£º162	 i:5 	 global-step:3245	 l-p:0.055958546698093414
epoch£º162	 i:6 	 global-step:3246	 l-p:0.05591948330402374
epoch£º162	 i:7 	 global-step:3247	 l-p:0.055891234427690506
epoch£º162	 i:8 	 global-step:3248	 l-p:0.05595385283231735
epoch£º162	 i:9 	 global-step:3249	 l-p:0.055982112884521484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4676, 35.2811, 39.6762],
        [27.4676, 27.4676, 27.4676],
        [27.4676, 30.9382, 31.2364],
        [27.4676, 27.4678, 27.4676]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.05613413825631142 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05613413825631142 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4813], device='cuda:0')), ('power', tensor([-0.4743], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.05613413825631142
epoch£º163	 i:1 	 global-step:3261	 l-p:0.0558951236307621
epoch£º163	 i:2 	 global-step:3262	 l-p:0.0558132641017437
epoch£º163	 i:3 	 global-step:3263	 l-p:0.055834218859672546
epoch£º163	 i:4 	 global-step:3264	 l-p:0.055842772126197815
epoch£º163	 i:5 	 global-step:3265	 l-p:0.05580676347017288
epoch£º163	 i:6 	 global-step:3266	 l-p:0.05577893927693367
epoch£º163	 i:7 	 global-step:3267	 l-p:0.05596171319484711
epoch£º163	 i:8 	 global-step:3268	 l-p:0.055880699306726456
epoch£º163	 i:9 	 global-step:3269	 l-p:0.05611836165189743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5482, 30.1597, 29.9795],
        [27.5482, 27.6222, 27.5558],
        [27.5482, 27.5647, 27.5489],
        [27.5482, 27.9180, 27.6507]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.05584706738591194 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05584706738591194 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5789], device='cuda:0')), ('power', tensor([-0.6083], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.05584706738591194
epoch£º164	 i:1 	 global-step:3281	 l-p:0.05609912797808647
epoch£º164	 i:2 	 global-step:3282	 l-p:0.056034013628959656
epoch£º164	 i:3 	 global-step:3283	 l-p:0.05609159171581268
epoch£º164	 i:4 	 global-step:3284	 l-p:0.05580776557326317
epoch£º164	 i:5 	 global-step:3285	 l-p:0.055837854743003845
epoch£º164	 i:6 	 global-step:3286	 l-p:0.05575115978717804
epoch£º164	 i:7 	 global-step:3287	 l-p:0.055799681693315506
epoch£º164	 i:8 	 global-step:3288	 l-p:0.055799681693315506
epoch£º164	 i:9 	 global-step:3289	 l-p:0.05581100285053253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6299, 27.6298, 27.6298],
        [27.6299, 34.7188, 38.2627],
        [27.6299, 32.9763, 34.7494],
        [27.6299, 29.9388, 29.6317]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.05582622438669205 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05582622438669205 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5816], device='cuda:0')), ('power', tensor([-0.5410], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.05582622438669205
epoch£º165	 i:1 	 global-step:3301	 l-p:0.05619869753718376
epoch£º165	 i:2 	 global-step:3302	 l-p:0.05576865002512932
epoch£º165	 i:3 	 global-step:3303	 l-p:0.05580158904194832
epoch£º165	 i:4 	 global-step:3304	 l-p:0.055872224271297455
epoch£º165	 i:5 	 global-step:3305	 l-p:0.05575406923890114
epoch£º165	 i:6 	 global-step:3306	 l-p:0.0558500662446022
epoch£º165	 i:7 	 global-step:3307	 l-p:0.05582910403609276
epoch£º165	 i:8 	 global-step:3308	 l-p:0.055755410343408585
epoch£º165	 i:9 	 global-step:3309	 l-p:0.056032970547676086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7111, 27.7444, 27.7133],
        [27.7111, 28.7395, 28.2557],
        [27.7111, 28.8580, 28.3616],
        [27.7111, 27.7116, 27.7111]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.055997345596551895 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055997345596551895 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5309], device='cuda:0')), ('power', tensor([-0.3026], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.055997345596551895
epoch£º166	 i:1 	 global-step:3321	 l-p:0.05595787987112999
epoch£º166	 i:2 	 global-step:3322	 l-p:0.055728521198034286
epoch£º166	 i:3 	 global-step:3323	 l-p:0.05592107027769089
epoch£º166	 i:4 	 global-step:3324	 l-p:0.05578950420022011
epoch£º166	 i:5 	 global-step:3325	 l-p:0.05579862371087074
epoch£º166	 i:6 	 global-step:3326	 l-p:0.0557134710252285
epoch£º166	 i:7 	 global-step:3327	 l-p:0.055985964834690094
epoch£º166	 i:8 	 global-step:3328	 l-p:0.055835045874118805
epoch£º166	 i:9 	 global-step:3329	 l-p:0.05577435344457626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7931, 27.7931, 27.7931],
        [27.7931, 29.7441, 29.3211],
        [27.7931, 38.0010, 45.3475],
        [27.7931, 28.0026, 27.8335]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.05590275675058365 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05590275675058365 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5573], device='cuda:0')), ('power', tensor([-0.2969], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.05590275675058365
epoch£º167	 i:1 	 global-step:3341	 l-p:0.056098245084285736
epoch£º167	 i:2 	 global-step:3342	 l-p:0.055808134377002716
epoch£º167	 i:3 	 global-step:3343	 l-p:0.055754274129867554
epoch£º167	 i:4 	 global-step:3344	 l-p:0.05577084422111511
epoch£º167	 i:5 	 global-step:3345	 l-p:0.05576327070593834
epoch£º167	 i:6 	 global-step:3346	 l-p:0.05573681369423866
epoch£º167	 i:7 	 global-step:3347	 l-p:0.055710285902023315
epoch£º167	 i:8 	 global-step:3348	 l-p:0.05574856698513031
epoch£º167	 i:9 	 global-step:3349	 l-p:0.05602065846323967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8754, 27.8890, 27.8760],
        [27.8754, 37.3247, 43.6526],
        [27.8754, 27.8756, 27.8754],
        [27.8754, 27.8756, 27.8754]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.056025438010692596 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056025438010692596 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5245], device='cuda:0')), ('power', tensor([-0.0946], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.056025438010692596
epoch£º168	 i:1 	 global-step:3361	 l-p:0.05575163662433624
epoch£º168	 i:2 	 global-step:3362	 l-p:0.05574248358607292
epoch£º168	 i:3 	 global-step:3363	 l-p:0.05581699684262276
epoch£º168	 i:4 	 global-step:3364	 l-p:0.055753931403160095
epoch£º168	 i:5 	 global-step:3365	 l-p:0.055766768753528595
epoch£º168	 i:6 	 global-step:3366	 l-p:0.05593568831682205
epoch£º168	 i:7 	 global-step:3367	 l-p:0.055736005306243896
epoch£º168	 i:8 	 global-step:3368	 l-p:0.05572561174631119
epoch£º168	 i:9 	 global-step:3369	 l-p:0.05587201938033104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9582, 27.9634, 27.9583],
        [27.9582, 28.3066, 28.0503],
        [27.9582, 29.7969, 29.3434],
        [27.9582, 28.7943, 28.3443]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.055702198296785355 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055702198296785355 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6444], device='cuda:0')), ('power', tensor([-0.2898], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.055702198296785355
epoch£º169	 i:1 	 global-step:3381	 l-p:0.055860333144664764
epoch£º169	 i:2 	 global-step:3382	 l-p:0.055910613387823105
epoch£º169	 i:3 	 global-step:3383	 l-p:0.05577380955219269
epoch£º169	 i:4 	 global-step:3384	 l-p:0.05588053539395332
epoch£º169	 i:5 	 global-step:3385	 l-p:0.05575112998485565
epoch£º169	 i:6 	 global-step:3386	 l-p:0.05570530146360397
epoch£º169	 i:7 	 global-step:3387	 l-p:0.05575867369771004
epoch£º169	 i:8 	 global-step:3388	 l-p:0.05591081455349922
epoch£º169	 i:9 	 global-step:3389	 l-p:0.055686067789793015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0413, 30.5553, 30.3087],
        [28.0413, 28.0610, 28.0422],
        [28.0413, 28.4675, 28.1691],
        [28.0413, 28.0560, 28.0419]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.05596589297056198 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05596589297056198 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.6790e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5447], device='cuda:0')), ('power', tensor([0.0936], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.05596589297056198
epoch£º170	 i:1 	 global-step:3401	 l-p:0.05596759542822838
epoch£º170	 i:2 	 global-step:3402	 l-p:0.05566267669200897
epoch£º170	 i:3 	 global-step:3403	 l-p:0.05570956692099571
epoch£º170	 i:4 	 global-step:3404	 l-p:0.05568195879459381
epoch£º170	 i:5 	 global-step:3405	 l-p:0.05566589534282684
epoch£º170	 i:6 	 global-step:3406	 l-p:0.05568179860711098
epoch£º170	 i:7 	 global-step:3407	 l-p:0.05574952811002731
epoch£º170	 i:8 	 global-step:3408	 l-p:0.05576435476541519
epoch£º170	 i:9 	 global-step:3409	 l-p:0.055903829634189606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1246, 33.3379, 34.9322],
        [28.1246, 28.1558, 28.1265],
        [28.1246, 28.5784, 28.2659],
        [28.1246, 28.1250, 28.1246]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.05592932924628258 
model_pd.l_d.mean(): 7.147637006710283e-07 
model_pd.lagr.mean(): 0.05593004450201988 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2453e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5276], device='cuda:0')), ('power', tensor([0.1593], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.05592932924628258
epoch£º171	 i:1 	 global-step:3421	 l-p:0.05578949674963951
epoch£º171	 i:2 	 global-step:3422	 l-p:0.055662114173173904
epoch£º171	 i:3 	 global-step:3423	 l-p:0.055701520293951035
epoch£º171	 i:4 	 global-step:3424	 l-p:0.05591420829296112
epoch£º171	 i:5 	 global-step:3425	 l-p:0.055731553584337234
epoch£º171	 i:6 	 global-step:3426	 l-p:0.055671367794275284
epoch£º171	 i:7 	 global-step:3427	 l-p:0.05576541647315025
epoch£º171	 i:8 	 global-step:3428	 l-p:0.055663321167230606
epoch£º171	 i:9 	 global-step:3429	 l-p:0.055740077048540115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2071, 28.2215, 28.2077],
        [28.2071, 28.3368, 28.2255],
        [28.2071, 31.4589, 31.5714],
        [28.2071, 28.2071, 28.2071]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.05564255267381668 
model_pd.l_d.mean(): -1.0742702443167218e-06 
model_pd.lagr.mean(): 0.055641479790210724 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.9170e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6574], device='cuda:0')), ('power', tensor([-0.0496], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.05564255267381668
epoch£º172	 i:1 	 global-step:3441	 l-p:0.055687274783849716
epoch£º172	 i:2 	 global-step:3442	 l-p:0.05566687509417534
epoch£º172	 i:3 	 global-step:3443	 l-p:0.05565241724252701
epoch£º172	 i:4 	 global-step:3444	 l-p:0.05595513805747032
epoch£º172	 i:5 	 global-step:3445	 l-p:0.055864881724119186
epoch£º172	 i:6 	 global-step:3446	 l-p:0.055845871567726135
epoch£º172	 i:7 	 global-step:3447	 l-p:0.05565638095140457
epoch£º172	 i:8 	 global-step:3448	 l-p:0.055674877017736435
epoch£º172	 i:9 	 global-step:3449	 l-p:0.0557420440018177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[28.2850, 38.0109, 44.6068],
        [28.2850, 29.6815, 29.1684],
        [28.2850, 34.9080, 37.8514],
        [28.2850, 29.8207, 29.3149]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.05575290694832802 
model_pd.l_d.mean(): 1.9518865883583203e-05 
model_pd.lagr.mean(): 0.05577242746949196 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.1228e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5637], device='cuda:0')), ('power', tensor([0.2475], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.05575290694832802
epoch£º173	 i:1 	 global-step:3461	 l-p:0.055835284292697906
epoch£º173	 i:2 	 global-step:3462	 l-p:0.05565176531672478
epoch£º173	 i:3 	 global-step:3463	 l-p:0.05562146008014679
epoch£º173	 i:4 	 global-step:3464	 l-p:0.05561134219169617
epoch£º173	 i:5 	 global-step:3465	 l-p:0.05567587912082672
epoch£º173	 i:6 	 global-step:3466	 l-p:0.05565212294459343
epoch£º173	 i:7 	 global-step:3467	 l-p:0.055792298167943954
epoch£º173	 i:8 	 global-step:3468	 l-p:0.05575454980134964
epoch£º173	 i:9 	 global-step:3469	 l-p:0.05588334798812866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3458, 36.4210, 40.9644],
        [28.3458, 28.6432, 28.4163],
        [28.3458, 29.6001, 29.0871],
        [28.3458, 30.3409, 29.9100]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.055666692554950714 
model_pd.l_d.mean(): 3.076139182667248e-05 
model_pd.lagr.mean(): 0.055697452276945114 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6184], device='cuda:0')), ('power', tensor([0.1794], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.055666692554950714
epoch£º174	 i:1 	 global-step:3481	 l-p:0.055720195174217224
epoch£º174	 i:2 	 global-step:3482	 l-p:0.05565319582819939
epoch£º174	 i:3 	 global-step:3483	 l-p:0.05565361678600311
epoch£º174	 i:4 	 global-step:3484	 l-p:0.05585778132081032
epoch£º174	 i:5 	 global-step:3485	 l-p:0.05570051446557045
epoch£º174	 i:6 	 global-step:3486	 l-p:0.055602606385946274
epoch£º174	 i:7 	 global-step:3487	 l-p:0.05582784116268158
epoch£º174	 i:8 	 global-step:3488	 l-p:0.05564796179533005
epoch£º174	 i:9 	 global-step:3489	 l-p:0.05579432472586632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3775, 28.4333, 28.3822],
        [28.3775, 28.5938, 28.4195],
        [28.3775, 29.2201, 28.7646],
        [28.3775, 29.5955, 29.0837]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.05562392994761467 
model_pd.l_d.mean(): 2.918249629146885e-05 
model_pd.lagr.mean(): 0.05565311387181282 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6652], device='cuda:0')), ('power', tensor([0.1012], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.05562392994761467
epoch£º175	 i:1 	 global-step:3501	 l-p:0.05564132705330849
epoch£º175	 i:2 	 global-step:3502	 l-p:0.055747050791978836
epoch£º175	 i:3 	 global-step:3503	 l-p:0.05561550334095955
epoch£º175	 i:4 	 global-step:3504	 l-p:0.055815838277339935
epoch£º175	 i:5 	 global-step:3505	 l-p:0.055852994322776794
epoch£º175	 i:6 	 global-step:3506	 l-p:0.05565877631306648
epoch£º175	 i:7 	 global-step:3507	 l-p:0.05577530711889267
epoch£º175	 i:8 	 global-step:3508	 l-p:0.055716343224048615
epoch£º175	 i:9 	 global-step:3509	 l-p:0.05564585700631142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3700, 28.3700, 28.3700],
        [28.3700, 28.7515, 28.4757],
        [28.3700, 34.1152, 36.1712],
        [28.3700, 31.1551, 31.0115]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.05580592527985573 
model_pd.l_d.mean(): 0.00013250779011286795 
model_pd.lagr.mean(): 0.05593843385577202 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5870], device='cuda:0')), ('power', tensor([0.3214], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.05580592527985573
epoch£º176	 i:1 	 global-step:3521	 l-p:0.05560394749045372
epoch£º176	 i:2 	 global-step:3522	 l-p:0.05566764622926712
epoch£º176	 i:3 	 global-step:3523	 l-p:0.05560462176799774
epoch£º176	 i:4 	 global-step:3524	 l-p:0.055699918419122696
epoch£º176	 i:5 	 global-step:3525	 l-p:0.05561821907758713
epoch£º176	 i:6 	 global-step:3526	 l-p:0.05603737756609917
epoch£º176	 i:7 	 global-step:3527	 l-p:0.0557696558535099
epoch£º176	 i:8 	 global-step:3528	 l-p:0.055672235786914825
epoch£º176	 i:9 	 global-step:3529	 l-p:0.0556742288172245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3170, 38.5436, 45.7914],
        [28.3170, 28.3348, 28.3178],
        [28.3170, 29.9332, 29.4345],
        [28.3170, 28.3253, 28.3172]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.05572426691651344 
model_pd.l_d.mean(): 9.19601006899029e-05 
model_pd.lagr.mean(): 0.055816225707530975 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5914], device='cuda:0')), ('power', tensor([0.1760], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.05572426691651344
epoch£º177	 i:1 	 global-step:3541	 l-p:0.05591466277837753
epoch£º177	 i:2 	 global-step:3542	 l-p:0.05563606694340706
epoch£º177	 i:3 	 global-step:3543	 l-p:0.055830564349889755
epoch£º177	 i:4 	 global-step:3544	 l-p:0.055670805275440216
epoch£º177	 i:5 	 global-step:3545	 l-p:0.055665988475084305
epoch£º177	 i:6 	 global-step:3546	 l-p:0.05572026968002319
epoch£º177	 i:7 	 global-step:3547	 l-p:0.05562872812151909
epoch£º177	 i:8 	 global-step:3548	 l-p:0.055810537189245224
epoch£º177	 i:9 	 global-step:3549	 l-p:0.05571103096008301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2204, 28.2257, 28.2205],
        [28.2204, 28.2208, 28.2204],
        [28.2204, 28.2204, 28.2204],
        [28.2204, 29.3896, 28.8836]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.0560157373547554 
model_pd.l_d.mean(): 0.00016071728896349669 
model_pd.lagr.mean(): 0.056176453828811646 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5444], device='cuda:0')), ('power', tensor([0.2693], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.0560157373547554
epoch£º178	 i:1 	 global-step:3561	 l-p:0.05568275228142738
epoch£º178	 i:2 	 global-step:3562	 l-p:0.05562714487314224
epoch£º178	 i:3 	 global-step:3563	 l-p:0.055790796875953674
epoch£º178	 i:4 	 global-step:3564	 l-p:0.05582160875201225
epoch£º178	 i:5 	 global-step:3565	 l-p:0.055677808821201324
epoch£º178	 i:6 	 global-step:3566	 l-p:0.05585423484444618
epoch£º178	 i:7 	 global-step:3567	 l-p:0.05572747066617012
epoch£º178	 i:8 	 global-step:3568	 l-p:0.055685434490442276
epoch£º178	 i:9 	 global-step:3569	 l-p:0.05567753314971924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0913, 28.1223, 28.0932],
        [28.0913, 28.5574, 28.2391],
        [28.0913, 28.5345, 28.2274],
        [28.0913, 28.1043, 28.0918]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.05568700656294823 
model_pd.l_d.mean(): -6.56069241813384e-05 
model_pd.lagr.mean(): 0.055621400475502014 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6385], device='cuda:0')), ('power', tensor([-0.1067], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.05568700656294823
epoch£º179	 i:1 	 global-step:3581	 l-p:0.055819157510995865
epoch£º179	 i:2 	 global-step:3582	 l-p:0.05574338138103485
epoch£º179	 i:3 	 global-step:3583	 l-p:0.055751584470272064
epoch£º179	 i:4 	 global-step:3584	 l-p:0.055791862308979034
epoch£º179	 i:5 	 global-step:3585	 l-p:0.055678196251392365
epoch£º179	 i:6 	 global-step:3586	 l-p:0.05600786209106445
epoch£º179	 i:7 	 global-step:3587	 l-p:0.055764395743608475
epoch£º179	 i:8 	 global-step:3588	 l-p:0.05569778382778168
epoch£º179	 i:9 	 global-step:3589	 l-p:0.0559283047914505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9487, 27.9709, 27.9498],
        [27.9487, 27.9644, 27.9493],
        [27.9487, 30.6006, 30.4178],
        [27.9487, 27.9487, 27.9487]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.0556856207549572 
model_pd.l_d.mean(): -0.00017285297508351505 
model_pd.lagr.mean(): 0.055512767285108566 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6632], device='cuda:0')), ('power', tensor([-0.3057], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.0556856207549572
epoch£º180	 i:1 	 global-step:3601	 l-p:0.05570623278617859
epoch£º180	 i:2 	 global-step:3602	 l-p:0.05597110092639923
epoch£º180	 i:3 	 global-step:3603	 l-p:0.05573101341724396
epoch£º180	 i:4 	 global-step:3604	 l-p:0.056057628244161606
epoch£º180	 i:5 	 global-step:3605	 l-p:0.055742282420396805
epoch£º180	 i:6 	 global-step:3606	 l-p:0.055967170745134354
epoch£º180	 i:7 	 global-step:3607	 l-p:0.05572708323597908
epoch£º180	 i:8 	 global-step:3608	 l-p:0.05583401024341583
epoch£º180	 i:9 	 global-step:3609	 l-p:0.0557609498500824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8173, 27.9794, 27.8440],
        [27.8173, 29.0539, 28.5506],
        [27.8173, 32.3801, 33.4502],
        [27.8173, 34.5430, 37.6641]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.05575299263000488 
model_pd.l_d.mean(): -0.0001744844630593434 
model_pd.lagr.mean(): 0.055578507483005524 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6082], device='cuda:0')), ('power', tensor([-0.3906], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.05575299263000488
epoch£º181	 i:1 	 global-step:3621	 l-p:0.05574833229184151
epoch£º181	 i:2 	 global-step:3622	 l-p:0.055944815278053284
epoch£º181	 i:3 	 global-step:3623	 l-p:0.05579075217247009
epoch£º181	 i:4 	 global-step:3624	 l-p:0.05593731626868248
epoch£º181	 i:5 	 global-step:3625	 l-p:0.055792298167943954
epoch£º181	 i:6 	 global-step:3626	 l-p:0.05578186362981796
epoch£º181	 i:7 	 global-step:3627	 l-p:0.055891234427690506
epoch£º181	 i:8 	 global-step:3628	 l-p:0.05603427439928055
epoch£º181	 i:9 	 global-step:3629	 l-p:0.055777739733457565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7212, 27.9179, 27.7577],
        [27.7212, 28.6343, 28.1697],
        [27.7212, 29.5833, 29.1422],
        [27.7212, 27.7212, 27.7212]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.05581674352288246 
model_pd.l_d.mean(): -0.00013297962141223252 
model_pd.lagr.mean(): 0.055683765560388565 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6107], device='cuda:0')), ('power', tensor([-0.4931], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.05581674352288246
epoch£º182	 i:1 	 global-step:3641	 l-p:0.05600672587752342
epoch£º182	 i:2 	 global-step:3642	 l-p:0.05576036497950554
epoch£º182	 i:3 	 global-step:3643	 l-p:0.05577385798096657
epoch£º182	 i:4 	 global-step:3644	 l-p:0.05580238625407219
epoch£º182	 i:5 	 global-step:3645	 l-p:0.05611320957541466
epoch£º182	 i:6 	 global-step:3646	 l-p:0.05577446520328522
epoch£º182	 i:7 	 global-step:3647	 l-p:0.05575491860508919
epoch£º182	 i:8 	 global-step:3648	 l-p:0.05579612776637077
epoch£º182	 i:9 	 global-step:3649	 l-p:0.05601571127772331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6837, 31.4207, 31.8800],
        [27.6837, 29.6713, 29.2615],
        [27.6837, 27.8693, 27.7170],
        [27.6837, 34.7230, 38.2037]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.055798258632421494 
model_pd.l_d.mean(): -2.9237064154585823e-05 
model_pd.lagr.mean(): 0.055769022554159164 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.1852e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6031], device='cuda:0')), ('power', tensor([-0.5099], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.055798258632421494
epoch£º183	 i:1 	 global-step:3661	 l-p:0.05577366054058075
epoch£º183	 i:2 	 global-step:3662	 l-p:0.05575720965862274
epoch£º183	 i:3 	 global-step:3663	 l-p:0.05578844994306564
epoch£º183	 i:4 	 global-step:3664	 l-p:0.055947501212358475
epoch£º183	 i:5 	 global-step:3665	 l-p:0.05582903325557709
epoch£º183	 i:6 	 global-step:3666	 l-p:0.055816616863012314
epoch£º183	 i:7 	 global-step:3667	 l-p:0.055785682052373886
epoch£º183	 i:8 	 global-step:3668	 l-p:0.05604662373661995
epoch£º183	 i:9 	 global-step:3669	 l-p:0.056084465235471725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7150, 28.9467, 28.4454],
        [27.7150, 35.3809, 39.5609],
        [27.7150, 28.3807, 27.9824],
        [27.7150, 27.7149, 27.7150]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.056001923978328705 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056001923978328705 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5366], device='cuda:0')), ('power', tensor([-0.3170], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.056001923978328705
epoch£º184	 i:1 	 global-step:3681	 l-p:0.05590049922466278
epoch£º184	 i:2 	 global-step:3682	 l-p:0.056001532822847366
epoch£º184	 i:3 	 global-step:3683	 l-p:0.05579245835542679
epoch£º184	 i:4 	 global-step:3684	 l-p:0.055755164474248886
epoch£º184	 i:5 	 global-step:3685	 l-p:0.05593402311205864
epoch£º184	 i:6 	 global-step:3686	 l-p:0.055793397128582
epoch£º184	 i:7 	 global-step:3687	 l-p:0.05582122504711151
epoch£º184	 i:8 	 global-step:3688	 l-p:0.05577382817864418
epoch£º184	 i:9 	 global-step:3689	 l-p:0.055734485387802124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7862, 29.6448, 29.2009],
        [27.7862, 27.7862, 27.7862],
        [27.7862, 28.1324, 27.8777],
        [27.7862, 35.1489, 38.9718]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.05591017007827759 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05591017007827759 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5456], device='cuda:0')), ('power', tensor([-0.2599], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.05591017007827759
epoch£º185	 i:1 	 global-step:3701	 l-p:0.05577187240123749
epoch£º185	 i:2 	 global-step:3702	 l-p:0.055981118232011795
epoch£º185	 i:3 	 global-step:3703	 l-p:0.05578114837408066
epoch£º185	 i:4 	 global-step:3704	 l-p:0.055754926055669785
epoch£º185	 i:5 	 global-step:3705	 l-p:0.05593850836157799
epoch£º185	 i:6 	 global-step:3706	 l-p:0.05578523501753807
epoch£º185	 i:7 	 global-step:3707	 l-p:0.05573667585849762
epoch£º185	 i:8 	 global-step:3708	 l-p:0.05593402683734894
epoch£º185	 i:9 	 global-step:3709	 l-p:0.055735308676958084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8707, 27.8707, 27.8707],
        [27.8707, 32.1866, 33.0549],
        [27.8707, 31.7928, 32.3666],
        [27.8707, 29.6009, 29.1300]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.055765390396118164 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055765390396118164 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5951], device='cuda:0')), ('power', tensor([-0.2632], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.055765390396118164
epoch£º186	 i:1 	 global-step:3721	 l-p:0.05589699000120163
epoch£º186	 i:2 	 global-step:3722	 l-p:0.0557393915951252
epoch£º186	 i:3 	 global-step:3723	 l-p:0.05572149157524109
epoch£º186	 i:4 	 global-step:3724	 l-p:0.055885475128889084
epoch£º186	 i:5 	 global-step:3725	 l-p:0.05598466098308563
epoch£º186	 i:6 	 global-step:3726	 l-p:0.05568157136440277
epoch£º186	 i:7 	 global-step:3727	 l-p:0.05596845969557762
epoch£º186	 i:8 	 global-step:3728	 l-p:0.055779360234737396
epoch£º186	 i:9 	 global-step:3729	 l-p:0.05570795014500618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9602, 27.9618, 27.9602],
        [27.9602, 30.4071, 30.1373],
        [27.9602, 31.7366, 32.2009],
        [27.9602, 27.9603, 27.9602]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.055888112634420395 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055888112634420395 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5756], device='cuda:0')), ('power', tensor([-0.0807], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.055888112634420395
epoch£º187	 i:1 	 global-step:3741	 l-p:0.05574817582964897
epoch£º187	 i:2 	 global-step:3742	 l-p:0.055784814059734344
epoch£º187	 i:3 	 global-step:3743	 l-p:0.055785663425922394
epoch£º187	 i:4 	 global-step:3744	 l-p:0.05569521710276604
epoch£º187	 i:5 	 global-step:3745	 l-p:0.05569590628147125
epoch£º187	 i:6 	 global-step:3746	 l-p:0.05596093088388443
epoch£º187	 i:7 	 global-step:3747	 l-p:0.055699288845062256
epoch£º187	 i:8 	 global-step:3748	 l-p:0.05573540925979614
epoch£º187	 i:9 	 global-step:3749	 l-p:0.055932458490133286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0517, 35.8164, 40.0514],
        [28.0517, 29.4086, 28.8995],
        [28.0517, 28.0522, 28.0517],
        [28.0517, 35.7255, 39.8562]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.055687956511974335 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055687956511974335 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6379], device='cuda:0')), ('power', tensor([-0.1749], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.055687956511974335
epoch£º188	 i:1 	 global-step:3761	 l-p:0.05570431426167488
epoch£º188	 i:2 	 global-step:3762	 l-p:0.05567045509815216
epoch£º188	 i:3 	 global-step:3763	 l-p:0.055726900696754456
epoch£º188	 i:4 	 global-step:3764	 l-p:0.055854108184576035
epoch£º188	 i:5 	 global-step:3765	 l-p:0.055769987404346466
epoch£º188	 i:6 	 global-step:3766	 l-p:0.055682819336652756
epoch£º188	 i:7 	 global-step:3767	 l-p:0.05572665482759476
epoch£º188	 i:8 	 global-step:3768	 l-p:0.05585245415568352
epoch£º188	 i:9 	 global-step:3769	 l-p:0.056043241173028946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1438, 37.4401, 43.5145],
        [28.1438, 37.5553, 43.7765],
        [28.1438, 28.1442, 28.1438],
        [28.1438, 28.1438, 28.1438]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.05579114332795143 
model_pd.l_d.mean(): 9.454116423057712e-08 
model_pd.lagr.mean(): 0.05579123646020889 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.6660e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5789], device='cuda:0')), ('power', tensor([0.0058], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.05579114332795143
epoch£º189	 i:1 	 global-step:3781	 l-p:0.055781323462724686
epoch£º189	 i:2 	 global-step:3782	 l-p:0.05569816380739212
epoch£º189	 i:3 	 global-step:3783	 l-p:0.05592029541730881
epoch£º189	 i:4 	 global-step:3784	 l-p:0.05563556030392647
epoch£º189	 i:5 	 global-step:3785	 l-p:0.05580570921301842
epoch£º189	 i:6 	 global-step:3786	 l-p:0.05567775294184685
epoch£º189	 i:7 	 global-step:3787	 l-p:0.0558522529900074
epoch£º189	 i:8 	 global-step:3788	 l-p:0.05568898096680641
epoch£º189	 i:9 	 global-step:3789	 l-p:0.05566641688346863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2333, 28.2334, 28.2333],
        [28.2333, 28.2334, 28.2333],
        [28.2333, 28.2335, 28.2333],
        [28.2333, 28.5854, 28.3264]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.055697083473205566 
model_pd.l_d.mean(): 2.0636568933696253e-06 
model_pd.lagr.mean(): 0.055699147284030914 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.7121e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6209], device='cuda:0')), ('power', tensor([0.0460], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.055697083473205566
epoch£º190	 i:1 	 global-step:3801	 l-p:0.05569816008210182
epoch£º190	 i:2 	 global-step:3802	 l-p:0.05582248419523239
epoch£º190	 i:3 	 global-step:3803	 l-p:0.055651191622018814
epoch£º190	 i:4 	 global-step:3804	 l-p:0.05565783381462097
epoch£º190	 i:5 	 global-step:3805	 l-p:0.056078050285577774
epoch£º190	 i:6 	 global-step:3806	 l-p:0.05566532164812088
epoch£º190	 i:7 	 global-step:3807	 l-p:0.05567800998687744
epoch£º190	 i:8 	 global-step:3808	 l-p:0.05572052672505379
epoch£º190	 i:9 	 global-step:3809	 l-p:0.05566071718931198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3112, 37.3635, 43.0957],
        [28.3112, 29.3165, 28.8285],
        [28.3112, 28.3127, 28.3112],
        [28.3112, 28.3112, 28.3112]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.055838555097579956 
model_pd.l_d.mean(): 4.008250107290223e-05 
model_pd.lagr.mean(): 0.055878639221191406 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5171], device='cuda:0')), ('power', tensor([0.3473], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.055838555097579956
epoch£º191	 i:1 	 global-step:3821	 l-p:0.05567336827516556
epoch£º191	 i:2 	 global-step:3822	 l-p:0.05577757582068443
epoch£º191	 i:3 	 global-step:3823	 l-p:0.05574972927570343
epoch£º191	 i:4 	 global-step:3824	 l-p:0.05564485490322113
epoch£º191	 i:5 	 global-step:3825	 l-p:0.05570034310221672
epoch£º191	 i:6 	 global-step:3826	 l-p:0.05565202608704567
epoch£º191	 i:7 	 global-step:3827	 l-p:0.05566081032156944
epoch£º191	 i:8 	 global-step:3828	 l-p:0.05563855916261673
epoch£º191	 i:9 	 global-step:3829	 l-p:0.05584488809108734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3636, 28.7872, 28.4891],
        [28.3636, 28.6431, 28.4273],
        [28.3636, 36.8169, 41.8051],
        [28.3636, 28.9278, 28.5643]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.05566311627626419 
model_pd.l_d.mean(): 4.6419372665695846e-05 
model_pd.lagr.mean(): 0.055709537118673325 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6169], device='cuda:0')), ('power', tensor([0.2114], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.05566311627626419
epoch£º192	 i:1 	 global-step:3841	 l-p:0.055671870708465576
epoch£º192	 i:2 	 global-step:3842	 l-p:0.055713851004838943
epoch£º192	 i:3 	 global-step:3843	 l-p:0.05559879168868065
epoch£º192	 i:4 	 global-step:3844	 l-p:0.055917877703905106
epoch£º192	 i:5 	 global-step:3845	 l-p:0.05588492378592491
epoch£º192	 i:6 	 global-step:3846	 l-p:0.05576876178383827
epoch£º192	 i:7 	 global-step:3847	 l-p:0.05561115965247154
epoch£º192	 i:8 	 global-step:3848	 l-p:0.05563728138804436
epoch£º192	 i:9 	 global-step:3849	 l-p:0.055633243173360825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3787, 36.3522, 40.7711],
        [28.3787, 28.7705, 28.4892],
        [28.3787, 28.4418, 28.3845],
        [28.3787, 28.3827, 28.3788]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.05562597140669823 
model_pd.l_d.mean(): 5.3879572078585625e-05 
model_pd.lagr.mean(): 0.05567985028028488 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6412], device='cuda:0')), ('power', tensor([0.1576], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.05562597140669823
epoch£º193	 i:1 	 global-step:3861	 l-p:0.05558459088206291
epoch£º193	 i:2 	 global-step:3862	 l-p:0.05570517107844353
epoch£º193	 i:3 	 global-step:3863	 l-p:0.05591427534818649
epoch£º193	 i:4 	 global-step:3864	 l-p:0.05565638467669487
epoch£º193	 i:5 	 global-step:3865	 l-p:0.05562799796462059
epoch£º193	 i:6 	 global-step:3866	 l-p:0.055785030126571655
epoch£º193	 i:7 	 global-step:3867	 l-p:0.0556887723505497
epoch£º193	 i:8 	 global-step:3868	 l-p:0.05590500682592392
epoch£º193	 i:9 	 global-step:3869	 l-p:0.05561968684196472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3476, 28.3476, 28.3476],
        [28.3476, 34.3328, 36.6230],
        [28.3476, 31.1245, 30.9782],
        [28.3476, 29.0553, 28.6387]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.05560219660401344 
model_pd.l_d.mean(): 4.29016217822209e-05 
model_pd.lagr.mean(): 0.05564509704709053 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6756], device='cuda:0')), ('power', tensor([0.0930], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.05560219660401344
epoch£º194	 i:1 	 global-step:3881	 l-p:0.05575694888830185
epoch£º194	 i:2 	 global-step:3882	 l-p:0.05562876537442207
epoch£º194	 i:3 	 global-step:3883	 l-p:0.055763211101293564
epoch£º194	 i:4 	 global-step:3884	 l-p:0.05560487508773804
epoch£º194	 i:5 	 global-step:3885	 l-p:0.05569280683994293
epoch£º194	 i:6 	 global-step:3886	 l-p:0.05577196925878525
epoch£º194	 i:7 	 global-step:3887	 l-p:0.05589289963245392
epoch£º194	 i:8 	 global-step:3888	 l-p:0.05581164360046387
epoch£º194	 i:9 	 global-step:3889	 l-p:0.055703867226839066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2684, 29.5191, 29.0075],
        [28.2684, 33.8486, 35.7616],
        [28.2684, 28.3431, 28.2760],
        [28.2684, 33.0325, 34.2231]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.055671557784080505 
model_pd.l_d.mean(): 4.285777686163783e-05 
model_pd.lagr.mean(): 0.05571441724896431 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6258], device='cuda:0')), ('power', tensor([0.0773], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.055671557784080505
epoch£º195	 i:1 	 global-step:3901	 l-p:0.0558711513876915
epoch£º195	 i:2 	 global-step:3902	 l-p:0.05568535253405571
epoch£º195	 i:3 	 global-step:3903	 l-p:0.05573472008109093
epoch£º195	 i:4 	 global-step:3904	 l-p:0.05578448250889778
epoch£º195	 i:5 	 global-step:3905	 l-p:0.05564023554325104
epoch£º195	 i:6 	 global-step:3906	 l-p:0.055630311369895935
epoch£º195	 i:7 	 global-step:3907	 l-p:0.055734071880578995
epoch£º195	 i:8 	 global-step:3908	 l-p:0.05563114210963249
epoch£º195	 i:9 	 global-step:3909	 l-p:0.05606430768966675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1465, 28.1467, 28.1465],
        [28.1465, 28.3273, 28.1780],
        [28.1465, 28.1545, 28.1468],
        [28.1465, 28.2842, 28.1668]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.0557706393301487 
model_pd.l_d.mean(): 4.3187359551666304e-05 
model_pd.lagr.mean(): 0.055813826620578766 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5485], device='cuda:0')), ('power', tensor([0.0721], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.0557706393301487
epoch£º196	 i:1 	 global-step:3921	 l-p:0.05571794509887695
epoch£º196	 i:2 	 global-step:3922	 l-p:0.055703382939100266
epoch£º196	 i:3 	 global-step:3923	 l-p:0.05583970993757248
epoch£º196	 i:4 	 global-step:3924	 l-p:0.055744655430316925
epoch£º196	 i:5 	 global-step:3925	 l-p:0.05565882474184036
epoch£º196	 i:6 	 global-step:3926	 l-p:0.055908508598804474
epoch£º196	 i:7 	 global-step:3927	 l-p:0.055762022733688354
epoch£º196	 i:8 	 global-step:3928	 l-p:0.05570831149816513
epoch£º196	 i:9 	 global-step:3929	 l-p:0.055933669209480286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9998, 28.0141, 28.0004],
        [27.9998, 32.9914, 34.4041],
        [27.9998, 27.9998, 27.9998],
        [27.9998, 28.3814, 28.1066]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.05591895803809166 
model_pd.l_d.mean(): -1.2170261470600963e-05 
model_pd.lagr.mean(): 0.055906787514686584 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5403], device='cuda:0')), ('power', tensor([-0.0211], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.05591895803809166
epoch£º197	 i:1 	 global-step:3941	 l-p:0.05571206286549568
epoch£º197	 i:2 	 global-step:3942	 l-p:0.05571841076016426
epoch£º197	 i:3 	 global-step:3943	 l-p:0.055879514664411545
epoch£º197	 i:4 	 global-step:3944	 l-p:0.05577092990279198
epoch£º197	 i:5 	 global-step:3945	 l-p:0.05568185821175575
epoch£º197	 i:6 	 global-step:3946	 l-p:0.055750198662281036
epoch£º197	 i:7 	 global-step:3947	 l-p:0.055827490985393524
epoch£º197	 i:8 	 global-step:3948	 l-p:0.05601554736495018
epoch£º197	 i:9 	 global-step:3949	 l-p:0.055806007236242294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8538, 28.3733, 28.0313],
        [27.8538, 27.8577, 27.8539],
        [27.8538, 27.8538, 27.8538],
        [27.8538, 27.8873, 27.8560]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.05574505776166916 
model_pd.l_d.mean(): -0.0001818494638428092 
model_pd.lagr.mean(): 0.05556320771574974 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6220], device='cuda:0')), ('power', tensor([-0.3788], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.05574505776166916
epoch£º198	 i:1 	 global-step:3961	 l-p:0.055846937000751495
epoch£º198	 i:2 	 global-step:3962	 l-p:0.055913809686899185
epoch£º198	 i:3 	 global-step:3963	 l-p:0.05574966222047806
epoch£º198	 i:4 	 global-step:3964	 l-p:0.055770404636859894
epoch£º198	 i:5 	 global-step:3965	 l-p:0.05576050654053688
epoch£º198	 i:6 	 global-step:3966	 l-p:0.055816907435655594
epoch£º198	 i:7 	 global-step:3967	 l-p:0.055824246257543564
epoch£º198	 i:8 	 global-step:3968	 l-p:0.05582011118531227
epoch£º198	 i:9 	 global-step:3969	 l-p:0.05614037811756134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7386, 27.7719, 27.7407],
        [27.7386, 28.0453, 27.8138],
        [27.7386, 27.7387, 27.7386],
        [27.7386, 29.6854, 29.2634]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.056028783321380615 
model_pd.l_d.mean(): -9.295887866755947e-05 
model_pd.lagr.mean(): 0.055935826152563095 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5541], device='cuda:0')), ('power', tensor([-0.2933], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.056028783321380615
epoch£º199	 i:1 	 global-step:3981	 l-p:0.055786099284887314
epoch£º199	 i:2 	 global-step:3982	 l-p:0.05585424602031708
epoch£º199	 i:3 	 global-step:3983	 l-p:0.05573152005672455
epoch£º199	 i:4 	 global-step:3984	 l-p:0.055764179676771164
epoch£º199	 i:5 	 global-step:3985	 l-p:0.05577342212200165
epoch£º199	 i:6 	 global-step:3986	 l-p:0.05580328032374382
epoch£º199	 i:7 	 global-step:3987	 l-p:0.05594165623188019
epoch£º199	 i:8 	 global-step:3988	 l-p:0.055954378098249435
epoch£º199	 i:9 	 global-step:3989	 l-p:0.05595732480287552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6816, 27.6815, 27.6816],
        [27.6816, 27.6993, 27.6823],
        [27.6816, 28.0260, 27.7725],
        [27.6816, 31.9133, 32.7339]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.05577779561281204 
model_pd.l_d.mean(): -5.688200326403603e-05 
model_pd.lagr.mean(): 0.05572091415524483 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.2681e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6170], device='cuda:0')), ('power', tensor([-0.5227], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.05577779561281204
epoch£º200	 i:1 	 global-step:4001	 l-p:0.05574476718902588
epoch£º200	 i:2 	 global-step:4002	 l-p:0.05591142922639847
epoch£º200	 i:3 	 global-step:4003	 l-p:0.05581355839967728
epoch£º200	 i:4 	 global-step:4004	 l-p:0.05577273294329643
epoch£º200	 i:5 	 global-step:4005	 l-p:0.055782753974199295
epoch£º200	 i:6 	 global-step:4006	 l-p:0.05586827173829079
epoch£º200	 i:7 	 global-step:4007	 l-p:0.05580204725265503
epoch£º200	 i:8 	 global-step:4008	 l-p:0.0561213493347168
epoch£º200	 i:9 	 global-step:4009	 l-p:0.05605694279074669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6893, 27.6937, 27.6894],
        [27.6893, 33.2592, 35.2337],
        [27.6893, 27.6896, 27.6893],
        [27.6893, 29.4076, 28.9399]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.05607032775878906 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05607032775878906 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5178], device='cuda:0')), ('power', tensor([-0.2253], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.05607032775878906
epoch£º201	 i:1 	 global-step:4021	 l-p:0.05600946396589279
epoch£º201	 i:2 	 global-step:4022	 l-p:0.05576802045106888
epoch£º201	 i:3 	 global-step:4023	 l-p:0.05578969419002533
epoch£º201	 i:4 	 global-step:4024	 l-p:0.055765945464372635
epoch£º201	 i:5 	 global-step:4025	 l-p:0.05575987696647644
epoch£º201	 i:6 	 global-step:4026	 l-p:0.05581196770071983
epoch£º201	 i:7 	 global-step:4027	 l-p:0.05602055788040161
epoch£º201	 i:8 	 global-step:4028	 l-p:0.05584008991718292
epoch£º201	 i:9 	 global-step:4029	 l-p:0.05577251315116882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7183, 27.7292, 27.7187],
        [27.7183, 27.7226, 27.7184],
        [27.7183, 29.3526, 28.8716],
        [27.7183, 28.0905, 27.8215]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.05583387613296509 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05583387613296509 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5811], device='cuda:0')), ('power', tensor([-0.4712], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.05583387613296509
epoch£º202	 i:1 	 global-step:4041	 l-p:0.056045178323984146
epoch£º202	 i:2 	 global-step:4042	 l-p:0.055856846272945404
epoch£º202	 i:3 	 global-step:4043	 l-p:0.055957235395908356
epoch£º202	 i:4 	 global-step:4044	 l-p:0.055968329310417175
epoch£º202	 i:5 	 global-step:4045	 l-p:0.05576711520552635
epoch£º202	 i:6 	 global-step:4046	 l-p:0.05575265735387802
epoch£º202	 i:7 	 global-step:4047	 l-p:0.05573473498225212
epoch£º202	 i:8 	 global-step:4048	 l-p:0.055853888392448425
epoch£º202	 i:9 	 global-step:4049	 l-p:0.05576292797923088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7550, 33.9996, 36.6288],
        [27.7550, 29.4986, 29.0333],
        [27.7550, 28.9886, 28.4865],
        [27.7550, 27.7658, 27.7553]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.056043099611997604 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056043099611997604 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5468], device='cuda:0')), ('power', tensor([-0.2900], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.056043099611997604
epoch£º203	 i:1 	 global-step:4061	 l-p:0.055935218930244446
epoch£º203	 i:2 	 global-step:4062	 l-p:0.05574968084692955
epoch£º203	 i:3 	 global-step:4063	 l-p:0.055763471871614456
epoch£º203	 i:4 	 global-step:4064	 l-p:0.055728547275066376
epoch£º203	 i:5 	 global-step:4065	 l-p:0.0558936670422554
epoch£º203	 i:6 	 global-step:4066	 l-p:0.05596153065562248
epoch£º203	 i:7 	 global-step:4067	 l-p:0.05579988285899162
epoch£º203	 i:8 	 global-step:4068	 l-p:0.05576308071613312
epoch£º203	 i:9 	 global-step:4069	 l-p:0.055807266384363174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7943, 33.7005, 35.9862],
        [27.7943, 27.7942, 27.7943],
        [27.7943, 33.6218, 35.8304],
        [27.7943, 29.7461, 29.3233]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.05576367303729057 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576367303729057 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6004], device='cuda:0')), ('power', tensor([-0.3567], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.05576367303729057
epoch£º204	 i:1 	 global-step:4081	 l-p:0.05585653707385063
epoch£º204	 i:2 	 global-step:4082	 l-p:0.055840060114860535
epoch£º204	 i:3 	 global-step:4083	 l-p:0.055728696286678314
epoch£º204	 i:4 	 global-step:4084	 l-p:0.05576635152101517
epoch£º204	 i:5 	 global-step:4085	 l-p:0.055790938436985016
epoch£º204	 i:6 	 global-step:4086	 l-p:0.05602739751338959
epoch£º204	 i:7 	 global-step:4087	 l-p:0.05574803054332733
epoch£º204	 i:8 	 global-step:4088	 l-p:0.05592966824769974
epoch£º204	 i:9 	 global-step:4089	 l-p:0.05590248852968216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8345, 29.2376, 28.7340],
        [27.8345, 27.8345, 27.8345],
        [27.8345, 27.8428, 27.8347],
        [27.8345, 27.8345, 27.8345]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.055846843868494034 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055846843868494034 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5527], device='cuda:0')), ('power', tensor([-0.2908], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.055846843868494034
epoch£º205	 i:1 	 global-step:4101	 l-p:0.05573933571577072
epoch£º205	 i:2 	 global-step:4102	 l-p:0.056091394275426865
epoch£º205	 i:3 	 global-step:4103	 l-p:0.05573510006070137
epoch£º205	 i:4 	 global-step:4104	 l-p:0.055721428245306015
epoch£º205	 i:5 	 global-step:4105	 l-p:0.05574873462319374
epoch£º205	 i:6 	 global-step:4106	 l-p:0.055754754692316055
epoch£º205	 i:7 	 global-step:4107	 l-p:0.055734165012836456
epoch£º205	 i:8 	 global-step:4108	 l-p:0.05611675977706909
epoch£º205	 i:9 	 global-step:4109	 l-p:0.05577319115400314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8755, 28.7857, 28.3200],
        [27.8755, 36.9942, 42.9001],
        [27.8755, 34.1485, 36.7898],
        [27.8755, 27.8756, 27.8755]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.0557551272213459 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0557551272213459 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5870], device='cuda:0')), ('power', tensor([-0.2414], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.0557551272213459
epoch£º206	 i:1 	 global-step:4121	 l-p:0.0557938851416111
epoch£º206	 i:2 	 global-step:4122	 l-p:0.05581159517168999
epoch£º206	 i:3 	 global-step:4123	 l-p:0.05585518851876259
epoch£º206	 i:4 	 global-step:4124	 l-p:0.055917833000421524
epoch£º206	 i:5 	 global-step:4125	 l-p:0.05574750155210495
epoch£º206	 i:6 	 global-step:4126	 l-p:0.05569047853350639
epoch£º206	 i:7 	 global-step:4127	 l-p:0.05591505765914917
epoch£º206	 i:8 	 global-step:4128	 l-p:0.055938415229320526
epoch£º206	 i:9 	 global-step:4129	 l-p:0.05574317276477814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9167, 28.5261, 28.1469],
        [27.9167, 27.9167, 27.9167],
        [27.9167, 31.8456, 32.4205],
        [27.9167, 30.6549, 30.5136]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.05583377182483673 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05583377182483673 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5956], device='cuda:0')), ('power', tensor([-0.2443], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.05583377182483673
epoch£º207	 i:1 	 global-step:4141	 l-p:0.0557219535112381
epoch£º207	 i:2 	 global-step:4142	 l-p:0.05614115670323372
epoch£º207	 i:3 	 global-step:4143	 l-p:0.05571964010596275
epoch£º207	 i:4 	 global-step:4144	 l-p:0.05572941154241562
epoch£º207	 i:5 	 global-step:4145	 l-p:0.05578826740384102
epoch£º207	 i:6 	 global-step:4146	 l-p:0.055757369846105576
epoch£º207	 i:7 	 global-step:4147	 l-p:0.05578644573688507
epoch£º207	 i:8 	 global-step:4148	 l-p:0.055725838989019394
epoch£º207	 i:9 	 global-step:4149	 l-p:0.055870991200208664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9584, 27.9645, 27.9585],
        [27.9584, 33.8532, 36.1061],
        [27.9584, 30.3083, 30.0018],
        [27.9584, 27.9584, 27.9584]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.05569074675440788 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05569074675440788 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6546], device='cuda:0')), ('power', tensor([-0.3282], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.05569074675440788
epoch£º208	 i:1 	 global-step:4161	 l-p:0.05597183108329773
epoch£º208	 i:2 	 global-step:4162	 l-p:0.05569407343864441
epoch£º208	 i:3 	 global-step:4163	 l-p:0.05580427870154381
epoch£º208	 i:4 	 global-step:4164	 l-p:0.05592171847820282
epoch£º208	 i:5 	 global-step:4165	 l-p:0.055758036673069
epoch£º208	 i:6 	 global-step:4166	 l-p:0.05575030297040939
epoch£º208	 i:7 	 global-step:4167	 l-p:0.05576660856604576
epoch£º208	 i:8 	 global-step:4168	 l-p:0.05570756644010544
epoch£º208	 i:9 	 global-step:4169	 l-p:0.05591537058353424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[28.0001, 32.6258, 33.7291],
        [28.0001, 37.4185, 43.6796],
        [28.0001, 29.2383, 28.7318],
        [28.0001, 31.7629, 32.2146]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.05572566017508507 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05572566017508507 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5989], device='cuda:0')), ('power', tensor([-0.0751], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.05572566017508507
epoch£º209	 i:1 	 global-step:4181	 l-p:0.0558462031185627
epoch£º209	 i:2 	 global-step:4182	 l-p:0.055739421397447586
epoch£º209	 i:3 	 global-step:4183	 l-p:0.05576072260737419
epoch£º209	 i:4 	 global-step:4184	 l-p:0.05566968023777008
epoch£º209	 i:5 	 global-step:4185	 l-p:0.055756013840436935
epoch£º209	 i:6 	 global-step:4186	 l-p:0.05582898110151291
epoch£º209	 i:7 	 global-step:4187	 l-p:0.05583041161298752
epoch£º209	 i:8 	 global-step:4188	 l-p:0.05599294602870941
epoch£º209	 i:9 	 global-step:4189	 l-p:0.055736273527145386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0420, 28.0420, 28.0420],
        [28.0420, 28.0420, 28.0420],
        [28.0420, 30.6056, 30.3797],
        [28.0420, 28.0420, 28.0420]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.055686965584754944 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055686965584754944 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6371], device='cuda:0')), ('power', tensor([-0.1755], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.055686965584754944
epoch£º210	 i:1 	 global-step:4201	 l-p:0.05581101402640343
epoch£º210	 i:2 	 global-step:4202	 l-p:0.05595419928431511
epoch£º210	 i:3 	 global-step:4203	 l-p:0.05599643290042877
epoch£º210	 i:4 	 global-step:4204	 l-p:0.05571736395359039
epoch£º210	 i:5 	 global-step:4205	 l-p:0.05566532164812088
epoch£º210	 i:6 	 global-step:4206	 l-p:0.05570567026734352
epoch£º210	 i:7 	 global-step:4207	 l-p:0.05570866912603378
epoch£º210	 i:8 	 global-step:4208	 l-p:0.055866558104753494
epoch£º210	 i:9 	 global-step:4209	 l-p:0.055680569261312485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0842, 33.0458, 34.4235],
        [28.0842, 28.0850, 28.0842],
        [28.0842, 30.4453, 30.1373],
        [28.0842, 28.0845, 28.0842]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.055852245539426804 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055852245539426804 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5949], device='cuda:0')), ('power', tensor([-0.0261], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.055852245539426804
epoch£º211	 i:1 	 global-step:4221	 l-p:0.05569837614893913
epoch£º211	 i:2 	 global-step:4222	 l-p:0.055790312588214874
epoch£º211	 i:3 	 global-step:4223	 l-p:0.0559409037232399
epoch£º211	 i:4 	 global-step:4224	 l-p:0.05570783466100693
epoch£º211	 i:5 	 global-step:4225	 l-p:0.055816005915403366
epoch£º211	 i:6 	 global-step:4226	 l-p:0.05570446327328682
epoch£º211	 i:7 	 global-step:4227	 l-p:0.055710580199956894
epoch£º211	 i:8 	 global-step:4228	 l-p:0.05579793453216553
epoch£º211	 i:9 	 global-step:4229	 l-p:0.05567995086312294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1264, 33.8352, 35.8870],
        [28.1264, 28.1374, 28.1268],
        [28.1264, 28.1264, 28.1264],
        [28.1264, 33.5739, 35.3809]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.05564272776246071 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05564272776246071 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6750], device='cuda:0')), ('power', tensor([-0.1594], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.05564272776246071
epoch£º212	 i:1 	 global-step:4241	 l-p:0.05570121482014656
epoch£º212	 i:2 	 global-step:4242	 l-p:0.055910270661115646
epoch£º212	 i:3 	 global-step:4243	 l-p:0.05568482354283333
epoch£º212	 i:4 	 global-step:4244	 l-p:0.0557413324713707
epoch£º212	 i:5 	 global-step:4245	 l-p:0.05574788153171539
epoch£º212	 i:6 	 global-step:4246	 l-p:0.05579739436507225
epoch£º212	 i:7 	 global-step:4247	 l-p:0.055871717631816864
epoch£º212	 i:8 	 global-step:4248	 l-p:0.055846355855464935
epoch£º212	 i:9 	 global-step:4249	 l-p:0.05566029250621796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1686, 28.2198, 28.1728],
        [28.1686, 28.1686, 28.1686],
        [28.1686, 30.9329, 30.7903],
        [28.1686, 28.5195, 28.2613]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.055783458054065704 
model_pd.l_d.mean(): 2.3209927348943893e-06 
model_pd.lagr.mean(): 0.055785778909921646 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6903e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5530], device='cuda:0')), ('power', tensor([0.1079], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.055783458054065704
epoch£º213	 i:1 	 global-step:4261	 l-p:0.05610934644937515
epoch£º213	 i:2 	 global-step:4262	 l-p:0.055705197155475616
epoch£º213	 i:3 	 global-step:4263	 l-p:0.05576830729842186
epoch£º213	 i:4 	 global-step:4264	 l-p:0.055660560727119446
epoch£º213	 i:5 	 global-step:4265	 l-p:0.05568499118089676
epoch£º213	 i:6 	 global-step:4266	 l-p:0.05567551404237747
epoch£º213	 i:7 	 global-step:4267	 l-p:0.05568508803844452
epoch£º213	 i:8 	 global-step:4268	 l-p:0.05565079674124718
epoch£º213	 i:9 	 global-step:4269	 l-p:0.055789828300476074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2082, 28.2195, 28.2086],
        [28.2082, 29.0455, 28.5928],
        [28.2082, 30.8755, 30.6863],
        [28.2082, 28.2082, 28.2082]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.055661391466856 
model_pd.l_d.mean(): -8.267277848972299e-07 
model_pd.lagr.mean(): 0.055660564452409744 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.0180e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6424], device='cuda:0')), ('power', tensor([-0.0162], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.055661391466856
epoch£º214	 i:1 	 global-step:4281	 l-p:0.05567285418510437
epoch£º214	 i:2 	 global-step:4282	 l-p:0.055770888924598694
epoch£º214	 i:3 	 global-step:4283	 l-p:0.05570203810930252
epoch£º214	 i:4 	 global-step:4284	 l-p:0.05586736649274826
epoch£º214	 i:5 	 global-step:4285	 l-p:0.055896151810884476
epoch£º214	 i:6 	 global-step:4286	 l-p:0.05571530759334564
epoch£º214	 i:7 	 global-step:4287	 l-p:0.055827587842941284
epoch£º214	 i:8 	 global-step:4288	 l-p:0.05563913285732269
epoch£º214	 i:9 	 global-step:4289	 l-p:0.05567668378353119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2431, 30.9140, 30.7245],
        [28.2431, 28.2623, 28.2440],
        [28.2431, 28.6648, 28.3681],
        [28.2431, 28.2445, 28.2431]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.05565245449542999 
model_pd.l_d.mean(): 4.3340569391148165e-06 
model_pd.lagr.mean(): 0.055656787008047104 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6321], device='cuda:0')), ('power', tensor([0.0438], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.05565245449542999
epoch£º215	 i:1 	 global-step:4301	 l-p:0.05567295849323273
epoch£º215	 i:2 	 global-step:4302	 l-p:0.055737655609846115
epoch£º215	 i:3 	 global-step:4303	 l-p:0.05572935938835144
epoch£º215	 i:4 	 global-step:4304	 l-p:0.0557539165019989
epoch£º215	 i:5 	 global-step:4305	 l-p:0.05584494024515152
epoch£º215	 i:6 	 global-step:4306	 l-p:0.05578601360321045
epoch£º215	 i:7 	 global-step:4307	 l-p:0.05566900596022606
epoch£º215	 i:8 	 global-step:4308	 l-p:0.05565251410007477
epoch£º215	 i:9 	 global-step:4309	 l-p:0.05586040019989014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2703, 28.3136, 28.2735],
        [28.2703, 32.8142, 33.8246],
        [28.2703, 28.7720, 28.4363],
        [28.2703, 28.2703, 28.2703]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.055897217243909836 
model_pd.l_d.mean(): 3.209994247299619e-05 
model_pd.lagr.mean(): 0.05592931807041168 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5859], device='cuda:0')), ('power', tensor([0.1971], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.055897217243909836
epoch£º216	 i:1 	 global-step:4321	 l-p:0.05578850209712982
epoch£º216	 i:2 	 global-step:4322	 l-p:0.05581331253051758
epoch£º216	 i:3 	 global-step:4323	 l-p:0.05562056601047516
epoch£º216	 i:4 	 global-step:4324	 l-p:0.05585980787873268
epoch£º216	 i:5 	 global-step:4325	 l-p:0.05563347041606903
epoch£º216	 i:6 	 global-step:4326	 l-p:0.05564369261264801
epoch£º216	 i:7 	 global-step:4327	 l-p:0.055676721036434174
epoch£º216	 i:8 	 global-step:4328	 l-p:0.05565914884209633
epoch£º216	 i:9 	 global-step:4329	 l-p:0.05571744963526726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2864, 28.6392, 28.3797],
        [28.2864, 33.8391, 35.7243],
        [28.2864, 31.3601, 31.3675],
        [28.2864, 28.2865, 28.2864]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.05564271658658981 
model_pd.l_d.mean(): 1.2089164556527976e-05 
model_pd.lagr.mean(): 0.05565480515360832 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6572], device='cuda:0')), ('power', tensor([0.0508], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.05564271658658981
epoch£º217	 i:1 	 global-step:4341	 l-p:0.05582475662231445
epoch£º217	 i:2 	 global-step:4342	 l-p:0.05577690899372101
epoch£º217	 i:3 	 global-step:4343	 l-p:0.0558723621070385
epoch£º217	 i:4 	 global-step:4344	 l-p:0.05566103011369705
epoch£º217	 i:5 	 global-step:4345	 l-p:0.05587422102689743
epoch£º217	 i:6 	 global-step:4346	 l-p:0.055665962398052216
epoch£º217	 i:7 	 global-step:4347	 l-p:0.0556335523724556
epoch£º217	 i:8 	 global-step:4348	 l-p:0.055700358003377914
epoch£º217	 i:9 	 global-step:4349	 l-p:0.0556359738111496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2885, 28.4263, 28.3087],
        [28.2885, 28.2885, 28.2885],
        [28.2885, 28.2947, 28.2886],
        [28.2885, 28.2885, 28.2885]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.05563807487487793 
model_pd.l_d.mean(): 9.906773811962921e-06 
model_pd.lagr.mean(): 0.05564798042178154 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6515], device='cuda:0')), ('power', tensor([0.0312], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.05563807487487793
epoch£º218	 i:1 	 global-step:4361	 l-p:0.05564853549003601
epoch£º218	 i:2 	 global-step:4362	 l-p:0.05574553459882736
epoch£º218	 i:3 	 global-step:4363	 l-p:0.05561642348766327
epoch£º218	 i:4 	 global-step:4364	 l-p:0.05581034719944
epoch£º218	 i:5 	 global-step:4365	 l-p:0.05592477694153786
epoch£º218	 i:6 	 global-step:4366	 l-p:0.055633410811424255
epoch£º218	 i:7 	 global-step:4367	 l-p:0.05597276613116264
epoch£º218	 i:8 	 global-step:4368	 l-p:0.05568579584360123
epoch£º218	 i:9 	 global-step:4369	 l-p:0.05562261492013931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2758, 28.4788, 28.3137],
        [28.2758, 28.6593, 28.3827],
        [28.2758, 30.9605, 30.7756],
        [28.2758, 38.2356, 45.1393]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.055924031883478165 
model_pd.l_d.mean(): 9.572891576681286e-05 
model_pd.lagr.mean(): 0.05601976066827774 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5623], device='cuda:0')), ('power', tensor([0.2421], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.055924031883478165
epoch£º219	 i:1 	 global-step:4381	 l-p:0.055697791278362274
epoch£º219	 i:2 	 global-step:4382	 l-p:0.05569568648934364
epoch£º219	 i:3 	 global-step:4383	 l-p:0.05574239790439606
epoch£º219	 i:4 	 global-step:4384	 l-p:0.05577104538679123
epoch£º219	 i:5 	 global-step:4385	 l-p:0.05566959083080292
epoch£º219	 i:6 	 global-step:4386	 l-p:0.05567673593759537
epoch£º219	 i:7 	 global-step:4387	 l-p:0.05570308491587639
epoch£º219	 i:8 	 global-step:4388	 l-p:0.05582473799586296
epoch£º219	 i:9 	 global-step:4389	 l-p:0.05563536658883095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2481, 28.2641, 28.2487],
        [28.2481, 29.6426, 29.1302],
        [28.2481, 28.2480, 28.2481],
        [28.2481, 28.6698, 28.3730]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.05567782372236252 
model_pd.l_d.mean(): 1.7287673472310416e-05 
model_pd.lagr.mean(): 0.05569511279463768 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6270], device='cuda:0')), ('power', tensor([0.0373], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.05567782372236252
epoch£º220	 i:1 	 global-step:4401	 l-p:0.05572518706321716
epoch£º220	 i:2 	 global-step:4402	 l-p:0.0559975802898407
epoch£º220	 i:3 	 global-step:4403	 l-p:0.055654458701610565
epoch£º220	 i:4 	 global-step:4404	 l-p:0.055659934878349304
epoch£º220	 i:5 	 global-step:4405	 l-p:0.055780086666345596
epoch£º220	 i:6 	 global-step:4406	 l-p:0.05566258728504181
epoch£º220	 i:7 	 global-step:4407	 l-p:0.055660054087638855
epoch£º220	 i:8 	 global-step:4408	 l-p:0.05586196482181549
epoch£º220	 i:9 	 global-step:4409	 l-p:0.055736541748046875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2063, 33.9321, 35.9901],
        [28.2063, 32.3490, 33.0550],
        [28.2063, 30.3663, 29.9859],
        [28.2063, 28.2166, 28.2066]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.055705394595861435 
model_pd.l_d.mean(): 4.6084282075753435e-05 
model_pd.lagr.mean(): 0.05575148016214371 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6178], device='cuda:0')), ('power', tensor([0.0896], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.055705394595861435
epoch£º221	 i:1 	 global-step:4421	 l-p:0.05589152127504349
epoch£º221	 i:2 	 global-step:4422	 l-p:0.055737026035785675
epoch£º221	 i:3 	 global-step:4423	 l-p:0.05570414289832115
epoch£º221	 i:4 	 global-step:4424	 l-p:0.05567743629217148
epoch£º221	 i:5 	 global-step:4425	 l-p:0.0556480772793293
epoch£º221	 i:6 	 global-step:4426	 l-p:0.05602429434657097
epoch£º221	 i:7 	 global-step:4427	 l-p:0.05570724233984947
epoch£º221	 i:8 	 global-step:4428	 l-p:0.05575539171695709
epoch£º221	 i:9 	 global-step:4429	 l-p:0.05566968023777008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[28.1533, 38.1609, 45.1559],
        [28.1533, 34.5121, 37.2020],
        [28.1533, 37.8375, 44.4083],
        [28.1533, 34.1502, 36.4776]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.055767230689525604 
model_pd.l_d.mean(): 5.772194708697498e-05 
model_pd.lagr.mean(): 0.05582495406270027 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5443], device='cuda:0')), ('power', tensor([0.1065], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.055767230689525604
epoch£º222	 i:1 	 global-step:4441	 l-p:0.05567917227745056
epoch£º222	 i:2 	 global-step:4442	 l-p:0.055674511939287186
epoch£º222	 i:3 	 global-step:4443	 l-p:0.05567941069602966
epoch£º222	 i:4 	 global-step:4444	 l-p:0.055886074900627136
epoch£º222	 i:5 	 global-step:4445	 l-p:0.05574257671833038
epoch£º222	 i:6 	 global-step:4446	 l-p:0.05572519078850746
epoch£º222	 i:7 	 global-step:4447	 l-p:0.05575470253825188
epoch£º222	 i:8 	 global-step:4448	 l-p:0.0556832030415535
epoch£º222	 i:9 	 global-step:4449	 l-p:0.05605528876185417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0930, 28.1187, 28.0944],
        [28.0930, 30.3342, 29.9841],
        [28.0930, 31.3699, 31.5048],
        [28.0930, 28.5200, 28.2210]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.05566604807972908 
model_pd.l_d.mean(): -5.508391768671572e-05 
model_pd.lagr.mean(): 0.05561096593737602 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6489], device='cuda:0')), ('power', tensor([-0.1017], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.05566604807972908
epoch£º223	 i:1 	 global-step:4461	 l-p:0.055721089243888855
epoch£º223	 i:2 	 global-step:4462	 l-p:0.055725663900375366
epoch£º223	 i:3 	 global-step:4463	 l-p:0.05573606863617897
epoch£º223	 i:4 	 global-step:4464	 l-p:0.0559072382748127
epoch£º223	 i:5 	 global-step:4465	 l-p:0.05569858476519585
epoch£º223	 i:6 	 global-step:4466	 l-p:0.05608285963535309
epoch£º223	 i:7 	 global-step:4467	 l-p:0.05586199462413788
epoch£º223	 i:8 	 global-step:4468	 l-p:0.05565891042351723
epoch£º223	 i:9 	 global-step:4469	 l-p:0.05572500079870224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0316, 28.0318, 28.0316],
        [28.0316, 28.0316, 28.0316],
        [28.0316, 28.8874, 28.4319],
        [28.0316, 28.0810, 28.0355]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.055708687752485275 
model_pd.l_d.mean(): -0.00013588367437478155 
model_pd.lagr.mean(): 0.05557280406355858 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6328], device='cuda:0')), ('power', tensor([-0.2659], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.055708687752485275
epoch£º224	 i:1 	 global-step:4481	 l-p:0.05571705102920532
epoch£º224	 i:2 	 global-step:4482	 l-p:0.05570339411497116
epoch£º224	 i:3 	 global-step:4483	 l-p:0.055865295231342316
epoch£º224	 i:4 	 global-step:4484	 l-p:0.055731918662786484
epoch£º224	 i:5 	 global-step:4485	 l-p:0.05572941154241562
epoch£º224	 i:6 	 global-step:4486	 l-p:0.055963799357414246
epoch£º224	 i:7 	 global-step:4487	 l-p:0.05598153546452522
epoch£º224	 i:8 	 global-step:4488	 l-p:0.05580698698759079
epoch£º224	 i:9 	 global-step:4489	 l-p:0.055710308253765106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9743, 27.9743, 27.9743],
        [27.9743, 27.9743, 27.9743],
        [27.9743, 27.9744, 27.9743],
        [27.9743, 30.3541, 30.0580]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.05582944676280022 
model_pd.l_d.mean(): -7.802348409313709e-05 
model_pd.lagr.mean(): 0.05575142428278923 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5897], device='cuda:0')), ('power', tensor([-0.1732], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.05582944676280022
epoch£º225	 i:1 	 global-step:4501	 l-p:0.05595606565475464
epoch£º225	 i:2 	 global-step:4502	 l-p:0.05574582889676094
epoch£º225	 i:3 	 global-step:4503	 l-p:0.05573219060897827
epoch£º225	 i:4 	 global-step:4504	 l-p:0.05570544674992561
epoch£º225	 i:5 	 global-step:4505	 l-p:0.05572662129998207
epoch£º225	 i:6 	 global-step:4506	 l-p:0.055747661739587784
epoch£º225	 i:7 	 global-step:4507	 l-p:0.05596171319484711
epoch£º225	 i:8 	 global-step:4508	 l-p:0.05580565333366394
epoch£º225	 i:9 	 global-step:4509	 l-p:0.05582724139094353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9263, 30.7020, 30.5778],
        [27.9263, 34.2114, 36.8578],
        [27.9263, 29.4413, 28.9423],
        [27.9263, 27.9324, 27.9265]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.055716779083013535 
model_pd.l_d.mean(): -0.00010023696086136624 
model_pd.lagr.mean(): 0.05561654269695282 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6330], device='cuda:0')), ('power', tensor([-0.2758], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.055716779083013535
epoch£º226	 i:1 	 global-step:4521	 l-p:0.05570105463266373
epoch£º226	 i:2 	 global-step:4522	 l-p:0.05599299445748329
epoch£º226	 i:3 	 global-step:4523	 l-p:0.055826909840106964
epoch£º226	 i:4 	 global-step:4524	 l-p:0.055738840252161026
epoch£º226	 i:5 	 global-step:4525	 l-p:0.055765580385923386
epoch£º226	 i:6 	 global-step:4526	 l-p:0.055963676422834396
epoch£º226	 i:7 	 global-step:4527	 l-p:0.05593092367053032
epoch£º226	 i:8 	 global-step:4528	 l-p:0.05573992431163788
epoch£º226	 i:9 	 global-step:4529	 l-p:0.055755615234375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8933, 30.1191, 29.7720],
        [27.8933, 27.9742, 27.9020],
        [27.8933, 28.6145, 28.1968],
        [27.8933, 27.9027, 27.8936]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.055811312049627304 
model_pd.l_d.mean(): -5.919430259382352e-05 
model_pd.lagr.mean(): 0.055752117186784744 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6041], device='cuda:0')), ('power', tensor([-0.2315], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.055811312049627304
epoch£º227	 i:1 	 global-step:4541	 l-p:0.055711615830659866
epoch£º227	 i:2 	 global-step:4542	 l-p:0.055791787803173065
epoch£º227	 i:3 	 global-step:4543	 l-p:0.055870942771434784
epoch£º227	 i:4 	 global-step:4544	 l-p:0.055781278759241104
epoch£º227	 i:5 	 global-step:4545	 l-p:0.05574673414230347
epoch£º227	 i:6 	 global-step:4546	 l-p:0.05605994164943695
epoch£º227	 i:7 	 global-step:4547	 l-p:0.05576751008629799
epoch£º227	 i:8 	 global-step:4548	 l-p:0.05593358725309372
epoch£º227	 i:9 	 global-step:4549	 l-p:0.05571337416768074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8797, 27.8802, 27.8797],
        [27.8797, 27.8816, 27.8797],
        [27.8797, 27.8976, 27.8805],
        [27.8797, 28.7607, 28.3010]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.055731143802404404 
model_pd.l_d.mean(): -4.192007327219471e-05 
model_pd.lagr.mean(): 0.05568922311067581 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6209], device='cuda:0')), ('power', tensor([-0.3085], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.055731143802404404
epoch£º228	 i:1 	 global-step:4561	 l-p:0.05579240620136261
epoch£º228	 i:2 	 global-step:4562	 l-p:0.05591944605112076
epoch£º228	 i:3 	 global-step:4563	 l-p:0.05575929582118988
epoch£º228	 i:4 	 global-step:4564	 l-p:0.05596103519201279
epoch£º228	 i:5 	 global-step:4565	 l-p:0.05591195821762085
epoch£º228	 i:6 	 global-step:4566	 l-p:0.05569269880652428
epoch£º228	 i:7 	 global-step:4567	 l-p:0.05571660026907921
epoch£º228	 i:8 	 global-step:4568	 l-p:0.05587846785783768
epoch£º228	 i:9 	 global-step:4569	 l-p:0.05583314597606659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8888, 27.9057, 27.8895],
        [27.8888, 34.4042, 37.2934],
        [27.8888, 28.4089, 28.0665],
        [27.8888, 28.2688, 27.9951]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.05603639781475067 
model_pd.l_d.mean(): -1.3289953813000466e-06 
model_pd.lagr.mean(): 0.05603506788611412 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.5520e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5186], device='cuda:0')), ('power', tensor([-0.0934], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.05603639781475067
epoch£º229	 i:1 	 global-step:4581	 l-p:0.055810313671827316
epoch£º229	 i:2 	 global-step:4582	 l-p:0.05601652339100838
epoch£º229	 i:3 	 global-step:4583	 l-p:0.05589764565229416
epoch£º229	 i:4 	 global-step:4584	 l-p:0.05576841160655022
epoch£º229	 i:5 	 global-step:4585	 l-p:0.05570337548851967
epoch£º229	 i:6 	 global-step:4586	 l-p:0.05573366954922676
epoch£º229	 i:7 	 global-step:4587	 l-p:0.0557258278131485
epoch£º229	 i:8 	 global-step:4588	 l-p:0.05573021620512009
epoch£º229	 i:9 	 global-step:4589	 l-p:0.05572965368628502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[27.9201, 28.5944, 28.1918],
        [27.9201, 37.3851, 43.7237],
        [27.9201, 28.7723, 28.3187],
        [27.9201, 31.1130, 31.2103]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.05594963580369949 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05594963580369949 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5412], device='cuda:0')), ('power', tensor([-0.1184], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.05594963580369949
epoch£º230	 i:1 	 global-step:4601	 l-p:0.055749937891960144
epoch£º230	 i:2 	 global-step:4602	 l-p:0.055725499987602234
epoch£º230	 i:3 	 global-step:4603	 l-p:0.05580909550189972
epoch£º230	 i:4 	 global-step:4604	 l-p:0.05588323622941971
epoch£º230	 i:5 	 global-step:4605	 l-p:0.05571092665195465
epoch£º230	 i:6 	 global-step:4606	 l-p:0.05576193332672119
epoch£º230	 i:7 	 global-step:4607	 l-p:0.0557028204202652
epoch£º230	 i:8 	 global-step:4608	 l-p:0.05579004064202309
epoch£º230	 i:9 	 global-step:4609	 l-p:0.05598524585366249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9614, 27.9616, 27.9614],
        [27.9614, 27.9816, 27.9624],
        [27.9614, 37.2165, 43.2772],
        [27.9614, 29.3137, 28.8063]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.05575495958328247 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575495958328247 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5855], device='cuda:0')), ('power', tensor([-0.1577], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.05575495958328247
epoch£º231	 i:1 	 global-step:4621	 l-p:0.05571701005101204
epoch£º231	 i:2 	 global-step:4622	 l-p:0.056165050715208054
epoch£º231	 i:3 	 global-step:4623	 l-p:0.05569657310843468
epoch£º231	 i:4 	 global-step:4624	 l-p:0.055664271116256714
epoch£º231	 i:5 	 global-step:4625	 l-p:0.05572908744215965
epoch£º231	 i:6 	 global-step:4626	 l-p:0.055854376405477524
epoch£º231	 i:7 	 global-step:4627	 l-p:0.055749114602804184
epoch£º231	 i:8 	 global-step:4628	 l-p:0.05591105297207832
epoch£º231	 i:9 	 global-step:4629	 l-p:0.055728886276483536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0070, 28.0129, 28.0071],
        [28.0070, 28.9260, 28.4571],
        [28.0070, 33.2979, 34.9758],
        [28.0070, 28.1357, 28.0252]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.055693190544843674 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055693190544843674 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6337], device='cuda:0')), ('power', tensor([-0.1895], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.055693190544843674
epoch£º232	 i:1 	 global-step:4641	 l-p:0.05573198199272156
epoch£º232	 i:2 	 global-step:4642	 l-p:0.055700309574604034
epoch£º232	 i:3 	 global-step:4643	 l-p:0.05572968348860741
epoch£º232	 i:4 	 global-step:4644	 l-p:0.055818650871515274
epoch£º232	 i:5 	 global-step:4645	 l-p:0.055832069367170334
epoch£º232	 i:6 	 global-step:4646	 l-p:0.055924635380506516
epoch£º232	 i:7 	 global-step:4647	 l-p:0.05575000122189522
epoch£º232	 i:8 	 global-step:4648	 l-p:0.05588487908244133
epoch£º232	 i:9 	 global-step:4649	 l-p:0.05580047890543938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0538, 29.1865, 28.6861],
        [28.0538, 28.6921, 28.3014],
        [28.0538, 32.3171, 33.1277],
        [28.0538, 36.5935, 41.7453]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.055741146206855774 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055741146206855774 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5825], device='cuda:0')), ('power', tensor([-0.0144], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.055741146206855774
epoch£º233	 i:1 	 global-step:4661	 l-p:0.05593591555953026
epoch£º233	 i:2 	 global-step:4662	 l-p:0.0558040477335453
epoch£º233	 i:3 	 global-step:4663	 l-p:0.05568419396877289
epoch£º233	 i:4 	 global-step:4664	 l-p:0.05568363890051842
epoch£º233	 i:5 	 global-step:4665	 l-p:0.055830586701631546
epoch£º233	 i:6 	 global-step:4666	 l-p:0.055766914039850235
epoch£º233	 i:7 	 global-step:4667	 l-p:0.055652789771556854
epoch£º233	 i:8 	 global-step:4668	 l-p:0.055870600044727325
epoch£º233	 i:9 	 global-step:4669	 l-p:0.05579061433672905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1017, 28.2624, 28.1278],
        [28.1017, 28.5094, 28.2203],
        [28.1017, 28.6672, 28.3045],
        [28.1017, 28.1042, 28.1017]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.055736616253852844 
model_pd.l_d.mean(): -1.1398632732140257e-10 
model_pd.lagr.mean(): 0.055736616253852844 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5820], device='cuda:0')), ('power', tensor([-0.0165], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.055736616253852844
epoch£º234	 i:1 	 global-step:4681	 l-p:0.05574043095111847
epoch£º234	 i:2 	 global-step:4682	 l-p:0.05584472045302391
epoch£º234	 i:3 	 global-step:4683	 l-p:0.05571551248431206
epoch£º234	 i:4 	 global-step:4684	 l-p:0.05566861853003502
epoch£º234	 i:5 	 global-step:4685	 l-p:0.05586187168955803
epoch£º234	 i:6 	 global-step:4686	 l-p:0.055753812193870544
epoch£º234	 i:7 	 global-step:4687	 l-p:0.05571207404136658
epoch£º234	 i:8 	 global-step:4688	 l-p:0.05595678091049194
epoch£º234	 i:9 	 global-step:4689	 l-p:0.05566297844052315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1496, 35.8515, 39.9973],
        [28.1496, 28.1499, 28.1496],
        [28.1496, 28.3496, 28.1867],
        [28.1496, 28.1499, 28.1496]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.055685997009277344 
model_pd.l_d.mean(): -6.987315970263808e-08 
model_pd.lagr.mean(): 0.05568592622876167 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6358], device='cuda:0')), ('power', tensor([-0.0837], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.055685997009277344
epoch£º235	 i:1 	 global-step:4701	 l-p:0.055688925087451935
epoch£º235	 i:2 	 global-step:4702	 l-p:0.0557439923286438
epoch£º235	 i:3 	 global-step:4703	 l-p:0.05578836426138878
epoch£º235	 i:4 	 global-step:4704	 l-p:0.05587321147322655
epoch£º235	 i:5 	 global-step:4705	 l-p:0.05568436160683632
epoch£º235	 i:6 	 global-step:4706	 l-p:0.05604000762104988
epoch£º235	 i:7 	 global-step:4707	 l-p:0.05571017041802406
epoch£º235	 i:8 	 global-step:4708	 l-p:0.055656079202890396
epoch£º235	 i:9 	 global-step:4709	 l-p:0.05567590519785881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1973, 31.7651, 32.0721],
        [28.1973, 28.2203, 28.1985],
        [28.1973, 30.8910, 30.7143],
        [28.1973, 29.0564, 28.5986]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.055670689791440964 
model_pd.l_d.mean(): 5.82604286591959e-07 
model_pd.lagr.mean(): 0.055671270936727524 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.8555e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6369], device='cuda:0')), ('power', tensor([0.0212], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.055670689791440964
epoch£º236	 i:1 	 global-step:4721	 l-p:0.05564017966389656
epoch£º236	 i:2 	 global-step:4722	 l-p:0.055699482560157776
epoch£º236	 i:3 	 global-step:4723	 l-p:0.055866606533527374
epoch£º236	 i:4 	 global-step:4724	 l-p:0.05571858584880829
epoch£º236	 i:5 	 global-step:4725	 l-p:0.055936165153980255
epoch£º236	 i:6 	 global-step:4726	 l-p:0.055634696036577225
epoch£º236	 i:7 	 global-step:4727	 l-p:0.055764131247997284
epoch£º236	 i:8 	 global-step:4728	 l-p:0.055827755481004715
epoch£º236	 i:9 	 global-step:4729	 l-p:0.05568605661392212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2413, 36.6565, 41.6221],
        [28.2413, 30.2347, 29.8069],
        [28.2413, 28.2413, 28.2413],
        [28.2413, 30.9407, 30.7642]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.05578776076436043 
model_pd.l_d.mean(): 1.252059155376628e-05 
model_pd.lagr.mean(): 0.05580028146505356 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.0841e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5820], device='cuda:0')), ('power', tensor([0.1735], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.05578776076436043
epoch£º237	 i:1 	 global-step:4741	 l-p:0.05568547174334526
epoch£º237	 i:2 	 global-step:4742	 l-p:0.055661603808403015
epoch£º237	 i:3 	 global-step:4743	 l-p:0.05565543845295906
epoch£º237	 i:4 	 global-step:4744	 l-p:0.05572573468089104
epoch£º237	 i:5 	 global-step:4745	 l-p:0.05564260482788086
epoch£º237	 i:6 	 global-step:4746	 l-p:0.05581514164805412
epoch£º237	 i:7 	 global-step:4747	 l-p:0.05599277466535568
epoch£º237	 i:8 	 global-step:4748	 l-p:0.05567769333720207
epoch£º237	 i:9 	 global-step:4749	 l-p:0.055709753185510635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2776, 37.5339, 43.5294],
        [28.2776, 29.6460, 29.1326],
        [28.2776, 34.5827, 37.2006],
        [28.2776, 28.2776, 28.2776]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.05613553896546364 
model_pd.l_d.mean(): 6.56118500046432e-05 
model_pd.lagr.mean(): 0.05620115250349045 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4569], device='cuda:0')), ('power', tensor([0.4784], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.05613553896546364
epoch£º238	 i:1 	 global-step:4761	 l-p:0.05575976148247719
epoch£º238	 i:2 	 global-step:4762	 l-p:0.055617064237594604
epoch£º238	 i:3 	 global-step:4763	 l-p:0.05569123476743698
epoch£º238	 i:4 	 global-step:4764	 l-p:0.05564741790294647
epoch£º238	 i:5 	 global-step:4765	 l-p:0.05563744530081749
epoch£º238	 i:6 	 global-step:4766	 l-p:0.05566365644335747
epoch£º238	 i:7 	 global-step:4767	 l-p:0.05566190183162689
epoch£º238	 i:8 	 global-step:4768	 l-p:0.05567555129528046
epoch£º238	 i:9 	 global-step:4769	 l-p:0.05579661577939987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[28.3012, 34.5220, 37.0517],
        [28.3012, 36.9196, 42.1193],
        [28.3012, 30.2935, 29.8634],
        [28.3012, 33.3032, 34.6922]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.05565398186445236 
model_pd.l_d.mean(): 3.823214501608163e-05 
model_pd.lagr.mean(): 0.05569221451878548 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6217], device='cuda:0')), ('power', tensor([0.1757], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.05565398186445236
epoch£º239	 i:1 	 global-step:4781	 l-p:0.055718354880809784
epoch£º239	 i:2 	 global-step:4782	 l-p:0.05565659701824188
epoch£º239	 i:3 	 global-step:4783	 l-p:0.055639490485191345
epoch£º239	 i:4 	 global-step:4784	 l-p:0.05593569576740265
epoch£º239	 i:5 	 global-step:4785	 l-p:0.05569833517074585
epoch£º239	 i:6 	 global-step:4786	 l-p:0.05570229887962341
epoch£º239	 i:7 	 global-step:4787	 l-p:0.05561362951993942
epoch£º239	 i:8 	 global-step:4788	 l-p:0.055773522704839706
epoch£º239	 i:9 	 global-step:4789	 l-p:0.05585767328739166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3087, 33.2969, 34.6731],
        [28.3087, 29.5530, 29.0410],
        [28.3087, 30.1240, 29.6553],
        [28.3087, 28.3280, 28.3096]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.055636659264564514 
model_pd.l_d.mean(): 1.4995144738350064e-05 
model_pd.lagr.mean(): 0.05565165355801582 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6569], device='cuda:0')), ('power', tensor([0.0490], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.055636659264564514
epoch£º240	 i:1 	 global-step:4801	 l-p:0.05575527995824814
epoch£º240	 i:2 	 global-step:4802	 l-p:0.05588590353727341
epoch£º240	 i:3 	 global-step:4803	 l-p:0.0556553415954113
epoch£º240	 i:4 	 global-step:4804	 l-p:0.0557047538459301
epoch£º240	 i:5 	 global-step:4805	 l-p:0.055715322494506836
epoch£º240	 i:6 	 global-step:4806	 l-p:0.05564166605472565
epoch£º240	 i:7 	 global-step:4807	 l-p:0.055636756122112274
epoch£º240	 i:8 	 global-step:4808	 l-p:0.05582224577665329
epoch£º240	 i:9 	 global-step:4809	 l-p:0.055796559900045395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2984, 29.7606, 29.2497],
        [28.2984, 28.2984, 28.2984],
        [28.2984, 34.6916, 37.3962],
        [28.2984, 36.7169, 41.6758]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.05565487593412399 
model_pd.l_d.mean(): 4.329665534896776e-05 
model_pd.lagr.mean(): 0.05569817125797272 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6328], device='cuda:0')), ('power', tensor([0.1098], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.05565487593412399
epoch£º241	 i:1 	 global-step:4821	 l-p:0.05566120892763138
epoch£º241	 i:2 	 global-step:4822	 l-p:0.05590951815247536
epoch£º241	 i:3 	 global-step:4823	 l-p:0.05561748519539833
epoch£º241	 i:4 	 global-step:4824	 l-p:0.05564482510089874
epoch£º241	 i:5 	 global-step:4825	 l-p:0.05580241605639458
epoch£º241	 i:6 	 global-step:4826	 l-p:0.05563556030392647
epoch£º241	 i:7 	 global-step:4827	 l-p:0.05563603341579437
epoch£º241	 i:8 	 global-step:4828	 l-p:0.0559467114508152
epoch£º241	 i:9 	 global-step:4829	 l-p:0.0557837188243866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2689, 32.5953, 33.4346],
        [28.2689, 28.9521, 28.5442],
        [28.2689, 28.2742, 28.2690],
        [28.2689, 36.0049, 40.1693]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.05567430704832077 
model_pd.l_d.mean(): 5.385278564062901e-05 
model_pd.lagr.mean(): 0.055728159844875336 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6214], device='cuda:0')), ('power', tensor([0.1138], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.05567430704832077
epoch£º242	 i:1 	 global-step:4841	 l-p:0.05568375438451767
epoch£º242	 i:2 	 global-step:4842	 l-p:0.05587132275104523
epoch£º242	 i:3 	 global-step:4843	 l-p:0.05586358904838562
epoch£º242	 i:4 	 global-step:4844	 l-p:0.055662669241428375
epoch£º242	 i:5 	 global-step:4845	 l-p:0.0557929202914238
epoch£º242	 i:6 	 global-step:4846	 l-p:0.055632781237363815
epoch£º242	 i:7 	 global-step:4847	 l-p:0.05563981831073761
epoch£º242	 i:8 	 global-step:4848	 l-p:0.05587111413478851
epoch£º242	 i:9 	 global-step:4849	 l-p:0.05568273365497589
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2213, 29.1632, 28.6874],
        [28.2213, 28.2257, 28.2214],
        [28.2213, 28.2213, 28.2213],
        [28.2213, 35.4693, 39.0934]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.05573582649230957 
model_pd.l_d.mean(): 3.394237864995375e-05 
model_pd.lagr.mean(): 0.05576976761221886 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6227], device='cuda:0')), ('power', tensor([0.0636], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.05573582649230957
epoch£º243	 i:1 	 global-step:4861	 l-p:0.055671386420726776
epoch£º243	 i:2 	 global-step:4862	 l-p:0.0557892844080925
epoch£º243	 i:3 	 global-step:4863	 l-p:0.055708035826683044
epoch£º243	 i:4 	 global-step:4864	 l-p:0.05569133535027504
epoch£º243	 i:5 	 global-step:4865	 l-p:0.05580982565879822
epoch£º243	 i:6 	 global-step:4866	 l-p:0.05604958534240723
epoch£º243	 i:7 	 global-step:4867	 l-p:0.05566168576478958
epoch£º243	 i:8 	 global-step:4868	 l-p:0.055664677172899246
epoch£º243	 i:9 	 global-step:4869	 l-p:0.05571483075618744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[28.1585, 28.8389, 28.4327],
        [28.1585, 32.6028, 33.5446],
        [28.1585, 29.3250, 28.8202],
        [28.1585, 34.9707, 38.1322]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.055677030235528946 
model_pd.l_d.mean(): -1.0184679922531359e-05 
model_pd.lagr.mean(): 0.05566684529185295 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6256], device='cuda:0')), ('power', tensor([-0.0180], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.055677030235528946
epoch£º244	 i:1 	 global-step:4881	 l-p:0.055694177746772766
epoch£º244	 i:2 	 global-step:4882	 l-p:0.05564812570810318
epoch£º244	 i:3 	 global-step:4883	 l-p:0.05571920424699783
epoch£º244	 i:4 	 global-step:4884	 l-p:0.05570774897933006
epoch£º244	 i:5 	 global-step:4885	 l-p:0.05568070337176323
epoch£º244	 i:6 	 global-step:4886	 l-p:0.05596993863582611
epoch£º244	 i:7 	 global-step:4887	 l-p:0.05569874495267868
epoch£º244	 i:8 	 global-step:4888	 l-p:0.056156039237976074
epoch£º244	 i:9 	 global-step:4889	 l-p:0.055696118623018265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0867, 28.0870, 28.0867],
        [28.0867, 31.0239, 30.9711],
        [28.0867, 32.5936, 33.5921],
        [28.0867, 28.0867, 28.0867]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.055743422359228134 
model_pd.l_d.mean(): -8.694441930856556e-06 
model_pd.lagr.mean(): 0.055734727531671524 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5789], device='cuda:0')), ('power', tensor([-0.0154], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.055743422359228134
epoch£º245	 i:1 	 global-step:4901	 l-p:0.05569106340408325
epoch£º245	 i:2 	 global-step:4902	 l-p:0.05571623519062996
epoch£º245	 i:3 	 global-step:4903	 l-p:0.05576149746775627
epoch£º245	 i:4 	 global-step:4904	 l-p:0.05569610372185707
epoch£º245	 i:5 	 global-step:4905	 l-p:0.0557895265519619
epoch£º245	 i:6 	 global-step:4906	 l-p:0.055733632296323776
epoch£º245	 i:7 	 global-step:4907	 l-p:0.05571366846561432
epoch£º245	 i:8 	 global-step:4908	 l-p:0.056046947836875916
epoch£º245	 i:9 	 global-step:4909	 l-p:0.0559186153113842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0131, 33.8888, 36.1158],
        [28.0131, 28.0288, 28.0137],
        [28.0131, 29.3590, 28.8506],
        [28.0131, 28.0135, 28.0131]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.055841293185949326 
model_pd.l_d.mean(): -2.8570466383825988e-05 
model_pd.lagr.mean(): 0.05581272393465042 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5580], device='cuda:0')), ('power', tensor([-0.0539], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.055841293185949326
epoch£º246	 i:1 	 global-step:4921	 l-p:0.05576015263795853
epoch£º246	 i:2 	 global-step:4922	 l-p:0.055704936385154724
epoch£º246	 i:3 	 global-step:4923	 l-p:0.055712249130010605
epoch£º246	 i:4 	 global-step:4924	 l-p:0.05582529306411743
epoch£º246	 i:5 	 global-step:4925	 l-p:0.05601688101887703
epoch£º246	 i:6 	 global-step:4926	 l-p:0.05576807260513306
epoch£º246	 i:7 	 global-step:4927	 l-p:0.05587992072105408
epoch£º246	 i:8 	 global-step:4928	 l-p:0.05574238672852516
epoch£º246	 i:9 	 global-step:4929	 l-p:0.05571966990828514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9444, 27.9444, 27.9443],
        [27.9444, 28.3851, 28.0797],
        [27.9444, 29.9538, 29.5405],
        [27.9444, 27.9445, 27.9444]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.05580892413854599 
model_pd.l_d.mean(): -9.676047193352133e-05 
model_pd.lagr.mean(): 0.05571216344833374 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5997], device='cuda:0')), ('power', tensor([-0.2115], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.05580892413854599
epoch£º247	 i:1 	 global-step:4941	 l-p:0.05572105199098587
epoch£º247	 i:2 	 global-step:4942	 l-p:0.05571969971060753
epoch£º247	 i:3 	 global-step:4943	 l-p:0.05599168688058853
epoch£º247	 i:4 	 global-step:4944	 l-p:0.055798210203647614
epoch£º247	 i:5 	 global-step:4945	 l-p:0.05571684613823891
epoch£º247	 i:6 	 global-step:4946	 l-p:0.055993784219026566
epoch£º247	 i:7 	 global-step:4947	 l-p:0.055706996470689774
epoch£º247	 i:8 	 global-step:4948	 l-p:0.055762555450201035
epoch£º247	 i:9 	 global-step:4949	 l-p:0.055895641446113586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8878, 27.8879, 27.8878],
        [27.8878, 27.8880, 27.8878],
        [27.8878, 28.3094, 28.0139],
        [27.8878, 27.8880, 27.8878]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.05572204291820526 
model_pd.l_d.mean(): -0.00012198028707643971 
model_pd.lagr.mean(): 0.055600062012672424 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6226], device='cuda:0')), ('power', tensor([-0.3450], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.05572204291820526
epoch£º248	 i:1 	 global-step:4961	 l-p:0.055739544332027435
epoch£º248	 i:2 	 global-step:4962	 l-p:0.05570434406399727
epoch£º248	 i:3 	 global-step:4963	 l-p:0.05596810579299927
epoch£º248	 i:4 	 global-step:4964	 l-p:0.055980291217565536
epoch£º248	 i:5 	 global-step:4965	 l-p:0.05574002489447594
epoch£º248	 i:6 	 global-step:4966	 l-p:0.05571502447128296
epoch£º248	 i:7 	 global-step:4967	 l-p:0.05581707879900932
epoch£º248	 i:8 	 global-step:4968	 l-p:0.05577123537659645
epoch£º248	 i:9 	 global-step:4969	 l-p:0.0560666061937809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[27.8518, 33.3457, 35.2287],
        [27.8518, 33.7709, 36.0616],
        [27.8518, 29.6831, 29.2314],
        [27.8518, 36.1455, 41.0390]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.055723533034324646 
model_pd.l_d.mean(): -7.52326741348952e-05 
model_pd.lagr.mean(): 0.05564830079674721 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6337], device='cuda:0')), ('power', tensor([-0.3331], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.055723533034324646
epoch£º249	 i:1 	 global-step:4981	 l-p:0.05575186759233475
epoch£º249	 i:2 	 global-step:4982	 l-p:0.05596829205751419
epoch£º249	 i:3 	 global-step:4983	 l-p:0.0557054802775383
epoch£º249	 i:4 	 global-step:4984	 l-p:0.055807046592235565
epoch£º249	 i:5 	 global-step:4985	 l-p:0.055774759501218796
epoch£º249	 i:6 	 global-step:4986	 l-p:0.05580098181962967
epoch£º249	 i:7 	 global-step:4987	 l-p:0.05586963891983032
epoch£º249	 i:8 	 global-step:4988	 l-p:0.05615883693099022
epoch£º249	 i:9 	 global-step:4989	 l-p:0.05571908876299858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8423, 31.1141, 31.2626],
        [27.8423, 27.8423, 27.8423],
        [27.8423, 28.4622, 28.0795],
        [27.8423, 27.8448, 27.8424]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.05587787553668022 
model_pd.l_d.mean(): -1.889200575533323e-05 
model_pd.lagr.mean(): 0.05585898458957672 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.5069e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5581], device='cuda:0')), ('power', tensor([-0.2196], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.05587787553668022
epoch£º250	 i:1 	 global-step:5001	 l-p:0.05600443854928017
epoch£º250	 i:2 	 global-step:5002	 l-p:0.05577985569834709
epoch£º250	 i:3 	 global-step:5003	 l-p:0.05584524944424629
epoch£º250	 i:4 	 global-step:5004	 l-p:0.055795133113861084
epoch£º250	 i:5 	 global-step:5005	 l-p:0.05571839213371277
epoch£º250	 i:6 	 global-step:5006	 l-p:0.05574946850538254
epoch£º250	 i:7 	 global-step:5007	 l-p:0.055710725486278534
epoch£º250	 i:8 	 global-step:5008	 l-p:0.05606118217110634
epoch£º250	 i:9 	 global-step:5009	 l-p:0.055728763341903687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8619, 30.7742, 30.7218],
        [27.8619, 27.8686, 27.8621],
        [27.8619, 29.7340, 29.2906],
        [27.8619, 27.8619, 27.8619]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.05577382817864418 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05577382817864418 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5880], device='cuda:0')), ('power', tensor([-0.2651], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.05577382817864418
epoch£º251	 i:1 	 global-step:5021	 l-p:0.055746402591466904
epoch£º251	 i:2 	 global-step:5022	 l-p:0.055924300104379654
epoch£º251	 i:3 	 global-step:5023	 l-p:0.05615498498082161
epoch£º251	 i:4 	 global-step:5024	 l-p:0.05575483292341232
epoch£º251	 i:5 	 global-step:5025	 l-p:0.05580877512693405
epoch£º251	 i:6 	 global-step:5026	 l-p:0.055748652666807175
epoch£º251	 i:7 	 global-step:5027	 l-p:0.0557071752846241
epoch£º251	 i:8 	 global-step:5028	 l-p:0.055767159909009933
epoch£º251	 i:9 	 global-step:5029	 l-p:0.05581635236740112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9026, 27.9146, 27.9031],
        [27.9026, 37.2500, 43.4409],
        [27.9026, 29.7625, 29.3151],
        [27.9026, 30.9323, 30.9395]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.05576419085264206 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576419085264206 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5723], device='cuda:0')), ('power', tensor([-0.1930], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.05576419085264206
epoch£º252	 i:1 	 global-step:5041	 l-p:0.05570656806230545
epoch£º252	 i:2 	 global-step:5042	 l-p:0.05576693266630173
epoch£º252	 i:3 	 global-step:5043	 l-p:0.055729497224092484
epoch£º252	 i:4 	 global-step:5044	 l-p:0.05571707710623741
epoch£º252	 i:5 	 global-step:5045	 l-p:0.05608430132269859
epoch£º252	 i:6 	 global-step:5046	 l-p:0.05573440343141556
epoch£º252	 i:7 	 global-step:5047	 l-p:0.055744800716638565
epoch£º252	 i:8 	 global-step:5048	 l-p:0.05594458431005478
epoch£º252	 i:9 	 global-step:5049	 l-p:0.05590720847249031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[27.9514, 37.2029, 43.2613],
        [27.9514, 29.9593, 29.5454],
        [27.9514, 29.8216, 29.3749],
        [27.9514, 33.6146, 35.6451]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.055715445429086685 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055715445429086685 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6312], device='cuda:0')), ('power', tensor([-0.2423], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.055715445429086685
epoch£º253	 i:1 	 global-step:5061	 l-p:0.05591493099927902
epoch£º253	 i:2 	 global-step:5062	 l-p:0.055937450379133224
epoch£º253	 i:3 	 global-step:5063	 l-p:0.055801067501306534
epoch£º253	 i:4 	 global-step:5064	 l-p:0.05570351332426071
epoch£º253	 i:5 	 global-step:5065	 l-p:0.05606997758150101
epoch£º253	 i:6 	 global-step:5066	 l-p:0.05568280071020126
epoch£º253	 i:7 	 global-step:5067	 l-p:0.055736567825078964
epoch£º253	 i:8 	 global-step:5068	 l-p:0.05573107302188873
epoch£º253	 i:9 	 global-step:5069	 l-p:0.055693693459033966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0037, 28.5672, 28.2058],
        [28.0037, 29.3207, 28.8124],
        [28.0037, 28.0039, 28.0037],
        [28.0037, 28.0039, 28.0037]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.05574415251612663 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574415251612663 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5987], device='cuda:0')), ('power', tensor([-0.1605], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.05574415251612663
epoch£º254	 i:1 	 global-step:5081	 l-p:0.055695995688438416
epoch£º254	 i:2 	 global-step:5082	 l-p:0.05597493052482605
epoch£º254	 i:3 	 global-step:5083	 l-p:0.05577933043241501
epoch£º254	 i:4 	 global-step:5084	 l-p:0.05586060881614685
epoch£º254	 i:5 	 global-step:5085	 l-p:0.05606262385845184
epoch£º254	 i:6 	 global-step:5086	 l-p:0.05568993464112282
epoch£º254	 i:7 	 global-step:5087	 l-p:0.055691733956336975
epoch£º254	 i:8 	 global-step:5088	 l-p:0.055674076080322266
epoch£º254	 i:9 	 global-step:5089	 l-p:0.05569419264793396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[28.0571, 37.3453, 43.4278],
        [28.0571, 30.7379, 30.5625],
        [28.0571, 33.8061, 35.9051],
        [28.0571, 38.4453, 45.9717]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.05597258359193802 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05597258359193802 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.1401e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5391], device='cuda:0')), ('power', tensor([0.0628], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.05597258359193802
epoch£º255	 i:1 	 global-step:5101	 l-p:0.0557221882045269
epoch£º255	 i:2 	 global-step:5102	 l-p:0.05575846508145332
epoch£º255	 i:3 	 global-step:5103	 l-p:0.0557052306830883
epoch£º255	 i:4 	 global-step:5104	 l-p:0.05590728670358658
epoch£º255	 i:5 	 global-step:5105	 l-p:0.05566779524087906
epoch£º255	 i:6 	 global-step:5106	 l-p:0.055717162787914276
epoch£º255	 i:7 	 global-step:5107	 l-p:0.055776871740818024
epoch£º255	 i:8 	 global-step:5108	 l-p:0.055845532566308975
epoch£º255	 i:9 	 global-step:5109	 l-p:0.055674370378255844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1109, 29.2461, 28.7446],
        [28.1109, 28.3251, 28.1525],
        [28.1109, 28.1109, 28.1109],
        [28.1109, 33.3215, 34.9151]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.05567564070224762 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05567564070224762 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6402], device='cuda:0')), ('power', tensor([-0.1062], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.05567564070224762
epoch£º256	 i:1 	 global-step:5121	 l-p:0.05575546994805336
epoch£º256	 i:2 	 global-step:5122	 l-p:0.055637046694755554
epoch£º256	 i:3 	 global-step:5123	 l-p:0.055801309645175934
epoch£º256	 i:4 	 global-step:5124	 l-p:0.0557776615023613
epoch£º256	 i:5 	 global-step:5125	 l-p:0.05590997636318207
epoch£º256	 i:6 	 global-step:5126	 l-p:0.05571949481964111
epoch£º256	 i:7 	 global-step:5127	 l-p:0.055705443024635315
epoch£º256	 i:8 	 global-step:5128	 l-p:0.05596303939819336
epoch£º256	 i:9 	 global-step:5129	 l-p:0.05568172410130501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1647, 28.1660, 28.1647],
        [28.1647, 28.1646, 28.1647],
        [28.1647, 31.5628, 31.7655],
        [28.1647, 28.5771, 28.2854]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.055836357176303864 
model_pd.l_d.mean(): 7.934067980386317e-07 
model_pd.lagr.mean(): 0.055837150663137436 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.1141e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6025], device='cuda:0')), ('power', tensor([0.0416], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.055836357176303864
epoch£º257	 i:1 	 global-step:5141	 l-p:0.055647313594818115
epoch£º257	 i:2 	 global-step:5142	 l-p:0.05580564960837364
epoch£º257	 i:3 	 global-step:5143	 l-p:0.05574902519583702
epoch£º257	 i:4 	 global-step:5144	 l-p:0.05567193776369095
epoch£º257	 i:5 	 global-step:5145	 l-p:0.0560452826321125
epoch£º257	 i:6 	 global-step:5146	 l-p:0.055680617690086365
epoch£º257	 i:7 	 global-step:5147	 l-p:0.05564989149570465
epoch£º257	 i:8 	 global-step:5148	 l-p:0.05571339651942253
epoch£º257	 i:9 	 global-step:5149	 l-p:0.05571027472615242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2161, 28.2338, 28.2169],
        [28.2161, 29.3557, 28.8523],
        [28.2161, 28.2161, 28.2161],
        [28.2161, 33.5026, 35.1523]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.05570985749363899 
model_pd.l_d.mean(): 3.7843619793420658e-06 
model_pd.lagr.mean(): 0.05571364238858223 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.3057e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6051], device='cuda:0')), ('power', tensor([0.0769], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.05570985749363899
epoch£º258	 i:1 	 global-step:5161	 l-p:0.055810898542404175
epoch£º258	 i:2 	 global-step:5162	 l-p:0.055708397179841995
epoch£º258	 i:3 	 global-step:5163	 l-p:0.05562717467546463
epoch£º258	 i:4 	 global-step:5164	 l-p:0.05566098913550377
epoch£º258	 i:5 	 global-step:5165	 l-p:0.05562479794025421
epoch£º258	 i:6 	 global-step:5166	 l-p:0.05596499145030975
epoch£º258	 i:7 	 global-step:5167	 l-p:0.05570722371339798
epoch£º258	 i:8 	 global-step:5168	 l-p:0.05581451207399368
epoch£º258	 i:9 	 global-step:5169	 l-p:0.0557723268866539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2614, 33.5534, 35.2027],
        [28.2614, 28.2614, 28.2614],
        [28.2614, 28.2626, 28.2614],
        [28.2614, 30.1212, 29.6626]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.05591800436377525 
model_pd.l_d.mean(): 2.1906065740040503e-05 
model_pd.lagr.mean(): 0.05593990907073021 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5711], device='cuda:0')), ('power', tensor([0.2115], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.05591800436377525
epoch£º259	 i:1 	 global-step:5181	 l-p:0.05567260459065437
epoch£º259	 i:2 	 global-step:5182	 l-p:0.05563731491565704
epoch£º259	 i:3 	 global-step:5183	 l-p:0.05567600205540657
epoch£º259	 i:4 	 global-step:5184	 l-p:0.05565434321761131
epoch£º259	 i:5 	 global-step:5185	 l-p:0.055838897824287415
epoch£º259	 i:6 	 global-step:5186	 l-p:0.05577889457345009
epoch£º259	 i:7 	 global-step:5187	 l-p:0.05565981566905975
epoch£º259	 i:8 	 global-step:5188	 l-p:0.055862512439489365
epoch£º259	 i:9 	 global-step:5189	 l-p:0.0556124784052372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2962, 35.1100, 38.2526],
        [28.2962, 32.6815, 33.5640],
        [28.2962, 28.2962, 28.2962],
        [28.2962, 30.3577, 29.9457]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.05567825213074684 
model_pd.l_d.mean(): 3.769430622924119e-05 
model_pd.lagr.mean(): 0.05571594461798668 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6049], device='cuda:0')), ('power', tensor([0.2114], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.05567825213074684
epoch£º260	 i:1 	 global-step:5201	 l-p:0.05606410279870033
epoch£º260	 i:2 	 global-step:5202	 l-p:0.05561976134777069
epoch£º260	 i:3 	 global-step:5203	 l-p:0.05579482391476631
epoch£º260	 i:4 	 global-step:5204	 l-p:0.055760081857442856
epoch£º260	 i:5 	 global-step:5205	 l-p:0.055666904896497726
epoch£º260	 i:6 	 global-step:5206	 l-p:0.05559903010725975
epoch£º260	 i:7 	 global-step:5207	 l-p:0.055792730301618576
epoch£º260	 i:8 	 global-step:5208	 l-p:0.05566355586051941
epoch£º260	 i:9 	 global-step:5209	 l-p:0.055611275136470795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3133, 28.3134, 28.3133],
        [28.3133, 33.2246, 34.5346],
        [28.3133, 33.6155, 35.2681],
        [28.3133, 28.7910, 28.4663]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.055713899433612823 
model_pd.l_d.mean(): 4.480564530240372e-05 
model_pd.lagr.mean(): 0.055758703500032425 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6304], device='cuda:0')), ('power', tensor([0.1680], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.055713899433612823
epoch£º261	 i:1 	 global-step:5221	 l-p:0.05562816560268402
epoch£º261	 i:2 	 global-step:5222	 l-p:0.055770404636859894
epoch£º261	 i:3 	 global-step:5223	 l-p:0.05571373924612999
epoch£º261	 i:4 	 global-step:5224	 l-p:0.05594003200531006
epoch£º261	 i:5 	 global-step:5225	 l-p:0.05562655255198479
epoch£º261	 i:6 	 global-step:5226	 l-p:0.05563756823539734
epoch£º261	 i:7 	 global-step:5227	 l-p:0.05577496066689491
epoch£º261	 i:8 	 global-step:5228	 l-p:0.055818431079387665
epoch£º261	 i:9 	 global-step:5229	 l-p:0.055609796196222305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3095, 28.4715, 28.3358],
        [28.3095, 28.9540, 28.5595],
        [28.3095, 28.3601, 28.3136],
        [28.3095, 28.3515, 28.3125]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.05578058212995529 
model_pd.l_d.mean(): 7.931231812108308e-05 
model_pd.lagr.mean(): 0.055859893560409546 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5881], device='cuda:0')), ('power', tensor([0.2210], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.05578058212995529
epoch£º262	 i:1 	 global-step:5241	 l-p:0.05563768371939659
epoch£º262	 i:2 	 global-step:5242	 l-p:0.055605582892894745
epoch£º262	 i:3 	 global-step:5243	 l-p:0.05572780594229698
epoch£º262	 i:4 	 global-step:5244	 l-p:0.05594192445278168
epoch£º262	 i:5 	 global-step:5245	 l-p:0.05580785498023033
epoch£º262	 i:6 	 global-step:5246	 l-p:0.05567871034145355
epoch£º262	 i:7 	 global-step:5247	 l-p:0.05580020695924759
epoch£º262	 i:8 	 global-step:5248	 l-p:0.055640414357185364
epoch£º262	 i:9 	 global-step:5249	 l-p:0.05564199760556221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2845, 34.5086, 37.0437],
        [28.2845, 32.8248, 33.8308],
        [28.2845, 28.6317, 28.3754],
        [28.2845, 34.0187, 36.0750]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.055834777653217316 
model_pd.l_d.mean(): 9.02770334505476e-05 
model_pd.lagr.mean(): 0.05592505633831024 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5833], device='cuda:0')), ('power', tensor([0.2031], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.055834777653217316
epoch£º263	 i:1 	 global-step:5261	 l-p:0.055667027831077576
epoch£º263	 i:2 	 global-step:5262	 l-p:0.05570954829454422
epoch£º263	 i:3 	 global-step:5263	 l-p:0.05566687509417534
epoch£º263	 i:4 	 global-step:5264	 l-p:0.05574468523263931
epoch£º263	 i:5 	 global-step:5265	 l-p:0.055660612881183624
epoch£º263	 i:6 	 global-step:5266	 l-p:0.05578802898526192
epoch£º263	 i:7 	 global-step:5267	 l-p:0.05587435141205788
epoch£º263	 i:8 	 global-step:5268	 l-p:0.055640753358602524
epoch£º263	 i:9 	 global-step:5269	 l-p:0.05575253441929817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2379, 28.3465, 28.2517],
        [28.2379, 34.5965, 37.2742],
        [28.2379, 36.6375, 41.5853],
        [28.2379, 33.2281, 34.6137]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.05565674602985382 
model_pd.l_d.mean(): 1.973770122276619e-05 
model_pd.lagr.mean(): 0.05567648261785507 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6314], device='cuda:0')), ('power', tensor([0.0385], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.05565674602985382
epoch£º264	 i:1 	 global-step:5281	 l-p:0.055678267031908035
epoch£º264	 i:2 	 global-step:5282	 l-p:0.05568443238735199
epoch£º264	 i:3 	 global-step:5283	 l-p:0.05569877475500107
epoch£º264	 i:4 	 global-step:5284	 l-p:0.055690400302410126
epoch£º264	 i:5 	 global-step:5285	 l-p:0.055886682122945786
epoch£º264	 i:6 	 global-step:5286	 l-p:0.05583886057138443
epoch£º264	 i:7 	 global-step:5287	 l-p:0.05565611645579338
epoch£º264	 i:8 	 global-step:5288	 l-p:0.055819980800151825
epoch£º264	 i:9 	 global-step:5289	 l-p:0.05585196241736412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1731, 37.1793, 42.8821],
        [28.1731, 31.2272, 31.2311],
        [28.1731, 28.1732, 28.1731],
        [28.1731, 28.1756, 28.1732]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.05574587732553482 
model_pd.l_d.mean(): 4.7832927521085367e-05 
model_pd.lagr.mean(): 0.05579371005296707 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5689], device='cuda:0')), ('power', tensor([0.0864], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.05574587732553482
epoch£º265	 i:1 	 global-step:5301	 l-p:0.05592787638306618
epoch£º265	 i:2 	 global-step:5302	 l-p:0.05569964274764061
epoch£º265	 i:3 	 global-step:5303	 l-p:0.05567090958356857
epoch£º265	 i:4 	 global-step:5304	 l-p:0.05576078221201897
epoch£º265	 i:5 	 global-step:5305	 l-p:0.05590764805674553
epoch£º265	 i:6 	 global-step:5306	 l-p:0.05570308491587639
epoch£º265	 i:7 	 global-step:5307	 l-p:0.055681828409433365
epoch£º265	 i:8 	 global-step:5308	 l-p:0.05567963793873787
epoch£º265	 i:9 	 global-step:5309	 l-p:0.055840346962213516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0958, 28.1532, 28.1008],
        [28.0958, 28.1068, 28.0962],
        [28.0958, 28.3905, 28.1657],
        [28.0958, 28.1001, 28.0959]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.055704545229673386 
model_pd.l_d.mean(): -5.0654824008233845e-05 
model_pd.lagr.mean(): 0.0556538887321949 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6105], device='cuda:0')), ('power', tensor([-0.0905], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.055704545229673386
epoch£º266	 i:1 	 global-step:5321	 l-p:0.05592632666230202
epoch£º266	 i:2 	 global-step:5322	 l-p:0.05569985881447792
epoch£º266	 i:3 	 global-step:5323	 l-p:0.05576726794242859
epoch£º266	 i:4 	 global-step:5324	 l-p:0.05575818568468094
epoch£º266	 i:5 	 global-step:5325	 l-p:0.05573809891939163
epoch£º266	 i:6 	 global-step:5326	 l-p:0.056090936064720154
epoch£º266	 i:7 	 global-step:5327	 l-p:0.055684398859739304
epoch£º266	 i:8 	 global-step:5328	 l-p:0.055718764662742615
epoch£º266	 i:9 	 global-step:5329	 l-p:0.05570892617106438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0135, 28.0136, 28.0135],
        [28.0135, 30.2480, 29.8989],
        [28.0135, 28.0135, 28.0135],
        [28.0135, 28.2124, 28.0505]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.05576031655073166 
model_pd.l_d.mean(): -3.947455115849152e-05 
model_pd.lagr.mean(): 0.055720843374729156 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5756], device='cuda:0')), ('power', tensor([-0.0751], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.05576031655073166
epoch£º267	 i:1 	 global-step:5341	 l-p:0.05569647252559662
epoch£º267	 i:2 	 global-step:5342	 l-p:0.05596345663070679
epoch£º267	 i:3 	 global-step:5343	 l-p:0.05588574334979057
epoch£º267	 i:4 	 global-step:5344	 l-p:0.055877361446619034
epoch£º267	 i:5 	 global-step:5345	 l-p:0.055855054408311844
epoch£º267	 i:6 	 global-step:5346	 l-p:0.0557524748146534
epoch£º267	 i:7 	 global-step:5347	 l-p:0.055748939514160156
epoch£º267	 i:8 	 global-step:5348	 l-p:0.055741727352142334
epoch£º267	 i:9 	 global-step:5349	 l-p:0.05569685250520706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9359, 27.9411, 27.9360],
        [27.9359, 31.7088, 32.1726],
        [27.9359, 28.1486, 27.9771],
        [27.9359, 27.9359, 27.9359]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.055750664323568344 
model_pd.l_d.mean(): -9.420016431249678e-05 
model_pd.lagr.mean(): 0.05565646290779114 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5919], device='cuda:0')), ('power', tensor([-0.2086], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.055750664323568344
epoch£º268	 i:1 	 global-step:5361	 l-p:0.05604025721549988
epoch£º268	 i:2 	 global-step:5362	 l-p:0.055850014090538025
epoch£º268	 i:3 	 global-step:5363	 l-p:0.05587296187877655
epoch£º268	 i:4 	 global-step:5364	 l-p:0.05582454055547714
epoch£º268	 i:5 	 global-step:5365	 l-p:0.055784113705158234
epoch£º268	 i:6 	 global-step:5366	 l-p:0.0557888001203537
epoch£º268	 i:7 	 global-step:5367	 l-p:0.05572289973497391
epoch£º268	 i:8 	 global-step:5368	 l-p:0.05578044801950455
epoch£º268	 i:9 	 global-step:5369	 l-p:0.055726032704114914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8725, 27.8838, 27.8729],
        [27.8725, 27.9605, 27.8825],
        [27.8725, 28.6177, 28.1929],
        [27.8725, 28.3423, 28.0230]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.05573596805334091 
model_pd.l_d.mean(): -8.515399531461298e-05 
model_pd.lagr.mean(): 0.05565081536769867 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6081], device='cuda:0')), ('power', tensor([-0.2491], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.05573596805334091
epoch£º269	 i:1 	 global-step:5381	 l-p:0.05570702254772186
epoch£º269	 i:2 	 global-step:5382	 l-p:0.055832259356975555
epoch£º269	 i:3 	 global-step:5383	 l-p:0.05579950660467148
epoch£º269	 i:4 	 global-step:5384	 l-p:0.05576510727405548
epoch£º269	 i:5 	 global-step:5385	 l-p:0.05590157210826874
epoch£º269	 i:6 	 global-step:5386	 l-p:0.055908627808094025
epoch£º269	 i:7 	 global-step:5387	 l-p:0.05577840283513069
epoch£º269	 i:8 	 global-step:5388	 l-p:0.05599050223827362
epoch£º269	 i:9 	 global-step:5389	 l-p:0.05584464967250824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8334, 27.9069, 27.8409],
        [27.8334, 32.1433, 33.0103],
        [27.8334, 27.8446, 27.8338],
        [27.8334, 30.4735, 30.2914]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.0557364895939827 
model_pd.l_d.mean(): -8.250098471762612e-05 
model_pd.lagr.mean(): 0.05565398931503296 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6231], device='cuda:0')), ('power', tensor([-0.4012], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.0557364895939827
epoch£º270	 i:1 	 global-step:5401	 l-p:0.05580976977944374
epoch£º270	 i:2 	 global-step:5402	 l-p:0.055855538696050644
epoch£º270	 i:3 	 global-step:5403	 l-p:0.055721815675497055
epoch£º270	 i:4 	 global-step:5404	 l-p:0.05572538822889328
epoch£º270	 i:5 	 global-step:5405	 l-p:0.05624733865261078
epoch£º270	 i:6 	 global-step:5406	 l-p:0.05590757727622986
epoch£º270	 i:7 	 global-step:5407	 l-p:0.055788908153772354
epoch£º270	 i:8 	 global-step:5408	 l-p:0.055760953575372696
epoch£º270	 i:9 	 global-step:5409	 l-p:0.05576564744114876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8269, 27.8269, 27.8269],
        [27.8269, 30.2383, 29.9609],
        [27.8269, 33.4555, 35.4687],
        [27.8269, 36.1215, 41.0208]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.055749110877513885 
model_pd.l_d.mean(): -1.9442297343630344e-05 
model_pd.lagr.mean(): 0.05572966858744621 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.0152e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6056], device='cuda:0')), ('power', tensor([-0.3401], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.055749110877513885
epoch£º271	 i:1 	 global-step:5421	 l-p:0.05573407933115959
epoch£º271	 i:2 	 global-step:5422	 l-p:0.055758003145456314
epoch£º271	 i:3 	 global-step:5423	 l-p:0.056175339967012405
epoch£º271	 i:4 	 global-step:5424	 l-p:0.055745676159858704
epoch£º271	 i:5 	 global-step:5425	 l-p:0.05596451088786125
epoch£º271	 i:6 	 global-step:5426	 l-p:0.05577905476093292
epoch£º271	 i:7 	 global-step:5427	 l-p:0.05586814880371094
epoch£º271	 i:8 	 global-step:5428	 l-p:0.05577918887138367
epoch£º271	 i:9 	 global-step:5429	 l-p:0.05574559047818184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8546, 28.0413, 27.8881],
        [27.8546, 34.2023, 36.9228],
        [27.8546, 28.2387, 27.9629],
        [27.8546, 27.9705, 27.8701]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.05598185583949089 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05598185583949089 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5602], device='cuda:0')), ('power', tensor([-0.1915], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.05598185583949089
epoch£º272	 i:1 	 global-step:5441	 l-p:0.05590984225273132
epoch£º272	 i:2 	 global-step:5442	 l-p:0.05577973648905754
epoch£º272	 i:3 	 global-step:5443	 l-p:0.05583762004971504
epoch£º272	 i:4 	 global-step:5444	 l-p:0.05573726445436478
epoch£º272	 i:5 	 global-step:5445	 l-p:0.0557585135102272
epoch£º272	 i:6 	 global-step:5446	 l-p:0.055779047310352325
epoch£º272	 i:7 	 global-step:5447	 l-p:0.0557917021214962
epoch£º272	 i:8 	 global-step:5448	 l-p:0.05574494227766991
epoch£º272	 i:9 	 global-step:5449	 l-p:0.05589032918214798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9030, 29.8656, 29.4418],
        [27.9030, 27.9339, 27.9049],
        [27.9030, 28.3755, 28.0548],
        [27.9030, 28.0821, 27.9343]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.056036438792943954 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056036438792943954 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5202], device='cuda:0')), ('power', tensor([-0.0516], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.056036438792943954
epoch£º273	 i:1 	 global-step:5461	 l-p:0.05575018376111984
epoch£º273	 i:2 	 global-step:5462	 l-p:0.055723875761032104
epoch£º273	 i:3 	 global-step:5463	 l-p:0.05600299686193466
epoch£º273	 i:4 	 global-step:5464	 l-p:0.055750854313373566
epoch£º273	 i:5 	 global-step:5465	 l-p:0.05578882247209549
epoch£º273	 i:6 	 global-step:5466	 l-p:0.05586806312203407
epoch£º273	 i:7 	 global-step:5467	 l-p:0.055696167051792145
epoch£º273	 i:8 	 global-step:5468	 l-p:0.05569455027580261
epoch£º273	 i:9 	 global-step:5469	 l-p:0.055780600756406784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9590, 31.8942, 32.4699],
        [27.9590, 36.2861, 41.1995],
        [27.9590, 28.0004, 27.9620],
        [27.9590, 36.2719, 41.1682]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.055706847459077835 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055706847459077835 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6316], device='cuda:0')), ('power', tensor([-0.2723], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.055706847459077835
epoch£º274	 i:1 	 global-step:5481	 l-p:0.05571352690458298
epoch£º274	 i:2 	 global-step:5482	 l-p:0.05571554973721504
epoch£º274	 i:3 	 global-step:5483	 l-p:0.05569939315319061
epoch£º274	 i:4 	 global-step:5484	 l-p:0.05608213320374489
epoch£º274	 i:5 	 global-step:5485	 l-p:0.055733293294906616
epoch£º274	 i:6 	 global-step:5486	 l-p:0.055768344551324844
epoch£º274	 i:7 	 global-step:5487	 l-p:0.05598001182079315
epoch£º274	 i:8 	 global-step:5488	 l-p:0.05577707290649414
epoch£º274	 i:9 	 global-step:5489	 l-p:0.0557861328125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0173, 32.6459, 33.7500],
        [28.0173, 32.8746, 34.1700],
        [28.0173, 28.0173, 28.0173],
        [28.0173, 28.1972, 28.0487]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.05568532273173332 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568532273173332 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6471], device='cuda:0')), ('power', tensor([-0.2389], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.05568532273173332
epoch£º275	 i:1 	 global-step:5501	 l-p:0.05612114444375038
epoch£º275	 i:2 	 global-step:5502	 l-p:0.05571331828832626
epoch£º275	 i:3 	 global-step:5503	 l-p:0.05583387240767479
epoch£º275	 i:4 	 global-step:5504	 l-p:0.05570434406399727
epoch£º275	 i:5 	 global-step:5505	 l-p:0.05566544458270073
epoch£º275	 i:6 	 global-step:5506	 l-p:0.05573858320713043
epoch£º275	 i:7 	 global-step:5507	 l-p:0.05586794763803482
epoch£º275	 i:8 	 global-step:5508	 l-p:0.055774468928575516
epoch£º275	 i:9 	 global-step:5509	 l-p:0.05572616308927536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0770, 35.8407, 40.0702],
        [28.0770, 28.2840, 28.1164],
        [28.0770, 28.0772, 28.0770],
        [28.0770, 28.0975, 28.0780]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.055664900690317154 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055664900690317154 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6573], device='cuda:0')), ('power', tensor([-0.1832], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.055664900690317154
epoch£º276	 i:1 	 global-step:5521	 l-p:0.05569884553551674
epoch£º276	 i:2 	 global-step:5522	 l-p:0.05624620243906975
epoch£º276	 i:3 	 global-step:5523	 l-p:0.05564817786216736
epoch£º276	 i:4 	 global-step:5524	 l-p:0.05575103312730789
epoch£º276	 i:5 	 global-step:5525	 l-p:0.05567193776369095
epoch£º276	 i:6 	 global-step:5526	 l-p:0.055896710604429245
epoch£º276	 i:7 	 global-step:5527	 l-p:0.05570705607533455
epoch£º276	 i:8 	 global-step:5528	 l-p:0.05574537068605423
epoch£º276	 i:9 	 global-step:5529	 l-p:0.055666644126176834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1369, 34.5521, 37.3018],
        [28.1369, 35.2604, 38.7612],
        [28.1369, 28.1394, 28.1369],
        [28.1369, 33.6038, 35.4274]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.055706873536109924 
model_pd.l_d.mean(): 5.15026279401809e-08 
model_pd.lagr.mean(): 0.0557069256901741 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.2276e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6020], device='cuda:0')), ('power', tensor([0.0289], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.055706873536109924
epoch£º277	 i:1 	 global-step:5541	 l-p:0.05567903444170952
epoch£º277	 i:2 	 global-step:5542	 l-p:0.055936381220817566
epoch£º277	 i:3 	 global-step:5543	 l-p:0.055653076618909836
epoch£º277	 i:4 	 global-step:5544	 l-p:0.0557420514523983
epoch£º277	 i:5 	 global-step:5545	 l-p:0.055718839168548584
epoch£º277	 i:6 	 global-step:5546	 l-p:0.05570591241121292
epoch£º277	 i:7 	 global-step:5547	 l-p:0.05564306676387787
epoch£º277	 i:8 	 global-step:5548	 l-p:0.0559304878115654
epoch£º277	 i:9 	 global-step:5549	 l-p:0.05584762617945671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1964, 28.2902, 28.2073],
        [28.1964, 28.5480, 28.2893],
        [28.1964, 29.2392, 28.7472],
        [28.1964, 35.0181, 38.1842]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.055908650159835815 
model_pd.l_d.mean(): 3.036744601558894e-06 
model_pd.lagr.mean(): 0.05591168627142906 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.7452e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5827], device='cuda:0')), ('power', tensor([0.1536], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.055908650159835815
epoch£º278	 i:1 	 global-step:5561	 l-p:0.055757131427526474
epoch£º278	 i:2 	 global-step:5562	 l-p:0.05571269989013672
epoch£º278	 i:3 	 global-step:5563	 l-p:0.05580451339483261
epoch£º278	 i:4 	 global-step:5564	 l-p:0.055826082825660706
epoch£º278	 i:5 	 global-step:5565	 l-p:0.05575455725193024
epoch£º278	 i:6 	 global-step:5566	 l-p:0.055677466094493866
epoch£º278	 i:7 	 global-step:5567	 l-p:0.05565301328897476
epoch£º278	 i:8 	 global-step:5568	 l-p:0.05568697676062584
epoch£º278	 i:9 	 global-step:5569	 l-p:0.05565368011593819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2524, 29.8647, 29.3671],
        [28.2524, 28.6893, 28.3848],
        [28.2524, 34.6952, 37.4569],
        [28.2524, 29.6778, 29.1663]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.055659227073192596 
model_pd.l_d.mean(): 2.829392542480491e-06 
model_pd.lagr.mean(): 0.05566205829381943 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.8976e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6314], device='cuda:0')), ('power', tensor([0.0423], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.055659227073192596
epoch£º279	 i:1 	 global-step:5581	 l-p:0.05567795783281326
epoch£º279	 i:2 	 global-step:5582	 l-p:0.056048259139060974
epoch£º279	 i:3 	 global-step:5583	 l-p:0.05576859042048454
epoch£º279	 i:4 	 global-step:5584	 l-p:0.05566605553030968
epoch£º279	 i:5 	 global-step:5585	 l-p:0.0557575486600399
epoch£º279	 i:6 	 global-step:5586	 l-p:0.05562644451856613
epoch£º279	 i:7 	 global-step:5587	 l-p:0.055870771408081055
epoch£º279	 i:8 	 global-step:5588	 l-p:0.055634789168834686
epoch£º279	 i:9 	 global-step:5589	 l-p:0.05561111867427826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2972, 28.3087, 28.2976],
        [28.2972, 28.7254, 28.4252],
        [28.2972, 29.1519, 28.6942],
        [28.2972, 29.7742, 29.2640]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.055722426623106 
model_pd.l_d.mean(): 1.858033283497207e-05 
model_pd.lagr.mean(): 0.05574100837111473 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6267], device='cuda:0')), ('power', tensor([0.1333], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.055722426623106
epoch£º280	 i:1 	 global-step:5601	 l-p:0.05576576292514801
epoch£º280	 i:2 	 global-step:5602	 l-p:0.05566391721367836
epoch£º280	 i:3 	 global-step:5603	 l-p:0.055774983018636703
epoch£º280	 i:4 	 global-step:5604	 l-p:0.05577829107642174
epoch£º280	 i:5 	 global-step:5605	 l-p:0.05565500259399414
epoch£º280	 i:6 	 global-step:5606	 l-p:0.05562876537442207
epoch£º280	 i:7 	 global-step:5607	 l-p:0.055872391909360886
epoch£º280	 i:8 	 global-step:5608	 l-p:0.055696915835142136
epoch£º280	 i:9 	 global-step:5609	 l-p:0.055679261684417725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3254, 31.0148, 30.8294],
        [28.3254, 28.3316, 28.3255],
        [28.3254, 28.4836, 28.3507],
        [28.3254, 29.1304, 28.6851]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.05563196912407875 
model_pd.l_d.mean(): 2.968515400425531e-05 
model_pd.lagr.mean(): 0.055661655962467194 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6434], device='cuda:0')), ('power', tensor([0.1287], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.05563196912407875
epoch£º281	 i:1 	 global-step:5621	 l-p:0.05562010779976845
epoch£º281	 i:2 	 global-step:5622	 l-p:0.05569884553551674
epoch£º281	 i:3 	 global-step:5623	 l-p:0.055825114250183105
epoch£º281	 i:4 	 global-step:5624	 l-p:0.05587835609912872
epoch£º281	 i:5 	 global-step:5625	 l-p:0.05570916831493378
epoch£º281	 i:6 	 global-step:5626	 l-p:0.055622730404138565
epoch£º281	 i:7 	 global-step:5627	 l-p:0.05565427616238594
epoch£º281	 i:8 	 global-step:5628	 l-p:0.055937737226486206
epoch£º281	 i:9 	 global-step:5629	 l-p:0.055617913603782654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[28.3320, 29.1878, 28.7295],
        [28.3320, 31.3996, 31.4010],
        [28.3320, 29.3222, 28.8365],
        [28.3320, 36.4031, 40.9441]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.05589199438691139 
model_pd.l_d.mean(): 0.00014665901835542172 
model_pd.lagr.mean(): 0.05603865161538124 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5135], device='cuda:0')), ('power', tensor([0.4427], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.05589199438691139
epoch£º282	 i:1 	 global-step:5641	 l-p:0.05583052337169647
epoch£º282	 i:2 	 global-step:5642	 l-p:0.05561848357319832
epoch£º282	 i:3 	 global-step:5643	 l-p:0.05578550323843956
epoch£º282	 i:4 	 global-step:5644	 l-p:0.05576779320836067
epoch£º282	 i:5 	 global-step:5645	 l-p:0.055657509714365005
epoch£º282	 i:6 	 global-step:5646	 l-p:0.05564704164862633
epoch£º282	 i:7 	 global-step:5647	 l-p:0.05566493049263954
epoch£º282	 i:8 	 global-step:5648	 l-p:0.055686186999082565
epoch£º282	 i:9 	 global-step:5649	 l-p:0.055656798183918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3120, 32.6457, 33.4866],
        [28.3120, 36.3772, 40.9148],
        [28.3120, 32.6453, 33.4861],
        [28.3120, 28.3626, 28.3161]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.05568508058786392 
model_pd.l_d.mean(): 8.361550862900913e-05 
model_pd.lagr.mean(): 0.0557686947286129 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5991], device='cuda:0')), ('power', tensor([0.1947], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.05568508058786392
epoch£º283	 i:1 	 global-step:5661	 l-p:0.05567722022533417
epoch£º283	 i:2 	 global-step:5662	 l-p:0.05564437806606293
epoch£º283	 i:3 	 global-step:5663	 l-p:0.05568728223443031
epoch£º283	 i:4 	 global-step:5664	 l-p:0.05569453537464142
epoch£º283	 i:5 	 global-step:5665	 l-p:0.05578036978840828
epoch£º283	 i:6 	 global-step:5666	 l-p:0.056014034897089005
epoch£º283	 i:7 	 global-step:5667	 l-p:0.05570176988840103
epoch£º283	 i:8 	 global-step:5668	 l-p:0.05561584234237671
epoch£º283	 i:9 	 global-step:5669	 l-p:0.05577816441655159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2652, 28.4269, 28.2914],
        [28.2652, 28.3533, 28.2751],
        [28.2652, 28.2692, 28.2653],
        [28.2652, 31.4658, 31.5450]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.05562685430049896 
model_pd.l_d.mean(): 2.1245383322821e-05 
model_pd.lagr.mean(): 0.05564809963107109 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6602], device='cuda:0')), ('power', tensor([0.0415], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.05562685430049896
epoch£º284	 i:1 	 global-step:5681	 l-p:0.05572538077831268
epoch£º284	 i:2 	 global-step:5682	 l-p:0.05568597838282585
epoch£º284	 i:3 	 global-step:5683	 l-p:0.05566134303808212
epoch£º284	 i:4 	 global-step:5684	 l-p:0.05564989894628525
epoch£º284	 i:5 	 global-step:5685	 l-p:0.05585198104381561
epoch£º284	 i:6 	 global-step:5686	 l-p:0.05564764514565468
epoch£º284	 i:7 	 global-step:5687	 l-p:0.05571163445711136
epoch£º284	 i:8 	 global-step:5688	 l-p:0.05599826201796532
epoch£º284	 i:9 	 global-step:5689	 l-p:0.05584762617945671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1951, 28.1951, 28.1951],
        [28.1951, 28.3247, 28.2135],
        [28.1951, 29.0541, 28.5963],
        [28.1951, 32.3360, 33.0417]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.055671002715826035 
model_pd.l_d.mean(): 1.121761215472361e-06 
model_pd.lagr.mean(): 0.05567212402820587 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6323], device='cuda:0')), ('power', tensor([0.0020], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.055671002715826035
epoch£º285	 i:1 	 global-step:5701	 l-p:0.05571921169757843
epoch£º285	 i:2 	 global-step:5702	 l-p:0.0559355765581131
epoch£º285	 i:3 	 global-step:5703	 l-p:0.055745553225278854
epoch£º285	 i:4 	 global-step:5704	 l-p:0.05564434826374054
epoch£º285	 i:5 	 global-step:5705	 l-p:0.05593385919928551
epoch£º285	 i:6 	 global-step:5706	 l-p:0.055821724236011505
epoch£º285	 i:7 	 global-step:5707	 l-p:0.055702533572912216
epoch£º285	 i:8 	 global-step:5708	 l-p:0.05572505667805672
epoch£º285	 i:9 	 global-step:5709	 l-p:0.05567888543009758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1084, 28.3425, 28.1564],
        [28.1084, 28.1087, 28.1083],
        [28.1084, 28.1083, 28.1084],
        [28.1084, 28.1374, 28.1101]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.05567973107099533 
model_pd.l_d.mean(): -7.314098183996975e-05 
model_pd.lagr.mean(): 0.05560658872127533 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6354], device='cuda:0')), ('power', tensor([-0.1261], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.05567973107099533
epoch£º286	 i:1 	 global-step:5721	 l-p:0.05583982169628143
epoch£º286	 i:2 	 global-step:5722	 l-p:0.05580510199069977
epoch£º286	 i:3 	 global-step:5723	 l-p:0.05570533499121666
epoch£º286	 i:4 	 global-step:5724	 l-p:0.0557372160255909
epoch£º286	 i:5 	 global-step:5725	 l-p:0.055907443165779114
epoch£º286	 i:6 	 global-step:5726	 l-p:0.0559721365571022
epoch£º286	 i:7 	 global-step:5727	 l-p:0.05571087822318077
epoch£º286	 i:8 	 global-step:5728	 l-p:0.055695485323667526
epoch£º286	 i:9 	 global-step:5729	 l-p:0.05572867766022682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0133, 31.8300, 32.3179],
        [28.0133, 32.6412, 33.7451],
        [28.0133, 29.3682, 28.8598],
        [28.0133, 28.3997, 28.1222]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.05571598932147026 
model_pd.l_d.mean(): -9.427402255823836e-05 
model_pd.lagr.mean(): 0.05562171712517738 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6141], device='cuda:0')), ('power', tensor([-0.1716], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.05571598932147026
epoch£º287	 i:1 	 global-step:5741	 l-p:0.05572289228439331
epoch£º287	 i:2 	 global-step:5742	 l-p:0.055693309754133224
epoch£º287	 i:3 	 global-step:5743	 l-p:0.05571133643388748
epoch£º287	 i:4 	 global-step:5744	 l-p:0.05571037530899048
epoch£º287	 i:5 	 global-step:5745	 l-p:0.05582837015390396
epoch£º287	 i:6 	 global-step:5746	 l-p:0.05601586773991585
epoch£º287	 i:7 	 global-step:5747	 l-p:0.05590754747390747
epoch£º287	 i:8 	 global-step:5748	 l-p:0.05590099096298218
epoch£º287	 i:9 	 global-step:5749	 l-p:0.05578861013054848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9226, 28.1353, 27.9639],
        [27.9226, 27.9226, 27.9226],
        [27.9226, 27.9387, 27.9233],
        [27.9226, 31.1787, 31.3125]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.055720552802085876 
model_pd.l_d.mean(): -0.00012711752788163722 
model_pd.lagr.mean(): 0.05559343472123146 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6202], device='cuda:0')), ('power', tensor([-0.2693], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.055720552802085876
epoch£º288	 i:1 	 global-step:5761	 l-p:0.055693455040454865
epoch£º288	 i:2 	 global-step:5762	 l-p:0.055737197399139404
epoch£º288	 i:3 	 global-step:5763	 l-p:0.055957991629838943
epoch£º288	 i:4 	 global-step:5764	 l-p:0.05578773841261864
epoch£º288	 i:5 	 global-step:5765	 l-p:0.05574880540370941
epoch£º288	 i:6 	 global-step:5766	 l-p:0.0558602400124073
epoch£º288	 i:7 	 global-step:5767	 l-p:0.05574925243854523
epoch£º288	 i:8 	 global-step:5768	 l-p:0.055774886161088943
epoch£º288	 i:9 	 global-step:5769	 l-p:0.05615346133708954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8498, 33.1096, 34.7774],
        [27.8498, 27.8654, 27.8505],
        [27.8498, 27.8610, 27.8502],
        [27.8498, 28.0551, 27.8888]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.055945269763469696 
model_pd.l_d.mean(): -6.49478824925609e-05 
model_pd.lagr.mean(): 0.05588032305240631 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5213], device='cuda:0')), ('power', tensor([-0.1838], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.055945269763469696
epoch£º289	 i:1 	 global-step:5781	 l-p:0.055788516998291016
epoch£º289	 i:2 	 global-step:5782	 l-p:0.05583183467388153
epoch£º289	 i:3 	 global-step:5783	 l-p:0.055703867226839066
epoch£º289	 i:4 	 global-step:5784	 l-p:0.05576030910015106
epoch£º289	 i:5 	 global-step:5785	 l-p:0.05574648827314377
epoch£º289	 i:6 	 global-step:5786	 l-p:0.05594285950064659
epoch£º289	 i:7 	 global-step:5787	 l-p:0.05586154758930206
epoch£º289	 i:8 	 global-step:5788	 l-p:0.05577852949500084
epoch£º289	 i:9 	 global-step:5789	 l-p:0.05596165359020233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8049, 27.8050, 27.8049],
        [27.8049, 27.8540, 27.8089],
        [27.8049, 27.8049, 27.8049],
        [27.8049, 29.1120, 28.6075]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.05575088411569595 
model_pd.l_d.mean(): -9.253089956473559e-05 
model_pd.lagr.mean(): 0.05565835162997246 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6242], device='cuda:0')), ('power', tensor([-0.4522], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.05575088411569595
epoch£º290	 i:1 	 global-step:5801	 l-p:0.055721063166856766
epoch£º290	 i:2 	 global-step:5802	 l-p:0.055770423263311386
epoch£º290	 i:3 	 global-step:5803	 l-p:0.0558127760887146
epoch£º290	 i:4 	 global-step:5804	 l-p:0.05590300261974335
epoch£º290	 i:5 	 global-step:5805	 l-p:0.05573726445436478
epoch£º290	 i:6 	 global-step:5806	 l-p:0.05579283460974693
epoch£º290	 i:7 	 global-step:5807	 l-p:0.055795229971408844
epoch£º290	 i:8 	 global-step:5808	 l-p:0.05614989250898361
epoch£º290	 i:9 	 global-step:5809	 l-p:0.055953335016965866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7979, 29.4371, 28.9546],
        [27.7979, 32.2560, 33.2434],
        [27.7979, 28.0138, 27.8403],
        [27.7979, 27.7982, 27.7978]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.055723924189805984 
model_pd.l_d.mean(): -2.0300931282690726e-05 
model_pd.lagr.mean(): 0.055703625082969666 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.7114e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6495], device='cuda:0')), ('power', tensor([-0.4886], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.055723924189805984
epoch£º291	 i:1 	 global-step:5821	 l-p:0.055903512984514236
epoch£º291	 i:2 	 global-step:5822	 l-p:0.055764663964509964
epoch£º291	 i:3 	 global-step:5823	 l-p:0.05586865171790123
epoch£º291	 i:4 	 global-step:5824	 l-p:0.055746372789144516
epoch£º291	 i:5 	 global-step:5825	 l-p:0.056004785001277924
epoch£º291	 i:6 	 global-step:5826	 l-p:0.05597614496946335
epoch£º291	 i:7 	 global-step:5827	 l-p:0.05584995076060295
epoch£º291	 i:8 	 global-step:5828	 l-p:0.055771395564079285
epoch£º291	 i:9 	 global-step:5829	 l-p:0.05574933439493179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8317, 27.8319, 27.8317],
        [27.8317, 27.9137, 27.8406],
        [27.8317, 27.8376, 27.8319],
        [27.8317, 27.8318, 27.8317]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.05580837279558182 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05580837279558182 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5915], device='cuda:0')), ('power', tensor([-0.3196], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.05580837279558182
epoch£º292	 i:1 	 global-step:5841	 l-p:0.056007470935583115
epoch£º292	 i:2 	 global-step:5842	 l-p:0.055741291493177414
epoch£º292	 i:3 	 global-step:5843	 l-p:0.05586365610361099
epoch£º292	 i:4 	 global-step:5844	 l-p:0.05573970824480057
epoch£º292	 i:5 	 global-step:5845	 l-p:0.055774763226509094
epoch£º292	 i:6 	 global-step:5846	 l-p:0.055719245225191116
epoch£º292	 i:7 	 global-step:5847	 l-p:0.05612485855817795
epoch£º292	 i:8 	 global-step:5848	 l-p:0.05575292557477951
epoch£º292	 i:9 	 global-step:5849	 l-p:0.0557238943874836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[27.8866, 29.8917, 29.4793],
        [27.8866, 29.8445, 29.4201],
        [27.8866, 28.7357, 28.2832],
        [27.8866, 28.5601, 28.1580]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.05572838708758354 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05572838708758354 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6387], device='cuda:0')), ('power', tensor([-0.3324], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.05572838708758354
epoch£º293	 i:1 	 global-step:5861	 l-p:0.05592159926891327
epoch£º293	 i:2 	 global-step:5862	 l-p:0.05573122203350067
epoch£º293	 i:3 	 global-step:5863	 l-p:0.05597342550754547
epoch£º293	 i:4 	 global-step:5864	 l-p:0.05570779740810394
epoch£º293	 i:5 	 global-step:5865	 l-p:0.05575408786535263
epoch£º293	 i:6 	 global-step:5866	 l-p:0.05591275170445442
epoch£º293	 i:7 	 global-step:5867	 l-p:0.05589760094881058
epoch£º293	 i:8 	 global-step:5868	 l-p:0.05569074675440788
epoch£º293	 i:9 	 global-step:5869	 l-p:0.055804841220378876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[27.9491, 28.9822, 28.4947],
        [27.9491, 32.7940, 34.0859],
        [27.9491, 33.4709, 35.3683],
        [27.9491, 30.2863, 29.9755]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.05589983984827995 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05589983984827995 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5784], device='cuda:0')), ('power', tensor([-0.1446], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.05589983984827995
epoch£º294	 i:1 	 global-step:5881	 l-p:0.05574101209640503
epoch£º294	 i:2 	 global-step:5882	 l-p:0.055831048637628555
epoch£º294	 i:3 	 global-step:5883	 l-p:0.055930450558662415
epoch£º294	 i:4 	 global-step:5884	 l-p:0.0557815358042717
epoch£º294	 i:5 	 global-step:5885	 l-p:0.05574198439717293
epoch£º294	 i:6 	 global-step:5886	 l-p:0.05572165548801422
epoch£º294	 i:7 	 global-step:5887	 l-p:0.055705830454826355
epoch£º294	 i:8 	 global-step:5888	 l-p:0.05576176941394806
epoch£º294	 i:9 	 global-step:5889	 l-p:0.05586313083767891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0145, 29.5351, 29.0345],
        [28.0145, 28.0178, 28.0145],
        [28.0145, 28.0374, 28.0157],
        [28.0145, 28.0146, 28.0145]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.05567120388150215 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05567120388150215 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6629], device='cuda:0')), ('power', tensor([-0.2146], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.05567120388150215
epoch£º295	 i:1 	 global-step:5901	 l-p:0.055905550718307495
epoch£º295	 i:2 	 global-step:5902	 l-p:0.05579393357038498
epoch£º295	 i:3 	 global-step:5903	 l-p:0.055703554302453995
epoch£º295	 i:4 	 global-step:5904	 l-p:0.055749595165252686
epoch£º295	 i:5 	 global-step:5905	 l-p:0.055655889213085175
epoch£º295	 i:6 	 global-step:5906	 l-p:0.055846042931079865
epoch£º295	 i:7 	 global-step:5907	 l-p:0.05606590583920479
epoch£º295	 i:8 	 global-step:5908	 l-p:0.055725838989019394
epoch£º295	 i:9 	 global-step:5909	 l-p:0.05571208894252777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0809, 29.8824, 29.4179],
        [28.0809, 28.1770, 28.0923],
        [28.0809, 28.0922, 28.0813],
        [28.0809, 28.0809, 28.0809]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.05569745972752571 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05569745972752571 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6227], device='cuda:0')), ('power', tensor([-0.1264], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.05569745972752571
epoch£º296	 i:1 	 global-step:5921	 l-p:0.05570881813764572
epoch£º296	 i:2 	 global-step:5922	 l-p:0.055724456906318665
epoch£º296	 i:3 	 global-step:5923	 l-p:0.05592626333236694
epoch£º296	 i:4 	 global-step:5924	 l-p:0.05589359626173973
epoch£º296	 i:5 	 global-step:5925	 l-p:0.05578140541911125
epoch£º296	 i:6 	 global-step:5926	 l-p:0.055829957127571106
epoch£º296	 i:7 	 global-step:5927	 l-p:0.05564586818218231
epoch£º296	 i:8 	 global-step:5928	 l-p:0.055795829743146896
epoch£º296	 i:9 	 global-step:5929	 l-p:0.05567719787359238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1479, 36.5196, 41.4507],
        [28.1479, 29.5007, 28.9897],
        [28.1479, 28.3602, 28.1889],
        [28.1479, 28.1592, 28.1483]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.055808525532484055 
model_pd.l_d.mean(): 1.1649195386098654e-07 
model_pd.lagr.mean(): 0.05580864101648331 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.6216e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5629], device='cuda:0')), ('power', tensor([0.0850], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.055808525532484055
epoch£º297	 i:1 	 global-step:5941	 l-p:0.05571430176496506
epoch£º297	 i:2 	 global-step:5942	 l-p:0.05584712699055672
epoch£º297	 i:3 	 global-step:5943	 l-p:0.056075360625982285
epoch£º297	 i:4 	 global-step:5944	 l-p:0.05572010576725006
epoch£º297	 i:5 	 global-step:5945	 l-p:0.05567682161927223
epoch£º297	 i:6 	 global-step:5946	 l-p:0.055664870887994766
epoch£º297	 i:7 	 global-step:5947	 l-p:0.055636242032051086
epoch£º297	 i:8 	 global-step:5948	 l-p:0.05565689131617546
epoch£º297	 i:9 	 global-step:5949	 l-p:0.05573302134871483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2136, 28.2138, 28.2136],
        [28.2136, 28.8555, 28.4625],
        [28.2136, 28.2155, 28.2137],
        [28.2136, 37.9438, 44.5610]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.05576803907752037 
model_pd.l_d.mean(): 1.7814010107031208e-06 
model_pd.lagr.mean(): 0.055769819766283035 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.9793e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5934], device='cuda:0')), ('power', tensor([0.0674], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.05576803907752037
epoch£º298	 i:1 	 global-step:5961	 l-p:0.055655669420957565
epoch£º298	 i:2 	 global-step:5962	 l-p:0.055903807282447815
epoch£º298	 i:3 	 global-step:5963	 l-p:0.05581633374094963
epoch£º298	 i:4 	 global-step:5964	 l-p:0.05572105944156647
epoch£º298	 i:5 	 global-step:5965	 l-p:0.055745381861925125
epoch£º298	 i:6 	 global-step:5966	 l-p:0.0556427463889122
epoch£º298	 i:7 	 global-step:5967	 l-p:0.055689021944999695
epoch£º298	 i:8 	 global-step:5968	 l-p:0.055663201957941055
epoch£º298	 i:9 	 global-step:5969	 l-p:0.05578678101301193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2739, 28.4318, 28.2992],
        [28.2739, 29.3244, 28.8303],
        [28.2739, 30.3064, 29.8875],
        [28.2739, 31.5339, 31.6467]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.05565408989787102 
model_pd.l_d.mean(): 6.828030109318206e-06 
model_pd.lagr.mean(): 0.0556609183549881 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.7091e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6275], device='cuda:0')), ('power', tensor([0.0823], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.05565408989787102
epoch£º299	 i:1 	 global-step:5981	 l-p:0.055664777755737305
epoch£º299	 i:2 	 global-step:5982	 l-p:0.055675361305475235
epoch£º299	 i:3 	 global-step:5983	 l-p:0.055856335908174515
epoch£º299	 i:4 	 global-step:5984	 l-p:0.055604834109544754
epoch£º299	 i:5 	 global-step:5985	 l-p:0.05567942559719086
epoch£º299	 i:6 	 global-step:5986	 l-p:0.05561841279268265
epoch£º299	 i:7 	 global-step:5987	 l-p:0.05566981062293053
epoch£º299	 i:8 	 global-step:5988	 l-p:0.05613350123167038
epoch£º299	 i:9 	 global-step:5989	 l-p:0.05571290850639343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3217, 36.6758, 41.5532],
        [28.3217, 28.7789, 28.4641],
        [28.3217, 28.3217, 28.3217],
        [28.3217, 28.7598, 28.4545]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.05562712624669075 
model_pd.l_d.mean(): 1.5559844541712664e-05 
model_pd.lagr.mean(): 0.055642686784267426 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6498], device='cuda:0')), ('power', tensor([0.0932], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.05562712624669075
epoch£º300	 i:1 	 global-step:6001	 l-p:0.05616268143057823
epoch£º300	 i:2 	 global-step:6002	 l-p:0.055703893303871155
epoch£º300	 i:3 	 global-step:6003	 l-p:0.05564039573073387
epoch£º300	 i:4 	 global-step:6004	 l-p:0.05559932067990303
epoch£º300	 i:5 	 global-step:6005	 l-p:0.055675581097602844
epoch£º300	 i:6 	 global-step:6006	 l-p:0.05565982311964035
epoch£º300	 i:7 	 global-step:6007	 l-p:0.05567912012338638
epoch£º300	 i:8 	 global-step:6008	 l-p:0.05583791434764862
epoch£º300	 i:9 	 global-step:6009	 l-p:0.05559851974248886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3480, 29.3388, 28.8528],
        [28.3480, 28.3480, 28.3480],
        [28.3480, 31.9359, 32.2447],
        [28.3480, 28.3480, 28.3480]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.05586014688014984 
model_pd.l_d.mean(): 9.10585731617175e-05 
model_pd.lagr.mean(): 0.055951204150915146 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5871], device='cuda:0')), ('power', tensor([0.3369], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.05586014688014984
epoch£º301	 i:1 	 global-step:6021	 l-p:0.055642999708652496
epoch£º301	 i:2 	 global-step:6022	 l-p:0.055830445140600204
epoch£º301	 i:3 	 global-step:6023	 l-p:0.05562889948487282
epoch£º301	 i:4 	 global-step:6024	 l-p:0.05575897917151451
epoch£º301	 i:5 	 global-step:6025	 l-p:0.05578678846359253
epoch£º301	 i:6 	 global-step:6026	 l-p:0.0556279756128788
epoch£º301	 i:7 	 global-step:6027	 l-p:0.05567246302962303
epoch£º301	 i:8 	 global-step:6028	 l-p:0.055654700845479965
epoch£º301	 i:9 	 global-step:6029	 l-p:0.055691830813884735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3445, 29.5904, 29.0777],
        [28.3445, 28.3608, 28.3452],
        [28.3445, 36.1929, 40.4730],
        [28.3445, 28.3590, 28.3451]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.05562041327357292 
model_pd.l_d.mean(): 4.421621633809991e-05 
model_pd.lagr.mean(): 0.055664628744125366 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6606], device='cuda:0')), ('power', tensor([0.1163], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.05562041327357292
epoch£º302	 i:1 	 global-step:6041	 l-p:0.05562983825802803
epoch£º302	 i:2 	 global-step:6042	 l-p:0.055680062621831894
epoch£º302	 i:3 	 global-step:6043	 l-p:0.05588896572589874
epoch£º302	 i:4 	 global-step:6044	 l-p:0.05583809316158295
epoch£º302	 i:5 	 global-step:6045	 l-p:0.055648479610681534
epoch£º302	 i:6 	 global-step:6046	 l-p:0.05569104105234146
epoch£º302	 i:7 	 global-step:6047	 l-p:0.055675897747278214
epoch£º302	 i:8 	 global-step:6048	 l-p:0.05582525581121445
epoch£º302	 i:9 	 global-step:6049	 l-p:0.055696532130241394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3092, 28.3093, 28.3092],
        [28.3092, 29.4247, 28.9223],
        [28.3092, 37.8363, 44.1700],
        [28.3092, 30.2132, 29.7623]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.055853407829999924 
model_pd.l_d.mean(): 0.00010887909593293443 
model_pd.lagr.mean(): 0.05596228688955307 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5748], device='cuda:0')), ('power', tensor([0.2263], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.055853407829999924
epoch£º303	 i:1 	 global-step:6061	 l-p:0.05565136298537254
epoch£º303	 i:2 	 global-step:6062	 l-p:0.05564647912979126
epoch£º303	 i:3 	 global-step:6063	 l-p:0.05566965416073799
epoch£º303	 i:4 	 global-step:6064	 l-p:0.05564841628074646
epoch£º303	 i:5 	 global-step:6065	 l-p:0.05564865097403526
epoch£º303	 i:6 	 global-step:6066	 l-p:0.055724408477544785
epoch£º303	 i:7 	 global-step:6067	 l-p:0.055627334862947464
epoch£º303	 i:8 	 global-step:6068	 l-p:0.05591990426182747
epoch£º303	 i:9 	 global-step:6069	 l-p:0.055913619697093964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2435, 28.2435, 28.2435],
        [28.2435, 28.2435, 28.2435],
        [28.2435, 28.2590, 28.2442],
        [28.2435, 28.2488, 28.2437]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.055918436497449875 
model_pd.l_d.mean(): 0.00010818463488249108 
model_pd.lagr.mean(): 0.05602662265300751 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5849], device='cuda:0')), ('power', tensor([0.1939], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.055918436497449875
epoch£º304	 i:1 	 global-step:6081	 l-p:0.05581769719719887
epoch£º304	 i:2 	 global-step:6082	 l-p:0.055794861167669296
epoch£º304	 i:3 	 global-step:6083	 l-p:0.055717963725328445
epoch£º304	 i:4 	 global-step:6084	 l-p:0.05571749433875084
epoch£º304	 i:5 	 global-step:6085	 l-p:0.05580786615610123
epoch£º304	 i:6 	 global-step:6086	 l-p:0.05568479374051094
epoch£º304	 i:7 	 global-step:6087	 l-p:0.05571094527840614
epoch£º304	 i:8 	 global-step:6088	 l-p:0.055646222084760666
epoch£º304	 i:9 	 global-step:6089	 l-p:0.05565500259399414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1522, 30.1389, 29.7125],
        [28.1522, 28.1538, 28.1522],
        [28.1522, 36.3587, 41.0921],
        [28.1522, 29.0048, 28.5489]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.0557112842798233 
model_pd.l_d.mean(): 6.6017960307362955e-06 
model_pd.lagr.mean(): 0.05571788549423218 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5964], device='cuda:0')), ('power', tensor([0.0111], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.0557112842798233
epoch£º305	 i:1 	 global-step:6101	 l-p:0.055631015449762344
epoch£º305	 i:2 	 global-step:6102	 l-p:0.05570788308978081
epoch£º305	 i:3 	 global-step:6103	 l-p:0.05568041652441025
epoch£º305	 i:4 	 global-step:6104	 l-p:0.05572596937417984
epoch£º305	 i:5 	 global-step:6105	 l-p:0.05613130331039429
epoch£º305	 i:6 	 global-step:6106	 l-p:0.05568717420101166
epoch£º305	 i:7 	 global-step:6107	 l-p:0.05584659427404404
epoch£º305	 i:8 	 global-step:6108	 l-p:0.05583546683192253
epoch£º305	 i:9 	 global-step:6109	 l-p:0.055740270763635635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0445, 28.0444, 28.0445],
        [28.0445, 36.0299, 40.5223],
        [28.0445, 28.0445, 28.0444],
        [28.0445, 30.8808, 30.7797]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.0559241957962513 
model_pd.l_d.mean(): 7.460554570570821e-06 
model_pd.lagr.mean(): 0.055931657552719116 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5369], device='cuda:0')), ('power', tensor([0.0128], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.0559241957962513
epoch£º306	 i:1 	 global-step:6121	 l-p:0.055770065635442734
epoch£º306	 i:2 	 global-step:6122	 l-p:0.05572693794965744
epoch£º306	 i:3 	 global-step:6123	 l-p:0.05571011081337929
epoch£º306	 i:4 	 global-step:6124	 l-p:0.055804576724767685
epoch£º306	 i:5 	 global-step:6125	 l-p:0.055757999420166016
epoch£º306	 i:6 	 global-step:6126	 l-p:0.055712249130010605
epoch£º306	 i:7 	 global-step:6127	 l-p:0.05597059056162834
epoch£º306	 i:8 	 global-step:6128	 l-p:0.055746372789144516
epoch£º306	 i:9 	 global-step:6129	 l-p:0.05581718683242798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9356, 27.9356, 27.9356],
        [27.9356, 28.4976, 28.1371],
        [27.9356, 34.2184, 36.8611],
        [27.9356, 28.4310, 28.0995]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.05571409687399864 
model_pd.l_d.mean(): -0.00012044631876051426 
model_pd.lagr.mean(): 0.05559365078806877 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6323], device='cuda:0')), ('power', tensor([-0.2320], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.05571409687399864
epoch£º307	 i:1 	 global-step:6141	 l-p:0.055832915008068085
epoch£º307	 i:2 	 global-step:6142	 l-p:0.05575316399335861
epoch£º307	 i:3 	 global-step:6143	 l-p:0.05599166825413704
epoch£º307	 i:4 	 global-step:6144	 l-p:0.05583103001117706
epoch£º307	 i:5 	 global-step:6145	 l-p:0.05576327070593834
epoch£º307	 i:6 	 global-step:6146	 l-p:0.055800411850214005
epoch£º307	 i:7 	 global-step:6147	 l-p:0.05600320175290108
epoch£º307	 i:8 	 global-step:6148	 l-p:0.05572305619716644
epoch£º307	 i:9 	 global-step:6149	 l-p:0.055762261152267456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8408, 27.9762, 27.8607],
        [27.8408, 28.0384, 27.8775],
        [27.8408, 28.5130, 28.1116],
        [27.8408, 30.5711, 30.4301]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.055706556886434555 
model_pd.l_d.mean(): -0.00017750501865521073 
model_pd.lagr.mean(): 0.05552905052900314 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6627], device='cuda:0')), ('power', tensor([-0.4415], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.055706556886434555
epoch£º308	 i:1 	 global-step:6161	 l-p:0.05597849562764168
epoch£º308	 i:2 	 global-step:6162	 l-p:0.055796269327402115
epoch£º308	 i:3 	 global-step:6163	 l-p:0.05582557991147041
epoch£º308	 i:4 	 global-step:6164	 l-p:0.055993612855672836
epoch£º308	 i:5 	 global-step:6165	 l-p:0.05591602623462677
epoch£º308	 i:6 	 global-step:6166	 l-p:0.055832814425230026
epoch£º308	 i:7 	 global-step:6167	 l-p:0.055735547095537186
epoch£º308	 i:8 	 global-step:6168	 l-p:0.05574934929609299
epoch£º308	 i:9 	 global-step:6169	 l-p:0.055828407406806946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7759, 29.1187, 28.6148],
        [27.7759, 32.3619, 33.4553],
        [27.7759, 27.7792, 27.7760],
        [27.7759, 33.6344, 35.8757]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.05599404126405716 
model_pd.l_d.mean(): -6.943985499674454e-05 
model_pd.lagr.mean(): 0.05592460185289383 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5324], device='cuda:0')), ('power', tensor([-0.2845], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.05599404126405716
epoch£º309	 i:1 	 global-step:6181	 l-p:0.05577228590846062
epoch£º309	 i:2 	 global-step:6182	 l-p:0.05575258284807205
epoch£º309	 i:3 	 global-step:6183	 l-p:0.055804021656513214
epoch£º309	 i:4 	 global-step:6184	 l-p:0.055879924446344376
epoch£º309	 i:5 	 global-step:6185	 l-p:0.05577562376856804
epoch£º309	 i:6 	 global-step:6186	 l-p:0.05615488067269325
epoch£º309	 i:7 	 global-step:6187	 l-p:0.05577044188976288
epoch£º309	 i:8 	 global-step:6188	 l-p:0.055786073207855225
epoch£º309	 i:9 	 global-step:6189	 l-p:0.05577925220131874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7537, 27.8665, 27.7686],
        [27.7537, 27.7562, 27.7538],
        [27.7537, 32.2103, 33.2009],
        [27.7537, 34.6374, 37.9376]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.055735502392053604 
model_pd.l_d.mean(): -2.8373176974128e-05 
model_pd.lagr.mean(): 0.05570713058114052 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.0597e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6389], device='cuda:0')), ('power', tensor([-0.4498], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.055735502392053604
epoch£º310	 i:1 	 global-step:6201	 l-p:0.05572131648659706
epoch£º310	 i:2 	 global-step:6202	 l-p:0.05580221116542816
epoch£º310	 i:3 	 global-step:6203	 l-p:0.055782120674848557
epoch£º310	 i:4 	 global-step:6204	 l-p:0.0558607280254364
epoch£º310	 i:5 	 global-step:6205	 l-p:0.055811136960983276
epoch£º310	 i:6 	 global-step:6206	 l-p:0.05605079233646393
epoch£º310	 i:7 	 global-step:6207	 l-p:0.055782489478588104
epoch£º310	 i:8 	 global-step:6208	 l-p:0.056187305599451065
epoch£º310	 i:9 	 global-step:6209	 l-p:0.05573514848947525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[27.7808, 29.8026, 29.3984],
        [27.7808, 29.7339, 29.3120],
        [27.7808, 29.0004, 28.4985],
        [27.7808, 30.2111, 29.9431]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.056023672223091125 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056023672223091125 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5523], device='cuda:0')), ('power', tensor([-0.2476], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.056023672223091125
epoch£º311	 i:1 	 global-step:6221	 l-p:0.05578828603029251
epoch£º311	 i:2 	 global-step:6222	 l-p:0.055794741958379745
epoch£º311	 i:3 	 global-step:6223	 l-p:0.055754050612449646
epoch£º311	 i:4 	 global-step:6224	 l-p:0.05578333139419556
epoch£º311	 i:5 	 global-step:6225	 l-p:0.05596000701189041
epoch£º311	 i:6 	 global-step:6226	 l-p:0.055900100618600845
epoch£º311	 i:7 	 global-step:6227	 l-p:0.05593142285943031
epoch£º311	 i:8 	 global-step:6228	 l-p:0.05571209639310837
epoch£º311	 i:9 	 global-step:6229	 l-p:0.05572294071316719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8383, 29.9231, 29.5344],
        [27.8383, 27.9309, 27.8491],
        [27.8383, 27.8383, 27.8382],
        [27.8383, 34.5692, 37.6928]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.05571998655796051 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05571998655796051 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6378], device='cuda:0')), ('power', tensor([-0.3863], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.05571998655796051
epoch£º312	 i:1 	 global-step:6241	 l-p:0.055743295699357986
epoch£º312	 i:2 	 global-step:6242	 l-p:0.055835943669080734
epoch£º312	 i:3 	 global-step:6243	 l-p:0.05578588321805
epoch£º312	 i:4 	 global-step:6244	 l-p:0.055994682013988495
epoch£º312	 i:5 	 global-step:6245	 l-p:0.055716950446367264
epoch£º312	 i:6 	 global-step:6246	 l-p:0.05573388934135437
epoch£º312	 i:7 	 global-step:6247	 l-p:0.05579306185245514
epoch£º312	 i:8 	 global-step:6248	 l-p:0.05573590099811554
epoch£º312	 i:9 	 global-step:6249	 l-p:0.056166600435972214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9057, 28.4262, 28.0835],
        [27.9057, 35.6198, 39.8220],
        [27.9057, 29.9959, 29.6062],
        [27.9057, 28.0683, 27.9324]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.05571509152650833 
model_pd.l_d.mean(): -5.053704512647528e-07 
model_pd.lagr.mean(): 0.05571458488702774 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6291], device='cuda:0')), ('power', tensor([-0.3166], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.05571509152650833
epoch£º313	 i:1 	 global-step:6261	 l-p:0.05573005974292755
epoch£º313	 i:2 	 global-step:6262	 l-p:0.055936411023139954
epoch£º313	 i:3 	 global-step:6263	 l-p:0.05578700080513954
epoch£º313	 i:4 	 global-step:6264	 l-p:0.05583501607179642
epoch£º313	 i:5 	 global-step:6265	 l-p:0.055683210492134094
epoch£º313	 i:6 	 global-step:6266	 l-p:0.05578792095184326
epoch£º313	 i:7 	 global-step:6267	 l-p:0.055756185203790665
epoch£º313	 i:8 	 global-step:6268	 l-p:0.05583928897976875
epoch£º313	 i:9 	 global-step:6269	 l-p:0.05599839240312576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9774, 27.9774, 27.9774],
        [27.9774, 28.1884, 28.0181],
        [27.9774, 28.8075, 28.3588],
        [27.9774, 29.1360, 28.6346]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.055767353624105453 
model_pd.l_d.mean(): -9.073892215383239e-08 
model_pd.lagr.mean(): 0.05576726421713829 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5581], device='cuda:0')), ('power', tensor([-0.1331], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.055767353624105453
epoch£º314	 i:1 	 global-step:6281	 l-p:0.05567312240600586
epoch£º314	 i:2 	 global-step:6282	 l-p:0.05589880794286728
epoch£º314	 i:3 	 global-step:6283	 l-p:0.055803604423999786
epoch£º314	 i:4 	 global-step:6284	 l-p:0.05569983273744583
epoch£º314	 i:5 	 global-step:6285	 l-p:0.055897586047649384
epoch£º314	 i:6 	 global-step:6286	 l-p:0.055936217308044434
epoch£º314	 i:7 	 global-step:6287	 l-p:0.055725496262311935
epoch£º314	 i:8 	 global-step:6288	 l-p:0.05582578480243683
epoch£º314	 i:9 	 global-step:6289	 l-p:0.05567784607410431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0511, 32.3424, 33.1748],
        [28.0511, 29.9284, 29.4801],
        [28.0511, 28.0511, 28.0511],
        [28.0511, 34.2786, 36.8493]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.05573863908648491 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05573863908648491 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6118], device='cuda:0')), ('power', tensor([-0.1048], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.05573863908648491
epoch£º315	 i:1 	 global-step:6301	 l-p:0.056066520512104034
epoch£º315	 i:2 	 global-step:6302	 l-p:0.05569593235850334
epoch£º315	 i:3 	 global-step:6303	 l-p:0.05573198199272156
epoch£º315	 i:4 	 global-step:6304	 l-p:0.0556844137609005
epoch£º315	 i:5 	 global-step:6305	 l-p:0.05567484721541405
epoch£º315	 i:6 	 global-step:6306	 l-p:0.055939506739377975
epoch£º315	 i:7 	 global-step:6307	 l-p:0.05568959191441536
epoch£º315	 i:8 	 global-step:6308	 l-p:0.055832814425230026
epoch£º315	 i:9 	 global-step:6309	 l-p:0.055686235427856445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1252, 28.2628, 28.1455],
        [28.1252, 28.1455, 28.1262],
        [28.1252, 30.5186, 30.2208],
        [28.1252, 28.6913, 28.3282]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.05564271658658981 
model_pd.l_d.mean(): -4.1714181975294196e-07 
model_pd.lagr.mean(): 0.055642299354076385 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6753], device='cuda:0')), ('power', tensor([-0.1620], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.05564271658658981
epoch£º316	 i:1 	 global-step:6321	 l-p:0.05568251386284828
epoch£º316	 i:2 	 global-step:6322	 l-p:0.05617428570985794
epoch£º316	 i:3 	 global-step:6323	 l-p:0.05571307614445686
epoch£º316	 i:4 	 global-step:6324	 l-p:0.05567927286028862
epoch£º316	 i:5 	 global-step:6325	 l-p:0.05565672740340233
epoch£º316	 i:6 	 global-step:6326	 l-p:0.05564351752400398
epoch£º316	 i:7 	 global-step:6327	 l-p:0.05570618063211441
epoch£º316	 i:8 	 global-step:6328	 l-p:0.055649206042289734
epoch£º316	 i:9 	 global-step:6329	 l-p:0.05602823197841644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1978, 28.8395, 28.4467],
        [28.1978, 28.2115, 28.1983],
        [28.1978, 29.5531, 29.0412],
        [28.1978, 29.3498, 28.8456]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.055685508996248245 
model_pd.l_d.mean(): 1.2273422953512636e-06 
model_pd.lagr.mean(): 0.05568673461675644 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.1345e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6134], device='cuda:0')), ('power', tensor([0.0420], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.055685508996248245
epoch£º317	 i:1 	 global-step:6341	 l-p:0.05569671466946602
epoch£º317	 i:2 	 global-step:6342	 l-p:0.0556771345436573
epoch£º317	 i:3 	 global-step:6343	 l-p:0.05593642219901085
epoch£º317	 i:4 	 global-step:6344	 l-p:0.05568230524659157
epoch£º317	 i:5 	 global-step:6345	 l-p:0.05566218122839928
epoch£º317	 i:6 	 global-step:6346	 l-p:0.0559597946703434
epoch£º317	 i:7 	 global-step:6347	 l-p:0.05568930134177208
epoch£º317	 i:8 	 global-step:6348	 l-p:0.05561382696032524
epoch£º317	 i:9 	 global-step:6349	 l-p:0.055816397070884705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2647, 28.9444, 28.5377],
        [28.2647, 28.2672, 28.2647],
        [28.2647, 28.4101, 28.2868],
        [28.2647, 28.2731, 28.2649]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.05568581074476242 
model_pd.l_d.mean(): 9.960007446352392e-06 
model_pd.lagr.mean(): 0.05569577217102051 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.5730e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6133], device='cuda:0')), ('power', tensor([0.1253], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.05568581074476242
epoch£º318	 i:1 	 global-step:6361	 l-p:0.055627793073654175
epoch£º318	 i:2 	 global-step:6362	 l-p:0.0557168573141098
epoch£º318	 i:3 	 global-step:6363	 l-p:0.056081388145685196
epoch£º318	 i:4 	 global-step:6364	 l-p:0.05583693087100983
epoch£º318	 i:5 	 global-step:6365	 l-p:0.055681969970464706
epoch£º318	 i:6 	 global-step:6366	 l-p:0.05562572181224823
epoch£º318	 i:7 	 global-step:6367	 l-p:0.05574300140142441
epoch£º318	 i:8 	 global-step:6368	 l-p:0.0556330531835556
epoch£º318	 i:9 	 global-step:6369	 l-p:0.05565224215388298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3177, 29.4912, 28.9833],
        [28.3177, 30.1336, 29.6647],
        [28.3177, 28.3177, 28.3177],
        [28.3177, 28.3178, 28.3177]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.05563763156533241 
model_pd.l_d.mean(): 2.368806781305466e-05 
model_pd.lagr.mean(): 0.05566132068634033 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6416], device='cuda:0')), ('power', tensor([0.1479], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.05563763156533241
epoch£º319	 i:1 	 global-step:6381	 l-p:0.055908579379320145
epoch£º319	 i:2 	 global-step:6382	 l-p:0.055694330483675
epoch£º319	 i:3 	 global-step:6383	 l-p:0.05574658885598183
epoch£º319	 i:4 	 global-step:6384	 l-p:0.05561735853552818
epoch£º319	 i:5 	 global-step:6385	 l-p:0.05575902387499809
epoch£º319	 i:6 	 global-step:6386	 l-p:0.05562384054064751
epoch£º319	 i:7 	 global-step:6387	 l-p:0.055771272629499435
epoch£º319	 i:8 	 global-step:6388	 l-p:0.05582205951213837
epoch£º319	 i:9 	 global-step:6389	 l-p:0.05560972914099693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3468, 30.1103, 29.6313],
        [28.3468, 31.0279, 30.8378],
        [28.3468, 30.8059, 30.5231],
        [28.3468, 28.4047, 28.3518]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.055702656507492065 
model_pd.l_d.mean(): 4.447199899004772e-05 
model_pd.lagr.mean(): 0.055747129023075104 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6385], device='cuda:0')), ('power', tensor([0.1697], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.055702656507492065
epoch£º320	 i:1 	 global-step:6401	 l-p:0.055755097419023514
epoch£º320	 i:2 	 global-step:6402	 l-p:0.05580490455031395
epoch£º320	 i:3 	 global-step:6403	 l-p:0.055681437253952026
epoch£º320	 i:4 	 global-step:6404	 l-p:0.055638473480939865
epoch£º320	 i:5 	 global-step:6405	 l-p:0.05563032999634743
epoch£º320	 i:6 	 global-step:6406	 l-p:0.05560286343097687
epoch£º320	 i:7 	 global-step:6407	 l-p:0.05559607222676277
epoch£º320	 i:8 	 global-step:6408	 l-p:0.05610254034399986
epoch£º320	 i:9 	 global-step:6409	 l-p:0.05564187839627266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3448, 28.6752, 28.4285],
        [28.3448, 33.9487, 35.8747],
        [28.3448, 28.4271, 28.3536],
        [28.3448, 28.3487, 28.3448]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.05577302724123001 
model_pd.l_d.mean(): 9.010137000586838e-05 
model_pd.lagr.mean(): 0.05586312711238861 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5857], device='cuda:0')), ('power', tensor([0.2423], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.05577302724123001
epoch£º321	 i:1 	 global-step:6421	 l-p:0.05564629286527634
epoch£º321	 i:2 	 global-step:6422	 l-p:0.055636242032051086
epoch£º321	 i:3 	 global-step:6423	 l-p:0.05562421306967735
epoch£º321	 i:4 	 global-step:6424	 l-p:0.05570659041404724
epoch£º321	 i:5 	 global-step:6425	 l-p:0.055702775716781616
epoch£º321	 i:6 	 global-step:6426	 l-p:0.05564463511109352
epoch£º321	 i:7 	 global-step:6427	 l-p:0.05575614422559738
epoch£º321	 i:8 	 global-step:6428	 l-p:0.055877432227134705
epoch£º321	 i:9 	 global-step:6429	 l-p:0.055827122181653976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3090, 38.0491, 44.6581],
        [28.3090, 28.3106, 28.3090],
        [28.3090, 28.3158, 28.3091],
        [28.3090, 28.3089, 28.3089]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.05563401058316231 
model_pd.l_d.mean(): 4.208047175779939e-05 
model_pd.lagr.mean(): 0.055676091462373734 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6496], device='cuda:0')), ('power', tensor([0.0890], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.05563401058316231
epoch£º322	 i:1 	 global-step:6441	 l-p:0.05561224743723869
epoch£º322	 i:2 	 global-step:6442	 l-p:0.05575200915336609
epoch£º322	 i:3 	 global-step:6443	 l-p:0.055638350546360016
epoch£º322	 i:4 	 global-step:6444	 l-p:0.055745575577020645
epoch£º322	 i:5 	 global-step:6445	 l-p:0.05571915581822395
epoch£º322	 i:6 	 global-step:6446	 l-p:0.055878426879644394
epoch£º322	 i:7 	 global-step:6447	 l-p:0.05584089085459709
epoch£º322	 i:8 	 global-step:6448	 l-p:0.055739156901836395
epoch£º322	 i:9 	 global-step:6449	 l-p:0.055746521800756454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2403, 28.2405, 28.2402],
        [28.2403, 28.3222, 28.2491],
        [28.2403, 32.1068, 32.6108],
        [28.2403, 28.6300, 28.3501]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.05567106977105141 
model_pd.l_d.mean(): 4.2331830627517775e-05 
model_pd.lagr.mean(): 0.05571340024471283 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6180], device='cuda:0')), ('power', tensor([0.0771], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.05567106977105141
epoch£º323	 i:1 	 global-step:6461	 l-p:0.055962856858968735
epoch£º323	 i:2 	 global-step:6462	 l-p:0.05588649958372116
epoch£º323	 i:3 	 global-step:6463	 l-p:0.055807288736104965
epoch£º323	 i:4 	 global-step:6464	 l-p:0.055647172033786774
epoch£º323	 i:5 	 global-step:6465	 l-p:0.05567687749862671
epoch£º323	 i:6 	 global-step:6466	 l-p:0.05564824864268303
epoch£º323	 i:7 	 global-step:6467	 l-p:0.05579400807619095
epoch£º323	 i:8 	 global-step:6468	 l-p:0.055702198296785355
epoch£º323	 i:9 	 global-step:6469	 l-p:0.05568908900022507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1426, 28.1428, 28.1426],
        [28.1426, 28.1428, 28.1426],
        [28.1426, 28.3223, 28.1738],
        [28.1426, 29.3499, 28.8426]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.0557565875351429 
model_pd.l_d.mean(): 6.905614282004535e-05 
model_pd.lagr.mean(): 0.05582564324140549 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5506], device='cuda:0')), ('power', tensor([0.1182], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.0557565875351429
epoch£º324	 i:1 	 global-step:6481	 l-p:0.05591756850481033
epoch£º324	 i:2 	 global-step:6482	 l-p:0.055647630244493484
epoch£º324	 i:3 	 global-step:6483	 l-p:0.05573122948408127
epoch£º324	 i:4 	 global-step:6484	 l-p:0.055708084255456924
epoch£º324	 i:5 	 global-step:6485	 l-p:0.0556633397936821
epoch£º324	 i:6 	 global-step:6486	 l-p:0.055772121995687485
epoch£º324	 i:7 	 global-step:6487	 l-p:0.05584665760397911
epoch£º324	 i:8 	 global-step:6488	 l-p:0.055895548313856125
epoch£º324	 i:9 	 global-step:6489	 l-p:0.0557861402630806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0276, 29.9967, 29.5703],
        [28.0276, 28.2075, 28.0590],
        [28.0276, 28.0301, 28.0276],
        [28.0276, 31.2342, 31.3323]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.05591459199786186 
model_pd.l_d.mean(): -2.5985866159317084e-05 
model_pd.lagr.mean(): 0.05588860437273979 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5756], device='cuda:0')), ('power', tensor([-0.0459], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.05591459199786186
epoch£º325	 i:1 	 global-step:6501	 l-p:0.055950164794921875
epoch£º325	 i:2 	 global-step:6502	 l-p:0.05582215264439583
epoch£º325	 i:3 	 global-step:6503	 l-p:0.05574200674891472
epoch£º325	 i:4 	 global-step:6504	 l-p:0.055788639932870865
epoch£º325	 i:5 	 global-step:6505	 l-p:0.055739663541316986
epoch£º325	 i:6 	 global-step:6506	 l-p:0.055744294077157974
epoch£º325	 i:7 	 global-step:6507	 l-p:0.0557086355984211
epoch£º325	 i:8 	 global-step:6508	 l-p:0.05584350600838661
epoch£º325	 i:9 	 global-step:6509	 l-p:0.055728863924741745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9130, 30.9373, 30.9410],
        [27.9130, 27.9286, 27.9136],
        [27.9130, 37.2494, 43.4244],
        [27.9130, 30.7352, 30.6346]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.055756740272045135 
model_pd.l_d.mean(): -0.0001231838105013594 
model_pd.lagr.mean(): 0.055633556097745895 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6014], device='cuda:0')), ('power', tensor([-0.2508], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.055756740272045135
epoch£º326	 i:1 	 global-step:6521	 l-p:0.055721215903759
epoch£º326	 i:2 	 global-step:6522	 l-p:0.05584431439638138
epoch£º326	 i:3 	 global-step:6523	 l-p:0.05584922060370445
epoch£º326	 i:4 	 global-step:6524	 l-p:0.05578925088047981
epoch£º326	 i:5 	 global-step:6525	 l-p:0.05576448142528534
epoch£º326	 i:6 	 global-step:6526	 l-p:0.05572135001420975
epoch£º326	 i:7 	 global-step:6527	 l-p:0.05621311068534851
epoch£º326	 i:8 	 global-step:6528	 l-p:0.05584898963570595
epoch£º326	 i:9 	 global-step:6529	 l-p:0.05572184547781944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8164, 37.2447, 43.5585],
        [27.8164, 27.8189, 27.8164],
        [27.8164, 28.6557, 28.2062],
        [27.8164, 27.8333, 27.8171]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.05580282583832741 
model_pd.l_d.mean(): -0.00012198365584481508 
model_pd.lagr.mean(): 0.05568084120750427 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5889], device='cuda:0')), ('power', tensor([-0.3368], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.05580282583832741
epoch£º327	 i:1 	 global-step:6541	 l-p:0.055828116834163666
epoch£º327	 i:2 	 global-step:6542	 l-p:0.05576711520552635
epoch£º327	 i:3 	 global-step:6543	 l-p:0.055867522954940796
epoch£º327	 i:4 	 global-step:6544	 l-p:0.05597788840532303
epoch£º327	 i:5 	 global-step:6545	 l-p:0.05572734773159027
epoch£º327	 i:6 	 global-step:6546	 l-p:0.05609126016497612
epoch£º327	 i:7 	 global-step:6547	 l-p:0.05579884350299835
epoch£º327	 i:8 	 global-step:6548	 l-p:0.055776968598365784
epoch£º327	 i:9 	 global-step:6549	 l-p:0.05577658489346504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7569, 27.8724, 27.7724],
        [27.7569, 27.7995, 27.7601],
        [27.7569, 30.6348, 30.5709],
        [27.7569, 34.7796, 38.2305]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.055808842182159424 
model_pd.l_d.mean(): -7.00778909958899e-05 
model_pd.lagr.mean(): 0.05573876574635506 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5725], device='cuda:0')), ('power', tensor([-0.3630], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.055808842182159424
epoch£º328	 i:1 	 global-step:6561	 l-p:0.0557539276778698
epoch£º328	 i:2 	 global-step:6562	 l-p:0.05580971762537956
epoch£º328	 i:3 	 global-step:6563	 l-p:0.05576750636100769
epoch£º328	 i:4 	 global-step:6564	 l-p:0.05573003739118576
epoch£º328	 i:5 	 global-step:6565	 l-p:0.05628693848848343
epoch£º328	 i:6 	 global-step:6566	 l-p:0.05574334040284157
epoch£º328	 i:7 	 global-step:6567	 l-p:0.05578993633389473
epoch£º328	 i:8 	 global-step:6568	 l-p:0.05573934316635132
epoch£º328	 i:9 	 global-step:6569	 l-p:0.056071024388074875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7487, 27.7488, 27.7487],
        [27.7487, 32.5571, 33.8391],
        [27.7487, 27.7628, 27.7492],
        [27.7487, 28.4152, 28.0164]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.055990103632211685 
model_pd.l_d.mean(): -1.213855739479186e-06 
model_pd.lagr.mean(): 0.05598888918757439 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5248], device='cuda:0')), ('power', tensor([-0.2299], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.055990103632211685
epoch£º329	 i:1 	 global-step:6581	 l-p:0.05603070557117462
epoch£º329	 i:2 	 global-step:6582	 l-p:0.05574502795934677
epoch£º329	 i:3 	 global-step:6583	 l-p:0.055838603526353836
epoch£º329	 i:4 	 global-step:6584	 l-p:0.055813588201999664
epoch£º329	 i:5 	 global-step:6585	 l-p:0.055765390396118164
epoch£º329	 i:6 	 global-step:6586	 l-p:0.055731937289237976
epoch£º329	 i:7 	 global-step:6587	 l-p:0.056020498275756836
epoch£º329	 i:8 	 global-step:6588	 l-p:0.055718131363391876
epoch£º329	 i:9 	 global-step:6589	 l-p:0.05580715090036392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7932, 27.8051, 27.7936],
        [27.7932, 27.8111, 27.7940],
        [27.7932, 27.8033, 27.7935],
        [27.7932, 27.7932, 27.7932]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.05575942248106003 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575942248106003 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6180], device='cuda:0')), ('power', tensor([-0.4107], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.05575942248106003
epoch£º330	 i:1 	 global-step:6601	 l-p:0.05590632185339928
epoch£º330	 i:2 	 global-step:6602	 l-p:0.0557253360748291
epoch£º330	 i:3 	 global-step:6603	 l-p:0.05581379309296608
epoch£º330	 i:4 	 global-step:6604	 l-p:0.055729955434799194
epoch£º330	 i:5 	 global-step:6605	 l-p:0.05577374994754791
epoch£º330	 i:6 	 global-step:6606	 l-p:0.055814776569604874
epoch£º330	 i:7 	 global-step:6607	 l-p:0.05577663332223892
epoch£º330	 i:8 	 global-step:6608	 l-p:0.055772870779037476
epoch£º330	 i:9 	 global-step:6609	 l-p:0.056257009506225586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8606, 30.4933, 30.3064],
        [27.8606, 36.8772, 42.6573],
        [27.8606, 27.9153, 27.8653],
        [27.8606, 36.0724, 40.8662]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.0557699128985405 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0557699128985405 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5855], device='cuda:0')), ('power', tensor([-0.2927], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.0557699128985405
epoch£º331	 i:1 	 global-step:6621	 l-p:0.05570370703935623
epoch£º331	 i:2 	 global-step:6622	 l-p:0.055961236357688904
epoch£º331	 i:3 	 global-step:6623	 l-p:0.055980656296014786
epoch£º331	 i:4 	 global-step:6624	 l-p:0.05589095875620842
epoch£º331	 i:5 	 global-step:6625	 l-p:0.055708691477775574
epoch£º331	 i:6 	 global-step:6626	 l-p:0.05568249523639679
epoch£º331	 i:7 	 global-step:6627	 l-p:0.055745504796504974
epoch£º331	 i:8 	 global-step:6628	 l-p:0.055761564522981644
epoch£º331	 i:9 	 global-step:6629	 l-p:0.05596178025007248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9374, 29.6994, 29.2320],
        [27.9374, 31.9830, 32.6407],
        [27.9374, 30.5882, 30.4055],
        [27.9374, 27.9844, 27.9411]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.05574049800634384 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574049800634384 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5965], device='cuda:0')), ('power', tensor([-0.1883], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.05574049800634384
epoch£º332	 i:1 	 global-step:6641	 l-p:0.055735472589731216
epoch£º332	 i:2 	 global-step:6642	 l-p:0.05604692921042442
epoch£º332	 i:3 	 global-step:6643	 l-p:0.05580391734838486
epoch£º332	 i:4 	 global-step:6644	 l-p:0.05568639934062958
epoch£º332	 i:5 	 global-step:6645	 l-p:0.05569244921207428
epoch£º332	 i:6 	 global-step:6646	 l-p:0.05569682642817497
epoch£º332	 i:7 	 global-step:6647	 l-p:0.05595044046640396
epoch£º332	 i:8 	 global-step:6648	 l-p:0.05590486526489258
epoch£º332	 i:9 	 global-step:6649	 l-p:0.05573153868317604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0173, 28.0173, 28.0173],
        [28.0173, 32.8356, 34.0979],
        [28.0173, 28.0173, 28.0173],
        [28.0173, 28.0175, 28.0173]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.055724237114191055 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055724237114191055 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6192], device='cuda:0')), ('power', tensor([-0.1496], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.055724237114191055
epoch£º333	 i:1 	 global-step:6661	 l-p:0.05598307400941849
epoch£º333	 i:2 	 global-step:6662	 l-p:0.0557016059756279
epoch£º333	 i:3 	 global-step:6663	 l-p:0.055689964443445206
epoch£º333	 i:4 	 global-step:6664	 l-p:0.05572251230478287
epoch£º333	 i:5 	 global-step:6665	 l-p:0.05570063367486
epoch£º333	 i:6 	 global-step:6666	 l-p:0.055814050137996674
epoch£º333	 i:7 	 global-step:6667	 l-p:0.05571035295724869
epoch£º333	 i:8 	 global-step:6668	 l-p:0.055853649973869324
epoch£º333	 i:9 	 global-step:6669	 l-p:0.055908203125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0983, 28.1045, 28.0985],
        [28.0983, 28.1414, 28.1015],
        [28.0983, 30.3550, 30.0098],
        [28.0983, 29.4576, 28.9476]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.05565376952290535 
model_pd.l_d.mean(): -9.565635537001071e-07 
model_pd.lagr.mean(): 0.055652812123298645 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6658], device='cuda:0')), ('power', tensor([-0.1562], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.05565376952290535
epoch£º334	 i:1 	 global-step:6681	 l-p:0.055828195065259933
epoch£º334	 i:2 	 global-step:6682	 l-p:0.05571369081735611
epoch£º334	 i:3 	 global-step:6683	 l-p:0.05571557953953743
epoch£º334	 i:4 	 global-step:6684	 l-p:0.05568484216928482
epoch£º334	 i:5 	 global-step:6685	 l-p:0.05578715726733208
epoch£º334	 i:6 	 global-step:6686	 l-p:0.055695369839668274
epoch£º334	 i:7 	 global-step:6687	 l-p:0.05579182505607605
epoch£º334	 i:8 	 global-step:6688	 l-p:0.05604621767997742
epoch£º334	 i:9 	 global-step:6689	 l-p:0.05571015179157257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1796, 29.0160, 28.5638],
        [28.1796, 37.1879, 42.8921],
        [28.1796, 28.1796, 28.1796],
        [28.1796, 37.7635, 44.1987]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.05583690479397774 
model_pd.l_d.mean(): 1.6471228718728526e-06 
model_pd.lagr.mean(): 0.05583855137228966 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.0234e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5382], device='cuda:0')), ('power', tensor([0.1129], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.05583690479397774
epoch£º335	 i:1 	 global-step:6701	 l-p:0.055690668523311615
epoch£º335	 i:2 	 global-step:6702	 l-p:0.05569520965218544
epoch£º335	 i:3 	 global-step:6703	 l-p:0.05568031221628189
epoch£º335	 i:4 	 global-step:6704	 l-p:0.055707529187202454
epoch£º335	 i:5 	 global-step:6705	 l-p:0.05578303337097168
epoch£º335	 i:6 	 global-step:6706	 l-p:0.05567588284611702
epoch£º335	 i:7 	 global-step:6707	 l-p:0.0559573769569397
epoch£º335	 i:8 	 global-step:6708	 l-p:0.05561985820531845
epoch£º335	 i:9 	 global-step:6709	 l-p:0.0558023527264595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[28.2574, 29.0514, 28.6096],
        [28.2574, 29.7173, 29.2072],
        [28.2574, 33.1856, 34.5159],
        [28.2574, 29.1851, 28.7118]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.055882591754198074 
model_pd.l_d.mean(): 8.149773748300504e-06 
model_pd.lagr.mean(): 0.05589074268937111 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.5170e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6022], device='cuda:0')), ('power', tensor([0.1401], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.055882591754198074
epoch£º336	 i:1 	 global-step:6721	 l-p:0.05573975294828415
epoch£º336	 i:2 	 global-step:6722	 l-p:0.05579365789890289
epoch£º336	 i:3 	 global-step:6723	 l-p:0.05573507025837898
epoch£º336	 i:4 	 global-step:6724	 l-p:0.055757805705070496
epoch£º336	 i:5 	 global-step:6725	 l-p:0.0556400865316391
epoch£º336	 i:6 	 global-step:6726	 l-p:0.05581528693437576
epoch£º336	 i:7 	 global-step:6727	 l-p:0.05562513321638107
epoch£º336	 i:8 	 global-step:6728	 l-p:0.055621545761823654
epoch£º336	 i:9 	 global-step:6729	 l-p:0.05567903444170952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3217, 38.8121, 46.4130],
        [28.3217, 28.9345, 28.5518],
        [28.3217, 31.6268, 31.7629],
        [28.3217, 35.3530, 38.7247]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.0559944324195385 
model_pd.l_d.mean(): 4.7948873543646187e-05 
model_pd.lagr.mean(): 0.05604238063097 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5532], device='cuda:0')), ('power', tensor([0.3479], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.0559944324195385
epoch£º337	 i:1 	 global-step:6741	 l-p:0.05569547414779663
epoch£º337	 i:2 	 global-step:6742	 l-p:0.05570486932992935
epoch£º337	 i:3 	 global-step:6743	 l-p:0.055632445961236954
epoch£º337	 i:4 	 global-step:6744	 l-p:0.05574085935950279
epoch£º337	 i:5 	 global-step:6745	 l-p:0.05565967410802841
epoch£º337	 i:6 	 global-step:6746	 l-p:0.05581335350871086
epoch£º337	 i:7 	 global-step:6747	 l-p:0.05569921433925629
epoch£º337	 i:8 	 global-step:6748	 l-p:0.05559531971812248
epoch£º337	 i:9 	 global-step:6749	 l-p:0.0556362122297287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3606, 29.4099, 28.9148],
        [28.3606, 28.7919, 28.4899],
        [28.3606, 28.4369, 28.3684],
        [28.3606, 28.3606, 28.3605]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.055661533027887344 
model_pd.l_d.mean(): 4.855501902056858e-05 
model_pd.lagr.mean(): 0.0557100884616375 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6130], device='cuda:0')), ('power', tensor([0.1989], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.055661533027887344
epoch£º338	 i:1 	 global-step:6761	 l-p:0.05565223470330238
epoch£º338	 i:2 	 global-step:6762	 l-p:0.05562220513820648
epoch£º338	 i:3 	 global-step:6763	 l-p:0.05569751188158989
epoch£º338	 i:4 	 global-step:6764	 l-p:0.05567752569913864
epoch£º338	 i:5 	 global-step:6765	 l-p:0.05580035597085953
epoch£º338	 i:6 	 global-step:6766	 l-p:0.05584916099905968
epoch£º338	 i:7 	 global-step:6767	 l-p:0.05570150166749954
epoch£º338	 i:8 	 global-step:6768	 l-p:0.0556240938603878
epoch£º338	 i:9 	 global-step:6769	 l-p:0.05583285167813301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3654, 36.2210, 40.5059],
        [28.3654, 29.0735, 28.6567],
        [28.3654, 28.8132, 28.5029],
        [28.3654, 28.3654, 28.3654]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.055640943348407745 
model_pd.l_d.mean(): 7.103345706127584e-05 
model_pd.lagr.mean(): 0.05571197718381882 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6279], device='cuda:0')), ('power', tensor([0.1961], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.055640943348407745
epoch£º339	 i:1 	 global-step:6781	 l-p:0.05562901869416237
epoch£º339	 i:2 	 global-step:6782	 l-p:0.05570141226053238
epoch£º339	 i:3 	 global-step:6783	 l-p:0.05580877140164375
epoch£º339	 i:4 	 global-step:6784	 l-p:0.055606842041015625
epoch£º339	 i:5 	 global-step:6785	 l-p:0.05574788153171539
epoch£º339	 i:6 	 global-step:6786	 l-p:0.05560196936130524
epoch£º339	 i:7 	 global-step:6787	 l-p:0.05591878667473793
epoch£º339	 i:8 	 global-step:6788	 l-p:0.055803488940000534
epoch£º339	 i:9 	 global-step:6789	 l-p:0.05568750202655792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3308, 34.6483, 37.2714],
        [28.3308, 28.3650, 28.3330],
        [28.3308, 28.3329, 28.3308],
        [28.3308, 29.3210, 28.8353]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.055705804377794266 
model_pd.l_d.mean(): 0.00010283262963639572 
model_pd.lagr.mean(): 0.05580863729119301 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5900], device='cuda:0')), ('power', tensor([0.2169], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.055705804377794266
epoch£º340	 i:1 	 global-step:6801	 l-p:0.055619437247514725
epoch£º340	 i:2 	 global-step:6802	 l-p:0.055842094123363495
epoch£º340	 i:3 	 global-step:6803	 l-p:0.05580345168709755
epoch£º340	 i:4 	 global-step:6804	 l-p:0.055640462785959244
epoch£º340	 i:5 	 global-step:6805	 l-p:0.05577027052640915
epoch£º340	 i:6 	 global-step:6806	 l-p:0.05567153915762901
epoch£º340	 i:7 	 global-step:6807	 l-p:0.05590161681175232
epoch£º340	 i:8 	 global-step:6808	 l-p:0.05561947077512741
epoch£º340	 i:9 	 global-step:6809	 l-p:0.05568864569067955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2549, 28.2662, 28.2553],
        [28.2549, 29.6805, 29.1689],
        [28.2549, 35.2689, 38.6322],
        [28.2549, 35.4097, 38.9260]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.055693455040454865 
model_pd.l_d.mean(): 8.652115502627566e-05 
model_pd.lagr.mean(): 0.05577997490763664 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5982], device='cuda:0')), ('power', tensor([0.1546], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.055693455040454865
epoch£º341	 i:1 	 global-step:6821	 l-p:0.05571912229061127
epoch£º341	 i:2 	 global-step:6822	 l-p:0.055782251060009
epoch£º341	 i:3 	 global-step:6823	 l-p:0.05566103756427765
epoch£º341	 i:4 	 global-step:6824	 l-p:0.05567794665694237
epoch£º341	 i:5 	 global-step:6825	 l-p:0.055691853165626526
epoch£º341	 i:6 	 global-step:6826	 l-p:0.055954158306121826
epoch£º341	 i:7 	 global-step:6827	 l-p:0.05570746213197708
epoch£º341	 i:8 	 global-step:6828	 l-p:0.05566076934337616
epoch£º341	 i:9 	 global-step:6829	 l-p:0.0559200644493103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1443, 28.1443, 28.1442],
        [28.1443, 35.9275, 40.1675],
        [28.1443, 37.4321, 43.4957],
        [28.1443, 35.9346, 40.1829]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.05566021427512169 
model_pd.l_d.mean(): -3.895326881320216e-05 
model_pd.lagr.mean(): 0.05562126263976097 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6469], device='cuda:0')), ('power', tensor([-0.0650], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.05566021427512169
epoch£º342	 i:1 	 global-step:6841	 l-p:0.055690377950668335
epoch£º342	 i:2 	 global-step:6842	 l-p:0.05569380521774292
epoch£º342	 i:3 	 global-step:6843	 l-p:0.05587572976946831
epoch£º342	 i:4 	 global-step:6844	 l-p:0.05591046065092087
epoch£º342	 i:5 	 global-step:6845	 l-p:0.055756576359272
epoch£º342	 i:6 	 global-step:6846	 l-p:0.055816009640693665
epoch£º342	 i:7 	 global-step:6847	 l-p:0.05568096041679382
epoch£º342	 i:8 	 global-step:6848	 l-p:0.055987801402807236
epoch£º342	 i:9 	 global-step:6849	 l-p:0.055663708597421646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0147, 35.7674, 39.9951],
        [28.0147, 28.0147, 28.0147],
        [28.0147, 37.1810, 43.1179],
        [28.0147, 28.3583, 28.1046]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.05568390712141991 
model_pd.l_d.mean(): -0.00013130169827491045 
model_pd.lagr.mean(): 0.05555260553956032 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6461], device='cuda:0')), ('power', tensor([-0.2267], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.05568390712141991
epoch£º343	 i:1 	 global-step:6861	 l-p:0.05573359876871109
epoch£º343	 i:2 	 global-step:6862	 l-p:0.05569400265812874
epoch£º343	 i:3 	 global-step:6863	 l-p:0.055989474058151245
epoch£º343	 i:4 	 global-step:6864	 l-p:0.05570607632398605
epoch£º343	 i:5 	 global-step:6865	 l-p:0.056069836020469666
epoch£º343	 i:6 	 global-step:6866	 l-p:0.05575859919190407
epoch£º343	 i:7 	 global-step:6867	 l-p:0.055763356387615204
epoch£º343	 i:8 	 global-step:6868	 l-p:0.05573846772313118
epoch£º343	 i:9 	 global-step:6869	 l-p:0.0558931902050972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[27.8857, 37.0079, 42.9161],
        [27.8857, 35.7480, 40.1254],
        [27.8857, 29.1968, 28.6907],
        [27.8857, 29.1185, 28.6142]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.05571164935827255 
model_pd.l_d.mean(): -0.00017788280092645437 
model_pd.lagr.mean(): 0.055533766746520996 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6407], device='cuda:0')), ('power', tensor([-0.3600], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.05571164935827255
epoch£º344	 i:1 	 global-step:6881	 l-p:0.055789269506931305
epoch£º344	 i:2 	 global-step:6882	 l-p:0.055975571274757385
epoch£º344	 i:3 	 global-step:6883	 l-p:0.055815961211919785
epoch£º344	 i:4 	 global-step:6884	 l-p:0.05602055415511131
epoch£º344	 i:5 	 global-step:6885	 l-p:0.05589372664690018
epoch£º344	 i:6 	 global-step:6886	 l-p:0.055806126445531845
epoch£º344	 i:7 	 global-step:6887	 l-p:0.05577855557203293
epoch£º344	 i:8 	 global-step:6888	 l-p:0.05571817234158516
epoch£º344	 i:9 	 global-step:6889	 l-p:0.055791452527046204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7800, 28.0032, 27.8248],
        [27.7800, 36.9507, 42.9425],
        [27.7800, 28.0711, 27.8490],
        [27.7800, 30.9567, 31.0538]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.05598164349794388 
model_pd.l_d.mean(): -0.0001143697154475376 
model_pd.lagr.mean(): 0.0558672733604908 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5857], device='cuda:0')), ('power', tensor([-0.3272], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.05598164349794388
epoch£º345	 i:1 	 global-step:6901	 l-p:0.055917561054229736
epoch£º345	 i:2 	 global-step:6902	 l-p:0.05579681321978569
epoch£º345	 i:3 	 global-step:6903	 l-p:0.055724117904901505
epoch£º345	 i:4 	 global-step:6904	 l-p:0.05579560622572899
epoch£º345	 i:5 	 global-step:6905	 l-p:0.055945735424757004
epoch£º345	 i:6 	 global-step:6906	 l-p:0.05590435862541199
epoch£º345	 i:7 	 global-step:6907	 l-p:0.05576665326952934
epoch£º345	 i:8 	 global-step:6908	 l-p:0.05587048456072807
epoch£º345	 i:9 	 global-step:6909	 l-p:0.05579858645796776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7179, 27.8452, 27.7360],
        [27.7179, 27.7192, 27.7179],
        [27.7179, 28.1253, 27.8375],
        [27.7179, 31.4407, 31.8874]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.05583721399307251 
model_pd.l_d.mean(): -5.8956240536645055e-05 
model_pd.lagr.mean(): 0.05577825754880905 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5438], device='cuda:0')), ('power', tensor([-0.3653], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.05583721399307251
epoch£º346	 i:1 	 global-step:6921	 l-p:0.055941104888916016
epoch£º346	 i:2 	 global-step:6922	 l-p:0.05575468763709068
epoch£º346	 i:3 	 global-step:6923	 l-p:0.05601637810468674
epoch£º346	 i:4 	 global-step:6924	 l-p:0.056146420538425446
epoch£º346	 i:5 	 global-step:6925	 l-p:0.055777981877326965
epoch£º346	 i:6 	 global-step:6926	 l-p:0.05577479302883148
epoch£º346	 i:7 	 global-step:6927	 l-p:0.05575313791632652
epoch£º346	 i:8 	 global-step:6928	 l-p:0.05579335615038872
epoch£º346	 i:9 	 global-step:6929	 l-p:0.055788639932870865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7173, 27.7173, 27.7173],
        [27.7173, 30.9480, 31.0807],
        [27.7173, 37.2392, 43.6960],
        [27.7173, 27.7692, 27.7216]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.05583447962999344 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05583447962999344 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5545], device='cuda:0')), ('power', tensor([-0.3848], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.05583447962999344
epoch£º347	 i:1 	 global-step:6941	 l-p:0.05588892847299576
epoch£º347	 i:2 	 global-step:6942	 l-p:0.05580455809831619
epoch£º347	 i:3 	 global-step:6943	 l-p:0.05594027042388916
epoch£º347	 i:4 	 global-step:6944	 l-p:0.0558164119720459
epoch£º347	 i:5 	 global-step:6945	 l-p:0.055772438645362854
epoch£º347	 i:6 	 global-step:6946	 l-p:0.05583962798118591
epoch£º347	 i:7 	 global-step:6947	 l-p:0.05572885274887085
epoch£º347	 i:8 	 global-step:6948	 l-p:0.056051142513751984
epoch£º347	 i:9 	 global-step:6949	 l-p:0.05584647133946419
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7707, 28.5130, 28.0898],
        [27.7707, 28.1535, 27.8786],
        [27.7707, 28.4392, 28.0395],
        [27.7707, 27.7707, 27.7707]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.05577864870429039 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05577864870429039 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6076], device='cuda:0')), ('power', tensor([-0.4335], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.05577864870429039
epoch£º348	 i:1 	 global-step:6961	 l-p:0.05576563626527786
epoch£º348	 i:2 	 global-step:6962	 l-p:0.05573653057217598
epoch£º348	 i:3 	 global-step:6963	 l-p:0.055782489478588104
epoch£º348	 i:4 	 global-step:6964	 l-p:0.055816344916820526
epoch£º348	 i:5 	 global-step:6965	 l-p:0.055925652384757996
epoch£º348	 i:6 	 global-step:6966	 l-p:0.0557798370718956
epoch£º348	 i:7 	 global-step:6967	 l-p:0.056002844125032425
epoch£º348	 i:8 	 global-step:6968	 l-p:0.05582094192504883
epoch£º348	 i:9 	 global-step:6969	 l-p:0.055963486433029175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8467, 27.8467, 27.8467],
        [27.8467, 27.8624, 27.8474],
        [27.8467, 27.8663, 27.8477],
        [27.8467, 28.2241, 27.9520]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.05602574348449707 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05602574348449707 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5308], device='cuda:0')), ('power', tensor([-0.1434], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.05602574348449707
epoch£º349	 i:1 	 global-step:6981	 l-p:0.05579274147748947
epoch£º349	 i:2 	 global-step:6982	 l-p:0.055710095912218094
epoch£º349	 i:3 	 global-step:6983	 l-p:0.056075725704431534
epoch£º349	 i:4 	 global-step:6984	 l-p:0.05580633878707886
epoch£º349	 i:5 	 global-step:6985	 l-p:0.05572659522294998
epoch£º349	 i:6 	 global-step:6986	 l-p:0.05571760982275009
epoch£º349	 i:7 	 global-step:6987	 l-p:0.055753231048583984
epoch£º349	 i:8 	 global-step:6988	 l-p:0.05571882054209709
epoch£º349	 i:9 	 global-step:6989	 l-p:0.05586349219083786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9320, 27.9515, 27.9329],
        [27.9320, 35.3350, 39.1791],
        [27.9320, 34.1082, 36.6438],
        [27.9320, 35.4079, 39.3340]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.05573178082704544 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05573178082704544 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6147], device='cuda:0')), ('power', tensor([-0.2115], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.05573178082704544
epoch£º350	 i:1 	 global-step:7001	 l-p:0.055728018283843994
epoch£º350	 i:2 	 global-step:7002	 l-p:0.05576375871896744
epoch£º350	 i:3 	 global-step:7003	 l-p:0.05602507293224335
epoch£º350	 i:4 	 global-step:7004	 l-p:0.055697184056043625
epoch£º350	 i:5 	 global-step:7005	 l-p:0.05573219060897827
epoch£º350	 i:6 	 global-step:7006	 l-p:0.05580032616853714
epoch£º350	 i:7 	 global-step:7007	 l-p:0.05582049861550331
epoch£º350	 i:8 	 global-step:7008	 l-p:0.05572478473186493
epoch£º350	 i:9 	 global-step:7009	 l-p:0.05596977099776268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0194, 28.6566, 28.2665],
        [28.0194, 28.9347, 28.4664],
        [28.0194, 32.3056, 33.1370],
        [28.0194, 32.5149, 33.5108]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.055739764124155045 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055739764124155045 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5865], device='cuda:0')), ('power', tensor([-0.0974], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.055739764124155045
epoch£º351	 i:1 	 global-step:7021	 l-p:0.0558554008603096
epoch£º351	 i:2 	 global-step:7022	 l-p:0.0557071715593338
epoch£º351	 i:3 	 global-step:7023	 l-p:0.05576866492629051
epoch£º351	 i:4 	 global-step:7024	 l-p:0.05568346381187439
epoch£º351	 i:5 	 global-step:7025	 l-p:0.055695027112960815
epoch£º351	 i:6 	 global-step:7026	 l-p:0.05571461468935013
epoch£º351	 i:7 	 global-step:7027	 l-p:0.055914606899023056
epoch£º351	 i:8 	 global-step:7028	 l-p:0.0556814931333065
epoch£º351	 i:9 	 global-step:7029	 l-p:0.05603482946753502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1083, 28.6740, 28.3112],
        [28.1083, 31.5601, 31.7999],
        [28.1083, 29.2434, 28.7419],
        [28.1083, 33.0593, 34.4252]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.055645670741796494 
model_pd.l_d.mean(): -1.860685301835474e-06 
model_pd.lagr.mean(): 0.05564381182193756 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.3425e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6782], device='cuda:0')), ('power', tensor([-0.1624], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.055645670741796494
epoch£º352	 i:1 	 global-step:7041	 l-p:0.055866580456495285
epoch£º352	 i:2 	 global-step:7042	 l-p:0.05574730411171913
epoch£º352	 i:3 	 global-step:7043	 l-p:0.05573153868317604
epoch£º352	 i:4 	 global-step:7044	 l-p:0.055689629167318344
epoch£º352	 i:5 	 global-step:7045	 l-p:0.05590950325131416
epoch£º352	 i:6 	 global-step:7046	 l-p:0.05572190135717392
epoch£º352	 i:7 	 global-step:7047	 l-p:0.05568694323301315
epoch£º352	 i:8 	 global-step:7048	 l-p:0.05591969192028046
epoch£º352	 i:9 	 global-step:7049	 l-p:0.055678751319646835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1969, 28.2934, 28.2084],
        [28.1969, 28.2296, 28.1990],
        [28.1969, 28.2261, 28.1986],
        [28.1969, 28.1970, 28.1969]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.05567468702793121 
model_pd.l_d.mean(): -6.959699927477914e-09 
model_pd.lagr.mean(): 0.055674679577350616 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.1860e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6290], device='cuda:0')), ('power', tensor([-0.0003], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.05567468702793121
epoch£º353	 i:1 	 global-step:7061	 l-p:0.0558791421353817
epoch£º353	 i:2 	 global-step:7062	 l-p:0.05583975836634636
epoch£º353	 i:3 	 global-step:7063	 l-p:0.05580480396747589
epoch£º353	 i:4 	 global-step:7064	 l-p:0.05564740672707558
epoch£º353	 i:5 	 global-step:7065	 l-p:0.05565563961863518
epoch£º353	 i:6 	 global-step:7066	 l-p:0.055843714624643326
epoch£º353	 i:7 	 global-step:7067	 l-p:0.05563997104763985
epoch£º353	 i:8 	 global-step:7068	 l-p:0.05569390952587128
epoch£º353	 i:9 	 global-step:7069	 l-p:0.05572740361094475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2796, 28.7526, 28.4303],
        [28.2796, 33.8308, 35.7155],
        [28.2796, 38.2736, 45.2216],
        [28.2796, 28.3737, 28.2906]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.055637042969465256 
model_pd.l_d.mean(): 1.4533385410686606e-06 
model_pd.lagr.mean(): 0.055638495832681656 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.6230e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6691], device='cuda:0')), ('power', tensor([0.0193], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.055637042969465256
epoch£º354	 i:1 	 global-step:7081	 l-p:0.05586249381303787
epoch£º354	 i:2 	 global-step:7082	 l-p:0.05569866672158241
epoch£º354	 i:3 	 global-step:7083	 l-p:0.05563990771770477
epoch£º354	 i:4 	 global-step:7084	 l-p:0.05565908923745155
epoch£º354	 i:5 	 global-step:7085	 l-p:0.05609201639890671
epoch£º354	 i:6 	 global-step:7086	 l-p:0.05565396323800087
epoch£º354	 i:7 	 global-step:7087	 l-p:0.055689532309770584
epoch£º354	 i:8 	 global-step:7088	 l-p:0.055649541318416595
epoch£º354	 i:9 	 global-step:7089	 l-p:0.05565652623772621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3453, 38.4243, 45.4693],
        [28.3453, 28.6246, 28.4089],
        [28.3453, 28.3954, 28.3493],
        [28.3453, 28.3623, 28.3460]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.05568763613700867 
model_pd.l_d.mean(): 2.6512703698244877e-05 
model_pd.lagr.mean(): 0.05571414902806282 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6075], device='cuda:0')), ('power', tensor([0.1594], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.05568763613700867
epoch£º355	 i:1 	 global-step:7101	 l-p:0.05580858513712883
epoch£º355	 i:2 	 global-step:7102	 l-p:0.05566473305225372
epoch£º355	 i:3 	 global-step:7103	 l-p:0.05579651892185211
epoch£º355	 i:4 	 global-step:7104	 l-p:0.05584853142499924
epoch£º355	 i:5 	 global-step:7105	 l-p:0.05562274903059006
epoch£º355	 i:6 	 global-step:7106	 l-p:0.055622681975364685
epoch£º355	 i:7 	 global-step:7107	 l-p:0.05574130266904831
epoch£º355	 i:8 	 global-step:7108	 l-p:0.05565955862402916
epoch£º355	 i:9 	 global-step:7109	 l-p:0.05567053705453873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3799, 29.3162, 28.8398],
        [28.3799, 35.6707, 39.3162],
        [28.3799, 28.3801, 28.3799],
        [28.3799, 29.3720, 28.8855]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.05574054643511772 
model_pd.l_d.mean(): 6.929953815415502e-05 
model_pd.lagr.mean(): 0.05580984428524971 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6182], device='cuda:0')), ('power', tensor([0.2444], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.05574054643511772
epoch£º356	 i:1 	 global-step:7121	 l-p:0.05560041964054108
epoch£º356	 i:2 	 global-step:7122	 l-p:0.05567796900868416
epoch£º356	 i:3 	 global-step:7123	 l-p:0.05589393898844719
epoch£º356	 i:4 	 global-step:7124	 l-p:0.05567065253853798
epoch£º356	 i:5 	 global-step:7125	 l-p:0.055629100650548935
epoch£º356	 i:6 	 global-step:7126	 l-p:0.05563710257411003
epoch£º356	 i:7 	 global-step:7127	 l-p:0.05571947991847992
epoch£º356	 i:8 	 global-step:7128	 l-p:0.05591168627142906
epoch£º356	 i:9 	 global-step:7129	 l-p:0.05560636892914772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3722, 38.1294, 44.7466],
        [28.3722, 28.3982, 28.3736],
        [28.3722, 28.5104, 28.3925],
        [28.3722, 29.8479, 29.3360]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.055681124329566956 
model_pd.l_d.mean(): 0.00011158861889271066 
model_pd.lagr.mean(): 0.055792711675167084 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5752], device='cuda:0')), ('power', tensor([0.2729], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.055681124329566956
epoch£º357	 i:1 	 global-step:7141	 l-p:0.055604808032512665
epoch£º357	 i:2 	 global-step:7142	 l-p:0.05564998835325241
epoch£º357	 i:3 	 global-step:7143	 l-p:0.0557459257543087
epoch£º357	 i:4 	 global-step:7144	 l-p:0.055675581097602844
epoch£º357	 i:5 	 global-step:7145	 l-p:0.05574006959795952
epoch£º357	 i:6 	 global-step:7146	 l-p:0.05584442988038063
epoch£º357	 i:7 	 global-step:7147	 l-p:0.05572722479701042
epoch£º357	 i:8 	 global-step:7148	 l-p:0.055636558681726456
epoch£º357	 i:9 	 global-step:7149	 l-p:0.055846136063337326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3160, 28.3275, 28.3164],
        [28.3160, 28.3162, 28.3160],
        [28.3160, 30.5896, 30.2410],
        [28.3160, 36.7632, 41.7532]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.05563708022236824 
model_pd.l_d.mean(): 5.2398343541426584e-05 
model_pd.lagr.mean(): 0.055689480155706406 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6428], device='cuda:0')), ('power', tensor([0.1008], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.05563708022236824
epoch£º358	 i:1 	 global-step:7161	 l-p:0.05578006058931351
epoch£º358	 i:2 	 global-step:7162	 l-p:0.0558500662446022
epoch£º358	 i:3 	 global-step:7163	 l-p:0.05570931360125542
epoch£º358	 i:4 	 global-step:7164	 l-p:0.05564470961689949
epoch£º358	 i:5 	 global-step:7165	 l-p:0.056009501218795776
epoch£º358	 i:6 	 global-step:7166	 l-p:0.05563506856560707
epoch£º358	 i:7 	 global-step:7167	 l-p:0.055671680718660355
epoch£º358	 i:8 	 global-step:7168	 l-p:0.05571826919913292
epoch£º358	 i:9 	 global-step:7169	 l-p:0.055664051324129105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2144, 28.5991, 28.3220],
        [28.2144, 28.2144, 28.2144],
        [28.2144, 30.0709, 29.6131],
        [28.2144, 28.2148, 28.2144]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.05569552257657051 
model_pd.l_d.mean(): 2.802610288199503e-05 
model_pd.lagr.mean(): 0.05572354793548584 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6126], device='cuda:0')), ('power', tensor([0.0473], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.05569552257657051
epoch£º359	 i:1 	 global-step:7181	 l-p:0.05566227436065674
epoch£º359	 i:2 	 global-step:7182	 l-p:0.05587455630302429
epoch£º359	 i:3 	 global-step:7183	 l-p:0.055874843150377274
epoch£º359	 i:4 	 global-step:7184	 l-p:0.05591954290866852
epoch£º359	 i:5 	 global-step:7185	 l-p:0.055683378130197525
epoch£º359	 i:6 	 global-step:7186	 l-p:0.05567474290728569
epoch£º359	 i:7 	 global-step:7187	 l-p:0.0557202510535717
epoch£º359	 i:8 	 global-step:7188	 l-p:0.05577462911605835
epoch£º359	 i:9 	 global-step:7189	 l-p:0.0557013601064682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0791, 29.4953, 28.9871],
        [28.0791, 29.5440, 29.0380],
        [28.0791, 29.0756, 28.5920],
        [28.0791, 28.0794, 28.0791]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.055737972259521484 
model_pd.l_d.mean(): -1.606345540494658e-05 
model_pd.lagr.mean(): 0.05572190880775452 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5885], device='cuda:0')), ('power', tensor([-0.0265], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.055737972259521484
epoch£º360	 i:1 	 global-step:7201	 l-p:0.05571667477488518
epoch£º360	 i:2 	 global-step:7202	 l-p:0.05580238252878189
epoch£º360	 i:3 	 global-step:7203	 l-p:0.05568736791610718
epoch£º360	 i:4 	 global-step:7204	 l-p:0.0559152252972126
epoch£º360	 i:5 	 global-step:7205	 l-p:0.05575612559914589
epoch£º360	 i:6 	 global-step:7206	 l-p:0.055698346346616745
epoch£º360	 i:7 	 global-step:7207	 l-p:0.05610406771302223
epoch£º360	 i:8 	 global-step:7208	 l-p:0.055772531777620316
epoch£º360	 i:9 	 global-step:7209	 l-p:0.055711373686790466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9313, 27.9313, 27.9313],
        [27.9313, 27.9315, 27.9313],
        [27.9313, 27.9333, 27.9314],
        [27.9313, 27.9514, 27.9323]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.05571524798870087 
model_pd.l_d.mean(): -0.00013892710558138788 
model_pd.lagr.mean(): 0.05557632073760033 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6206], device='cuda:0')), ('power', tensor([-0.2529], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.05571524798870087
epoch£º361	 i:1 	 global-step:7221	 l-p:0.05573797971010208
epoch£º361	 i:2 	 global-step:7222	 l-p:0.05581337958574295
epoch£º361	 i:3 	 global-step:7223	 l-p:0.055722493678331375
epoch£º361	 i:4 	 global-step:7224	 l-p:0.05621572211384773
epoch£º361	 i:5 	 global-step:7225	 l-p:0.05573641136288643
epoch£º361	 i:6 	 global-step:7226	 l-p:0.05582914128899574
epoch£º361	 i:7 	 global-step:7227	 l-p:0.05573909729719162
epoch£º361	 i:8 	 global-step:7228	 l-p:0.05593462660908699
epoch£º361	 i:9 	 global-step:7229	 l-p:0.055782631039619446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7971, 29.3741, 28.8841],
        [27.7971, 27.7972, 27.7971],
        [27.7971, 27.7973, 27.7971],
        [27.7971, 34.7378, 38.0931]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.055770549923181534 
model_pd.l_d.mean(): -0.00014496664516627789 
model_pd.lagr.mean(): 0.05562558397650719 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6042], device='cuda:0')), ('power', tensor([-0.3442], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.055770549923181534
epoch£º362	 i:1 	 global-step:7241	 l-p:0.055739257484674454
epoch£º362	 i:2 	 global-step:7242	 l-p:0.05578770488500595
epoch£º362	 i:3 	 global-step:7243	 l-p:0.055986881256103516
epoch£º362	 i:4 	 global-step:7244	 l-p:0.055854182690382004
epoch£º362	 i:5 	 global-step:7245	 l-p:0.05576630309224129
epoch£º362	 i:6 	 global-step:7246	 l-p:0.05610112473368645
epoch£º362	 i:7 	 global-step:7247	 l-p:0.055841244757175446
epoch£º362	 i:8 	 global-step:7248	 l-p:0.05578315630555153
epoch£º362	 i:9 	 global-step:7249	 l-p:0.055865172296762466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7043, 28.5400, 28.0925],
        [27.7043, 27.7122, 27.7046],
        [27.7043, 27.7237, 27.7052],
        [27.7043, 29.6522, 29.2315]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.055755823850631714 
model_pd.l_d.mean(): -0.0001246053579961881 
model_pd.lagr.mean(): 0.05563122034072876 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6264], device='cuda:0')), ('power', tensor([-0.5310], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.055755823850631714
epoch£º363	 i:1 	 global-step:7261	 l-p:0.05609681457281113
epoch£º363	 i:2 	 global-step:7262	 l-p:0.05583985149860382
epoch£º363	 i:3 	 global-step:7263	 l-p:0.05584077537059784
epoch£º363	 i:4 	 global-step:7264	 l-p:0.055875372141599655
epoch£º363	 i:5 	 global-step:7265	 l-p:0.055835023522377014
epoch£º363	 i:6 	 global-step:7266	 l-p:0.05576137453317642
epoch£º363	 i:7 	 global-step:7267	 l-p:0.05596618354320526
epoch£º363	 i:8 	 global-step:7268	 l-p:0.055745918303728104
epoch£º363	 i:9 	 global-step:7269	 l-p:0.05592703819274902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6765, 37.9175, 45.3368],
        [27.6765, 28.5138, 28.0661],
        [27.6765, 27.6780, 27.6765],
        [27.6765, 29.8827, 29.5379]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.05601746588945389 
model_pd.l_d.mean(): -5.863525075255893e-06 
model_pd.lagr.mean(): 0.05601160228252411 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5609], device='cuda:0')), ('power', tensor([-0.3688], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.05601746588945389
epoch£º364	 i:1 	 global-step:7281	 l-p:0.05594060942530632
epoch£º364	 i:2 	 global-step:7282	 l-p:0.055809468030929565
epoch£º364	 i:3 	 global-step:7283	 l-p:0.05593649297952652
epoch£º364	 i:4 	 global-step:7284	 l-p:0.05586469918489456
epoch£º364	 i:5 	 global-step:7285	 l-p:0.055752094835042953
epoch£º364	 i:6 	 global-step:7286	 l-p:0.05596644803881645
epoch£º364	 i:7 	 global-step:7287	 l-p:0.055760420858860016
epoch£º364	 i:8 	 global-step:7288	 l-p:0.05575629696249962
epoch£º364	 i:9 	 global-step:7289	 l-p:0.05582712963223457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7201, 27.7201, 27.7201],
        [27.7201, 29.6664, 29.2449],
        [27.7201, 29.1599, 28.6604],
        [27.7201, 27.8076, 27.7300]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.0557522289454937 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0557522289454937 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6267], device='cuda:0')), ('power', tensor([-0.5113], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.0557522289454937
epoch£º365	 i:1 	 global-step:7301	 l-p:0.05578756704926491
epoch£º365	 i:2 	 global-step:7302	 l-p:0.05574241653084755
epoch£º365	 i:3 	 global-step:7303	 l-p:0.056096144020557404
epoch£º365	 i:4 	 global-step:7304	 l-p:0.055789366364479065
epoch£º365	 i:5 	 global-step:7305	 l-p:0.05574902892112732
epoch£º365	 i:6 	 global-step:7306	 l-p:0.055944450199604034
epoch£º365	 i:7 	 global-step:7307	 l-p:0.055902957916259766
epoch£º365	 i:8 	 global-step:7308	 l-p:0.0559719055891037
epoch£º365	 i:9 	 global-step:7309	 l-p:0.055752430111169815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7977, 27.8498, 27.8020],
        [27.7977, 36.9746, 42.9704],
        [27.7977, 37.6139, 44.4376],
        [27.7977, 27.8388, 27.8007]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.056018322706222534 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056018322706222534 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4851], device='cuda:0')), ('power', tensor([-0.1213], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.056018322706222534
epoch£º366	 i:1 	 global-step:7321	 l-p:0.05574391782283783
epoch£º366	 i:2 	 global-step:7322	 l-p:0.05572619289159775
epoch£º366	 i:3 	 global-step:7323	 l-p:0.056036461144685745
epoch£º366	 i:4 	 global-step:7324	 l-p:0.055770911276340485
epoch£º366	 i:5 	 global-step:7325	 l-p:0.05569671839475632
epoch£º366	 i:6 	 global-step:7326	 l-p:0.05575688183307648
epoch£º366	 i:7 	 global-step:7327	 l-p:0.05586208403110504
epoch£º366	 i:8 	 global-step:7328	 l-p:0.055711813271045685
epoch£º366	 i:9 	 global-step:7329	 l-p:0.05597272887825966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[27.8883, 37.2161, 43.3853],
        [27.8883, 29.6767, 29.2156],
        [27.8883, 29.1129, 28.6089],
        [27.8883, 31.5225, 31.8961]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.05592678114771843 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05592678114771843 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5578], device='cuda:0')), ('power', tensor([-0.1263], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.05592678114771843
epoch£º367	 i:1 	 global-step:7341	 l-p:0.05568888038396835
epoch£º367	 i:2 	 global-step:7342	 l-p:0.05576150864362717
epoch£º367	 i:3 	 global-step:7343	 l-p:0.05574851483106613
epoch£º367	 i:4 	 global-step:7344	 l-p:0.055739954113960266
epoch£º367	 i:5 	 global-step:7345	 l-p:0.055732496082782745
epoch£º367	 i:6 	 global-step:7346	 l-p:0.05603012815117836
epoch£º367	 i:7 	 global-step:7347	 l-p:0.05569081008434296
epoch£º367	 i:8 	 global-step:7348	 l-p:0.056026212871074677
epoch£º367	 i:9 	 global-step:7349	 l-p:0.05573944002389908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9832, 27.9832, 27.9832],
        [27.9832, 27.9832, 27.9832],
        [27.9832, 28.0169, 27.9853],
        [27.9832, 30.9090, 30.8565]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.05572982504963875 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05572982504963875 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5955], device='cuda:0')), ('power', tensor([-0.1759], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.05572982504963875
epoch£º368	 i:1 	 global-step:7361	 l-p:0.05577884614467621
epoch£º368	 i:2 	 global-step:7362	 l-p:0.05568655952811241
epoch£º368	 i:3 	 global-step:7363	 l-p:0.05572567507624626
epoch£º368	 i:4 	 global-step:7364	 l-p:0.05610022321343422
epoch£º368	 i:5 	 global-step:7365	 l-p:0.055730998516082764
epoch£º368	 i:6 	 global-step:7366	 l-p:0.055731333792209625
epoch£º368	 i:7 	 global-step:7367	 l-p:0.05567930266261101
epoch£º368	 i:8 	 global-step:7368	 l-p:0.05577372759580612
epoch£º368	 i:9 	 global-step:7369	 l-p:0.05593223124742508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0799, 32.2031, 32.9057],
        [28.0799, 28.1036, 28.0812],
        [28.0799, 28.0799, 28.0799],
        [28.0799, 37.5264, 43.8062]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.055774789303541183 
model_pd.l_d.mean(): 4.9193694451332703e-08 
model_pd.lagr.mean(): 0.055774837732315063 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.1468e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5432], device='cuda:0')), ('power', tensor([0.0289], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.055774789303541183
epoch£º369	 i:1 	 global-step:7381	 l-p:0.05591903626918793
epoch£º369	 i:2 	 global-step:7382	 l-p:0.05567748844623566
epoch£º369	 i:3 	 global-step:7383	 l-p:0.055708616971969604
epoch£º369	 i:4 	 global-step:7384	 l-p:0.055708348751068115
epoch£º369	 i:5 	 global-step:7385	 l-p:0.05566294491291046
epoch£º369	 i:6 	 global-step:7386	 l-p:0.05586625635623932
epoch£º369	 i:7 	 global-step:7387	 l-p:0.055832963436841965
epoch£º369	 i:8 	 global-step:7388	 l-p:0.055753450840711594
epoch£º369	 i:9 	 global-step:7389	 l-p:0.055748358368873596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1774, 31.3675, 31.4465],
        [28.1774, 28.2101, 28.1795],
        [28.1774, 36.0917, 40.4775],
        [28.1774, 29.0982, 28.6271]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.05612220615148544 
model_pd.l_d.mean(): 3.4395334296277724e-06 
model_pd.lagr.mean(): 0.05612564459443092 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6289e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5238], device='cuda:0')), ('power', tensor([0.2807], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.05612220615148544
epoch£º370	 i:1 	 global-step:7401	 l-p:0.055686432868242264
epoch£º370	 i:2 	 global-step:7402	 l-p:0.0556858628988266
epoch£º370	 i:3 	 global-step:7403	 l-p:0.05586450546979904
epoch£º370	 i:4 	 global-step:7404	 l-p:0.05562050640583038
epoch£º370	 i:5 	 global-step:7405	 l-p:0.05564432591199875
epoch£º370	 i:6 	 global-step:7406	 l-p:0.05564935505390167
epoch£º370	 i:7 	 global-step:7407	 l-p:0.055770378559827805
epoch£º370	 i:8 	 global-step:7408	 l-p:0.055698461830616
epoch£º370	 i:9 	 global-step:7409	 l-p:0.0556982159614563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2704, 28.2706, 28.2704],
        [28.2704, 28.2865, 28.2711],
        [28.2704, 37.7261, 43.9768],
        [28.2704, 28.4510, 28.3018]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.05567850172519684 
model_pd.l_d.mean(): 5.8964274103345815e-06 
model_pd.lagr.mean(): 0.055684398859739304 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.3351e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6046], device='cuda:0')), ('power', tensor([0.1011], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.05567850172519684
epoch£º371	 i:1 	 global-step:7421	 l-p:0.055708128958940506
epoch£º371	 i:2 	 global-step:7422	 l-p:0.055669717490673065
epoch£º371	 i:3 	 global-step:7423	 l-p:0.0557139590382576
epoch£º371	 i:4 	 global-step:7424	 l-p:0.05588630586862564
epoch£º371	 i:5 	 global-step:7425	 l-p:0.055641695857048035
epoch£º371	 i:6 	 global-step:7426	 l-p:0.05568114295601845
epoch£º371	 i:7 	 global-step:7427	 l-p:0.05585232749581337
epoch£º371	 i:8 	 global-step:7428	 l-p:0.05574306473135948
epoch£º371	 i:9 	 global-step:7429	 l-p:0.055672015994787216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3483, 38.8146, 46.3766],
        [28.3483, 28.3485, 28.3483],
        [28.3483, 28.3653, 28.3490],
        [28.3483, 31.3144, 31.2613]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.05581645667552948 
model_pd.l_d.mean(): 4.19592070102226e-05 
model_pd.lagr.mean(): 0.055858414620161057 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5809], device='cuda:0')), ('power', tensor([0.2847], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.05581645667552948
epoch£º372	 i:1 	 global-step:7441	 l-p:0.0556512288749218
epoch£º372	 i:2 	 global-step:7442	 l-p:0.05567760765552521
epoch£º372	 i:3 	 global-step:7443	 l-p:0.0556534081697464
epoch£º372	 i:4 	 global-step:7444	 l-p:0.055625900626182556
epoch£º372	 i:5 	 global-step:7445	 l-p:0.055717986077070236
epoch£º372	 i:6 	 global-step:7446	 l-p:0.05566059798002243
epoch£º372	 i:7 	 global-step:7447	 l-p:0.05601110681891441
epoch£º372	 i:8 	 global-step:7448	 l-p:0.05558018013834953
epoch£º372	 i:9 	 global-step:7449	 l-p:0.055709708482027054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3945, 37.8994, 44.1863],
        [28.3945, 28.6925, 28.4651],
        [28.3945, 28.3945, 28.3945],
        [28.3945, 31.9886, 32.2980]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.05585883930325508 
model_pd.l_d.mean(): 9.669866994954646e-05 
model_pd.lagr.mean(): 0.05595553666353226 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5586], device='cuda:0')), ('power', tensor([0.3597], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.05585883930325508
epoch£º373	 i:1 	 global-step:7461	 l-p:0.055673997849226
epoch£º373	 i:2 	 global-step:7462	 l-p:0.05574540048837662
epoch£º373	 i:3 	 global-step:7463	 l-p:0.055630091577768326
epoch£º373	 i:4 	 global-step:7464	 l-p:0.055674389004707336
epoch£º373	 i:5 	 global-step:7465	 l-p:0.055619724094867706
epoch£º373	 i:6 	 global-step:7466	 l-p:0.05582491680979729
epoch£º373	 i:7 	 global-step:7467	 l-p:0.05576254799962044
epoch£º373	 i:8 	 global-step:7468	 l-p:0.05558806285262108
epoch£º373	 i:9 	 global-step:7469	 l-p:0.055669937282800674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3927, 30.9901, 30.7613],
        [28.3927, 28.8677, 28.5440],
        [28.3927, 28.7237, 28.4765],
        [28.3927, 38.3954, 45.3291]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.05562322586774826 
model_pd.l_d.mean(): 8.207505015889183e-05 
model_pd.lagr.mean(): 0.05570530146360397 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6560], device='cuda:0')), ('power', tensor([0.2036], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.05562322586774826
epoch£º374	 i:1 	 global-step:7481	 l-p:0.05591306462883949
epoch£º374	 i:2 	 global-step:7482	 l-p:0.05568648874759674
epoch£º374	 i:3 	 global-step:7483	 l-p:0.05565572530031204
epoch£º374	 i:4 	 global-step:7484	 l-p:0.05562176927924156
epoch£º374	 i:5 	 global-step:7485	 l-p:0.055603522807359695
epoch£º374	 i:6 	 global-step:7486	 l-p:0.05567099153995514
epoch£º374	 i:7 	 global-step:7487	 l-p:0.05580437555909157
epoch£º374	 i:8 	 global-step:7488	 l-p:0.055656664073467255
epoch£º374	 i:9 	 global-step:7489	 l-p:0.0558718666434288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3342, 28.3842, 28.3382],
        [28.3342, 28.3342, 28.3342],
        [28.3342, 28.3938, 28.3394],
        [28.3342, 28.3354, 28.3342]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.05566348508000374 
model_pd.l_d.mean(): 5.866389983566478e-05 
model_pd.lagr.mean(): 0.05572214722633362 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6428], device='cuda:0')), ('power', tensor([0.1120], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.05566348508000374
epoch£º375	 i:1 	 global-step:7501	 l-p:0.055701106786727905
epoch£º375	 i:2 	 global-step:7502	 l-p:0.05561979115009308
epoch£º375	 i:3 	 global-step:7503	 l-p:0.055781878530979156
epoch£º375	 i:4 	 global-step:7504	 l-p:0.05563291534781456
epoch£º375	 i:5 	 global-step:7505	 l-p:0.05589616298675537
epoch£º375	 i:6 	 global-step:7506	 l-p:0.05564417317509651
epoch£º375	 i:7 	 global-step:7507	 l-p:0.055685222148895264
epoch£º375	 i:8 	 global-step:7508	 l-p:0.05562205985188484
epoch£º375	 i:9 	 global-step:7509	 l-p:0.05604347214102745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2234, 28.9021, 28.4960],
        [28.2234, 31.5164, 31.6519],
        [28.2234, 28.2249, 28.2234],
        [28.2234, 30.1059, 29.6532]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.05576994642615318 
model_pd.l_d.mean(): 6.675360054941848e-05 
model_pd.lagr.mean(): 0.05583669990301132 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5671], device='cuda:0')), ('power', tensor([0.1106], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.05576994642615318
epoch£º376	 i:1 	 global-step:7521	 l-p:0.055811088532209396
epoch£º376	 i:2 	 global-step:7522	 l-p:0.05577533692121506
epoch£º376	 i:3 	 global-step:7523	 l-p:0.05568768456578255
epoch£º376	 i:4 	 global-step:7524	 l-p:0.05574849992990494
epoch£º376	 i:5 	 global-step:7525	 l-p:0.055823467671871185
epoch£º376	 i:6 	 global-step:7526	 l-p:0.0556899830698967
epoch£º376	 i:7 	 global-step:7527	 l-p:0.05594095215201378
epoch£º376	 i:8 	 global-step:7528	 l-p:0.05566496402025223
epoch£º376	 i:9 	 global-step:7529	 l-p:0.0556623637676239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0734, 30.1417, 29.7397],
        [28.0734, 28.7267, 28.3305],
        [28.0734, 28.5972, 28.2524],
        [28.0734, 37.2599, 43.2099]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.05573223903775215 
model_pd.l_d.mean(): -6.194469460751861e-05 
model_pd.lagr.mean(): 0.05567029491066933 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6149], device='cuda:0')), ('power', tensor([-0.1001], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.05573223903775215
epoch£º377	 i:1 	 global-step:7541	 l-p:0.05569742992520332
epoch£º377	 i:2 	 global-step:7542	 l-p:0.05603355914354324
epoch£º377	 i:3 	 global-step:7543	 l-p:0.05572701245546341
epoch£º377	 i:4 	 global-step:7544	 l-p:0.05577076971530914
epoch£º377	 i:5 	 global-step:7545	 l-p:0.05606408417224884
epoch£º377	 i:6 	 global-step:7546	 l-p:0.05571147799491882
epoch£º377	 i:7 	 global-step:7547	 l-p:0.055698174983263016
epoch£º377	 i:8 	 global-step:7548	 l-p:0.05570800602436066
epoch£º377	 i:9 	 global-step:7549	 l-p:0.05578833445906639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9076, 30.7292, 30.6286],
        [27.9076, 27.9076, 27.9075],
        [27.9076, 28.9436, 28.4562],
        [27.9076, 28.4024, 28.0712]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.055794209241867065 
model_pd.l_d.mean(): -0.0001002685894491151 
model_pd.lagr.mean(): 0.055693939328193665 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5600], device='cuda:0')), ('power', tensor([-0.1805], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.055794209241867065
epoch£º378	 i:1 	 global-step:7561	 l-p:0.05579693615436554
epoch£º378	 i:2 	 global-step:7562	 l-p:0.056054532527923584
epoch£º378	 i:3 	 global-step:7563	 l-p:0.05579955875873566
epoch£º378	 i:4 	 global-step:7564	 l-p:0.05576704815030098
epoch£º378	 i:5 	 global-step:7565	 l-p:0.055775608867406845
epoch£º378	 i:6 	 global-step:7566	 l-p:0.055800359696149826
epoch£º378	 i:7 	 global-step:7567	 l-p:0.055725276470184326
epoch£º378	 i:8 	 global-step:7568	 l-p:0.05588021129369736
epoch£º378	 i:9 	 global-step:7569	 l-p:0.05590299516916275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7576, 29.3944, 28.9126],
        [27.7576, 27.7577, 27.7576],
        [27.7576, 27.8852, 27.7757],
        [27.7576, 29.8013, 29.4040]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.05602322518825531 
model_pd.l_d.mean(): -9.146266529569402e-05 
model_pd.lagr.mean(): 0.05593176186084747 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5442], device='cuda:0')), ('power', tensor([-0.2221], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.05602322518825531
epoch£º379	 i:1 	 global-step:7581	 l-p:0.0558115690946579
epoch£º379	 i:2 	 global-step:7582	 l-p:0.05609871819615364
epoch£º379	 i:3 	 global-step:7583	 l-p:0.05577193200588226
epoch£º379	 i:4 	 global-step:7584	 l-p:0.055790990591049194
epoch£º379	 i:5 	 global-step:7585	 l-p:0.05585175007581711
epoch£º379	 i:6 	 global-step:7586	 l-p:0.05581038445234299
epoch£º379	 i:7 	 global-step:7587	 l-p:0.05578203871846199
epoch£º379	 i:8 	 global-step:7588	 l-p:0.055768441408872604
epoch£º379	 i:9 	 global-step:7589	 l-p:0.05588380992412567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6574, 32.8305, 34.4424],
        [27.6574, 27.8348, 27.6883],
        [27.6574, 27.6605, 27.6574],
        [27.6574, 27.6894, 27.6594]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.05581454560160637 
model_pd.l_d.mean(): -0.00010651047341525555 
model_pd.lagr.mean(): 0.055708035826683044 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5993], device='cuda:0')), ('power', tensor([-0.5230], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.05581454560160637
epoch£º380	 i:1 	 global-step:7601	 l-p:0.055773064494132996
epoch£º380	 i:2 	 global-step:7602	 l-p:0.05591442063450813
epoch£º380	 i:3 	 global-step:7603	 l-p:0.055765550583601
epoch£º380	 i:4 	 global-step:7604	 l-p:0.056033309549093246
epoch£º380	 i:5 	 global-step:7605	 l-p:0.055811408907175064
epoch£º380	 i:6 	 global-step:7606	 l-p:0.05580639839172363
epoch£º380	 i:7 	 global-step:7607	 l-p:0.05596309155225754
epoch£º380	 i:8 	 global-step:7608	 l-p:0.056051481515169144
epoch£º380	 i:9 	 global-step:7609	 l-p:0.0558164156973362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6365, 27.6373, 27.6365],
        [27.6365, 35.2726, 39.4321],
        [27.6365, 28.2336, 27.8606],
        [27.6365, 29.0716, 28.5737]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.05583889037370682 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05583889037370682 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5528], device='cuda:0')), ('power', tensor([-0.4302], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.05583889037370682
epoch£º381	 i:1 	 global-step:7621	 l-p:0.055857524275779724
epoch£º381	 i:2 	 global-step:7622	 l-p:0.05582780763506889
epoch£º381	 i:3 	 global-step:7623	 l-p:0.05587390437722206
epoch£º381	 i:4 	 global-step:7624	 l-p:0.05592099204659462
epoch£º381	 i:5 	 global-step:7625	 l-p:0.05603642389178276
epoch£º381	 i:6 	 global-step:7626	 l-p:0.05575421079993248
epoch£º381	 i:7 	 global-step:7627	 l-p:0.05588044226169586
epoch£º381	 i:8 	 global-step:7628	 l-p:0.05593505874276161
epoch£º381	 i:9 	 global-step:7629	 l-p:0.055784065276384354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6927, 28.6747, 28.1980],
        [27.6927, 27.6951, 27.6927],
        [27.6927, 27.7006, 27.6929],
        [27.6927, 29.0851, 28.5841]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.05575527250766754 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575527250766754 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6386], device='cuda:0')), ('power', tensor([-0.5908], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.05575527250766754
epoch£º382	 i:1 	 global-step:7641	 l-p:0.05582514777779579
epoch£º382	 i:2 	 global-step:7642	 l-p:0.055853221565485
epoch£º382	 i:3 	 global-step:7643	 l-p:0.05574781447649002
epoch£º382	 i:4 	 global-step:7644	 l-p:0.05587403103709221
epoch£º382	 i:5 	 global-step:7645	 l-p:0.055832747370004654
epoch£º382	 i:6 	 global-step:7646	 l-p:0.05573464184999466
epoch£º382	 i:7 	 global-step:7647	 l-p:0.05580681562423706
epoch£º382	 i:8 	 global-step:7648	 l-p:0.056101106107234955
epoch£º382	 i:9 	 global-step:7649	 l-p:0.0560087114572525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7804, 27.8137, 27.7825],
        [27.7804, 30.3187, 30.0948],
        [27.7804, 27.7804, 27.7803],
        [27.7804, 28.3807, 28.0058]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.055800121277570724 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055800121277570724 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5847], device='cuda:0')), ('power', tensor([-0.3788], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.055800121277570724
epoch£º383	 i:1 	 global-step:7661	 l-p:0.05585145950317383
epoch£º383	 i:2 	 global-step:7662	 l-p:0.055724408477544785
epoch£º383	 i:3 	 global-step:7663	 l-p:0.055747177451848984
epoch£º383	 i:4 	 global-step:7664	 l-p:0.05577988550066948
epoch£º383	 i:5 	 global-step:7665	 l-p:0.055935341864824295
epoch£º383	 i:6 	 global-step:7666	 l-p:0.05578894168138504
epoch£º383	 i:7 	 global-step:7667	 l-p:0.055701978504657745
epoch£º383	 i:8 	 global-step:7668	 l-p:0.05584096908569336
epoch£º383	 i:9 	 global-step:7669	 l-p:0.05615405738353729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8800, 36.0512, 40.7930],
        [27.8800, 29.6678, 29.2068],
        [27.8800, 33.7574, 36.0035],
        [27.8800, 28.2598, 27.9862]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.0557108074426651 
model_pd.l_d.mean(): -5.150508286533295e-07 
model_pd.lagr.mean(): 0.05571029335260391 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6404], device='cuda:0')), ('power', tensor([-0.3130], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.0557108074426651
epoch£º384	 i:1 	 global-step:7681	 l-p:0.055805739015340805
epoch£º384	 i:2 	 global-step:7682	 l-p:0.05573808401823044
epoch£º384	 i:3 	 global-step:7683	 l-p:0.055871304124593735
epoch£º384	 i:4 	 global-step:7684	 l-p:0.05597304180264473
epoch£º384	 i:5 	 global-step:7685	 l-p:0.055746257305145264
epoch£º384	 i:6 	 global-step:7686	 l-p:0.055766791105270386
epoch£º384	 i:7 	 global-step:7687	 l-p:0.055817537009716034
epoch£º384	 i:8 	 global-step:7688	 l-p:0.05581352114677429
epoch£º384	 i:9 	 global-step:7689	 l-p:0.05585072562098503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9843, 30.6566, 30.4811],
        [27.9843, 28.0066, 27.9855],
        [27.9843, 27.9846, 27.9843],
        [27.9843, 35.8934, 40.3075]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.05574897676706314 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574897676706314 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5946], device='cuda:0')), ('power', tensor([-0.1499], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.05574897676706314
epoch£º385	 i:1 	 global-step:7701	 l-p:0.05576011538505554
epoch£º385	 i:2 	 global-step:7702	 l-p:0.05590830743312836
epoch£º385	 i:3 	 global-step:7703	 l-p:0.055795200169086456
epoch£º385	 i:4 	 global-step:7704	 l-p:0.05566423386335373
epoch£º385	 i:5 	 global-step:7705	 l-p:0.05580935999751091
epoch£º385	 i:6 	 global-step:7706	 l-p:0.055734243243932724
epoch£º385	 i:7 	 global-step:7707	 l-p:0.05590318515896797
epoch£º385	 i:8 	 global-step:7708	 l-p:0.055683430284261703
epoch£º385	 i:9 	 global-step:7709	 l-p:0.05585018917918205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0904, 28.7657, 28.3616],
        [28.0904, 33.7761, 35.8106],
        [28.0904, 28.0904, 28.0904],
        [28.0904, 28.5641, 28.2421]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.05573003739118576 
model_pd.l_d.mean(): -2.803286935915139e-09 
model_pd.lagr.mean(): 0.05573003366589546 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5800], device='cuda:0')), ('power', tensor([-0.0151], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.05573003739118576
epoch£º386	 i:1 	 global-step:7721	 l-p:0.055680982768535614
epoch£º386	 i:2 	 global-step:7722	 l-p:0.05566002428531647
epoch£º386	 i:3 	 global-step:7723	 l-p:0.056053340435028076
epoch£º386	 i:4 	 global-step:7724	 l-p:0.055663153529167175
epoch£º386	 i:5 	 global-step:7725	 l-p:0.055762600153684616
epoch£º386	 i:6 	 global-step:7726	 l-p:0.055970728397369385
epoch£º386	 i:7 	 global-step:7727	 l-p:0.05566595867276192
epoch£º386	 i:8 	 global-step:7728	 l-p:0.05568254739046097
epoch£º386	 i:9 	 global-step:7729	 l-p:0.05575085058808327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1964, 28.5480, 28.2893],
        [28.1964, 35.7225, 39.6609],
        [28.1964, 28.2017, 28.1965],
        [28.1964, 28.9000, 28.4858]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.05565372481942177 
model_pd.l_d.mean(): -1.4647904222897523e-08 
model_pd.lagr.mean(): 0.055653709918260574 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.6358e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6514], device='cuda:0')), ('power', tensor([-0.0009], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.05565372481942177
epoch£º387	 i:1 	 global-step:7741	 l-p:0.05596043914556503
epoch£º387	 i:2 	 global-step:7742	 l-p:0.05569375306367874
epoch£º387	 i:3 	 global-step:7743	 l-p:0.05564773455262184
epoch£º387	 i:4 	 global-step:7744	 l-p:0.05565667897462845
epoch£º387	 i:5 	 global-step:7745	 l-p:0.05567050725221634
epoch£º387	 i:6 	 global-step:7746	 l-p:0.05591491237282753
epoch£º387	 i:7 	 global-step:7747	 l-p:0.055688340216875076
epoch£º387	 i:8 	 global-step:7748	 l-p:0.05564922094345093
epoch£º387	 i:9 	 global-step:7749	 l-p:0.05585417523980141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2964, 28.3306, 28.2986],
        [28.2964, 28.6596, 28.3942],
        [28.2964, 28.3383, 28.2994],
        [28.2964, 30.7509, 30.4686]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.05575914680957794 
model_pd.l_d.mean(): 2.1710951841669157e-05 
model_pd.lagr.mean(): 0.05578085780143738 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.8184e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5262], device='cuda:0')), ('power', tensor([0.2958], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.05575914680957794
epoch£º388	 i:1 	 global-step:7761	 l-p:0.05577317997813225
epoch£º388	 i:2 	 global-step:7762	 l-p:0.055966462939977646
epoch£º388	 i:3 	 global-step:7763	 l-p:0.055644504725933075
epoch£º388	 i:4 	 global-step:7764	 l-p:0.05565212294459343
epoch£º388	 i:5 	 global-step:7765	 l-p:0.05561268702149391
epoch£º388	 i:6 	 global-step:7766	 l-p:0.055671628564596176
epoch£º388	 i:7 	 global-step:7767	 l-p:0.05564429238438606
epoch£º388	 i:8 	 global-step:7768	 l-p:0.0556875541806221
epoch£º388	 i:9 	 global-step:7769	 l-p:0.05577670410275459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3741, 32.4144, 33.0307],
        [28.3741, 30.6405, 30.2872],
        [28.3741, 28.3762, 28.3741],
        [28.3741, 29.0366, 28.6352]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.05580993369221687 
model_pd.l_d.mean(): 5.952940045972355e-05 
model_pd.lagr.mean(): 0.055869463831186295 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5752], device='cuda:0')), ('power', tensor([0.3383], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.05580993369221687
epoch£º389	 i:1 	 global-step:7781	 l-p:0.05590074881911278
epoch£º389	 i:2 	 global-step:7782	 l-p:0.055616606026887894
epoch£º389	 i:3 	 global-step:7783	 l-p:0.055889833718538284
epoch£º389	 i:4 	 global-step:7784	 l-p:0.05561399832367897
epoch£º389	 i:5 	 global-step:7785	 l-p:0.05564941465854645
epoch£º389	 i:6 	 global-step:7786	 l-p:0.05562915652990341
epoch£º389	 i:7 	 global-step:7787	 l-p:0.05569753423333168
epoch£º389	 i:8 	 global-step:7788	 l-p:0.055648691952228546
epoch£º389	 i:9 	 global-step:7789	 l-p:0.05560098960995674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4094, 28.4119, 28.4094],
        [28.4094, 29.8432, 29.3287],
        [28.4094, 36.8768, 41.8735],
        [28.4094, 28.4119, 28.4094]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.055609941482543945 
model_pd.l_d.mean(): 5.969669291516766e-05 
model_pd.lagr.mean(): 0.0556696392595768 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6509], device='cuda:0')), ('power', tensor([0.1936], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.055609941482543945
epoch£º390	 i:1 	 global-step:7801	 l-p:0.055665090680122375
epoch£º390	 i:2 	 global-step:7802	 l-p:0.05565466731786728
epoch£º390	 i:3 	 global-step:7803	 l-p:0.05578602850437164
epoch£º390	 i:4 	 global-step:7804	 l-p:0.055649593472480774
epoch£º390	 i:5 	 global-step:7805	 l-p:0.05569107085466385
epoch£º390	 i:6 	 global-step:7806	 l-p:0.05575795844197273
epoch£º390	 i:7 	 global-step:7807	 l-p:0.05582008138298988
epoch£º390	 i:8 	 global-step:7808	 l-p:0.055592067539691925
epoch£º390	 i:9 	 global-step:7809	 l-p:0.05580775439739227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3880, 28.4852, 28.3996],
        [28.3880, 31.1010, 30.9230],
        [28.3880, 30.6541, 30.3002],
        [28.3880, 28.4301, 28.3911]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.05561178922653198 
model_pd.l_d.mean(): 7.036823080852628e-05 
model_pd.lagr.mean(): 0.05568215623497963 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6561], device='cuda:0')), ('power', tensor([0.1579], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.05561178922653198
epoch£º391	 i:1 	 global-step:7821	 l-p:0.05568030849099159
epoch£º391	 i:2 	 global-step:7822	 l-p:0.055709075182676315
epoch£º391	 i:3 	 global-step:7823	 l-p:0.055682387202978134
epoch£º391	 i:4 	 global-step:7824	 l-p:0.05562349036335945
epoch£º391	 i:5 	 global-step:7825	 l-p:0.05576901137828827
epoch£º391	 i:6 	 global-step:7826	 l-p:0.05562968924641609
epoch£º391	 i:7 	 global-step:7827	 l-p:0.05589504912495613
epoch£º391	 i:8 	 global-step:7828	 l-p:0.05565386638045311
epoch£º391	 i:9 	 global-step:7829	 l-p:0.055886607617139816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3061, 28.8949, 28.5217],
        [28.3061, 28.3086, 28.3061],
        [28.3061, 38.7107, 46.1996],
        [28.3061, 28.3061, 28.3061]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.0557582750916481 
model_pd.l_d.mean(): 8.219161099987105e-05 
model_pd.lagr.mean(): 0.05584046617150307 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6124], device='cuda:0')), ('power', tensor([0.1470], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.0557582750916481
epoch£º392	 i:1 	 global-step:7841	 l-p:0.055833134800195694
epoch£º392	 i:2 	 global-step:7842	 l-p:0.05563030764460564
epoch£º392	 i:3 	 global-step:7843	 l-p:0.05593542009592056
epoch£º392	 i:4 	 global-step:7844	 l-p:0.05566384270787239
epoch£º392	 i:5 	 global-step:7845	 l-p:0.05565114691853523
epoch£º392	 i:6 	 global-step:7846	 l-p:0.055662937462329865
epoch£º392	 i:7 	 global-step:7847	 l-p:0.05574091896414757
epoch£º392	 i:8 	 global-step:7848	 l-p:0.055781811475753784
epoch£º392	 i:9 	 global-step:7849	 l-p:0.055717211216688156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1690, 37.2899, 43.1373],
        [28.1690, 28.1690, 28.1689],
        [28.1690, 33.8867, 35.9418],
        [28.1690, 28.1690, 28.1690]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.05569315701723099 
model_pd.l_d.mean(): 2.524482442822773e-05 
model_pd.lagr.mean(): 0.055718403309583664 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6042], device='cuda:0')), ('power', tensor([0.0408], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.05569315701723099
epoch£º393	 i:1 	 global-step:7861	 l-p:0.05583670362830162
epoch£º393	 i:2 	 global-step:7862	 l-p:0.05565185844898224
epoch£º393	 i:3 	 global-step:7863	 l-p:0.05608033016324043
epoch£º393	 i:4 	 global-step:7864	 l-p:0.05573579668998718
epoch£º393	 i:5 	 global-step:7865	 l-p:0.05571497976779938
epoch£º393	 i:6 	 global-step:7866	 l-p:0.05573727563023567
epoch£º393	 i:7 	 global-step:7867	 l-p:0.055662333965301514
epoch£º393	 i:8 	 global-step:7868	 l-p:0.055910706520080566
epoch£º393	 i:9 	 global-step:7869	 l-p:0.05569831654429436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9950, 28.6319, 28.2420],
        [27.9950, 27.9990, 27.9951],
        [27.9950, 30.0923, 29.7013],
        [27.9950, 37.6223, 44.1544]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.05571063980460167 
model_pd.l_d.mean(): -0.00012497494753915817 
model_pd.lagr.mean(): 0.05558566376566887 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6280], device='cuda:0')), ('power', tensor([-0.2076], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.05571063980460167
epoch£º394	 i:1 	 global-step:7881	 l-p:0.05575920641422272
epoch£º394	 i:2 	 global-step:7882	 l-p:0.05571605637669563
epoch£º394	 i:3 	 global-step:7883	 l-p:0.055847570300102234
epoch£º394	 i:4 	 global-step:7884	 l-p:0.05574405565857887
epoch£º394	 i:5 	 global-step:7885	 l-p:0.05582873150706291
epoch£º394	 i:6 	 global-step:7886	 l-p:0.05571504309773445
epoch£º394	 i:7 	 global-step:7887	 l-p:0.05591794475913048
epoch£º394	 i:8 	 global-step:7888	 l-p:0.05589604750275612
epoch£º394	 i:9 	 global-step:7889	 l-p:0.055990733206272125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8191, 27.8347, 27.8197],
        [27.8191, 28.7357, 28.2693],
        [27.8191, 27.8191, 27.8191],
        [27.8191, 28.4384, 28.0560]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.05579864978790283 
model_pd.l_d.mean(): -0.00015278834325727075 
model_pd.lagr.mean(): 0.055645860731601715 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5682], device='cuda:0')), ('power', tensor([-0.3078], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.05579864978790283
epoch£º395	 i:1 	 global-step:7901	 l-p:0.05582417547702789
epoch£º395	 i:2 	 global-step:7902	 l-p:0.05571359395980835
epoch£º395	 i:3 	 global-step:7903	 l-p:0.05572505667805672
epoch£º395	 i:4 	 global-step:7904	 l-p:0.05609213560819626
epoch£º395	 i:5 	 global-step:7905	 l-p:0.055904045701026917
epoch£º395	 i:6 	 global-step:7906	 l-p:0.055750101804733276
epoch£º395	 i:7 	 global-step:7907	 l-p:0.05580858141183853
epoch£º395	 i:8 	 global-step:7908	 l-p:0.05604531988501549
epoch£º395	 i:9 	 global-step:7909	 l-p:0.05583037808537483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6819, 33.2797, 35.2817],
        [27.6819, 33.8477, 36.4072],
        [27.6819, 27.9043, 27.7265],
        [27.6819, 27.8091, 27.6999]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.05573379993438721 
model_pd.l_d.mean(): -0.00018567861116025597 
model_pd.lagr.mean(): 0.05554812029004097 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6630], device='cuda:0')), ('power', tensor([-0.5977], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.05573379993438721
epoch£º396	 i:1 	 global-step:7921	 l-p:0.05585853382945061
epoch£º396	 i:2 	 global-step:7922	 l-p:0.05605069175362587
epoch£º396	 i:3 	 global-step:7923	 l-p:0.05579119548201561
epoch£º396	 i:4 	 global-step:7924	 l-p:0.05583946406841278
epoch£º396	 i:5 	 global-step:7925	 l-p:0.05581087991595268
epoch£º396	 i:6 	 global-step:7926	 l-p:0.055965449661016464
epoch£º396	 i:7 	 global-step:7927	 l-p:0.05579473078250885
epoch£º396	 i:8 	 global-step:7928	 l-p:0.055785197764635086
epoch£º396	 i:9 	 global-step:7929	 l-p:0.05610694736242294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6181, 37.1100, 43.5496],
        [27.6181, 27.6745, 27.6230],
        [27.6181, 28.4536, 28.0068],
        [27.6181, 28.3561, 27.9354]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.055943526327610016 
model_pd.l_d.mean(): -2.7080295694759116e-05 
model_pd.lagr.mean(): 0.055916447192430496 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.3356e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4910], device='cuda:0')), ('power', tensor([-0.3754], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.055943526327610016
epoch£º397	 i:1 	 global-step:7941	 l-p:0.05618125945329666
epoch£º397	 i:2 	 global-step:7942	 l-p:0.05579547584056854
epoch£º397	 i:3 	 global-step:7943	 l-p:0.05578332766890526
epoch£º397	 i:4 	 global-step:7944	 l-p:0.0558185912668705
epoch£º397	 i:5 	 global-step:7945	 l-p:0.05579892545938492
epoch£º397	 i:6 	 global-step:7946	 l-p:0.0560666061937809
epoch£º397	 i:7 	 global-step:7947	 l-p:0.05577879771590233
epoch£º397	 i:8 	 global-step:7948	 l-p:0.05576281249523163
epoch£º397	 i:9 	 global-step:7949	 l-p:0.055856093764305115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6479, 33.5215, 35.7944],
        [27.6479, 27.6480, 27.6479],
        [27.6479, 27.6634, 27.6486],
        [27.6479, 32.8676, 34.5225]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.05597180128097534 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05597180128097534 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5360], device='cuda:0')), ('power', tensor([-0.3603], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.05597180128097534
epoch£º398	 i:1 	 global-step:7961	 l-p:0.056122880429029465
epoch£º398	 i:2 	 global-step:7962	 l-p:0.055751457810401917
epoch£º398	 i:3 	 global-step:7963	 l-p:0.05576566606760025
epoch£º398	 i:4 	 global-step:7964	 l-p:0.05574869364500046
epoch£º398	 i:5 	 global-step:7965	 l-p:0.05586108937859535
epoch£º398	 i:6 	 global-step:7966	 l-p:0.05577870085835457
epoch£º398	 i:7 	 global-step:7967	 l-p:0.055812470614910126
epoch£º398	 i:8 	 global-step:7968	 l-p:0.0558529756963253
epoch£º398	 i:9 	 global-step:7969	 l-p:0.055984437465667725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7321, 27.7364, 27.7322],
        [27.7321, 28.4984, 28.0687],
        [27.7321, 32.0781, 32.9830],
        [27.7321, 30.1579, 29.8903]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.05583250895142555 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05583250895142555 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5733], device='cuda:0')), ('power', tensor([-0.3942], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.05583250895142555
epoch£º399	 i:1 	 global-step:7981	 l-p:0.05602414906024933
epoch£º399	 i:2 	 global-step:7982	 l-p:0.05585421621799469
epoch£º399	 i:3 	 global-step:7983	 l-p:0.05574572831392288
epoch£º399	 i:4 	 global-step:7984	 l-p:0.056072864681482315
epoch£º399	 i:5 	 global-step:7985	 l-p:0.05573299154639244
epoch£º399	 i:6 	 global-step:7986	 l-p:0.055786848068237305
epoch£º399	 i:7 	 global-step:7987	 l-p:0.05586228147149086
epoch£º399	 i:8 	 global-step:7988	 l-p:0.05579337105154991
epoch£º399	 i:9 	 global-step:7989	 l-p:0.05572843924164772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8365, 30.9859, 31.0637],
        [27.8365, 32.7372, 34.0889],
        [27.8365, 28.0231, 27.8700],
        [27.8365, 30.7941, 30.7668]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.05591411143541336 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05591411143541336 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5831], device='cuda:0')), ('power', tensor([-0.2490], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.05591411143541336
epoch£º400	 i:1 	 global-step:8001	 l-p:0.05572165548801422
epoch£º400	 i:2 	 global-step:8002	 l-p:0.05586083233356476
epoch£º400	 i:3 	 global-step:8003	 l-p:0.05590393766760826
epoch£º400	 i:4 	 global-step:8004	 l-p:0.056025855243206024
epoch£º400	 i:5 	 global-step:8005	 l-p:0.05574466660618782
epoch£º400	 i:6 	 global-step:8006	 l-p:0.055766962468624115
epoch£º400	 i:7 	 global-step:8007	 l-p:0.055843908339738846
epoch£º400	 i:8 	 global-step:8008	 l-p:0.0557217113673687
epoch£º400	 i:9 	 global-step:8009	 l-p:0.055750492960214615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8811, 33.2787, 35.0688],
        [27.8811, 27.8832, 27.8811],
        [27.8811, 32.7136, 34.0022],
        [27.8811, 34.5902, 37.6840]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.05585328862071037 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05585328862071037 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5188], device='cuda:0')), ('power', tensor([-0.0945], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.05585328862071037
epoch£º401	 i:1 	 global-step:8021	 l-p:0.05571861192584038
epoch£º401	 i:2 	 global-step:8022	 l-p:0.055748555809259415
epoch£º401	 i:3 	 global-step:8023	 l-p:0.055946264415979385
epoch£º401	 i:4 	 global-step:8024	 l-p:0.0557267926633358
epoch£º401	 i:5 	 global-step:8025	 l-p:0.055858783423900604
epoch£º401	 i:6 	 global-step:8026	 l-p:0.05569242686033249
epoch£º401	 i:7 	 global-step:8027	 l-p:0.056167639791965485
epoch£º401	 i:8 	 global-step:8028	 l-p:0.055715255439281464
epoch£º401	 i:9 	 global-step:8029	 l-p:0.055723704397678375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9266, 27.9266, 27.9266],
        [27.9266, 27.9311, 27.9267],
        [27.9266, 28.7108, 28.2745],
        [27.9266, 27.9266, 27.9266]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.055712636560201645 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055712636560201645 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6393], device='cuda:0')), ('power', tensor([-0.3386], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.055712636560201645
epoch£º402	 i:1 	 global-step:8041	 l-p:0.05598803237080574
epoch£º402	 i:2 	 global-step:8042	 l-p:0.0560225173830986
epoch£º402	 i:3 	 global-step:8043	 l-p:0.055829357355833054
epoch£º402	 i:4 	 global-step:8044	 l-p:0.05570721626281738
epoch£º402	 i:5 	 global-step:8045	 l-p:0.0557243674993515
epoch£º402	 i:6 	 global-step:8046	 l-p:0.05572942644357681
epoch£º402	 i:7 	 global-step:8047	 l-p:0.055732741951942444
epoch£º402	 i:8 	 global-step:8048	 l-p:0.05585414171218872
epoch£º402	 i:9 	 global-step:8049	 l-p:0.05574760213494301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9730, 28.5958, 28.2112],
        [27.9730, 27.9733, 27.9729],
        [27.9730, 28.5289, 28.1707],
        [27.9730, 28.1525, 28.0043]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.055727288126945496 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055727288126945496 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6221], device='cuda:0')), ('power', tensor([-0.2553], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.055727288126945496
epoch£º403	 i:1 	 global-step:8061	 l-p:0.05572480708360672
epoch£º403	 i:2 	 global-step:8062	 l-p:0.05587586760520935
epoch£º403	 i:3 	 global-step:8063	 l-p:0.05610453337430954
epoch£º403	 i:4 	 global-step:8064	 l-p:0.055705513805150986
epoch£º403	 i:5 	 global-step:8065	 l-p:0.05572176352143288
epoch£º403	 i:6 	 global-step:8066	 l-p:0.05577464774250984
epoch£º403	 i:7 	 global-step:8067	 l-p:0.05571410059928894
epoch£º403	 i:8 	 global-step:8068	 l-p:0.05590226128697395
epoch£º403	 i:9 	 global-step:8069	 l-p:0.05569251999258995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0194, 32.8771, 34.1725],
        [28.0194, 35.9640, 40.4133],
        [28.0194, 28.4012, 28.1262],
        [28.0194, 28.0688, 28.0233]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.05591559410095215 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05591559410095215 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.9269e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5393], device='cuda:0')), ('power', tensor([0.0385], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.05591559410095215
epoch£º404	 i:1 	 global-step:8081	 l-p:0.05582568049430847
epoch£º404	 i:2 	 global-step:8082	 l-p:0.05580725520849228
epoch£º404	 i:3 	 global-step:8083	 l-p:0.055724382400512695
epoch£º404	 i:4 	 global-step:8084	 l-p:0.05595383048057556
epoch£º404	 i:5 	 global-step:8085	 l-p:0.05568612366914749
epoch£º404	 i:6 	 global-step:8086	 l-p:0.05573078989982605
epoch£º404	 i:7 	 global-step:8087	 l-p:0.05583401024341583
epoch£º404	 i:8 	 global-step:8088	 l-p:0.055659301578998566
epoch£º404	 i:9 	 global-step:8089	 l-p:0.05570213124155998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0662, 30.9899, 30.9315],
        [28.0662, 28.0662, 28.0662],
        [28.0662, 28.0662, 28.0662],
        [28.0662, 29.8305, 29.3598]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.05571144446730614 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05571144446730614 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6054], device='cuda:0')), ('power', tensor([-0.0936], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.05571144446730614
epoch£º405	 i:1 	 global-step:8101	 l-p:0.055925071239471436
epoch£º405	 i:2 	 global-step:8102	 l-p:0.05573250725865364
epoch£º405	 i:3 	 global-step:8103	 l-p:0.05566542595624924
epoch£º405	 i:4 	 global-step:8104	 l-p:0.05597163736820221
epoch£º405	 i:5 	 global-step:8105	 l-p:0.05575275793671608
epoch£º405	 i:6 	 global-step:8106	 l-p:0.05569709837436676
epoch£º405	 i:7 	 global-step:8107	 l-p:0.05566186457872391
epoch£º405	 i:8 	 global-step:8108	 l-p:0.05567828193306923
epoch£º405	 i:9 	 global-step:8109	 l-p:0.055937811732292175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1128, 28.9114, 28.4696],
        [28.1128, 28.3375, 28.1577],
        [28.1128, 29.8590, 29.3838],
        [28.1128, 33.4248, 35.1093]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.0558052621781826 
model_pd.l_d.mean(): 1.2237211421961547e-07 
model_pd.lagr.mean(): 0.05580538511276245 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.3751e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5734], device='cuda:0')), ('power', tensor([0.0327], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.0558052621781826
epoch£º406	 i:1 	 global-step:8121	 l-p:0.05571257323026657
epoch£º406	 i:2 	 global-step:8122	 l-p:0.05583686754107475
epoch£º406	 i:3 	 global-step:8123	 l-p:0.05569583177566528
epoch£º406	 i:4 	 global-step:8124	 l-p:0.05570448189973831
epoch£º406	 i:5 	 global-step:8125	 l-p:0.055681902915239334
epoch£º406	 i:6 	 global-step:8126	 l-p:0.05572842061519623
epoch£º406	 i:7 	 global-step:8127	 l-p:0.05569459870457649
epoch£º406	 i:8 	 global-step:8128	 l-p:0.055803332477808
epoch£º406	 i:9 	 global-step:8129	 l-p:0.05596647039055824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1595, 28.2220, 28.1652],
        [28.1595, 28.2148, 28.1642],
        [28.1595, 32.6846, 33.6907],
        [28.1595, 31.2185, 31.2259]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.05566718056797981 
model_pd.l_d.mean(): -7.675091069359041e-07 
model_pd.lagr.mean(): 0.05566641315817833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.0431e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6410], device='cuda:0')), ('power', tensor([-0.0630], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.05566718056797981
epoch£º407	 i:1 	 global-step:8141	 l-p:0.05584230273962021
epoch£º407	 i:2 	 global-step:8142	 l-p:0.055951278656721115
epoch£º407	 i:3 	 global-step:8143	 l-p:0.05573892593383789
epoch£º407	 i:4 	 global-step:8144	 l-p:0.05565506964921951
epoch£º407	 i:5 	 global-step:8145	 l-p:0.055677108466625214
epoch£º407	 i:6 	 global-step:8146	 l-p:0.05580630153417587
epoch£º407	 i:7 	 global-step:8147	 l-p:0.05585629492998123
epoch£º407	 i:8 	 global-step:8148	 l-p:0.055682335048913956
epoch£º407	 i:9 	 global-step:8149	 l-p:0.055650580674409866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2048, 28.6198, 28.3267],
        [28.2048, 28.5562, 28.2977],
        [28.2048, 29.9784, 29.5052],
        [28.2048, 32.4922, 33.3075]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.05565459281206131 
model_pd.l_d.mean(): 1.5331108897953527e-06 
model_pd.lagr.mean(): 0.055656127631664276 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.0393e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6399], device='cuda:0')), ('power', tensor([0.0399], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.05565459281206131
epoch£º408	 i:1 	 global-step:8161	 l-p:0.05579373240470886
epoch£º408	 i:2 	 global-step:8162	 l-p:0.055757418274879456
epoch£º408	 i:3 	 global-step:8163	 l-p:0.055787649005651474
epoch£º408	 i:4 	 global-step:8164	 l-p:0.05568615719676018
epoch£º408	 i:5 	 global-step:8165	 l-p:0.0556483119726181
epoch£º408	 i:6 	 global-step:8166	 l-p:0.05570419505238533
epoch£º408	 i:7 	 global-step:8167	 l-p:0.055684059858322144
epoch£º408	 i:8 	 global-step:8168	 l-p:0.05582747235894203
epoch£º408	 i:9 	 global-step:8169	 l-p:0.05588755011558533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2450, 28.4919, 28.2972],
        [28.2450, 28.5232, 28.3084],
        [28.2450, 28.2612, 28.2457],
        [28.2450, 28.2564, 28.2454]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.05565616488456726 
model_pd.l_d.mean(): 8.134981044349843e-07 
model_pd.lagr.mean(): 0.055656976997852325 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.6480e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6536], device='cuda:0')), ('power', tensor([0.0095], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.05565616488456726
epoch£º409	 i:1 	 global-step:8181	 l-p:0.055621325969696045
epoch£º409	 i:2 	 global-step:8182	 l-p:0.055646996945142746
epoch£º409	 i:3 	 global-step:8183	 l-p:0.05570269003510475
epoch£º409	 i:4 	 global-step:8184	 l-p:0.055643822997808456
epoch£º409	 i:5 	 global-step:8185	 l-p:0.055951111018657684
epoch£º409	 i:6 	 global-step:8186	 l-p:0.05566435307264328
epoch£º409	 i:7 	 global-step:8187	 l-p:0.05599585920572281
epoch£º409	 i:8 	 global-step:8188	 l-p:0.055769603699445724
epoch£º409	 i:9 	 global-step:8189	 l-p:0.055697277188301086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2779, 28.7001, 28.4030],
        [28.2779, 29.2218, 28.7450],
        [28.2779, 28.3374, 28.2832],
        [28.2779, 28.2779, 28.2779]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.055655211210250854 
model_pd.l_d.mean(): 1.5539766536676325e-05 
model_pd.lagr.mean(): 0.05567074939608574 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6264], device='cuda:0')), ('power', tensor([0.1022], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.055655211210250854
epoch£º410	 i:1 	 global-step:8201	 l-p:0.05568563938140869
epoch£º410	 i:2 	 global-step:8202	 l-p:0.056084081530570984
epoch£º410	 i:3 	 global-step:8203	 l-p:0.05565942823886871
epoch£º410	 i:4 	 global-step:8204	 l-p:0.05582808703184128
epoch£º410	 i:5 	 global-step:8205	 l-p:0.05563201755285263
epoch£º410	 i:6 	 global-step:8206	 l-p:0.055845957249403
epoch£º410	 i:7 	 global-step:8207	 l-p:0.055633869022130966
epoch£º410	 i:8 	 global-step:8208	 l-p:0.05560712516307831
epoch£º410	 i:9 	 global-step:8209	 l-p:0.05565692484378815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2985, 28.2985, 28.2985],
        [28.2985, 28.3045, 28.2986],
        [28.2985, 28.2998, 28.2985],
        [28.2985, 28.3095, 28.2988]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.055665090680122375 
model_pd.l_d.mean(): 2.9062639441690408e-05 
model_pd.lagr.mean(): 0.05569415166974068 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6092], device='cuda:0')), ('power', tensor([0.1254], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.055665090680122375
epoch£º411	 i:1 	 global-step:8221	 l-p:0.05577629804611206
epoch£º411	 i:2 	 global-step:8222	 l-p:0.055622462183237076
epoch£º411	 i:3 	 global-step:8223	 l-p:0.055673614144325256
epoch£º411	 i:4 	 global-step:8224	 l-p:0.05598561465740204
epoch£º411	 i:5 	 global-step:8225	 l-p:0.05573024973273277
epoch£º411	 i:6 	 global-step:8226	 l-p:0.055744607001543045
epoch£º411	 i:7 	 global-step:8227	 l-p:0.055630460381507874
epoch£º411	 i:8 	 global-step:8228	 l-p:0.05582751706242561
epoch£º411	 i:9 	 global-step:8229	 l-p:0.05560297891497612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3028, 28.3607, 28.3079],
        [28.3028, 38.7860, 46.3815],
        [28.3028, 28.3028, 28.3028],
        [28.3028, 29.2279, 28.7546]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.05588888004422188 
model_pd.l_d.mean(): 9.075423440663144e-05 
model_pd.lagr.mean(): 0.05597963556647301 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5702], device='cuda:0')), ('power', tensor([0.2851], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.05588888004422188
epoch£º412	 i:1 	 global-step:8241	 l-p:0.055640265345573425
epoch£º412	 i:2 	 global-step:8242	 l-p:0.055677831172943115
epoch£º412	 i:3 	 global-step:8243	 l-p:0.05566827952861786
epoch£º412	 i:4 	 global-step:8244	 l-p:0.055685676634311676
epoch£º412	 i:5 	 global-step:8245	 l-p:0.05567239969968796
epoch£º412	 i:6 	 global-step:8246	 l-p:0.05562492087483406
epoch£º412	 i:7 	 global-step:8247	 l-p:0.05580456182360649
epoch£º412	 i:8 	 global-step:8248	 l-p:0.055911820381879807
epoch£º412	 i:9 	 global-step:8249	 l-p:0.055692050606012344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2894, 28.9481, 28.5486],
        [28.2894, 28.2894, 28.2894],
        [28.2894, 38.3476, 45.3780],
        [28.2894, 28.2894, 28.2894]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.05572871118783951 
model_pd.l_d.mean(): 7.003235077718273e-05 
model_pd.lagr.mean(): 0.055798742920160294 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5908], device='cuda:0')), ('power', tensor([0.1738], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.05572871118783951
epoch£º413	 i:1 	 global-step:8261	 l-p:0.0559602789580822
epoch£º413	 i:2 	 global-step:8262	 l-p:0.055640533566474915
epoch£º413	 i:3 	 global-step:8263	 l-p:0.055711887776851654
epoch£º413	 i:4 	 global-step:8264	 l-p:0.05571111664175987
epoch£º413	 i:5 	 global-step:8265	 l-p:0.055990636348724365
epoch£º413	 i:6 	 global-step:8266	 l-p:0.05561821907758713
epoch£º413	 i:7 	 global-step:8267	 l-p:0.05564026162028313
epoch£º413	 i:8 	 global-step:8268	 l-p:0.0556599423289299
epoch£º413	 i:9 	 global-step:8269	 l-p:0.05565178394317627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2579, 28.2919, 28.2600],
        [28.2579, 33.4971, 35.0995],
        [28.2579, 28.2579, 28.2579],
        [28.2579, 28.5348, 28.3208]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.05580475553870201 
model_pd.l_d.mean(): 8.982535655377433e-05 
model_pd.lagr.mean(): 0.055894579738378525 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5778], device='cuda:0')), ('power', tensor([0.1883], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.05580475553870201
epoch£º414	 i:1 	 global-step:8281	 l-p:0.05562898516654968
epoch£º414	 i:2 	 global-step:8282	 l-p:0.05569354072213173
epoch£º414	 i:3 	 global-step:8283	 l-p:0.05571233481168747
epoch£º414	 i:4 	 global-step:8284	 l-p:0.05593714118003845
epoch£º414	 i:5 	 global-step:8285	 l-p:0.055709995329380035
epoch£º414	 i:6 	 global-step:8286	 l-p:0.05564199388027191
epoch£º414	 i:7 	 global-step:8287	 l-p:0.05568390712141991
epoch£º414	 i:8 	 global-step:8288	 l-p:0.05588606745004654
epoch£º414	 i:9 	 global-step:8289	 l-p:0.05570262670516968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2090, 33.0620, 34.3334],
        [28.2090, 36.5283, 41.3852],
        [28.2090, 32.4063, 33.1528],
        [28.2090, 28.3541, 28.2311]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.055729396641254425 
model_pd.l_d.mean(): 8.408458234043792e-05 
model_pd.lagr.mean(): 0.05581348016858101 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5558], device='cuda:0')), ('power', tensor([0.1582], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.055729396641254425
epoch£º415	 i:1 	 global-step:8301	 l-p:0.055641669780015945
epoch£º415	 i:2 	 global-step:8302	 l-p:0.05584162473678589
epoch£º415	 i:3 	 global-step:8303	 l-p:0.055636268109083176
epoch£º415	 i:4 	 global-step:8304	 l-p:0.055669158697128296
epoch£º415	 i:5 	 global-step:8305	 l-p:0.05588957667350769
epoch£º415	 i:6 	 global-step:8306	 l-p:0.055729422718286514
epoch£º415	 i:7 	 global-step:8307	 l-p:0.0557413287460804
epoch£º415	 i:8 	 global-step:8308	 l-p:0.05598238855600357
epoch£º415	 i:9 	 global-step:8309	 l-p:0.05566246807575226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1475, 28.5292, 28.2539],
        [28.1475, 28.1477, 28.1475],
        [28.1475, 32.7980, 33.9071],
        [28.1475, 28.1475, 28.1475]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.05566633120179176 
model_pd.l_d.mean(): -1.406986757501727e-05 
model_pd.lagr.mean(): 0.05565226078033447 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6398], device='cuda:0')), ('power', tensor([-0.0252], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.05566633120179176
epoch£º416	 i:1 	 global-step:8321	 l-p:0.05570788308978081
epoch£º416	 i:2 	 global-step:8322	 l-p:0.055687565356492996
epoch£º416	 i:3 	 global-step:8323	 l-p:0.05565831437706947
epoch£º416	 i:4 	 global-step:8324	 l-p:0.05603805556893349
epoch£º416	 i:5 	 global-step:8325	 l-p:0.05596185475587845
epoch£º416	 i:6 	 global-step:8326	 l-p:0.05572796240448952
epoch£º416	 i:7 	 global-step:8327	 l-p:0.05578921362757683
epoch£º416	 i:8 	 global-step:8328	 l-p:0.05570966750383377
epoch£º416	 i:9 	 global-step:8329	 l-p:0.05572143569588661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0785, 28.1612, 28.0874],
        [28.0785, 28.2668, 28.1122],
        [28.0785, 28.7336, 28.3367],
        [28.0785, 28.0785, 28.0784]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.05570812523365021 
model_pd.l_d.mean(): -6.103944906499237e-05 
model_pd.lagr.mean(): 0.05564708635210991 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6098], device='cuda:0')), ('power', tensor([-0.1103], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.05570812523365021
epoch£º417	 i:1 	 global-step:8341	 l-p:0.05582312121987343
epoch£º417	 i:2 	 global-step:8342	 l-p:0.055697001516819
epoch£º417	 i:3 	 global-step:8343	 l-p:0.05570673942565918
epoch£º417	 i:4 	 global-step:8344	 l-p:0.055701617151498795
epoch£º417	 i:5 	 global-step:8345	 l-p:0.055689964443445206
epoch£º417	 i:6 	 global-step:8346	 l-p:0.056126244366168976
epoch£º417	 i:7 	 global-step:8347	 l-p:0.05571556091308594
epoch£º417	 i:8 	 global-step:8348	 l-p:0.05585925281047821
epoch£º417	 i:9 	 global-step:8349	 l-p:0.055798012763261795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0082, 28.0096, 28.0082],
        [28.0082, 28.0090, 28.0081],
        [28.0082, 33.9181, 36.1792],
        [28.0082, 29.2467, 28.7400]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.0558718666434288 
model_pd.l_d.mean(): -2.6747502488433383e-05 
model_pd.lagr.mean(): 0.055845119059085846 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5851], device='cuda:0')), ('power', tensor([-0.0521], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.0558718666434288
epoch£º418	 i:1 	 global-step:8361	 l-p:0.055672358721494675
epoch£º418	 i:2 	 global-step:8362	 l-p:0.05593050271272659
epoch£º418	 i:3 	 global-step:8363	 l-p:0.05593671277165413
epoch£º418	 i:4 	 global-step:8364	 l-p:0.05571073293685913
epoch£º418	 i:5 	 global-step:8365	 l-p:0.05582893267273903
epoch£º418	 i:6 	 global-step:8366	 l-p:0.05576244369149208
epoch£º418	 i:7 	 global-step:8367	 l-p:0.05576251447200775
epoch£º418	 i:8 	 global-step:8368	 l-p:0.05578465387225151
epoch£º418	 i:9 	 global-step:8369	 l-p:0.05571603775024414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9440, 29.1712, 28.6661],
        [27.9440, 32.0461, 32.7450],
        [27.9440, 30.9783, 30.9855],
        [27.9440, 28.0322, 27.9539]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.05574801191687584 
model_pd.l_d.mean(): -7.324368925765157e-05 
model_pd.lagr.mean(): 0.05567476898431778 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5981], device='cuda:0')), ('power', tensor([-0.1666], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.05574801191687584
epoch£º419	 i:1 	 global-step:8381	 l-p:0.05600278824567795
epoch£º419	 i:2 	 global-step:8382	 l-p:0.055689532309770584
epoch£º419	 i:3 	 global-step:8383	 l-p:0.05588165670633316
epoch£º419	 i:4 	 global-step:8384	 l-p:0.05572826415300369
epoch£º419	 i:5 	 global-step:8385	 l-p:0.05587053298950195
epoch£º419	 i:6 	 global-step:8386	 l-p:0.05579005926847458
epoch£º419	 i:7 	 global-step:8387	 l-p:0.055756062269210815
epoch£º419	 i:8 	 global-step:8388	 l-p:0.05584233999252319
epoch£º419	 i:9 	 global-step:8389	 l-p:0.05580206587910652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8918, 33.2915, 35.0824],
        [27.8918, 28.9272, 28.4401],
        [27.8918, 29.6421, 29.1740],
        [27.8918, 27.8958, 27.8919]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.05588323622941971 
model_pd.l_d.mean(): -7.155424100346863e-05 
model_pd.lagr.mean(): 0.05581168085336685 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5715], device='cuda:0')), ('power', tensor([-0.2126], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.05588323622941971
epoch£º420	 i:1 	 global-step:8401	 l-p:0.05572435259819031
epoch£º420	 i:2 	 global-step:8402	 l-p:0.05598187446594238
epoch£º420	 i:3 	 global-step:8403	 l-p:0.05584600940346718
epoch£º420	 i:4 	 global-step:8404	 l-p:0.05568571761250496
epoch£º420	 i:5 	 global-step:8405	 l-p:0.055847253650426865
epoch£º420	 i:6 	 global-step:8406	 l-p:0.05575818940997124
epoch£º420	 i:7 	 global-step:8407	 l-p:0.05598988011479378
epoch£º420	 i:8 	 global-step:8408	 l-p:0.055729225277900696
epoch£º420	 i:9 	 global-step:8409	 l-p:0.055764708667993546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8596, 29.8607, 29.4481],
        [27.8596, 36.9730, 42.8753],
        [27.8596, 28.3089, 27.9995],
        [27.8596, 27.9182, 27.8648]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.05576886981725693 
model_pd.l_d.mean(): -6.892187957419083e-05 
model_pd.lagr.mean(): 0.055699948221445084 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5870], device='cuda:0')), ('power', tensor([-0.3255], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.05576886981725693
epoch£º421	 i:1 	 global-step:8421	 l-p:0.05588145926594734
epoch£º421	 i:2 	 global-step:8422	 l-p:0.0557873398065567
epoch£º421	 i:3 	 global-step:8423	 l-p:0.05595405772328377
epoch£º421	 i:4 	 global-step:8424	 l-p:0.05579255893826485
epoch£º421	 i:5 	 global-step:8425	 l-p:0.05590575560927391
epoch£º421	 i:6 	 global-step:8426	 l-p:0.05576136335730553
epoch£º421	 i:7 	 global-step:8427	 l-p:0.05572288855910301
epoch£º421	 i:8 	 global-step:8428	 l-p:0.05594918504357338
epoch£º421	 i:9 	 global-step:8429	 l-p:0.05573533475399017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8524, 28.0078, 27.8772],
        [27.8524, 27.8524, 27.8524],
        [27.8524, 27.8585, 27.8525],
        [27.8524, 31.6134, 32.0757]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.05577094107866287 
model_pd.l_d.mean(): -2.3254655388882384e-05 
model_pd.lagr.mean(): 0.05574768781661987 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.1210e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5829], device='cuda:0')), ('power', tensor([-0.3043], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.05577094107866287
epoch£º422	 i:1 	 global-step:8441	 l-p:0.055713504552841187
epoch£º422	 i:2 	 global-step:8442	 l-p:0.05585883930325508
epoch£º422	 i:3 	 global-step:8443	 l-p:0.05579076334834099
epoch£º422	 i:4 	 global-step:8444	 l-p:0.05603792145848274
epoch£º422	 i:5 	 global-step:8445	 l-p:0.055749986320734024
epoch£º422	 i:6 	 global-step:8446	 l-p:0.055894993245601654
epoch£º422	 i:7 	 global-step:8447	 l-p:0.05574195832014084
epoch£º422	 i:8 	 global-step:8448	 l-p:0.05585657060146332
epoch£º422	 i:9 	 global-step:8449	 l-p:0.055831458419561386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8733, 27.8736, 27.8733],
        [27.8733, 32.2702, 33.2017],
        [27.8733, 28.0610, 27.9070],
        [27.8733, 30.7868, 30.7344]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.05578713119029999 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05578713119029999 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5923], device='cuda:0')), ('power', tensor([-0.1924], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.05578713119029999
epoch£º423	 i:1 	 global-step:8461	 l-p:0.05571846663951874
epoch£º423	 i:2 	 global-step:8462	 l-p:0.055768754333257675
epoch£º423	 i:3 	 global-step:8463	 l-p:0.05572791025042534
epoch£º423	 i:4 	 global-step:8464	 l-p:0.05600406602025032
epoch£º423	 i:5 	 global-step:8465	 l-p:0.05605817213654518
epoch£º423	 i:6 	 global-step:8466	 l-p:0.05574113875627518
epoch£º423	 i:7 	 global-step:8467	 l-p:0.05574280023574829
epoch£º423	 i:8 	 global-step:8468	 l-p:0.05581368878483772
epoch£º423	 i:9 	 global-step:8469	 l-p:0.05581406503915787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9138, 29.0696, 28.5694],
        [27.9138, 27.9342, 27.9148],
        [27.9138, 34.2159, 36.8816],
        [27.9138, 27.9138, 27.9138]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.05576348304748535 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576348304748535 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5829], device='cuda:0')), ('power', tensor([-0.1683], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.05576348304748535
epoch£º424	 i:1 	 global-step:8481	 l-p:0.05600235238671303
epoch£º424	 i:2 	 global-step:8482	 l-p:0.05571427196264267
epoch£º424	 i:3 	 global-step:8483	 l-p:0.05584142729640007
epoch£º424	 i:4 	 global-step:8484	 l-p:0.05574086681008339
epoch£º424	 i:5 	 global-step:8485	 l-p:0.05574599653482437
epoch£º424	 i:6 	 global-step:8486	 l-p:0.05569829046726227
epoch£º424	 i:7 	 global-step:8487	 l-p:0.0557437390089035
epoch£º424	 i:8 	 global-step:8488	 l-p:0.055885642766952515
epoch£º424	 i:9 	 global-step:8489	 l-p:0.05593936890363693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9617, 28.9383, 28.4593],
        [27.9617, 30.0591, 29.6694],
        [27.9617, 33.3757, 35.1713],
        [27.9617, 27.9617, 27.9617]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.05593506619334221 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05593506619334221 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5921], device='cuda:0')), ('power', tensor([-0.0952], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.05593506619334221
epoch£º425	 i:1 	 global-step:8501	 l-p:0.05574234947562218
epoch£º425	 i:2 	 global-step:8502	 l-p:0.05572078004479408
epoch£º425	 i:3 	 global-step:8503	 l-p:0.05572861433029175
epoch£º425	 i:4 	 global-step:8504	 l-p:0.055781010538339615
epoch£º425	 i:5 	 global-step:8505	 l-p:0.055729541927576065
epoch£º425	 i:6 	 global-step:8506	 l-p:0.05585229769349098
epoch£º425	 i:7 	 global-step:8507	 l-p:0.0557917095720768
epoch£º425	 i:8 	 global-step:8508	 l-p:0.055816859006881714
epoch£º425	 i:9 	 global-step:8509	 l-p:0.05586595460772514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0124, 28.0936, 28.0211],
        [28.0124, 28.0183, 28.0125],
        [28.0124, 28.0353, 28.0135],
        [28.0124, 35.1393, 38.6637]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.05596648156642914 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05596648156642914 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.8837e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5392], device='cuda:0')), ('power', tensor([0.0577], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.05596648156642914
epoch£º426	 i:1 	 global-step:8521	 l-p:0.055692870169878006
epoch£º426	 i:2 	 global-step:8522	 l-p:0.055722687393426895
epoch£º426	 i:3 	 global-step:8523	 l-p:0.0556846559047699
epoch£º426	 i:4 	 global-step:8524	 l-p:0.05595741793513298
epoch£º426	 i:5 	 global-step:8525	 l-p:0.05578715726733208
epoch£º426	 i:6 	 global-step:8526	 l-p:0.0557282492518425
epoch£º426	 i:7 	 global-step:8527	 l-p:0.05586537346243858
epoch£º426	 i:8 	 global-step:8528	 l-p:0.05570134148001671
epoch£º426	 i:9 	 global-step:8529	 l-p:0.05574328452348709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0642, 28.1193, 28.0689],
        [28.0642, 28.0879, 28.0655],
        [28.0642, 28.7190, 28.3223],
        [28.0642, 31.6143, 31.9196]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.05584722012281418 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05584722012281418 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6083], device='cuda:0')), ('power', tensor([-0.0887], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.05584722012281418
epoch£º427	 i:1 	 global-step:8541	 l-p:0.05572056770324707
epoch£º427	 i:2 	 global-step:8542	 l-p:0.05572736635804176
epoch£º427	 i:3 	 global-step:8543	 l-p:0.05597427114844322
epoch£º427	 i:4 	 global-step:8544	 l-p:0.05589020252227783
epoch£º427	 i:5 	 global-step:8545	 l-p:0.05572905391454697
epoch£º427	 i:6 	 global-step:8546	 l-p:0.0556725412607193
epoch£º427	 i:7 	 global-step:8547	 l-p:0.05567227303981781
epoch£º427	 i:8 	 global-step:8548	 l-p:0.05577586963772774
epoch£º427	 i:9 	 global-step:8549	 l-p:0.05572384595870972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[28.1165, 36.3121, 41.0392],
        [28.1165, 30.2233, 29.8306],
        [28.1165, 35.2345, 38.7326],
        [28.1165, 30.5778, 30.3065]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.05568113550543785 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568113550543785 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6309], device='cuda:0')), ('power', tensor([-0.0827], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.05568113550543785
epoch£º428	 i:1 	 global-step:8561	 l-p:0.055752865970134735
epoch£º428	 i:2 	 global-step:8562	 l-p:0.05565972998738289
epoch£º428	 i:3 	 global-step:8563	 l-p:0.05588945373892784
epoch£º428	 i:4 	 global-step:8564	 l-p:0.055752284824848175
epoch£º428	 i:5 	 global-step:8565	 l-p:0.055725593119859695
epoch£º428	 i:6 	 global-step:8566	 l-p:0.05579933151602745
epoch£º428	 i:7 	 global-step:8567	 l-p:0.05566258355975151
epoch£º428	 i:8 	 global-step:8568	 l-p:0.055780112743377686
epoch£º428	 i:9 	 global-step:8569	 l-p:0.05591297522187233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1686, 31.0915, 31.0267],
        [28.1686, 28.1693, 28.1686],
        [28.1686, 34.0785, 36.3186],
        [28.1686, 38.1216, 45.0409]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.055806197226047516 
model_pd.l_d.mean(): 2.581095941422973e-06 
model_pd.lagr.mean(): 0.05580877885222435 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.3898e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5263], device='cuda:0')), ('power', tensor([0.1649], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.055806197226047516
epoch£º429	 i:1 	 global-step:8581	 l-p:0.056009694933891296
epoch£º429	 i:2 	 global-step:8582	 l-p:0.05577855929732323
epoch£º429	 i:3 	 global-step:8583	 l-p:0.05562765896320343
epoch£º429	 i:4 	 global-step:8584	 l-p:0.05570873245596886
epoch£º429	 i:5 	 global-step:8585	 l-p:0.055637702345848083
epoch£º429	 i:6 	 global-step:8586	 l-p:0.055687468498945236
epoch£º429	 i:7 	 global-step:8587	 l-p:0.055895354598760605
epoch£º429	 i:8 	 global-step:8588	 l-p:0.055667705833911896
epoch£º429	 i:9 	 global-step:8589	 l-p:0.05568375438451767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2183, 28.5702, 28.3113],
        [28.2183, 38.1569, 45.0458],
        [28.2183, 29.1488, 28.6753],
        [28.2183, 28.2296, 28.2187]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.055796366184949875 
model_pd.l_d.mean(): 1.085421990865143e-05 
model_pd.lagr.mean(): 0.05580722168087959 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.8862e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5181], device='cuda:0')), ('power', tensor([0.2289], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.055796366184949875
epoch£º430	 i:1 	 global-step:8601	 l-p:0.0557449534535408
epoch£º430	 i:2 	 global-step:8602	 l-p:0.05563492700457573
epoch£º430	 i:3 	 global-step:8603	 l-p:0.05566117540001869
epoch£º430	 i:4 	 global-step:8604	 l-p:0.0556594654917717
epoch£º430	 i:5 	 global-step:8605	 l-p:0.05570468306541443
epoch£º430	 i:6 	 global-step:8606	 l-p:0.055877361446619034
epoch£º430	 i:7 	 global-step:8607	 l-p:0.05586740002036095
epoch£º430	 i:8 	 global-step:8608	 l-p:0.05566706508398056
epoch£º430	 i:9 	 global-step:8609	 l-p:0.05578486621379852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2615, 33.5535, 35.2028],
        [28.2615, 38.3031, 45.3179],
        [28.2615, 28.3765, 28.2767],
        [28.2615, 30.9629, 30.7863]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.0557059571146965 
model_pd.l_d.mean(): 1.3607411347038578e-05 
model_pd.lagr.mean(): 0.055719565600156784 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5892], device='cuda:0')), ('power', tensor([0.1329], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.0557059571146965
epoch£º431	 i:1 	 global-step:8621	 l-p:0.05583871155977249
epoch£º431	 i:2 	 global-step:8622	 l-p:0.055745210498571396
epoch£º431	 i:3 	 global-step:8623	 l-p:0.055669743567705154
epoch£º431	 i:4 	 global-step:8624	 l-p:0.05563855916261673
epoch£º431	 i:5 	 global-step:8625	 l-p:0.055681340396404266
epoch£º431	 i:6 	 global-step:8626	 l-p:0.05565660446882248
epoch£º431	 i:7 	 global-step:8627	 l-p:0.05604281276464462
epoch£º431	 i:8 	 global-step:8628	 l-p:0.055671218782663345
epoch£º431	 i:9 	 global-step:8629	 l-p:0.055662259459495544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2944, 37.8162, 44.1464],
        [28.2944, 28.2949, 28.2944],
        [28.2944, 28.4762, 28.3261],
        [28.2944, 28.6668, 28.3962]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.055690452456474304 
model_pd.l_d.mean(): 3.047214704565704e-05 
model_pd.lagr.mean(): 0.05572092533111572 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5999], device='cuda:0')), ('power', tensor([0.1724], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.055690452456474304
epoch£º432	 i:1 	 global-step:8641	 l-p:0.055605776607990265
epoch£º432	 i:2 	 global-step:8642	 l-p:0.05570199713110924
epoch£º432	 i:3 	 global-step:8643	 l-p:0.05578684061765671
epoch£º432	 i:4 	 global-step:8644	 l-p:0.055786971002817154
epoch£º432	 i:5 	 global-step:8645	 l-p:0.0557214617729187
epoch£º432	 i:6 	 global-step:8646	 l-p:0.05564208701252937
epoch£º432	 i:7 	 global-step:8647	 l-p:0.055750176310539246
epoch£º432	 i:8 	 global-step:8648	 l-p:0.05564306676387787
epoch£º432	 i:9 	 global-step:8649	 l-p:0.05592574179172516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3120, 30.3493, 29.9304],
        [28.3120, 34.2850, 36.5681],
        [28.3120, 28.3119, 28.3119],
        [28.3120, 28.5895, 28.3750]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.055694155395030975 
model_pd.l_d.mean(): 5.2136390877421945e-05 
model_pd.lagr.mean(): 0.05574629083275795 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5958], device='cuda:0')), ('power', tensor([0.1973], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.055694155395030975
epoch£º433	 i:1 	 global-step:8661	 l-p:0.05564755201339722
epoch£º433	 i:2 	 global-step:8662	 l-p:0.055666904896497726
epoch£º433	 i:3 	 global-step:8663	 l-p:0.05563843995332718
epoch£º433	 i:4 	 global-step:8664	 l-p:0.055688951164484024
epoch£º433	 i:5 	 global-step:8665	 l-p:0.055821437388658524
epoch£º433	 i:6 	 global-step:8666	 l-p:0.05596655234694481
epoch£º433	 i:7 	 global-step:8667	 l-p:0.055842313915491104
epoch£º433	 i:8 	 global-step:8668	 l-p:0.0556645393371582
epoch£º433	 i:9 	 global-step:8669	 l-p:0.05560329928994179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[28.3106, 31.4731, 31.5279],
        [28.3106, 34.7673, 37.5351],
        [28.3106, 29.6718, 29.1576],
        [28.3106, 30.3000, 29.8689]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.05560757592320442 
model_pd.l_d.mean(): 3.123746864730492e-05 
model_pd.lagr.mean(): 0.055638812482357025 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6793], device='cuda:0')), ('power', tensor([0.0877], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.05560757592320442
epoch£º434	 i:1 	 global-step:8681	 l-p:0.05562661588191986
epoch£º434	 i:2 	 global-step:8682	 l-p:0.0558948777616024
epoch£º434	 i:3 	 global-step:8683	 l-p:0.0557086281478405
epoch£º434	 i:4 	 global-step:8684	 l-p:0.05564170330762863
epoch£º434	 i:5 	 global-step:8685	 l-p:0.055776480585336685
epoch£º434	 i:6 	 global-step:8686	 l-p:0.05584125220775604
epoch£º434	 i:7 	 global-step:8687	 l-p:0.055713605135679245
epoch£º434	 i:8 	 global-step:8688	 l-p:0.05579926818609238
epoch£º434	 i:9 	 global-step:8689	 l-p:0.05564826354384422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2878, 28.2891, 28.2878],
        [28.2878, 28.2878, 28.2878],
        [28.2878, 28.2882, 28.2878],
        [28.2878, 37.6338, 43.7409]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.05562498793005943 
model_pd.l_d.mean(): 1.2360834261926357e-05 
model_pd.lagr.mean(): 0.05563734844326973 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6613], device='cuda:0')), ('power', tensor([0.0279], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.05562498793005943
epoch£º435	 i:1 	 global-step:8701	 l-p:0.055648073554039
epoch£º435	 i:2 	 global-step:8702	 l-p:0.05571921169757843
epoch£º435	 i:3 	 global-step:8703	 l-p:0.05568161979317665
epoch£º435	 i:4 	 global-step:8704	 l-p:0.055723756551742554
epoch£º435	 i:5 	 global-step:8705	 l-p:0.05576128885149956
epoch£º435	 i:6 	 global-step:8706	 l-p:0.055824652314186096
epoch£º435	 i:7 	 global-step:8707	 l-p:0.05567052215337753
epoch£º435	 i:8 	 global-step:8708	 l-p:0.055672015994787216
epoch£º435	 i:9 	 global-step:8709	 l-p:0.05600406602025032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2442, 37.4167, 43.3135],
        [28.2442, 35.4329, 38.9881],
        [28.2442, 28.3161, 28.2513],
        [28.2442, 33.2203, 34.5932]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.0556601844727993 
model_pd.l_d.mean(): 6.238053174456581e-05 
model_pd.lagr.mean(): 0.055722564458847046 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6165], device='cuda:0')), ('power', tensor([0.1215], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.0556601844727993
epoch£º436	 i:1 	 global-step:8721	 l-p:0.05564184859395027
epoch£º436	 i:2 	 global-step:8722	 l-p:0.05576426908373833
epoch£º436	 i:3 	 global-step:8723	 l-p:0.05589687079191208
epoch£º436	 i:4 	 global-step:8724	 l-p:0.05574485659599304
epoch£º436	 i:5 	 global-step:8725	 l-p:0.05569036677479744
epoch£º436	 i:6 	 global-step:8726	 l-p:0.05573083460330963
epoch£º436	 i:7 	 global-step:8727	 l-p:0.055654510855674744
epoch£º436	 i:8 	 global-step:8728	 l-p:0.055870167911052704
epoch£º436	 i:9 	 global-step:8729	 l-p:0.05579013749957085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1828, 28.2168, 28.1849],
        [28.1828, 38.6195, 46.1812],
        [28.1828, 34.1692, 36.4822],
        [28.1828, 34.4657, 37.0742]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.055891409516334534 
model_pd.l_d.mean(): 5.325601523509249e-05 
model_pd.lagr.mean(): 0.055944666266441345 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5947], device='cuda:0')), ('power', tensor([0.0954], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.055891409516334534
epoch£º437	 i:1 	 global-step:8741	 l-p:0.05573340877890587
epoch£º437	 i:2 	 global-step:8742	 l-p:0.05572988837957382
epoch£º437	 i:3 	 global-step:8743	 l-p:0.05571630969643593
epoch£º437	 i:4 	 global-step:8744	 l-p:0.05567694455385208
epoch£º437	 i:5 	 global-step:8745	 l-p:0.055964548140764236
epoch£º437	 i:6 	 global-step:8746	 l-p:0.05564456433057785
epoch£º437	 i:7 	 global-step:8747	 l-p:0.05566024035215378
epoch£º437	 i:8 	 global-step:8748	 l-p:0.05577964708209038
epoch£º437	 i:9 	 global-step:8749	 l-p:0.05579686909914017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1078, 30.0828, 29.6552],
        [28.1078, 32.3796, 33.1919],
        [28.1078, 28.3832, 28.1703],
        [28.1078, 28.1077, 28.1078]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.05592162907123566 
model_pd.l_d.mean(): 5.125614279677393e-06 
model_pd.lagr.mean(): 0.05592675507068634 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5926], device='cuda:0')), ('power', tensor([0.0090], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.05592162907123566
epoch£º438	 i:1 	 global-step:8761	 l-p:0.05570476874709129
epoch£º438	 i:2 	 global-step:8762	 l-p:0.055887944996356964
epoch£º438	 i:3 	 global-step:8763	 l-p:0.055756375193595886
epoch£º438	 i:4 	 global-step:8764	 l-p:0.0557369701564312
epoch£º438	 i:5 	 global-step:8765	 l-p:0.05567903444170952
epoch£º438	 i:6 	 global-step:8766	 l-p:0.055736243724823
epoch£º438	 i:7 	 global-step:8767	 l-p:0.0557023361325264
epoch£º438	 i:8 	 global-step:8768	 l-p:0.055926136672496796
epoch£º438	 i:9 	 global-step:8769	 l-p:0.05571706220507622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0267, 30.8611, 30.7600],
        [28.0267, 31.2566, 31.3682],
        [28.0267, 31.3212, 31.4709],
        [28.0267, 28.0286, 28.0267]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.055678412318229675 
model_pd.l_d.mean(): -0.00012463463644962758 
model_pd.lagr.mean(): 0.055553779006004333 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6512], device='cuda:0')), ('power', tensor([-0.2300], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.055678412318229675
epoch£º439	 i:1 	 global-step:8781	 l-p:0.05601433664560318
epoch£º439	 i:2 	 global-step:8782	 l-p:0.055677708238363266
epoch£º439	 i:3 	 global-step:8783	 l-p:0.055760517716407776
epoch£º439	 i:4 	 global-step:8784	 l-p:0.05576084926724434
epoch£º439	 i:5 	 global-step:8785	 l-p:0.05574661120772362
epoch£º439	 i:6 	 global-step:8786	 l-p:0.055737752467393875
epoch£º439	 i:7 	 global-step:8787	 l-p:0.055833909660577774
epoch£º439	 i:8 	 global-step:8788	 l-p:0.05603249371051788
epoch£º439	 i:9 	 global-step:8789	 l-p:0.055707063525915146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9492, 28.2229, 28.0114],
        [27.9492, 28.1599, 27.9898],
        [27.9492, 29.3553, 28.8494],
        [27.9492, 33.6120, 35.6423]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.05595456808805466 
model_pd.l_d.mean(): -5.634866829495877e-05 
model_pd.lagr.mean(): 0.05589821934700012 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5713], device='cuda:0')), ('power', tensor([-0.1188], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.05595456808805466
epoch£º440	 i:1 	 global-step:8801	 l-p:0.05593836307525635
epoch£º440	 i:2 	 global-step:8802	 l-p:0.05584157258272171
epoch£º440	 i:3 	 global-step:8803	 l-p:0.05582840368151665
epoch£º440	 i:4 	 global-step:8804	 l-p:0.055689867585897446
epoch£º440	 i:5 	 global-step:8805	 l-p:0.055741507560014725
epoch£º440	 i:6 	 global-step:8806	 l-p:0.05581032484769821
epoch£º440	 i:7 	 global-step:8807	 l-p:0.055821046233177185
epoch£º440	 i:8 	 global-step:8808	 l-p:0.05569533631205559
epoch£º440	 i:9 	 global-step:8809	 l-p:0.05579018592834473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8845, 27.9554, 27.8915],
        [27.8845, 33.5265, 35.5451],
        [27.8845, 27.8912, 27.8847],
        [27.8845, 30.3836, 30.1384]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.055939681828022 
model_pd.l_d.mean(): -4.694413291872479e-05 
model_pd.lagr.mean(): 0.055892739444971085 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5292], device='cuda:0')), ('power', tensor([-0.1265], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.055939681828022
epoch£º441	 i:1 	 global-step:8821	 l-p:0.05593366548418999
epoch£º441	 i:2 	 global-step:8822	 l-p:0.05577588826417923
epoch£º441	 i:3 	 global-step:8823	 l-p:0.05581694841384888
epoch£º441	 i:4 	 global-step:8824	 l-p:0.05575830489397049
epoch£º441	 i:5 	 global-step:8825	 l-p:0.05571356043219566
epoch£º441	 i:6 	 global-step:8826	 l-p:0.055720433592796326
epoch£º441	 i:7 	 global-step:8827	 l-p:0.055810168385505676
epoch£º441	 i:8 	 global-step:8828	 l-p:0.05571959540247917
epoch£º441	 i:9 	 global-step:8829	 l-p:0.056051481515169144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8401, 27.8529, 27.8406],
        [27.8401, 35.4532, 39.5510],
        [27.8401, 31.9796, 32.7156],
        [27.8401, 28.0397, 27.8774]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.05578639358282089 
model_pd.l_d.mean(): -8.169205830199644e-05 
model_pd.lagr.mean(): 0.05570470169186592 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6099], device='cuda:0')), ('power', tensor([-0.3407], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.05578639358282089
epoch£º442	 i:1 	 global-step:8841	 l-p:0.055701036006212234
epoch£º442	 i:2 	 global-step:8842	 l-p:0.055754370987415314
epoch£º442	 i:3 	 global-step:8843	 l-p:0.05591779202222824
epoch£º442	 i:4 	 global-step:8844	 l-p:0.055753257125616074
epoch£º442	 i:5 	 global-step:8845	 l-p:0.05579061061143875
epoch£º442	 i:6 	 global-step:8846	 l-p:0.05590175837278366
epoch£º442	 i:7 	 global-step:8847	 l-p:0.05614296719431877
epoch£º442	 i:8 	 global-step:8848	 l-p:0.05576448142528534
epoch£º442	 i:9 	 global-step:8849	 l-p:0.05579986795783043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8256, 28.2527, 27.9545],
        [27.8256, 27.8314, 27.8257],
        [27.8256, 27.8338, 27.8258],
        [27.8256, 28.4451, 28.0626]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.055834826081991196 
model_pd.l_d.mean(): -3.122651469311677e-05 
model_pd.lagr.mean(): 0.05580360069870949 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.6008e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6002], device='cuda:0')), ('power', tensor([-0.3364], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.055834826081991196
epoch£º443	 i:1 	 global-step:8861	 l-p:0.05574293062090874
epoch£º443	 i:2 	 global-step:8862	 l-p:0.05586927384138107
epoch£º443	 i:3 	 global-step:8863	 l-p:0.05599275603890419
epoch£º443	 i:4 	 global-step:8864	 l-p:0.0557563453912735
epoch£º443	 i:5 	 global-step:8865	 l-p:0.05574877932667732
epoch£º443	 i:6 	 global-step:8866	 l-p:0.05575558543205261
epoch£º443	 i:7 	 global-step:8867	 l-p:0.05578353628516197
epoch£º443	 i:8 	 global-step:8868	 l-p:0.055944498628377914
epoch£º443	 i:9 	 global-step:8869	 l-p:0.055881816893815994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8449, 33.3450, 35.2349],
        [27.8449, 28.4467, 28.0708],
        [27.8449, 29.6756, 29.2241],
        [27.8449, 32.7470, 34.0992]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.056026000529527664 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056026000529527664 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5334], device='cuda:0')), ('power', tensor([-0.1311], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:0.056026000529527664
epoch£º444	 i:1 	 global-step:8881	 l-p:0.05572175234556198
epoch£º444	 i:2 	 global-step:8882	 l-p:0.05571955442428589
epoch£º444	 i:3 	 global-step:8883	 l-p:0.05574082210659981
epoch£º444	 i:4 	 global-step:8884	 l-p:0.055885620415210724
epoch£º444	 i:5 	 global-step:8885	 l-p:0.055747564882040024
epoch£º444	 i:6 	 global-step:8886	 l-p:0.055790975689888
epoch£º444	 i:7 	 global-step:8887	 l-p:0.05588265880942345
epoch£º444	 i:8 	 global-step:8888	 l-p:0.05592796579003334
epoch£º444	 i:9 	 global-step:8889	 l-p:0.05579537898302078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8883, 27.8916, 27.8883],
        [27.8883, 30.2319, 29.9262],
        [27.8883, 27.9810, 27.8991],
        [27.8883, 29.0139, 28.5166]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.05572659894824028 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05572659894824028 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6229], device='cuda:0')), ('power', tensor([-0.2972], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.05572659894824028
epoch£º445	 i:1 	 global-step:8901	 l-p:0.05578763037919998
epoch£º445	 i:2 	 global-step:8902	 l-p:0.055922459810972214
epoch£º445	 i:3 	 global-step:8903	 l-p:0.05595701187849045
epoch£º445	 i:4 	 global-step:8904	 l-p:0.05571721866726875
epoch£º445	 i:5 	 global-step:8905	 l-p:0.055735666304826736
epoch£º445	 i:6 	 global-step:8906	 l-p:0.05597012862563133
epoch£º445	 i:7 	 global-step:8907	 l-p:0.055714454501867294
epoch£º445	 i:8 	 global-step:8908	 l-p:0.05575425922870636
epoch£º445	 i:9 	 global-step:8909	 l-p:0.05584339797496796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[27.9409, 30.2774, 29.9667],
        [27.9409, 36.2481, 41.1411],
        [27.9409, 31.1604, 31.2716],
        [27.9409, 36.4446, 41.5746]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.055764827877283096 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055764827877283096 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5861], device='cuda:0')), ('power', tensor([-0.1830], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.055764827877283096
epoch£º446	 i:1 	 global-step:8921	 l-p:0.05583789572119713
epoch£º446	 i:2 	 global-step:8922	 l-p:0.05576084926724434
epoch£º446	 i:3 	 global-step:8923	 l-p:0.05572035536170006
epoch£º446	 i:4 	 global-step:8924	 l-p:0.05595611408352852
epoch£º446	 i:5 	 global-step:8925	 l-p:0.05577678978443146
epoch£º446	 i:6 	 global-step:8926	 l-p:0.055671803653240204
epoch£º446	 i:7 	 global-step:8927	 l-p:0.05609200522303581
epoch£º446	 i:8 	 global-step:8928	 l-p:0.05572229251265526
epoch£º446	 i:9 	 global-step:8929	 l-p:0.05570293217897415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9969, 28.0383, 27.9999],
        [27.9969, 28.4224, 28.1245],
        [27.9969, 30.6716, 30.4966],
        [27.9969, 30.7434, 30.6016]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.05573093146085739 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05573093146085739 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6049], device='cuda:0')), ('power', tensor([-0.1525], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.05573093146085739
epoch£º447	 i:1 	 global-step:8941	 l-p:0.055815115571022034
epoch£º447	 i:2 	 global-step:8942	 l-p:0.055866751819849014
epoch£º447	 i:3 	 global-step:8943	 l-p:0.055692996829748154
epoch£º447	 i:4 	 global-step:8944	 l-p:0.0557171106338501
epoch£º447	 i:5 	 global-step:8945	 l-p:0.055997446179389954
epoch£º447	 i:6 	 global-step:8946	 l-p:0.05570920929312706
epoch£º447	 i:7 	 global-step:8947	 l-p:0.05580318346619606
epoch£º447	 i:8 	 global-step:8948	 l-p:0.0558551624417305
epoch£º447	 i:9 	 global-step:8949	 l-p:0.05569058284163475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0542, 37.3195, 43.3736],
        [28.0542, 35.9322, 40.2978],
        [28.0542, 28.3484, 28.1239],
        [28.0542, 28.0548, 28.0542]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.05579141527414322 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05579141527414322 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5669], device='cuda:0')), ('power', tensor([-0.0223], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.05579141527414322
epoch£º448	 i:1 	 global-step:8961	 l-p:0.05578196793794632
epoch£º448	 i:2 	 global-step:8962	 l-p:0.055918268859386444
epoch£º448	 i:3 	 global-step:8963	 l-p:0.055799681693315506
epoch£º448	 i:4 	 global-step:8964	 l-p:0.05584889277815819
epoch£º448	 i:5 	 global-step:8965	 l-p:0.05575652793049812
epoch£º448	 i:6 	 global-step:8966	 l-p:0.05565890297293663
epoch£º448	 i:7 	 global-step:8967	 l-p:0.05582587420940399
epoch£º448	 i:8 	 global-step:8968	 l-p:0.05571189150214195
epoch£º448	 i:9 	 global-step:8969	 l-p:0.05565660446882248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1123, 28.1294, 28.1130],
        [28.1123, 29.5790, 29.0723],
        [28.1123, 28.5469, 28.2440],
        [28.1123, 28.6781, 28.3152]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.05566449835896492 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05566449835896492 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6443], device='cuda:0')), ('power', tensor([-0.0871], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.05566449835896492
epoch£º449	 i:1 	 global-step:8981	 l-p:0.055666856467723846
epoch£º449	 i:2 	 global-step:8982	 l-p:0.05565585941076279
epoch£º449	 i:3 	 global-step:8983	 l-p:0.05574385076761246
epoch£º449	 i:4 	 global-step:8984	 l-p:0.05590677261352539
epoch£º449	 i:5 	 global-step:8985	 l-p:0.055717919021844864
epoch£º449	 i:6 	 global-step:8986	 l-p:0.05587862804532051
epoch£º449	 i:7 	 global-step:8987	 l-p:0.055930640548467636
epoch£º449	 i:8 	 global-step:8988	 l-p:0.05577118694782257
epoch£º449	 i:9 	 global-step:8989	 l-p:0.055683355778455734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1700, 28.1733, 28.1700],
        [28.1700, 28.1712, 28.1700],
        [28.1700, 28.1958, 28.1714],
        [28.1700, 34.9519, 38.0797]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.05566013231873512 
model_pd.l_d.mean(): -6.983617595324176e-08 
model_pd.lagr.mean(): 0.05566006153821945 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.8835e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6423], device='cuda:0')), ('power', tensor([-0.0037], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.05566013231873512
epoch£º450	 i:1 	 global-step:9001	 l-p:0.055650241672992706
epoch£º450	 i:2 	 global-step:9002	 l-p:0.05580670386552811
epoch£º450	 i:3 	 global-step:9003	 l-p:0.05585698038339615
epoch£º450	 i:4 	 global-step:9004	 l-p:0.05567774549126625
epoch£º450	 i:5 	 global-step:9005	 l-p:0.055784568190574646
epoch£º450	 i:6 	 global-step:9006	 l-p:0.0559549480676651
epoch£º450	 i:7 	 global-step:9007	 l-p:0.05562435835599899
epoch£º450	 i:8 	 global-step:9008	 l-p:0.05570363998413086
epoch£º450	 i:9 	 global-step:9009	 l-p:0.05577455461025238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2251, 28.4470, 28.2690],
        [28.2251, 28.8838, 28.4847],
        [28.2251, 28.2257, 28.2250],
        [28.2251, 28.2270, 28.2251]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.055925171822309494 
model_pd.l_d.mean(): 1.483232972532278e-05 
model_pd.lagr.mean(): 0.05594000592827797 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.6692e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5192], device='cuda:0')), ('power', tensor([0.2820], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.055925171822309494
epoch£º451	 i:1 	 global-step:9021	 l-p:0.05568339303135872
epoch£º451	 i:2 	 global-step:9022	 l-p:0.05592051520943642
epoch£º451	 i:3 	 global-step:9023	 l-p:0.05563770979642868
epoch£º451	 i:4 	 global-step:9024	 l-p:0.05564945191144943
epoch£º451	 i:5 	 global-step:9025	 l-p:0.055686309933662415
epoch£º451	 i:6 	 global-step:9026	 l-p:0.055875346064567566
epoch£º451	 i:7 	 global-step:9027	 l-p:0.055657465010881424
epoch£º451	 i:8 	 global-step:9028	 l-p:0.05562629550695419
epoch£º451	 i:9 	 global-step:9029	 l-p:0.05571766570210457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2723, 29.1510, 28.6880],
        [28.2723, 28.2833, 28.2727],
        [28.2723, 28.2948, 28.2734],
        [28.2723, 28.2723, 28.2723]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.05570412427186966 
model_pd.l_d.mean(): 1.747738861013204e-05 
model_pd.lagr.mean(): 0.05572160333395004 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5914], device='cuda:0')), ('power', tensor([0.1560], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.05570412427186966
epoch£º452	 i:1 	 global-step:9041	 l-p:0.05563049018383026
epoch£º452	 i:2 	 global-step:9042	 l-p:0.055875785648822784
epoch£º452	 i:3 	 global-step:9043	 l-p:0.05561612918972969
epoch£º452	 i:4 	 global-step:9044	 l-p:0.05578269064426422
epoch£º452	 i:5 	 global-step:9045	 l-p:0.05569063872098923
epoch£º452	 i:6 	 global-step:9046	 l-p:0.05605475232005119
epoch£º452	 i:7 	 global-step:9047	 l-p:0.05563695728778839
epoch£º452	 i:8 	 global-step:9048	 l-p:0.055629387497901917
epoch£º452	 i:9 	 global-step:9049	 l-p:0.05566653609275818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3061, 28.3174, 28.3065],
        [28.3061, 28.3061, 28.3061],
        [28.3061, 28.3233, 28.3068],
        [28.3061, 28.7207, 28.4275]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.055627934634685516 
model_pd.l_d.mean(): 2.0801846403628588e-05 
model_pd.lagr.mean(): 0.05564873665571213 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6512], device='cuda:0')), ('power', tensor([0.1083], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.055627934634685516
epoch£º453	 i:1 	 global-step:9061	 l-p:0.05588145926594734
epoch£º453	 i:2 	 global-step:9062	 l-p:0.05582306161522865
epoch£º453	 i:3 	 global-step:9063	 l-p:0.05591161549091339
epoch£º453	 i:4 	 global-step:9064	 l-p:0.055637385696172714
epoch£º453	 i:5 	 global-step:9065	 l-p:0.05566343292593956
epoch£º453	 i:6 	 global-step:9066	 l-p:0.05572542920708656
epoch£º453	 i:7 	 global-step:9067	 l-p:0.05575144663453102
epoch£º453	 i:8 	 global-step:9068	 l-p:0.05560129135847092
epoch£º453	 i:9 	 global-step:9069	 l-p:0.05560749024152756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3214, 29.4788, 28.9722],
        [28.3214, 33.8085, 35.6287],
        [28.3214, 28.3445, 28.3226],
        [28.3214, 28.4303, 28.3353]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.05566779151558876 
model_pd.l_d.mean(): 2.605203917482868e-05 
model_pd.lagr.mean(): 0.055693842470645905 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6341], device='cuda:0')), ('power', tensor([0.0914], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.05566779151558876
epoch£º454	 i:1 	 global-step:9081	 l-p:0.05577806010842323
epoch£º454	 i:2 	 global-step:9082	 l-p:0.055776529014110565
epoch£º454	 i:3 	 global-step:9083	 l-p:0.05562656372785568
epoch£º454	 i:4 	 global-step:9084	 l-p:0.05585553124547005
epoch£º454	 i:5 	 global-step:9085	 l-p:0.055682223290205
epoch£º454	 i:6 	 global-step:9086	 l-p:0.05566803738474846
epoch£º454	 i:7 	 global-step:9087	 l-p:0.05580510199069977
epoch£º454	 i:8 	 global-step:9088	 l-p:0.05567857623100281
epoch£º454	 i:9 	 global-step:9089	 l-p:0.0556807778775692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3139, 30.7242, 30.4244],
        [28.3139, 28.3178, 28.3139],
        [28.3139, 28.3152, 28.3139],
        [28.3139, 28.3138, 28.3138]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.05568254366517067 
model_pd.l_d.mean(): 6.883158494019881e-05 
model_pd.lagr.mean(): 0.05575137585401535 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6054], device='cuda:0')), ('power', tensor([0.1810], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.05568254366517067
epoch£º455	 i:1 	 global-step:9101	 l-p:0.056024834513664246
epoch£º455	 i:2 	 global-step:9102	 l-p:0.05570712313055992
epoch£º455	 i:3 	 global-step:9103	 l-p:0.0556282214820385
epoch£º455	 i:4 	 global-step:9104	 l-p:0.0556558258831501
epoch£º455	 i:5 	 global-step:9105	 l-p:0.055661484599113464
epoch£º455	 i:6 	 global-step:9106	 l-p:0.055612046271562576
epoch£º455	 i:7 	 global-step:9107	 l-p:0.055682744830846786
epoch£º455	 i:8 	 global-step:9108	 l-p:0.055918268859386444
epoch£º455	 i:9 	 global-step:9109	 l-p:0.05568617582321167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2820, 28.2820, 28.2820],
        [28.2820, 29.1459, 28.6861],
        [28.2820, 28.2860, 28.2820],
        [28.2820, 28.3045, 28.2831]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.05567346140742302 
model_pd.l_d.mean(): 4.026270471513271e-05 
model_pd.lagr.mean(): 0.055713724344968796 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6226], device='cuda:0')), ('power', tensor([0.0863], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.05567346140742302
epoch£º456	 i:1 	 global-step:9121	 l-p:0.055862147361040115
epoch£º456	 i:2 	 global-step:9122	 l-p:0.05583428218960762
epoch£º456	 i:3 	 global-step:9123	 l-p:0.05583780258893967
epoch£º456	 i:4 	 global-step:9124	 l-p:0.055751606822013855
epoch£º456	 i:5 	 global-step:9125	 l-p:0.05563840642571449
epoch£º456	 i:6 	 global-step:9126	 l-p:0.05576632544398308
epoch£º456	 i:7 	 global-step:9127	 l-p:0.055641401559114456
epoch£º456	 i:8 	 global-step:9128	 l-p:0.055697228759527206
epoch£º456	 i:9 	 global-step:9129	 l-p:0.05564955249428749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2267, 34.4713, 37.0352],
        [28.2267, 29.2131, 28.7293],
        [28.2267, 32.5466, 33.3848],
        [28.2267, 28.2267, 28.2267]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.05563066899776459 
model_pd.l_d.mean(): 7.628017101524165e-06 
model_pd.lagr.mean(): 0.05563829839229584 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6657], device='cuda:0')), ('power', tensor([0.0143], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.05563066899776459
epoch£º457	 i:1 	 global-step:9141	 l-p:0.05581001192331314
epoch£º457	 i:2 	 global-step:9142	 l-p:0.055693335831165314
epoch£º457	 i:3 	 global-step:9143	 l-p:0.05567895621061325
epoch£º457	 i:4 	 global-step:9144	 l-p:0.05568931996822357
epoch£º457	 i:5 	 global-step:9145	 l-p:0.055803630501031876
epoch£º457	 i:6 	 global-step:9146	 l-p:0.055862583220005035
epoch£º457	 i:7 	 global-step:9147	 l-p:0.055703725665807724
epoch£º457	 i:8 	 global-step:9148	 l-p:0.055636920034885406
epoch£º457	 i:9 	 global-step:9149	 l-p:0.055987969040870667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1517, 28.1522, 28.1517],
        [28.1517, 28.6224, 28.3017],
        [28.1517, 28.1517, 28.1517],
        [28.1517, 28.1524, 28.1517]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.055657051503658295 
model_pd.l_d.mean(): -3.2790372642921284e-05 
model_pd.lagr.mean(): 0.05562426149845123 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6442], device='cuda:0')), ('power', tensor([-0.0580], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.055657051503658295
epoch£º458	 i:1 	 global-step:9161	 l-p:0.05579184740781784
epoch£º458	 i:2 	 global-step:9162	 l-p:0.0557546503841877
epoch£º458	 i:3 	 global-step:9163	 l-p:0.05565028637647629
epoch£º458	 i:4 	 global-step:9164	 l-p:0.05577214062213898
epoch£º458	 i:5 	 global-step:9165	 l-p:0.055751733481884
epoch£º458	 i:6 	 global-step:9166	 l-p:0.055691707879304886
epoch£º458	 i:7 	 global-step:9167	 l-p:0.05595025792717934
epoch£º458	 i:8 	 global-step:9168	 l-p:0.055722083896398544
epoch£º458	 i:9 	 global-step:9169	 l-p:0.055934250354766846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0663, 28.0663, 28.0663],
        [28.0663, 33.5187, 35.3375],
        [28.0663, 38.3390, 45.7076],
        [28.0663, 28.0665, 28.0663]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.055762872099876404 
model_pd.l_d.mean(): -6.597060564672574e-05 
model_pd.lagr.mean(): 0.05569690093398094 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6272], device='cuda:0')), ('power', tensor([-0.1182], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.055762872099876404
epoch£º459	 i:1 	 global-step:9181	 l-p:0.0556839182972908
epoch£º459	 i:2 	 global-step:9182	 l-p:0.05569777265191078
epoch£º459	 i:3 	 global-step:9183	 l-p:0.05579456314444542
epoch£º459	 i:4 	 global-step:9184	 l-p:0.055925607681274414
epoch£º459	 i:5 	 global-step:9185	 l-p:0.05596069246530533
epoch£º459	 i:6 	 global-step:9186	 l-p:0.05568363890051842
epoch£º459	 i:7 	 global-step:9187	 l-p:0.05584248527884483
epoch£º459	 i:8 	 global-step:9188	 l-p:0.05573836341500282
epoch£º459	 i:9 	 global-step:9189	 l-p:0.055778682231903076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9798, 28.0610, 27.9885],
        [27.9798, 30.3316, 30.0248],
        [27.9798, 37.8307, 44.6585],
        [27.9798, 32.3944, 33.3298]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.056013867259025574 
model_pd.l_d.mean(): 1.4280041796155274e-05 
model_pd.lagr.mean(): 0.05602814629673958 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5158], device='cuda:0')), ('power', tensor([0.0281], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.056013867259025574
epoch£º460	 i:1 	 global-step:9201	 l-p:0.05570570006966591
epoch£º460	 i:2 	 global-step:9202	 l-p:0.05570388585329056
epoch£º460	 i:3 	 global-step:9203	 l-p:0.05574558675289154
epoch£º460	 i:4 	 global-step:9204	 l-p:0.05589715763926506
epoch£º460	 i:5 	 global-step:9205	 l-p:0.05568477138876915
epoch£º460	 i:6 	 global-step:9206	 l-p:0.05591681972146034
epoch£º460	 i:7 	 global-step:9207	 l-p:0.05579935386776924
epoch£º460	 i:8 	 global-step:9208	 l-p:0.055841684341430664
epoch£º460	 i:9 	 global-step:9209	 l-p:0.05574744939804077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9016, 27.9173, 27.9023],
        [27.9016, 30.1281, 29.7809],
        [27.9016, 28.3963, 28.0653],
        [27.9016, 33.1228, 34.7498]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.0560835637152195 
model_pd.l_d.mean(): -2.2330847059492953e-05 
model_pd.lagr.mean(): 0.05606123432517052 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5174], device='cuda:0')), ('power', tensor([-0.0535], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.0560835637152195
epoch£º461	 i:1 	 global-step:9221	 l-p:0.05576607584953308
epoch£º461	 i:2 	 global-step:9222	 l-p:0.05570623651146889
epoch£º461	 i:3 	 global-step:9223	 l-p:0.05587311461567879
epoch£º461	 i:4 	 global-step:9224	 l-p:0.05589866638183594
epoch£º461	 i:5 	 global-step:9225	 l-p:0.05579090863466263
epoch£º461	 i:6 	 global-step:9226	 l-p:0.05572907626628876
epoch£º461	 i:7 	 global-step:9227	 l-p:0.055723417550325394
epoch£º461	 i:8 	 global-step:9228	 l-p:0.0558045469224453
epoch£º461	 i:9 	 global-step:9229	 l-p:0.05583896115422249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8430, 31.2129, 31.4208],
        [27.8430, 30.6099, 30.4861],
        [27.8430, 28.2732, 27.9734],
        [27.8430, 28.2661, 27.9699]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.05575544759631157 
model_pd.l_d.mean(): -9.171698184218258e-05 
model_pd.lagr.mean(): 0.05566373094916344 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6067], device='cuda:0')), ('power', tensor([-0.3144], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.05575544759631157
epoch£º462	 i:1 	 global-step:9241	 l-p:0.05590110644698143
epoch£º462	 i:2 	 global-step:9242	 l-p:0.05577175319194794
epoch£º462	 i:3 	 global-step:9243	 l-p:0.05576573312282562
epoch£º462	 i:4 	 global-step:9244	 l-p:0.05577994883060455
epoch£º462	 i:5 	 global-step:9245	 l-p:0.05587773770093918
epoch£º462	 i:6 	 global-step:9246	 l-p:0.05601523816585541
epoch£º462	 i:7 	 global-step:9247	 l-p:0.05586124211549759
epoch£º462	 i:8 	 global-step:9248	 l-p:0.05578956380486488
epoch£º462	 i:9 	 global-step:9249	 l-p:0.05580291524529457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8139, 27.8259, 27.8143],
        [27.8139, 27.9064, 27.8247],
        [27.8139, 27.8958, 27.8228],
        [27.8139, 27.8150, 27.8139]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.05611955374479294 
model_pd.l_d.mean(): -1.0082793778565247e-05 
model_pd.lagr.mean(): 0.056109469383955 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4714], device='cuda:0')), ('power', tensor([-0.0705], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.05611955374479294
epoch£º463	 i:1 	 global-step:9261	 l-p:0.055855296552181244
epoch£º463	 i:2 	 global-step:9262	 l-p:0.055842746049165726
epoch£º463	 i:3 	 global-step:9263	 l-p:0.05572180077433586
epoch£º463	 i:4 	 global-step:9264	 l-p:0.05574706941843033
epoch£º463	 i:5 	 global-step:9265	 l-p:0.05595940351486206
epoch£º463	 i:6 	 global-step:9266	 l-p:0.0558282732963562
epoch£º463	 i:7 	 global-step:9267	 l-p:0.05574636161327362
epoch£º463	 i:8 	 global-step:9268	 l-p:0.0557689405977726
epoch£º463	 i:9 	 global-step:9269	 l-p:0.055761002004146576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8207, 27.8495, 27.8224],
        [27.8207, 32.7787, 34.1818],
        [27.8207, 33.9116, 36.3766],
        [27.8207, 27.8207, 27.8207]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.05574443191289902 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574443191289902 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6160], device='cuda:0')), ('power', tensor([-0.3794], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.05574443191289902
epoch£º464	 i:1 	 global-step:9281	 l-p:0.05576474219560623
epoch£º464	 i:2 	 global-step:9282	 l-p:0.0557883195579052
epoch£º464	 i:3 	 global-step:9283	 l-p:0.05577502027153969
epoch£º464	 i:4 	 global-step:9284	 l-p:0.055723633617162704
epoch£º464	 i:5 	 global-step:9285	 l-p:0.05598932132124901
epoch£º464	 i:6 	 global-step:9286	 l-p:0.055856771767139435
epoch£º464	 i:7 	 global-step:9287	 l-p:0.05576956644654274
epoch£º464	 i:8 	 global-step:9288	 l-p:0.055742233991622925
epoch£º464	 i:9 	 global-step:9289	 l-p:0.05614408478140831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8605, 27.8605, 27.8605],
        [27.8605, 27.8717, 27.8609],
        [27.8605, 28.0472, 27.8939],
        [27.8605, 29.7243, 29.2791]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.055784158408641815 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055784158408641815 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5707], device='cuda:0')), ('power', tensor([-0.2422], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.055784158408641815
epoch£º465	 i:1 	 global-step:9301	 l-p:0.05578262358903885
epoch£º465	 i:2 	 global-step:9302	 l-p:0.05570540204644203
epoch£º465	 i:3 	 global-step:9303	 l-p:0.05578893423080444
epoch£º465	 i:4 	 global-step:9304	 l-p:0.05569535493850708
epoch£º465	 i:5 	 global-step:9305	 l-p:0.055781926959753036
epoch£º465	 i:6 	 global-step:9306	 l-p:0.05573856830596924
epoch£º465	 i:7 	 global-step:9307	 l-p:0.05594867095351219
epoch£º465	 i:8 	 global-step:9308	 l-p:0.055800072848796844
epoch£º465	 i:9 	 global-step:9309	 l-p:0.05616286024451256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9156, 29.2928, 28.7867],
        [27.9156, 29.7833, 29.3373],
        [27.9156, 28.0515, 27.9356],
        [27.9156, 31.9579, 32.6150]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.056017063558101654 
model_pd.l_d.mean(): -1.4099960310431925e-07 
model_pd.lagr.mean(): 0.05601692199707031 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5454], device='cuda:0')), ('power', tensor([-0.0600], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.056017063558101654
epoch£º466	 i:1 	 global-step:9321	 l-p:0.05582958459854126
epoch£º466	 i:2 	 global-step:9322	 l-p:0.05597921833395958
epoch£º466	 i:3 	 global-step:9323	 l-p:0.05569075047969818
epoch£º466	 i:4 	 global-step:9324	 l-p:0.05572152137756348
epoch£º466	 i:5 	 global-step:9325	 l-p:0.055768147110939026
epoch£º466	 i:6 	 global-step:9326	 l-p:0.05568811297416687
epoch£º466	 i:7 	 global-step:9327	 l-p:0.055679406970739365
epoch£º466	 i:8 	 global-step:9328	 l-p:0.05588921532034874
epoch£º466	 i:9 	 global-step:9329	 l-p:0.05579521134495735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9770, 27.9775, 27.9771],
        [27.9770, 33.5736, 35.5378],
        [27.9770, 28.1944, 28.0197],
        [27.9770, 29.6275, 29.1417]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.05574414134025574 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574414134025574 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6037], device='cuda:0')), ('power', tensor([-0.1776], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.05574414134025574
epoch£º467	 i:1 	 global-step:9341	 l-p:0.0558282844722271
epoch£º467	 i:2 	 global-step:9342	 l-p:0.05583375692367554
epoch£º467	 i:3 	 global-step:9343	 l-p:0.05586822330951691
epoch£º467	 i:4 	 global-step:9344	 l-p:0.05591948330402374
epoch£º467	 i:5 	 global-step:9345	 l-p:0.05596132576465607
epoch£º467	 i:6 	 global-step:9346	 l-p:0.05566779896616936
epoch£º467	 i:7 	 global-step:9347	 l-p:0.055682167410850525
epoch£º467	 i:8 	 global-step:9348	 l-p:0.055716123431921005
epoch£º467	 i:9 	 global-step:9349	 l-p:0.05569654703140259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0402, 28.3895, 28.1325],
        [28.0402, 33.7142, 35.7437],
        [28.0402, 28.6927, 28.2970],
        [28.0402, 37.7079, 44.2824]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.05569895729422569 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05569895729422569 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6272], device='cuda:0')), ('power', tensor([-0.1223], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.05569895729422569
epoch£º468	 i:1 	 global-step:9361	 l-p:0.055877216160297394
epoch£º468	 i:2 	 global-step:9362	 l-p:0.05586542934179306
epoch£º468	 i:3 	 global-step:9363	 l-p:0.05592666566371918
epoch£º468	 i:4 	 global-step:9364	 l-p:0.05575638636946678
epoch£º468	 i:5 	 global-step:9365	 l-p:0.05573231726884842
epoch£º468	 i:6 	 global-step:9366	 l-p:0.0556531623005867
epoch£º468	 i:7 	 global-step:9367	 l-p:0.055839527398347855
epoch£º468	 i:8 	 global-step:9368	 l-p:0.0556936152279377
epoch£º468	 i:9 	 global-step:9369	 l-p:0.05573203042149544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1040, 28.5739, 28.2537],
        [28.1040, 28.1040, 28.1040],
        [28.1040, 28.1270, 28.1052],
        [28.1040, 28.1040, 28.1040]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.0557013601064682 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0557013601064682 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6188], device='cuda:0')), ('power', tensor([-0.1419], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.0557013601064682
epoch£º469	 i:1 	 global-step:9381	 l-p:0.055674657225608826
epoch£º469	 i:2 	 global-step:9382	 l-p:0.05584626644849777
epoch£º469	 i:3 	 global-step:9383	 l-p:0.05572487786412239
epoch£º469	 i:4 	 global-step:9384	 l-p:0.05569218471646309
epoch£º469	 i:5 	 global-step:9385	 l-p:0.05580705404281616
epoch£º469	 i:6 	 global-step:9386	 l-p:0.05583905056118965
epoch£º469	 i:7 	 global-step:9387	 l-p:0.05569620430469513
epoch£º469	 i:8 	 global-step:9388	 l-p:0.055916327983140945
epoch£º469	 i:9 	 global-step:9389	 l-p:0.05573393777012825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1679, 28.1698, 28.1679],
        [28.1679, 29.6229, 29.1145],
        [28.1679, 28.7953, 28.4079],
        [28.1679, 28.1691, 28.1679]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.05567443370819092 
model_pd.l_d.mean(): -6.860578309897392e-07 
model_pd.lagr.mean(): 0.055673748254776 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.4065e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6309], device='cuda:0')), ('power', tensor([-0.0601], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.05567443370819092
epoch£º470	 i:1 	 global-step:9401	 l-p:0.055640265345573425
epoch£º470	 i:2 	 global-step:9402	 l-p:0.05577365681529045
epoch£º470	 i:3 	 global-step:9403	 l-p:0.055779263377189636
epoch£º470	 i:4 	 global-step:9404	 l-p:0.05572066083550453
epoch£º470	 i:5 	 global-step:9405	 l-p:0.0559537410736084
epoch£º470	 i:6 	 global-step:9406	 l-p:0.05564655363559723
epoch£º470	 i:7 	 global-step:9407	 l-p:0.05589734762907028
epoch£º470	 i:8 	 global-step:9408	 l-p:0.05567045882344246
epoch£º470	 i:9 	 global-step:9409	 l-p:0.05573482811450958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2304, 28.2304, 28.2304],
        [28.2304, 28.2304, 28.2304],
        [28.2304, 28.2304, 28.2304],
        [28.2304, 28.2304, 28.2304]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.05571674183011055 
model_pd.l_d.mean(): 5.2107570809312165e-06 
model_pd.lagr.mean(): 0.0557219535112381 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.1270e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5826], device='cuda:0')), ('power', tensor([0.1144], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.05571674183011055
epoch£º471	 i:1 	 global-step:9421	 l-p:0.05565442144870758
epoch£º471	 i:2 	 global-step:9422	 l-p:0.055716343224048615
epoch£º471	 i:3 	 global-step:9423	 l-p:0.055613573640584946
epoch£º471	 i:4 	 global-step:9424	 l-p:0.05592377483844757
epoch£º471	 i:5 	 global-step:9425	 l-p:0.055708449333906174
epoch£º471	 i:6 	 global-step:9426	 l-p:0.05590616166591644
epoch£º471	 i:7 	 global-step:9427	 l-p:0.05564286932349205
epoch£º471	 i:8 	 global-step:9428	 l-p:0.055717602372169495
epoch£º471	 i:9 	 global-step:9429	 l-p:0.05575951933860779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2852, 31.1472, 31.0453],
        [28.2852, 28.4752, 28.3193],
        [28.2852, 35.8602, 39.8386],
        [28.2852, 28.2852, 28.2852]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.055643193423748016 
model_pd.l_d.mean(): 9.02461488294648e-06 
model_pd.lagr.mean(): 0.05565221980214119 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6448], device='cuda:0')), ('power', tensor([0.0826], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.055643193423748016
epoch£º472	 i:1 	 global-step:9441	 l-p:0.05581103265285492
epoch£º472	 i:2 	 global-step:9442	 l-p:0.0556778721511364
epoch£º472	 i:3 	 global-step:9443	 l-p:0.05569826066493988
epoch£º472	 i:4 	 global-step:9444	 l-p:0.05571668595075607
epoch£º472	 i:5 	 global-step:9445	 l-p:0.05569720268249512
epoch£º472	 i:6 	 global-step:9446	 l-p:0.05587757006287575
epoch£º472	 i:7 	 global-step:9447	 l-p:0.05562590807676315
epoch£º472	 i:8 	 global-step:9448	 l-p:0.05584138631820679
epoch£º472	 i:9 	 global-step:9449	 l-p:0.0556635744869709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3246, 30.3630, 29.9438],
        [28.3246, 28.3248, 28.3246],
        [28.3246, 28.3249, 28.3246],
        [28.3246, 28.5259, 28.3620]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.05573474243283272 
model_pd.l_d.mean(): 6.64328908897005e-05 
model_pd.lagr.mean(): 0.05580117553472519 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5522], device='cuda:0')), ('power', tensor([0.3371], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.05573474243283272
epoch£º473	 i:1 	 global-step:9461	 l-p:0.05589093267917633
epoch£º473	 i:2 	 global-step:9462	 l-p:0.05561218038201332
epoch£º473	 i:3 	 global-step:9463	 l-p:0.05564923956990242
epoch£º473	 i:4 	 global-step:9464	 l-p:0.0557560957968235
epoch£º473	 i:5 	 global-step:9465	 l-p:0.055917251855134964
epoch£º473	 i:6 	 global-step:9466	 l-p:0.05571872740983963
epoch£º473	 i:7 	 global-step:9467	 l-p:0.055657923221588135
epoch£º473	 i:8 	 global-step:9468	 l-p:0.055612362921237946
epoch£º473	 i:9 	 global-step:9469	 l-p:0.055638689547777176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3407, 28.3562, 28.3414],
        [28.3407, 28.3407, 28.3407],
        [28.3407, 28.3407, 28.3407],
        [28.3407, 29.0622, 28.6412]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.055673059076070786 
model_pd.l_d.mean(): 6.469006621045992e-05 
model_pd.lagr.mean(): 0.05573774874210358 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6090], device='cuda:0')), ('power', tensor([0.2159], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.055673059076070786
epoch£º474	 i:1 	 global-step:9481	 l-p:0.055821493268013
epoch£º474	 i:2 	 global-step:9482	 l-p:0.05568663403391838
epoch£º474	 i:3 	 global-step:9483	 l-p:0.05568140000104904
epoch£º474	 i:4 	 global-step:9484	 l-p:0.055862411856651306
epoch£º474	 i:5 	 global-step:9485	 l-p:0.05574996769428253
epoch£º474	 i:6 	 global-step:9486	 l-p:0.05575725808739662
epoch£º474	 i:7 	 global-step:9487	 l-p:0.05563430115580559
epoch£º474	 i:8 	 global-step:9488	 l-p:0.0556144192814827
epoch£º474	 i:9 	 global-step:9489	 l-p:0.05569932982325554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3284, 29.3186, 28.8330],
        [28.3284, 29.8070, 29.2963],
        [28.3284, 28.3284, 28.3284],
        [28.3284, 28.3284, 28.3284]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.05569698289036751 
model_pd.l_d.mean(): 9.86656523309648e-05 
model_pd.lagr.mean(): 0.05579564720392227 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5904], device='cuda:0')), ('power', tensor([0.2444], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.05569698289036751
epoch£º475	 i:1 	 global-step:9501	 l-p:0.05577409639954567
epoch£º475	 i:2 	 global-step:9502	 l-p:0.05571826919913292
epoch£º475	 i:3 	 global-step:9503	 l-p:0.055890653282403946
epoch£º475	 i:4 	 global-step:9504	 l-p:0.05583430081605911
epoch£º475	 i:5 	 global-step:9505	 l-p:0.05562420189380646
epoch£º475	 i:6 	 global-step:9506	 l-p:0.055607184767723083
epoch£º475	 i:7 	 global-step:9507	 l-p:0.055681221187114716
epoch£º475	 i:8 	 global-step:9508	 l-p:0.05567970126867294
epoch£º475	 i:9 	 global-step:9509	 l-p:0.05572963505983353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2863, 36.0273, 40.1943],
        [28.2863, 28.2864, 28.2863],
        [28.2863, 35.4862, 39.0470],
        [28.2863, 28.8558, 28.4905]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.05565816909074783 
model_pd.l_d.mean(): 4.465838355827145e-05 
model_pd.lagr.mean(): 0.055702827870845795 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6271], device='cuda:0')), ('power', tensor([0.0902], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.05565816909074783
epoch£º476	 i:1 	 global-step:9521	 l-p:0.055864304304122925
epoch£º476	 i:2 	 global-step:9522	 l-p:0.0556609220802784
epoch£º476	 i:3 	 global-step:9523	 l-p:0.055612631142139435
epoch£º476	 i:4 	 global-step:9524	 l-p:0.05581861361861229
epoch£º476	 i:5 	 global-step:9525	 l-p:0.055655039846897125
epoch£º476	 i:6 	 global-step:9526	 l-p:0.055859316140413284
epoch£º476	 i:7 	 global-step:9527	 l-p:0.05565197765827179
epoch£º476	 i:8 	 global-step:9528	 l-p:0.0556345209479332
epoch£º476	 i:9 	 global-step:9529	 l-p:0.05594232305884361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2168, 28.2236, 28.2170],
        [28.2168, 33.5494, 35.2406],
        [28.2168, 28.6851, 28.3653],
        [28.2168, 30.2024, 29.7735]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.05586036667227745 
model_pd.l_d.mean(): 9.38548255362548e-05 
model_pd.lagr.mean(): 0.0559542216360569 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5617], device='cuda:0')), ('power', tensor([0.1677], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.05586036667227745
epoch£º477	 i:1 	 global-step:9541	 l-p:0.05564730614423752
epoch£º477	 i:2 	 global-step:9542	 l-p:0.05579250678420067
epoch£º477	 i:3 	 global-step:9543	 l-p:0.05562443286180496
epoch£º477	 i:4 	 global-step:9544	 l-p:0.05583951249718666
epoch£º477	 i:5 	 global-step:9545	 l-p:0.05565706640481949
epoch£º477	 i:6 	 global-step:9546	 l-p:0.05600888654589653
epoch£º477	 i:7 	 global-step:9547	 l-p:0.05566668510437012
epoch£º477	 i:8 	 global-step:9548	 l-p:0.05576854571700096
epoch£º477	 i:9 	 global-step:9549	 l-p:0.05566677823662758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1272, 28.1272, 28.1272],
        [28.1272, 36.4924, 41.4197],
        [28.1272, 28.1272, 28.1272],
        [28.1272, 31.1826, 31.1899]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.055987339466810226 
model_pd.l_d.mean(): 0.00011033434566343203 
model_pd.lagr.mean(): 0.05609767511487007 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5295], device='cuda:0')), ('power', tensor([0.1888], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.055987339466810226
epoch£º478	 i:1 	 global-step:9561	 l-p:0.055767256766557693
epoch£º478	 i:2 	 global-step:9562	 l-p:0.05570885166525841
epoch£º478	 i:3 	 global-step:9563	 l-p:0.055832453072071075
epoch£º478	 i:4 	 global-step:9564	 l-p:0.055691927671432495
epoch£º478	 i:5 	 global-step:9565	 l-p:0.055870890617370605
epoch£º478	 i:6 	 global-step:9566	 l-p:0.05573423206806183
epoch£º478	 i:7 	 global-step:9567	 l-p:0.05570756644010544
epoch£º478	 i:8 	 global-step:9568	 l-p:0.05571998655796051
epoch£º478	 i:9 	 global-step:9569	 l-p:0.055723026394844055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0261, 36.2889, 41.1126],
        [28.0261, 28.1625, 28.0461],
        [28.0261, 31.0587, 31.0600],
        [28.0261, 28.8576, 28.4081]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.05579141154885292 
model_pd.l_d.mean(): -4.119581717532128e-05 
model_pd.lagr.mean(): 0.05575021728873253 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5616], device='cuda:0')), ('power', tensor([-0.0733], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.05579141154885292
epoch£º479	 i:1 	 global-step:9581	 l-p:0.0557279996573925
epoch£º479	 i:2 	 global-step:9582	 l-p:0.05572477728128433
epoch£º479	 i:3 	 global-step:9583	 l-p:0.05571819096803665
epoch£º479	 i:4 	 global-step:9584	 l-p:0.05568589270114899
epoch£º479	 i:5 	 global-step:9585	 l-p:0.05598686262965202
epoch£º479	 i:6 	 global-step:9586	 l-p:0.05585838109254837
epoch£º479	 i:7 	 global-step:9587	 l-p:0.05597306787967682
epoch£º479	 i:8 	 global-step:9588	 l-p:0.055749960243701935
epoch£º479	 i:9 	 global-step:9589	 l-p:0.05575771629810333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9265, 27.9269, 27.9265],
        [27.9265, 29.3208, 28.8150],
        [27.9265, 27.9269, 27.9265],
        [27.9265, 28.5976, 28.1961]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.05597385764122009 
model_pd.l_d.mean(): -5.9632166085066274e-05 
model_pd.lagr.mean(): 0.05591422691941261 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5774], device='cuda:0')), ('power', tensor([-0.1218], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.05597385764122009
epoch£º480	 i:1 	 global-step:9601	 l-p:0.05576352775096893
epoch£º480	 i:2 	 global-step:9602	 l-p:0.05583604797720909
epoch£º480	 i:3 	 global-step:9603	 l-p:0.05577562749385834
epoch£º480	 i:4 	 global-step:9604	 l-p:0.0558997243642807
epoch£º480	 i:5 	 global-step:9605	 l-p:0.05574449524283409
epoch£º480	 i:6 	 global-step:9606	 l-p:0.05592801049351692
epoch£º480	 i:7 	 global-step:9607	 l-p:0.05575611814856529
epoch£º480	 i:8 	 global-step:9608	 l-p:0.05575085058808327
epoch£º480	 i:9 	 global-step:9609	 l-p:0.05575406923890114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8437, 28.1679, 27.9258],
        [27.8437, 27.8438, 27.8437],
        [27.8437, 27.8756, 27.8457],
        [27.8437, 27.8439, 27.8437]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.055714964866638184 
model_pd.l_d.mean(): -0.00013284602027852088 
model_pd.lagr.mean(): 0.05558211728930473 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6454], device='cuda:0')), ('power', tensor([-0.3585], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.055714964866638184
epoch£º481	 i:1 	 global-step:9621	 l-p:0.05575484037399292
epoch£º481	 i:2 	 global-step:9622	 l-p:0.055748939514160156
epoch£º481	 i:3 	 global-step:9623	 l-p:0.055859021842479706
epoch£º481	 i:4 	 global-step:9624	 l-p:0.05585721880197525
epoch£º481	 i:5 	 global-step:9625	 l-p:0.0557696707546711
epoch£º481	 i:6 	 global-step:9626	 l-p:0.055810149759054184
epoch£º481	 i:7 	 global-step:9627	 l-p:0.055848803371191025
epoch£º481	 i:8 	 global-step:9628	 l-p:0.055784109979867935
epoch£º481	 i:9 	 global-step:9629	 l-p:0.056198060512542725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7904, 27.9823, 27.8254],
        [27.7904, 28.2830, 27.9534],
        [27.7904, 30.4264, 30.2447],
        [27.7904, 32.9897, 34.6098]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.05572308599948883 
model_pd.l_d.mean(): -9.872941154753789e-05 
model_pd.lagr.mean(): 0.05562435835599899 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6467], device='cuda:0')), ('power', tensor([-0.4559], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.05572308599948883
epoch£º482	 i:1 	 global-step:9641	 l-p:0.0557648204267025
epoch£º482	 i:2 	 global-step:9642	 l-p:0.055751923471689224
epoch£º482	 i:3 	 global-step:9643	 l-p:0.05591575428843498
epoch£º482	 i:4 	 global-step:9644	 l-p:0.05604412034153938
epoch£º482	 i:5 	 global-step:9645	 l-p:0.05581580847501755
epoch£º482	 i:6 	 global-step:9646	 l-p:0.05593735724687576
epoch£º482	 i:7 	 global-step:9647	 l-p:0.05576246231794357
epoch£º482	 i:8 	 global-step:9648	 l-p:0.0557430237531662
epoch£º482	 i:9 	 global-step:9649	 l-p:0.055966053158044815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7790, 27.7791, 27.7790],
        [27.7790, 32.9795, 34.6020],
        [27.7790, 27.7821, 27.7791],
        [27.7790, 27.7790, 27.7790]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.05604872107505798 
model_pd.l_d.mean(): -8.7460202848888e-06 
model_pd.lagr.mean(): 0.056039974093437195 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.5655e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4980], device='cuda:0')), ('power', tensor([-0.1930], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.05604872107505798
epoch£º483	 i:1 	 global-step:9661	 l-p:0.05617593601346016
epoch£º483	 i:2 	 global-step:9662	 l-p:0.05572489649057388
epoch£º483	 i:3 	 global-step:9663	 l-p:0.05579588934779167
epoch£º483	 i:4 	 global-step:9664	 l-p:0.05573999136686325
epoch£º483	 i:5 	 global-step:9665	 l-p:0.0557999312877655
epoch£º483	 i:6 	 global-step:9666	 l-p:0.05577714741230011
epoch£º483	 i:7 	 global-step:9667	 l-p:0.05583946034312248
epoch£º483	 i:8 	 global-step:9668	 l-p:0.05575960874557495
epoch£º483	 i:9 	 global-step:9669	 l-p:0.055742960423231125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[27.8111, 34.3177, 37.2090],
        [27.8111, 30.3525, 30.1284],
        [27.8111, 31.3275, 31.6298],
        [27.8111, 34.5352, 37.6556]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.05585431307554245 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05585431307554245 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5190], device='cuda:0')), ('power', tensor([-0.2221], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.05585431307554245
epoch£º484	 i:1 	 global-step:9681	 l-p:0.05595342814922333
epoch£º484	 i:2 	 global-step:9682	 l-p:0.055955227464437485
epoch£º484	 i:3 	 global-step:9683	 l-p:0.05591963231563568
epoch£º484	 i:4 	 global-step:9684	 l-p:0.0557563453912735
epoch£º484	 i:5 	 global-step:9685	 l-p:0.0558745451271534
epoch£º484	 i:6 	 global-step:9686	 l-p:0.055750805884599686
epoch£º484	 i:7 	 global-step:9687	 l-p:0.055782001465559006
epoch£º484	 i:8 	 global-step:9688	 l-p:0.05571707710623741
epoch£º484	 i:9 	 global-step:9689	 l-p:0.055739182978868484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8675, 28.1596, 27.9368],
        [27.8675, 28.8975, 28.4115],
        [27.8675, 27.8675, 27.8675],
        [27.8675, 27.8675, 27.8675]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.05568745359778404 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568745359778404 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6816], device='cuda:0')), ('power', tensor([-0.4489], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.05568745359778404
epoch£º485	 i:1 	 global-step:9701	 l-p:0.05611468106508255
epoch£º485	 i:2 	 global-step:9702	 l-p:0.055762190371751785
epoch£º485	 i:3 	 global-step:9703	 l-p:0.055824678391218185
epoch£º485	 i:4 	 global-step:9704	 l-p:0.05584436282515526
epoch£º485	 i:5 	 global-step:9705	 l-p:0.05574088543653488
epoch£º485	 i:6 	 global-step:9706	 l-p:0.0556894950568676
epoch£º485	 i:7 	 global-step:9707	 l-p:0.05596539005637169
epoch£º485	 i:8 	 global-step:9708	 l-p:0.05577017739415169
epoch£º485	 i:9 	 global-step:9709	 l-p:0.05576401948928833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9326, 31.6858, 32.1363],
        [27.9326, 29.6856, 29.2168],
        [27.9326, 34.2989, 37.0275],
        [27.9326, 28.5423, 28.1629]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.05578776076436043 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05578776076436043 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5638], device='cuda:0')), ('power', tensor([-0.1481], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.05578776076436043
epoch£º486	 i:1 	 global-step:9721	 l-p:0.05569424480199814
epoch£º486	 i:2 	 global-step:9722	 l-p:0.05590534955263138
epoch£º486	 i:3 	 global-step:9723	 l-p:0.055686578154563904
epoch£º486	 i:4 	 global-step:9724	 l-p:0.05578199401497841
epoch£º486	 i:5 	 global-step:9725	 l-p:0.05575965717434883
epoch£º486	 i:6 	 global-step:9726	 l-p:0.055676333606243134
epoch£º486	 i:7 	 global-step:9727	 l-p:0.05601474642753601
epoch£º486	 i:8 	 global-step:9728	 l-p:0.05575581267476082
epoch£º486	 i:9 	 global-step:9729	 l-p:0.055948879569768906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0008, 29.7671, 29.2985],
        [28.0008, 28.0019, 28.0008],
        [28.0008, 28.0008, 28.0008],
        [28.0008, 32.8161, 34.0775]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.05576753616333008 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576753616333008 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5899], device='cuda:0')), ('power', tensor([-0.1305], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.05576753616333008
epoch£º487	 i:1 	 global-step:9741	 l-p:0.05570055916905403
epoch£º487	 i:2 	 global-step:9742	 l-p:0.055761925876140594
epoch£º487	 i:3 	 global-step:9743	 l-p:0.05583081766963005
epoch£º487	 i:4 	 global-step:9744	 l-p:0.055924296379089355
epoch£º487	 i:5 	 global-step:9745	 l-p:0.055736273527145386
epoch£º487	 i:6 	 global-step:9746	 l-p:0.05567966029047966
epoch£º487	 i:7 	 global-step:9747	 l-p:0.055826038122177124
epoch£º487	 i:8 	 global-step:9748	 l-p:0.05572717636823654
epoch£º487	 i:9 	 global-step:9749	 l-p:0.055902328342199326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0707, 28.0791, 28.0710],
        [28.0707, 31.3707, 31.5207],
        [28.0707, 28.2590, 28.1045],
        [28.0707, 35.8397, 40.0763]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.055711496621370316 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055711496621370316 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6070], device='cuda:0')), ('power', tensor([-0.0783], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.055711496621370316
epoch£º488	 i:1 	 global-step:9761	 l-p:0.05571979284286499
epoch£º488	 i:2 	 global-step:9762	 l-p:0.05568099021911621
epoch£º488	 i:3 	 global-step:9763	 l-p:0.05602451041340828
epoch£º488	 i:4 	 global-step:9764	 l-p:0.055758800357580185
epoch£º488	 i:5 	 global-step:9765	 l-p:0.05581105127930641
epoch£º488	 i:6 	 global-step:9766	 l-p:0.05587448924779892
epoch£º488	 i:7 	 global-step:9767	 l-p:0.05570187419652939
epoch£º488	 i:8 	 global-step:9768	 l-p:0.055730465799570084
epoch£º488	 i:9 	 global-step:9769	 l-p:0.05568655952811241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1412, 28.4364, 28.2112],
        [28.1412, 28.1437, 28.1413],
        [28.1412, 28.4692, 28.2243],
        [28.1412, 28.1412, 28.1412]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.05566735938191414 
model_pd.l_d.mean(): -4.4231649098946946e-07 
model_pd.lagr.mean(): 0.05566691607236862 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6453], device='cuda:0')), ('power', tensor([-0.1035], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.05566735938191414
epoch£º489	 i:1 	 global-step:9781	 l-p:0.05569807067513466
epoch£º489	 i:2 	 global-step:9782	 l-p:0.055728886276483536
epoch£º489	 i:3 	 global-step:9783	 l-p:0.05569760128855705
epoch£º489	 i:4 	 global-step:9784	 l-p:0.05583219230175018
epoch£º489	 i:5 	 global-step:9785	 l-p:0.055848173797130585
epoch£º489	 i:6 	 global-step:9786	 l-p:0.05601293593645096
epoch£º489	 i:7 	 global-step:9787	 l-p:0.05570029094815254
epoch£º489	 i:8 	 global-step:9788	 l-p:0.05568356439471245
epoch£º489	 i:9 	 global-step:9789	 l-p:0.055674485862255096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2109, 28.2109, 28.2109],
        [28.2109, 28.2142, 28.2109],
        [28.2109, 28.2162, 28.2110],
        [28.2109, 28.2109, 28.2109]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.055638596415519714 
model_pd.l_d.mean(): -8.935944038057642e-07 
model_pd.lagr.mean(): 0.055637702345848083 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6008e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6596], device='cuda:0')), ('power', tensor([-0.0323], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.055638596415519714
epoch£º490	 i:1 	 global-step:9801	 l-p:0.05567430332303047
epoch£º490	 i:2 	 global-step:9802	 l-p:0.055730145424604416
epoch£º490	 i:3 	 global-step:9803	 l-p:0.05571088194847107
epoch£º490	 i:4 	 global-step:9804	 l-p:0.05586518719792366
epoch£º490	 i:5 	 global-step:9805	 l-p:0.05563654378056526
epoch£º490	 i:6 	 global-step:9806	 l-p:0.05573928356170654
epoch£º490	 i:7 	 global-step:9807	 l-p:0.05574949085712433
epoch£º490	 i:8 	 global-step:9808	 l-p:0.0556420236825943
epoch£º490	 i:9 	 global-step:9809	 l-p:0.056006669998168945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2750, 36.5673, 41.3799],
        [28.2750, 29.8886, 29.3906],
        [28.2750, 28.6349, 28.3714],
        [28.2750, 34.5794, 37.1970]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.05587375536561012 
model_pd.l_d.mean(): 1.8225926396553405e-05 
model_pd.lagr.mean(): 0.0558919794857502 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.4536e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5906], device='cuda:0')), ('power', tensor([0.2179], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.05587375536561012
epoch£º491	 i:1 	 global-step:9821	 l-p:0.0556204579770565
epoch£º491	 i:2 	 global-step:9822	 l-p:0.05565639212727547
epoch£º491	 i:3 	 global-step:9823	 l-p:0.05575091391801834
epoch£º491	 i:4 	 global-step:9824	 l-p:0.055640894919633865
epoch£º491	 i:5 	 global-step:9825	 l-p:0.05568007379770279
epoch£º491	 i:6 	 global-step:9826	 l-p:0.05579502880573273
epoch£º491	 i:7 	 global-step:9827	 l-p:0.05571851134300232
epoch£º491	 i:8 	 global-step:9828	 l-p:0.05584970489144325
epoch£º491	 i:9 	 global-step:9829	 l-p:0.0556783564388752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3258, 28.3259, 28.3258],
        [28.3258, 31.4900, 31.5448],
        [28.3258, 28.3836, 28.3308],
        [28.3258, 28.3677, 28.3288]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.05562108755111694 
model_pd.l_d.mean(): 2.650241003721021e-05 
model_pd.lagr.mean(): 0.0556475892663002 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6518], device='cuda:0')), ('power', tensor([0.1569], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.05562108755111694
epoch£º492	 i:1 	 global-step:9841	 l-p:0.05568621680140495
epoch£º492	 i:2 	 global-step:9842	 l-p:0.05583370104432106
epoch£º492	 i:3 	 global-step:9843	 l-p:0.05566239356994629
epoch£º492	 i:4 	 global-step:9844	 l-p:0.05565684288740158
epoch£º492	 i:5 	 global-step:9845	 l-p:0.05574530363082886
epoch£º492	 i:6 	 global-step:9846	 l-p:0.05583420768380165
epoch£º492	 i:7 	 global-step:9847	 l-p:0.05560290813446045
epoch£º492	 i:8 	 global-step:9848	 l-p:0.05583412945270538
epoch£º492	 i:9 	 global-step:9849	 l-p:0.0556979738175869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3524, 38.8203, 46.3834],
        [28.3524, 28.3542, 28.3524],
        [28.3524, 29.7831, 29.2697],
        [28.3524, 37.6342, 43.6464]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.05611434951424599 
model_pd.l_d.mean(): 0.00014581030700355768 
model_pd.lagr.mean(): 0.056260161101818085 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4531], device='cuda:0')), ('power', tensor([0.5315], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.05611434951424599
epoch£º493	 i:1 	 global-step:9861	 l-p:0.05565405637025833
epoch£º493	 i:2 	 global-step:9862	 l-p:0.055593423545360565
epoch£º493	 i:3 	 global-step:9863	 l-p:0.05571076273918152
epoch£º493	 i:4 	 global-step:9864	 l-p:0.055666666477918625
epoch£º493	 i:5 	 global-step:9865	 l-p:0.0556480772793293
epoch£º493	 i:6 	 global-step:9866	 l-p:0.05578625202178955
epoch£º493	 i:7 	 global-step:9867	 l-p:0.05569786950945854
epoch£º493	 i:8 	 global-step:9868	 l-p:0.05561895668506622
epoch£º493	 i:9 	 global-step:9869	 l-p:0.055655982345342636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3474, 31.5141, 31.5690],
        [28.3474, 38.3335, 45.2555],
        [28.3474, 29.3962, 28.9013],
        [28.3474, 28.4910, 28.3690]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.055988993495702744 
model_pd.l_d.mean(): 0.0001894485903903842 
model_pd.lagr.mean(): 0.056178443133831024 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5004], device='cuda:0')), ('power', tensor([0.4905], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.055988993495702744
epoch£º494	 i:1 	 global-step:9881	 l-p:0.05571100115776062
epoch£º494	 i:2 	 global-step:9882	 l-p:0.05562323331832886
epoch£º494	 i:3 	 global-step:9883	 l-p:0.055745791643857956
epoch£º494	 i:4 	 global-step:9884	 l-p:0.055596303194761276
epoch£º494	 i:5 	 global-step:9885	 l-p:0.055662963539361954
epoch£º494	 i:6 	 global-step:9886	 l-p:0.055682793259620667
epoch£º494	 i:7 	 global-step:9887	 l-p:0.05566014349460602
epoch£º494	 i:8 	 global-step:9888	 l-p:0.05589248612523079
epoch£º494	 i:9 	 global-step:9889	 l-p:0.05562853813171387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3067, 29.6388, 29.1247],
        [28.3067, 28.3107, 28.3068],
        [28.3067, 35.8283, 39.7430],
        [28.3067, 28.3950, 28.3166]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.05566055700182915 
model_pd.l_d.mean(): 5.992055230308324e-05 
model_pd.lagr.mean(): 0.05572047829627991 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6305], device='cuda:0')), ('power', tensor([0.1229], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.05566055700182915
epoch£º495	 i:1 	 global-step:9901	 l-p:0.05573759973049164
epoch£º495	 i:2 	 global-step:9902	 l-p:0.05572816729545593
epoch£º495	 i:3 	 global-step:9903	 l-p:0.05585702881217003
epoch£º495	 i:4 	 global-step:9904	 l-p:0.055690038949251175
epoch£º495	 i:5 	 global-step:9905	 l-p:0.05588497593998909
epoch£º495	 i:6 	 global-step:9906	 l-p:0.05563317984342575
epoch£º495	 i:7 	 global-step:9907	 l-p:0.05577787384390831
epoch£º495	 i:8 	 global-step:9908	 l-p:0.05564184859395027
epoch£º495	 i:9 	 global-step:9909	 l-p:0.05570325627923012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2338, 28.2338, 28.2338],
        [28.2338, 28.8765, 28.4830],
        [28.2338, 28.2339, 28.2338],
        [28.2338, 35.7351, 39.6392]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.055658381432294846 
model_pd.l_d.mean(): 1.5989362509571947e-05 
model_pd.lagr.mean(): 0.055674370378255844 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6346], device='cuda:0')), ('power', tensor([0.0285], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.055658381432294846
epoch£º496	 i:1 	 global-step:9921	 l-p:0.056012172251939774
epoch£º496	 i:2 	 global-step:9922	 l-p:0.05566573888063431
epoch£º496	 i:3 	 global-step:9923	 l-p:0.05591457337141037
epoch£º496	 i:4 	 global-step:9924	 l-p:0.05565488338470459
epoch£º496	 i:5 	 global-step:9925	 l-p:0.055635564029216766
epoch£º496	 i:6 	 global-step:9926	 l-p:0.055801697075366974
epoch£º496	 i:7 	 global-step:9927	 l-p:0.05574401095509529
epoch£º496	 i:8 	 global-step:9928	 l-p:0.055746957659721375
epoch£º496	 i:9 	 global-step:9929	 l-p:0.05566782504320145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1352, 30.9812, 30.8798],
        [28.1352, 28.1352, 28.1352],
        [28.1352, 28.1354, 28.1352],
        [28.1352, 28.1465, 28.1356]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.0557076521217823 
model_pd.l_d.mean(): 9.480035259912256e-06 
model_pd.lagr.mean(): 0.05571713298559189 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6036], device='cuda:0')), ('power', tensor([0.0160], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.0557076521217823
epoch£º497	 i:1 	 global-step:9941	 l-p:0.05570056661963463
epoch£º497	 i:2 	 global-step:9942	 l-p:0.055661991238594055
epoch£º497	 i:3 	 global-step:9943	 l-p:0.05572928488254547
epoch£º497	 i:4 	 global-step:9944	 l-p:0.055681705474853516
epoch£º497	 i:5 	 global-step:9945	 l-p:0.055756304413080215
epoch£º497	 i:6 	 global-step:9946	 l-p:0.055694226175546646
epoch£º497	 i:7 	 global-step:9947	 l-p:0.05578974261879921
epoch£º497	 i:8 	 global-step:9948	 l-p:0.055847447365522385
epoch£º497	 i:9 	 global-step:9949	 l-p:0.05617310106754303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0228, 28.4436, 28.1480],
        [28.0228, 36.2378, 41.0052],
        [28.0228, 34.1601, 36.6441],
        [28.0228, 28.0773, 28.0274]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.05572894960641861 
model_pd.l_d.mean(): -7.342400203924626e-05 
model_pd.lagr.mean(): 0.055655524134635925 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6041], device='cuda:0')), ('power', tensor([-0.1283], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.05572894960641861
epoch£º498	 i:1 	 global-step:9961	 l-p:0.0557250902056694
epoch£º498	 i:2 	 global-step:9962	 l-p:0.05575269088149071
epoch£º498	 i:3 	 global-step:9963	 l-p:0.05571300908923149
epoch£º498	 i:4 	 global-step:9964	 l-p:0.05570507049560547
epoch£º498	 i:5 	 global-step:9965	 l-p:0.05589646100997925
epoch£º498	 i:6 	 global-step:9966	 l-p:0.05574209988117218
epoch£º498	 i:7 	 global-step:9967	 l-p:0.05579521879553795
epoch£º498	 i:8 	 global-step:9968	 l-p:0.05615033209323883
epoch£º498	 i:9 	 global-step:9969	 l-p:0.05578315630555153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9139, 27.9192, 27.9140],
        [27.9139, 35.8604, 40.3308],
        [27.9139, 27.9139, 27.9139],
        [27.9139, 27.9139, 27.9139]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.055688388645648956 
model_pd.l_d.mean(): -0.00018606739467941225 
model_pd.lagr.mean(): 0.05550232157111168 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6651], device='cuda:0')), ('power', tensor([-0.3753], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.055688388645648956
epoch£º499	 i:1 	 global-step:9981	 l-p:0.05603574588894844
epoch£º499	 i:2 	 global-step:9982	 l-p:0.0557173416018486
epoch£º499	 i:3 	 global-step:9983	 l-p:0.055841024965047836
epoch£º499	 i:4 	 global-step:9984	 l-p:0.0557168647646904
epoch£º499	 i:5 	 global-step:9985	 l-p:0.05588667839765549
epoch£º499	 i:6 	 global-step:9986	 l-p:0.05576791986823082
epoch£º499	 i:7 	 global-step:9987	 l-p:0.055744465440511703
epoch£º499	 i:8 	 global-step:9988	 l-p:0.055837564170360565
epoch£º499	 i:9 	 global-step:9989	 l-p:0.055985383689403534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8230, 28.7314, 28.2666],
        [27.8230, 30.5877, 30.4640],
        [27.8230, 28.2406, 27.9472],
        [27.8230, 33.3186, 35.2069]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.055718887597322464 
model_pd.l_d.mean(): -0.0001377285225316882 
model_pd.lagr.mean(): 0.05558115988969803 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6455], device='cuda:0')), ('power', tensor([-0.3736], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.055718887597322464
epoch£º500	 i:1 	 global-step:10001	 l-p:0.055734433233737946
epoch£º500	 i:2 	 global-step:10002	 l-p:0.055751997977495193
epoch£º500	 i:3 	 global-step:10003	 l-p:0.05601455643773079
epoch£º500	 i:4 	 global-step:10004	 l-p:0.055759329348802567
epoch£º500	 i:5 	 global-step:10005	 l-p:0.05589529499411583
epoch£º500	 i:6 	 global-step:10006	 l-p:0.056155577301979065
epoch£º500	 i:7 	 global-step:10007	 l-p:0.05580064654350281
epoch£º500	 i:8 	 global-step:10008	 l-p:0.055786386132240295
epoch£º500	 i:9 	 global-step:10009	 l-p:0.0557798407971859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7660, 28.9849, 28.4832],
        [27.7660, 30.6448, 30.5809],
        [27.7660, 31.0282, 31.1763],
        [27.7660, 27.9841, 27.8091]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.05583807826042175 
model_pd.l_d.mean(): -6.840596324764192e-05 
model_pd.lagr.mean(): 0.0557696707546711 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5655], device='cuda:0')), ('power', tensor([-0.3362], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.05583807826042175
epoch£º501	 i:1 	 global-step:10021	 l-p:0.05578729882836342
epoch£º501	 i:2 	 global-step:10022	 l-p:0.05573900416493416
epoch£º501	 i:3 	 global-step:10023	 l-p:0.05605001747608185
epoch£º501	 i:4 	 global-step:10024	 l-p:0.0557214729487896
epoch£º501	 i:5 	 global-step:10025	 l-p:0.05602226033806801
epoch£º501	 i:6 	 global-step:10026	 l-p:0.05578198656439781
epoch£º501	 i:7 	 global-step:10027	 l-p:0.05582434684038162
epoch£º501	 i:8 	 global-step:10028	 l-p:0.055775951594114304
epoch£º501	 i:9 	 global-step:10029	 l-p:0.05594010278582573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7559, 29.9701, 29.6247],
        [27.7559, 27.7559, 27.7559],
        [27.7559, 35.5799, 39.9357],
        [27.7559, 27.7575, 27.7559]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.05590742081403732 
model_pd.l_d.mean(): -6.726211722707376e-06 
model_pd.lagr.mean(): 0.0559006929397583 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.2268e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5421], device='cuda:0')), ('power', tensor([-0.3359], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.05590742081403732
epoch£º502	 i:1 	 global-step:10041	 l-p:0.05578744783997536
epoch£º502	 i:2 	 global-step:10042	 l-p:0.05573638901114464
epoch£º502	 i:3 	 global-step:10043	 l-p:0.05591045320034027
epoch£º502	 i:4 	 global-step:10044	 l-p:0.055824484676122665
epoch£º502	 i:5 	 global-step:10045	 l-p:0.05581521987915039
epoch£º502	 i:6 	 global-step:10046	 l-p:0.05616097152233124
epoch£º502	 i:7 	 global-step:10047	 l-p:0.05574425682425499
epoch£º502	 i:8 	 global-step:10048	 l-p:0.05579768866300583
epoch£º502	 i:9 	 global-step:10049	 l-p:0.05576570704579353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7952, 27.7957, 27.7952],
        [27.7952, 35.4855, 39.6796],
        [27.7952, 27.7952, 27.7952],
        [27.7952, 27.9816, 27.8286]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.05587706342339516 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05587706342339516 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5646], device='cuda:0')), ('power', tensor([-0.3412], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.05587706342339516
epoch£º503	 i:1 	 global-step:10061	 l-p:0.0557713657617569
epoch£º503	 i:2 	 global-step:10062	 l-p:0.055734191089868546
epoch£º503	 i:3 	 global-step:10063	 l-p:0.055775173008441925
epoch£º503	 i:4 	 global-step:10064	 l-p:0.05577601492404938
epoch£º503	 i:5 	 global-step:10065	 l-p:0.05590953677892685
epoch£º503	 i:6 	 global-step:10066	 l-p:0.055978819727897644
epoch£º503	 i:7 	 global-step:10067	 l-p:0.055793531239032745
epoch£º503	 i:8 	 global-step:10068	 l-p:0.055727917701005936
epoch£º503	 i:9 	 global-step:10069	 l-p:0.05598674714565277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8584, 27.9392, 27.8671],
        [27.8584, 30.7485, 30.6848],
        [27.8584, 28.1017, 27.9098],
        [27.8584, 29.0528, 28.5509]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.05575499311089516 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575499311089516 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5912], device='cuda:0')), ('power', tensor([-0.2888], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.05575499311089516
epoch£º504	 i:1 	 global-step:10081	 l-p:0.05594098940491676
epoch£º504	 i:2 	 global-step:10082	 l-p:0.05575167387723923
epoch£º504	 i:3 	 global-step:10083	 l-p:0.05571115389466286
epoch£º504	 i:4 	 global-step:10084	 l-p:0.05572136864066124
epoch£º504	 i:5 	 global-step:10085	 l-p:0.055878300219774246
epoch£º504	 i:6 	 global-step:10086	 l-p:0.05569853633642197
epoch£º504	 i:7 	 global-step:10087	 l-p:0.05575396120548248
epoch£º504	 i:8 	 global-step:10088	 l-p:0.05595150217413902
epoch£º504	 i:9 	 global-step:10089	 l-p:0.05601305887103081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9305, 29.4463, 28.9472],
        [27.9305, 28.3809, 28.0707],
        [27.9305, 27.9614, 27.9323],
        [27.9305, 28.6050, 28.2023]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.05572018399834633 
model_pd.l_d.mean(): -8.12718710108129e-08 
model_pd.lagr.mean(): 0.05572010204195976 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6222], device='cuda:0')), ('power', tensor([-0.2608], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.05572018399834633
epoch£º505	 i:1 	 global-step:10101	 l-p:0.055779289454221725
epoch£º505	 i:2 	 global-step:10102	 l-p:0.05570714920759201
epoch£º505	 i:3 	 global-step:10103	 l-p:0.055684491991996765
epoch£º505	 i:4 	 global-step:10104	 l-p:0.05576297640800476
epoch£º505	 i:5 	 global-step:10105	 l-p:0.055760785937309265
epoch£º505	 i:6 	 global-step:10106	 l-p:0.055832020938396454
epoch£º505	 i:7 	 global-step:10107	 l-p:0.056050196290016174
epoch£º505	 i:8 	 global-step:10108	 l-p:0.05575418472290039
epoch£º505	 i:9 	 global-step:10109	 l-p:0.05595715343952179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0058, 28.0870, 28.0145],
        [28.0058, 38.1150, 45.2790],
        [28.0058, 37.0973, 42.9417],
        [28.0058, 31.2331, 31.3446]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.05581307411193848 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05581307411193848 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5468], device='cuda:0')), ('power', tensor([-0.0750], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.05581307411193848
epoch£º506	 i:1 	 global-step:10121	 l-p:0.05567176640033722
epoch£º506	 i:2 	 global-step:10122	 l-p:0.055845893919467926
epoch£º506	 i:3 	 global-step:10123	 l-p:0.0556708425283432
epoch£º506	 i:4 	 global-step:10124	 l-p:0.055716607719659805
epoch£º506	 i:5 	 global-step:10125	 l-p:0.05587952584028244
epoch£º506	 i:6 	 global-step:10126	 l-p:0.055822018533945084
epoch£º506	 i:7 	 global-step:10127	 l-p:0.05581899359822273
epoch£º506	 i:8 	 global-step:10128	 l-p:0.05590895563364029
epoch£º506	 i:9 	 global-step:10129	 l-p:0.05569057911634445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0828, 28.4902, 28.2014],
        [28.0828, 29.9706, 29.5235],
        [28.0828, 30.5177, 30.2376],
        [28.0828, 37.7658, 44.3507]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.05570713058114052 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05570713058114052 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6096], device='cuda:0')), ('power', tensor([-0.0555], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.05570713058114052
epoch£º507	 i:1 	 global-step:10141	 l-p:0.055824682116508484
epoch£º507	 i:2 	 global-step:10142	 l-p:0.05588541924953461
epoch£º507	 i:3 	 global-step:10143	 l-p:0.055663544684648514
epoch£º507	 i:4 	 global-step:10144	 l-p:0.05593983456492424
epoch£º507	 i:5 	 global-step:10145	 l-p:0.0556618869304657
epoch£º507	 i:6 	 global-step:10146	 l-p:0.055697135627269745
epoch£º507	 i:7 	 global-step:10147	 l-p:0.055664509534835815
epoch£º507	 i:8 	 global-step:10148	 l-p:0.05594153329730034
epoch£º507	 i:9 	 global-step:10149	 l-p:0.05568042770028114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1604, 28.3502, 28.1945],
        [28.1604, 28.1758, 28.1610],
        [28.1604, 28.1643, 28.1604],
        [28.1604, 32.6857, 33.6918]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.055834703147411346 
model_pd.l_d.mean(): 1.1531741392900585e-06 
model_pd.lagr.mean(): 0.05583585798740387 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.5718e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5612], device='cuda:0')), ('power', tensor([0.1167], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.055834703147411346
epoch£º508	 i:1 	 global-step:10161	 l-p:0.055695611983537674
epoch£º508	 i:2 	 global-step:10162	 l-p:0.055878061801195145
epoch£º508	 i:3 	 global-step:10163	 l-p:0.05568075180053711
epoch£º508	 i:4 	 global-step:10164	 l-p:0.05562900751829147
epoch£º508	 i:5 	 global-step:10165	 l-p:0.055652741342782974
epoch£º508	 i:6 	 global-step:10166	 l-p:0.05568582937121391
epoch£º508	 i:7 	 global-step:10167	 l-p:0.0558292381465435
epoch£º508	 i:8 	 global-step:10168	 l-p:0.05580832064151764
epoch£º508	 i:9 	 global-step:10169	 l-p:0.055801134556531906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2354, 35.0338, 38.1692],
        [28.2354, 28.2385, 28.2354],
        [28.2354, 38.5727, 45.9878],
        [28.2354, 28.5875, 28.3284]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.05575631931424141 
model_pd.l_d.mean(): 5.730923021474155e-06 
model_pd.lagr.mean(): 0.055762048810720444 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.9799e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5961], device='cuda:0')), ('power', tensor([0.1328], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.05575631931424141
epoch£º509	 i:1 	 global-step:10181	 l-p:0.05604412779211998
epoch£º509	 i:2 	 global-step:10182	 l-p:0.05566784739494324
epoch£º509	 i:3 	 global-step:10183	 l-p:0.05569770559668541
epoch£º509	 i:4 	 global-step:10184	 l-p:0.05566525459289551
epoch£º509	 i:5 	 global-step:10185	 l-p:0.055681951344013214
epoch£º509	 i:6 	 global-step:10186	 l-p:0.055797550827264786
epoch£º509	 i:7 	 global-step:10187	 l-p:0.05562937259674072
epoch£º509	 i:8 	 global-step:10188	 l-p:0.05570690333843231
epoch£º509	 i:9 	 global-step:10189	 l-p:0.05569051206111908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3013, 28.3108, 28.3016],
        [28.3013, 34.2404, 36.4917],
        [28.3013, 34.5870, 37.1820],
        [28.3013, 32.9475, 34.0376]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.05601995810866356 
model_pd.l_d.mean(): 2.9089775125612505e-05 
model_pd.lagr.mean(): 0.05604904890060425 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5783], device='cuda:0')), ('power', tensor([0.2595], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.05601995810866356
epoch£º510	 i:1 	 global-step:10201	 l-p:0.05563521757721901
epoch£º510	 i:2 	 global-step:10202	 l-p:0.05566229671239853
epoch£º510	 i:3 	 global-step:10203	 l-p:0.05569044128060341
epoch£º510	 i:4 	 global-step:10204	 l-p:0.05574150010943413
epoch£º510	 i:5 	 global-step:10205	 l-p:0.05574212595820427
epoch£º510	 i:6 	 global-step:10206	 l-p:0.05564829707145691
epoch£º510	 i:7 	 global-step:10207	 l-p:0.05568620562553406
epoch£º510	 i:8 	 global-step:10208	 l-p:0.05573226884007454
epoch£º510	 i:9 	 global-step:10209	 l-p:0.05565251410007477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3470, 29.0806, 28.6558],
        [28.3470, 33.8567, 35.6948],
        [28.3470, 28.6249, 28.4101],
        [28.3470, 28.3470, 28.3470]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.05568965896964073 
model_pd.l_d.mean(): 5.877200237591751e-05 
model_pd.lagr.mean(): 0.05574842914938927 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5944], device='cuda:0')), ('power', tensor([0.2805], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.05568965896964073
epoch£º511	 i:1 	 global-step:10221	 l-p:0.055663738399744034
epoch£º511	 i:2 	 global-step:10222	 l-p:0.05575605109333992
epoch£º511	 i:3 	 global-step:10223	 l-p:0.05576317012310028
epoch£º511	 i:4 	 global-step:10224	 l-p:0.05562222748994827
epoch£º511	 i:5 	 global-step:10225	 l-p:0.0556337870657444
epoch£º511	 i:6 	 global-step:10226	 l-p:0.055617496371269226
epoch£º511	 i:7 	 global-step:10227	 l-p:0.056117162108421326
epoch£º511	 i:8 	 global-step:10228	 l-p:0.05566561594605446
epoch£º511	 i:9 	 global-step:10229	 l-p:0.055609330534935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3629, 28.4465, 28.3720],
        [28.3629, 28.7167, 28.4564],
        [28.3629, 33.6783, 35.3371],
        [28.3629, 29.5993, 29.0868]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.05563333258032799 
model_pd.l_d.mean(): 4.9744143325369805e-05 
model_pd.lagr.mean(): 0.05568307638168335 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6549], device='cuda:0')), ('power', tensor([0.1539], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.05563333258032799
epoch£º512	 i:1 	 global-step:10241	 l-p:0.05576961487531662
epoch£º512	 i:2 	 global-step:10242	 l-p:0.05566183105111122
epoch£º512	 i:3 	 global-step:10243	 l-p:0.05587070435285568
epoch£º512	 i:4 	 global-step:10244	 l-p:0.05588074401021004
epoch£º512	 i:5 	 global-step:10245	 l-p:0.055648744106292725
epoch£º512	 i:6 	 global-step:10246	 l-p:0.055678289383649826
epoch£º512	 i:7 	 global-step:10247	 l-p:0.055669497698545456
epoch£º512	 i:8 	 global-step:10248	 l-p:0.05560678243637085
epoch£º512	 i:9 	 global-step:10249	 l-p:0.05571812018752098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3426, 30.1643, 29.6958],
        [28.3426, 36.3576, 40.8313],
        [28.3426, 29.8821, 29.3753],
        [28.3426, 28.3570, 28.3432]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.05564810335636139 
model_pd.l_d.mean(): 7.859330798964947e-05 
model_pd.lagr.mean(): 0.05572669580578804 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6215], device='cuda:0')), ('power', tensor([0.1798], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.05564810335636139
epoch£º513	 i:1 	 global-step:10261	 l-p:0.05572005733847618
epoch£º513	 i:2 	 global-step:10262	 l-p:0.05562847852706909
epoch£º513	 i:3 	 global-step:10263	 l-p:0.055617187172174454
epoch£º513	 i:4 	 global-step:10264	 l-p:0.05591322481632233
epoch£º513	 i:5 	 global-step:10265	 l-p:0.05568058043718338
epoch£º513	 i:6 	 global-step:10266	 l-p:0.05566161870956421
epoch£º513	 i:7 	 global-step:10267	 l-p:0.05575095862150192
epoch£º513	 i:8 	 global-step:10268	 l-p:0.05575965717434883
epoch£º513	 i:9 	 global-step:10269	 l-p:0.05584242567420006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2830, 37.7635, 44.0432],
        [28.2830, 36.1135, 40.3838],
        [28.2830, 29.6139, 29.1003],
        [28.2830, 28.2843, 28.2830]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.055665720254182816 
model_pd.l_d.mean(): 4.131183595745824e-05 
model_pd.lagr.mean(): 0.05570703372359276 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6198], device='cuda:0')), ('power', tensor([0.0777], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.055665720254182816
epoch£º514	 i:1 	 global-step:10281	 l-p:0.05563398078083992
epoch£º514	 i:2 	 global-step:10282	 l-p:0.05562290549278259
epoch£º514	 i:3 	 global-step:10283	 l-p:0.0556640625
epoch£º514	 i:4 	 global-step:10284	 l-p:0.05604930594563484
epoch£º514	 i:5 	 global-step:10285	 l-p:0.05587365850806236
epoch£º514	 i:6 	 global-step:10286	 l-p:0.05576740950345993
epoch£º514	 i:7 	 global-step:10287	 l-p:0.055744368582963943
epoch£º514	 i:8 	 global-step:10288	 l-p:0.055709388107061386
epoch£º514	 i:9 	 global-step:10289	 l-p:0.05565650016069412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1900, 35.7382, 39.7026],
        [28.1900, 35.3275, 38.8352],
        [28.1900, 28.6578, 28.3383],
        [28.1900, 32.5039, 33.3410]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.055930834263563156 
model_pd.l_d.mean(): 0.00013444795331452042 
model_pd.lagr.mean(): 0.056065283715724945 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5451], device='cuda:0')), ('power', tensor([0.2280], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.055930834263563156
epoch£º515	 i:1 	 global-step:10301	 l-p:0.05583842098712921
epoch£º515	 i:2 	 global-step:10302	 l-p:0.05582522600889206
epoch£º515	 i:3 	 global-step:10303	 l-p:0.05566529929637909
epoch£º515	 i:4 	 global-step:10304	 l-p:0.05564645677804947
epoch£º515	 i:5 	 global-step:10305	 l-p:0.05584084987640381
epoch£º515	 i:6 	 global-step:10306	 l-p:0.05581545829772949
epoch£º515	 i:7 	 global-step:10307	 l-p:0.05567648634314537
epoch£º515	 i:8 	 global-step:10308	 l-p:0.05570286139845848
epoch£º515	 i:9 	 global-step:10309	 l-p:0.05567600578069687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0716, 28.1364, 28.0777],
        [28.0716, 29.5360, 29.0302],
        [28.0716, 35.5629, 39.4828],
        [28.0716, 28.1970, 28.0891]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.0557638444006443 
model_pd.l_d.mean(): 8.12372763903113e-06 
model_pd.lagr.mean(): 0.05577196925878525 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5732], device='cuda:0')), ('power', tensor([0.0137], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.0557638444006443
epoch£º516	 i:1 	 global-step:10321	 l-p:0.05572018399834633
epoch£º516	 i:2 	 global-step:10322	 l-p:0.055717676877975464
epoch£º516	 i:3 	 global-step:10323	 l-p:0.05579410493373871
epoch£º516	 i:4 	 global-step:10324	 l-p:0.05574341118335724
epoch£º516	 i:5 	 global-step:10325	 l-p:0.05588001012802124
epoch£º516	 i:6 	 global-step:10326	 l-p:0.055781830102205276
epoch£º516	 i:7 	 global-step:10327	 l-p:0.055720165371894836
epoch£º516	 i:8 	 global-step:10328	 l-p:0.05571407824754715
epoch£º516	 i:9 	 global-step:10329	 l-p:0.05606342852115631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9451, 27.9950, 27.9491],
        [27.9451, 30.9148, 30.8875],
        [27.9451, 30.6145, 30.4399],
        [27.9451, 27.9452, 27.9451]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.055810995399951935 
model_pd.l_d.mean(): -0.00010177443618886173 
model_pd.lagr.mean(): 0.055709220468997955 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5930], device='cuda:0')), ('power', tensor([-0.1888], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.055810995399951935
epoch£º517	 i:1 	 global-step:10341	 l-p:0.055755142122507095
epoch£º517	 i:2 	 global-step:10342	 l-p:0.055766087025403976
epoch£º517	 i:3 	 global-step:10343	 l-p:0.05571736395359039
epoch£º517	 i:4 	 global-step:10344	 l-p:0.05571972206234932
epoch£º517	 i:5 	 global-step:10345	 l-p:0.0557716079056263
epoch£º517	 i:6 	 global-step:10346	 l-p:0.05588535591959953
epoch£º517	 i:7 	 global-step:10347	 l-p:0.05577142909169197
epoch£º517	 i:8 	 global-step:10348	 l-p:0.05602561682462692
epoch£º517	 i:9 	 global-step:10349	 l-p:0.05595023185014725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[27.8329, 34.0095, 36.5591],
        [27.8329, 31.5912, 32.0531],
        [27.8329, 28.6651, 28.2171],
        [27.8329, 29.6947, 29.2500]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.05577046424150467 
model_pd.l_d.mean(): -0.00012431827781256288 
model_pd.lagr.mean(): 0.0556461475789547 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5859], device='cuda:0')), ('power', tensor([-0.2941], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.05577046424150467
epoch£º518	 i:1 	 global-step:10361	 l-p:0.05590246245265007
epoch£º518	 i:2 	 global-step:10362	 l-p:0.05583992227911949
epoch£º518	 i:3 	 global-step:10363	 l-p:0.05599310249090195
epoch£º518	 i:4 	 global-step:10364	 l-p:0.05576698109507561
epoch£º518	 i:5 	 global-step:10365	 l-p:0.055766861885786057
epoch£º518	 i:6 	 global-step:10366	 l-p:0.055808600038290024
epoch£º518	 i:7 	 global-step:10367	 l-p:0.05586723983287811
epoch£º518	 i:8 	 global-step:10368	 l-p:0.055774569511413574
epoch£º518	 i:9 	 global-step:10369	 l-p:0.05590556189417839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7540, 27.7725, 27.7548],
        [27.7540, 37.5540, 44.3663],
        [27.7540, 27.7742, 27.7549],
        [27.7540, 28.3994, 28.0079]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.055779457092285156 
model_pd.l_d.mean(): -0.00011017256474588066 
model_pd.lagr.mean(): 0.055669285356998444 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6053], device='cuda:0')), ('power', tensor([-0.4276], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.055779457092285156
epoch£º519	 i:1 	 global-step:10381	 l-p:0.05573791265487671
epoch£º519	 i:2 	 global-step:10382	 l-p:0.05582009255886078
epoch£º519	 i:3 	 global-step:10383	 l-p:0.05575115233659744
epoch£º519	 i:4 	 global-step:10384	 l-p:0.05578702688217163
epoch£º519	 i:5 	 global-step:10385	 l-p:0.056367795914411545
epoch£º519	 i:6 	 global-step:10386	 l-p:0.05576280504465103
epoch£º519	 i:7 	 global-step:10387	 l-p:0.05586857721209526
epoch£º519	 i:8 	 global-step:10388	 l-p:0.05585508793592453
epoch£º519	 i:9 	 global-step:10389	 l-p:0.05579933524131775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7248, 27.7248, 27.7248],
        [27.7248, 36.6953, 42.4458],
        [27.7248, 27.7248, 27.7248],
        [27.7248, 27.7742, 27.7287]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.05599292740225792 
model_pd.l_d.mean(): -2.338714875804726e-05 
model_pd.lagr.mean(): 0.055969540029764175 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.5408e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5833], device='cuda:0')), ('power', tensor([-0.3669], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.05599292740225792
epoch£º520	 i:1 	 global-step:10401	 l-p:0.05598355084657669
epoch£º520	 i:2 	 global-step:10402	 l-p:0.05577830970287323
epoch£º520	 i:3 	 global-step:10403	 l-p:0.05585077032446861
epoch£º520	 i:4 	 global-step:10404	 l-p:0.055977657437324524
epoch£º520	 i:5 	 global-step:10405	 l-p:0.055759601294994354
epoch£º520	 i:6 	 global-step:10406	 l-p:0.05588589608669281
epoch£º520	 i:7 	 global-step:10407	 l-p:0.05576368048787117
epoch£º520	 i:8 	 global-step:10408	 l-p:0.055783823132514954
epoch£º520	 i:9 	 global-step:10409	 l-p:0.05575789511203766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7545, 29.9805, 29.6391],
        [27.7545, 27.9637, 27.7949],
        [27.7545, 37.3193, 43.8234],
        [27.7545, 29.4771, 29.0083]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.0557485856115818 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0557485856115818 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6314], device='cuda:0')), ('power', tensor([-0.4712], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.0557485856115818
epoch£º521	 i:1 	 global-step:10421	 l-p:0.05585082992911339
epoch£º521	 i:2 	 global-step:10422	 l-p:0.05576007068157196
epoch£º521	 i:3 	 global-step:10423	 l-p:0.055827267467975616
epoch£º521	 i:4 	 global-step:10424	 l-p:0.05581861361861229
epoch£º521	 i:5 	 global-step:10425	 l-p:0.05605379119515419
epoch£º521	 i:6 	 global-step:10426	 l-p:0.05586904287338257
epoch£º521	 i:7 	 global-step:10427	 l-p:0.05597985163331032
epoch£º521	 i:8 	 global-step:10428	 l-p:0.05573742836713791
epoch£º521	 i:9 	 global-step:10429	 l-p:0.05577855929732323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8186, 27.8420, 27.8198],
        [27.8186, 27.8186, 27.8185],
        [27.8186, 27.8753, 27.8235],
        [27.8186, 27.8827, 27.8246]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.05573319271206856 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05573319271206856 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6281], device='cuda:0')), ('power', tensor([-0.3702], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.05573319271206856
epoch£º522	 i:1 	 global-step:10441	 l-p:0.055889442563056946
epoch£º522	 i:2 	 global-step:10442	 l-p:0.0557527057826519
epoch£º522	 i:3 	 global-step:10443	 l-p:0.055818069726228714
epoch£º522	 i:4 	 global-step:10444	 l-p:0.05578672140836716
epoch£º522	 i:5 	 global-step:10445	 l-p:0.055949270725250244
epoch£º522	 i:6 	 global-step:10446	 l-p:0.0557759664952755
epoch£º522	 i:7 	 global-step:10447	 l-p:0.05589300021529198
epoch£º522	 i:8 	 global-step:10448	 l-p:0.055905260145664215
epoch£º522	 i:9 	 global-step:10449	 l-p:0.05575861036777496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8954, 28.0236, 27.9136],
        [27.8954, 27.8954, 27.8954],
        [27.8954, 37.5109, 44.0497],
        [27.8954, 27.8965, 27.8954]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.05609432980418205 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05609432980418205 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.0078e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4756], device='cuda:0')), ('power', tensor([0.0202], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.05609432980418205
epoch£º523	 i:1 	 global-step:10461	 l-p:0.05572891980409622
epoch£º523	 i:2 	 global-step:10462	 l-p:0.05598031356930733
epoch£º523	 i:3 	 global-step:10463	 l-p:0.055729568004608154
epoch£º523	 i:4 	 global-step:10464	 l-p:0.05571376159787178
epoch£º523	 i:5 	 global-step:10465	 l-p:0.05574509873986244
epoch£º523	 i:6 	 global-step:10466	 l-p:0.05573780834674835
epoch£º523	 i:7 	 global-step:10467	 l-p:0.05574119836091995
epoch£º523	 i:8 	 global-step:10468	 l-p:0.055716559290885925
epoch£º523	 i:9 	 global-step:10469	 l-p:0.05589547008275986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9774, 28.0527, 27.9852],
        [27.9774, 35.4423, 39.3484],
        [27.9774, 27.9793, 27.9775],
        [27.9774, 28.0486, 27.9845]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.05575241521000862 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575241521000862 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6059], device='cuda:0')), ('power', tensor([-0.1648], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.05575241521000862
epoch£º524	 i:1 	 global-step:10481	 l-p:0.05570434406399727
epoch£º524	 i:2 	 global-step:10482	 l-p:0.05569913610816002
epoch£º524	 i:3 	 global-step:10483	 l-p:0.0557202510535717
epoch£º524	 i:4 	 global-step:10484	 l-p:0.055815860629081726
epoch£º524	 i:5 	 global-step:10485	 l-p:0.05598818138241768
epoch£º524	 i:6 	 global-step:10486	 l-p:0.0560297854244709
epoch£º524	 i:7 	 global-step:10487	 l-p:0.05574005842208862
epoch£º524	 i:8 	 global-step:10488	 l-p:0.05574730411171913
epoch£º524	 i:9 	 global-step:10489	 l-p:0.05569843947887421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0607, 28.0754, 28.0613],
        [28.0607, 28.0608, 28.0607],
        [28.0607, 28.0809, 28.0616],
        [28.0607, 30.0350, 29.6087]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.055835895240306854 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055835895240306854 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5857], device='cuda:0')), ('power', tensor([-0.0097], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.055835895240306854
epoch£º525	 i:1 	 global-step:10501	 l-p:0.05566918104887009
epoch£º525	 i:2 	 global-step:10502	 l-p:0.05574001371860504
epoch£º525	 i:3 	 global-step:10503	 l-p:0.05575348436832428
epoch£º525	 i:4 	 global-step:10504	 l-p:0.05585682392120361
epoch£º525	 i:5 	 global-step:10505	 l-p:0.055753666907548904
epoch£º525	 i:6 	 global-step:10506	 l-p:0.055671826004981995
epoch£º525	 i:7 	 global-step:10507	 l-p:0.055958978831768036
epoch£º525	 i:8 	 global-step:10508	 l-p:0.055764589458703995
epoch£º525	 i:9 	 global-step:10509	 l-p:0.05570419132709503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1448, 28.2192, 28.1524],
        [28.1448, 29.5986, 29.0906],
        [28.1448, 33.8492, 35.8946],
        [28.1448, 34.5618, 37.3124]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.05566772446036339 
model_pd.l_d.mean(): -7.973045512699173e-08 
model_pd.lagr.mean(): 0.05566764622926712 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.0078e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6346], device='cuda:0')), ('power', tensor([-0.0199], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.05566772446036339
epoch£º526	 i:1 	 global-step:10521	 l-p:0.055691540241241455
epoch£º526	 i:2 	 global-step:10522	 l-p:0.05585774406790733
epoch£º526	 i:3 	 global-step:10523	 l-p:0.056025877594947815
epoch£º526	 i:4 	 global-step:10524	 l-p:0.0556635782122612
epoch£º526	 i:5 	 global-step:10525	 l-p:0.05586985498666763
epoch£º526	 i:6 	 global-step:10526	 l-p:0.05566250532865524
epoch£º526	 i:7 	 global-step:10527	 l-p:0.05564383417367935
epoch£º526	 i:8 	 global-step:10528	 l-p:0.055764198303222656
epoch£º526	 i:9 	 global-step:10529	 l-p:0.055675625801086426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2277, 28.2277, 28.2277],
        [28.2277, 31.0835, 30.9818],
        [28.2277, 28.2277, 28.2277],
        [28.2277, 28.2317, 28.2278]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.05566936358809471 
model_pd.l_d.mean(): 8.057924105742131e-07 
model_pd.lagr.mean(): 0.05567016825079918 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.2628e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6338], device='cuda:0')), ('power', tensor([0.0257], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.05566936358809471
epoch£º527	 i:1 	 global-step:10541	 l-p:0.05588572472333908
epoch£º527	 i:2 	 global-step:10542	 l-p:0.05562959611415863
epoch£º527	 i:3 	 global-step:10543	 l-p:0.05565718933939934
epoch£º527	 i:4 	 global-step:10544	 l-p:0.05587645247578621
epoch£º527	 i:5 	 global-step:10545	 l-p:0.05576986074447632
epoch£º527	 i:6 	 global-step:10546	 l-p:0.055645592510700226
epoch£º527	 i:7 	 global-step:10547	 l-p:0.05567529425024986
epoch£º527	 i:8 	 global-step:10548	 l-p:0.05589897185564041
epoch£º527	 i:9 	 global-step:10549	 l-p:0.05563758313655853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3022, 28.6654, 28.4000],
        [28.3022, 28.3456, 28.3053],
        [28.3022, 36.1382, 40.4116],
        [28.3022, 36.1394, 40.4142]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.05575310066342354 
model_pd.l_d.mean(): 2.3537955712527037e-05 
model_pd.lagr.mean(): 0.05577663704752922 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5766], device='cuda:0')), ('power', tensor([0.2396], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:0.05575310066342354
epoch£º528	 i:1 	 global-step:10561	 l-p:0.05569469928741455
epoch£º528	 i:2 	 global-step:10562	 l-p:0.05565386638045311
epoch£º528	 i:3 	 global-step:10563	 l-p:0.05574259161949158
epoch£º528	 i:4 	 global-step:10564	 l-p:0.05588019639253616
epoch£º528	 i:5 	 global-step:10565	 l-p:0.055744774639606476
epoch£º528	 i:6 	 global-step:10566	 l-p:0.05569308251142502
epoch£º528	 i:7 	 global-step:10567	 l-p:0.05565563961863518
epoch£º528	 i:8 	 global-step:10568	 l-p:0.055774640291929245
epoch£º528	 i:9 	 global-step:10569	 l-p:0.055607374757528305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[28.3561, 31.4377, 31.4452],
        [28.3561, 32.8332, 33.7821],
        [28.3561, 29.0780, 28.6568],
        [28.3561, 35.5747, 39.1447]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.05562186241149902 
model_pd.l_d.mean(): 3.515080607030541e-05 
model_pd.lagr.mean(): 0.05565701425075531 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6451], device='cuda:0')), ('power', tensor([0.1775], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.05562186241149902
epoch£º529	 i:1 	 global-step:10581	 l-p:0.05594039708375931
epoch£º529	 i:2 	 global-step:10582	 l-p:0.05572005733847618
epoch£º529	 i:3 	 global-step:10583	 l-p:0.055664580315351486
epoch£º529	 i:4 	 global-step:10584	 l-p:0.05573186278343201
epoch£º529	 i:5 	 global-step:10585	 l-p:0.05564896762371063
epoch£º529	 i:6 	 global-step:10586	 l-p:0.05567757412791252
epoch£º529	 i:7 	 global-step:10587	 l-p:0.05562622845172882
epoch£º529	 i:8 	 global-step:10588	 l-p:0.055821504443883896
epoch£º529	 i:9 	 global-step:10589	 l-p:0.05565953999757767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3769, 28.3769, 28.3769],
        [28.3769, 30.6556, 30.3063],
        [28.3769, 28.3769, 28.3769],
        [28.3769, 34.6151, 37.1518]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.055658094584941864 
model_pd.l_d.mean(): 7.522823580075055e-05 
model_pd.lagr.mean(): 0.055733323097229004 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6177], device='cuda:0')), ('power', tensor([0.2368], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.055658094584941864
epoch£º530	 i:1 	 global-step:10601	 l-p:0.055659111589193344
epoch£º530	 i:2 	 global-step:10602	 l-p:0.05576776713132858
epoch£º530	 i:3 	 global-step:10603	 l-p:0.05567164719104767
epoch£º530	 i:4 	 global-step:10604	 l-p:0.055605556815862656
epoch£º530	 i:5 	 global-step:10605	 l-p:0.05570650473237038
epoch£º530	 i:6 	 global-step:10606	 l-p:0.055743150413036346
epoch£º530	 i:7 	 global-step:10607	 l-p:0.05572396144270897
epoch£º530	 i:8 	 global-step:10608	 l-p:0.05591583997011185
epoch£º530	 i:9 	 global-step:10609	 l-p:0.05565548315644264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3566, 28.3895, 28.3586],
        [28.3566, 28.3566, 28.3566],
        [28.3566, 28.3566, 28.3566],
        [28.3566, 28.3566, 28.3566]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.05586335062980652 
model_pd.l_d.mean(): 0.00012257539492566139 
model_pd.lagr.mean(): 0.05598592758178711 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6014], device='cuda:0')), ('power', tensor([0.2795], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.05586335062980652
epoch£º531	 i:1 	 global-step:10621	 l-p:0.05565723404288292
epoch£º531	 i:2 	 global-step:10622	 l-p:0.05579227954149246
epoch£º531	 i:3 	 global-step:10623	 l-p:0.05567125231027603
epoch£º531	 i:4 	 global-step:10624	 l-p:0.05586559697985649
epoch£º531	 i:5 	 global-step:10625	 l-p:0.05564357340335846
epoch£º531	 i:6 	 global-step:10626	 l-p:0.055642347782850266
epoch£º531	 i:7 	 global-step:10627	 l-p:0.05562586709856987
epoch£º531	 i:8 	 global-step:10628	 l-p:0.05569666624069214
epoch£º531	 i:9 	 global-step:10629	 l-p:0.05573516711592674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2920, 33.1994, 34.5082],
        [28.2920, 28.6724, 28.3974],
        [28.2920, 33.9547, 35.9424],
        [28.2920, 28.3098, 28.2928]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.05586986988782883 
model_pd.l_d.mean(): 0.00013830287207383662 
model_pd.lagr.mean(): 0.056008171290159225 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5925], device='cuda:0')), ('power', tensor([0.2563], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.05586986988782883
epoch£º532	 i:1 	 global-step:10641	 l-p:0.05596506968140602
epoch£º532	 i:2 	 global-step:10642	 l-p:0.05563383921980858
epoch£º532	 i:3 	 global-step:10643	 l-p:0.055642470717430115
epoch£º532	 i:4 	 global-step:10644	 l-p:0.05564626678824425
epoch£º532	 i:5 	 global-step:10645	 l-p:0.05573165789246559
epoch£º532	 i:6 	 global-step:10646	 l-p:0.055838748812675476
epoch£º532	 i:7 	 global-step:10647	 l-p:0.055682361125946045
epoch£º532	 i:8 	 global-step:10648	 l-p:0.055697962641716
epoch£º532	 i:9 	 global-step:10649	 l-p:0.05566893890500069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1861, 35.7093, 39.6461],
        [28.1861, 28.2066, 28.1871],
        [28.1861, 28.3661, 28.2174],
        [28.1861, 30.0659, 29.6138]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.05582571029663086 
model_pd.l_d.mean(): 5.907589729758911e-05 
model_pd.lagr.mean(): 0.05588478595018387 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6086], device='cuda:0')), ('power', tensor([0.0986], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.05582571029663086
epoch£º533	 i:1 	 global-step:10661	 l-p:0.055685196071863174
epoch£º533	 i:2 	 global-step:10662	 l-p:0.055636584758758545
epoch£º533	 i:3 	 global-step:10663	 l-p:0.05568462237715721
epoch£º533	 i:4 	 global-step:10664	 l-p:0.05588611587882042
epoch£º533	 i:5 	 global-step:10665	 l-p:0.0557510107755661
epoch£º533	 i:6 	 global-step:10666	 l-p:0.055987704545259476
epoch£º533	 i:7 	 global-step:10667	 l-p:0.05578063800930977
epoch£º533	 i:8 	 global-step:10668	 l-p:0.055715177208185196
epoch£º533	 i:9 	 global-step:10669	 l-p:0.05569064989686012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0539, 29.8149, 29.3440],
        [28.0539, 31.7108, 32.0868],
        [28.0539, 28.1064, 28.0582],
        [28.0539, 30.4003, 30.0883]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.05576517432928085 
model_pd.l_d.mean(): -8.804217031865846e-06 
model_pd.lagr.mean(): 0.05575637146830559 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5690], device='cuda:0')), ('power', tensor([-0.0147], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.05576517432928085
epoch£º534	 i:1 	 global-step:10681	 l-p:0.05569838359951973
epoch£º534	 i:2 	 global-step:10682	 l-p:0.05585726723074913
epoch£º534	 i:3 	 global-step:10683	 l-p:0.0556822344660759
epoch£º534	 i:4 	 global-step:10684	 l-p:0.05587870627641678
epoch£º534	 i:5 	 global-step:10685	 l-p:0.05573033168911934
epoch£º534	 i:6 	 global-step:10686	 l-p:0.05578560754656792
epoch£º534	 i:7 	 global-step:10687	 l-p:0.05597655102610588
epoch£º534	 i:8 	 global-step:10688	 l-p:0.05569782480597496
epoch£º534	 i:9 	 global-step:10689	 l-p:0.05587838217616081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9153, 27.9166, 27.9153],
        [27.9153, 28.0588, 27.9371],
        [27.9153, 27.9155, 27.9153],
        [27.9153, 28.2628, 28.0071]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.055765360593795776 
model_pd.l_d.mean(): -0.00015034872922115028 
model_pd.lagr.mean(): 0.05561501160264015 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5971], device='cuda:0')), ('power', tensor([-0.2827], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.055765360593795776
epoch£º535	 i:1 	 global-step:10701	 l-p:0.055751197040081024
epoch£º535	 i:2 	 global-step:10702	 l-p:0.056027572602033615
epoch£º535	 i:3 	 global-step:10703	 l-p:0.05571503937244415
epoch£º535	 i:4 	 global-step:10704	 l-p:0.05570041015744209
epoch£º535	 i:5 	 global-step:10705	 l-p:0.055898766964673996
epoch£º535	 i:6 	 global-step:10706	 l-p:0.0557958148419857
epoch£º535	 i:7 	 global-step:10707	 l-p:0.05590023100376129
epoch£º535	 i:8 	 global-step:10708	 l-p:0.05590628460049629
epoch£º535	 i:9 	 global-step:10709	 l-p:0.05578874796628952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7947, 27.8654, 27.8018],
        [27.7947, 27.8282, 27.7969],
        [27.7947, 27.8589, 27.8008],
        [27.7947, 33.8796, 36.3422]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.055935852229595184 
model_pd.l_d.mean(): -9.37421282287687e-05 
model_pd.lagr.mean(): 0.05584210902452469 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5660], device='cuda:0')), ('power', tensor([-0.2352], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.055935852229595184
epoch£º536	 i:1 	 global-step:10721	 l-p:0.05578603595495224
epoch£º536	 i:2 	 global-step:10722	 l-p:0.05575896054506302
epoch£º536	 i:3 	 global-step:10723	 l-p:0.05574064701795578
epoch£º536	 i:4 	 global-step:10724	 l-p:0.05582847446203232
epoch£º536	 i:5 	 global-step:10725	 l-p:0.0559193454682827
epoch£º536	 i:6 	 global-step:10726	 l-p:0.05579762160778046
epoch£º536	 i:7 	 global-step:10727	 l-p:0.05611056089401245
epoch£º536	 i:8 	 global-step:10728	 l-p:0.055760495364665985
epoch£º536	 i:9 	 global-step:10729	 l-p:0.055848244577646255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7155, 27.7157, 27.7154],
        [27.7155, 29.7075, 29.2977],
        [27.7155, 29.5771, 29.1361],
        [27.7155, 27.7154, 27.7154]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.055746596306562424 
model_pd.l_d.mean(): -0.000122129509691149 
model_pd.lagr.mean(): 0.05562446638941765 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6340], device='cuda:0')), ('power', tensor([-0.5707], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.055746596306562424
epoch£º537	 i:1 	 global-step:10741	 l-p:0.055925678461790085
epoch£º537	 i:2 	 global-step:10742	 l-p:0.05587748438119888
epoch£º537	 i:3 	 global-step:10743	 l-p:0.055812399834394455
epoch£º537	 i:4 	 global-step:10744	 l-p:0.05584577098488808
epoch£º537	 i:5 	 global-step:10745	 l-p:0.05595792084932327
epoch£º537	 i:6 	 global-step:10746	 l-p:0.055832307785749435
epoch£º537	 i:7 	 global-step:10747	 l-p:0.055803000926971436
epoch£º537	 i:8 	 global-step:10748	 l-p:0.05601062625646591
epoch£º537	 i:9 	 global-step:10749	 l-p:0.05579575523734093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6978, 27.9396, 27.7489],
        [27.6978, 27.7009, 27.6978],
        [27.6978, 27.6980, 27.6978],
        [27.6978, 28.1228, 27.8261]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.05576693266630173 
model_pd.l_d.mean(): -1.6317435438395478e-06 
model_pd.lagr.mean(): 0.055765300989151 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6178], device='cuda:0')), ('power', tensor([-0.5120], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.05576693266630173
epoch£º538	 i:1 	 global-step:10761	 l-p:0.055793873965740204
epoch£º538	 i:2 	 global-step:10762	 l-p:0.055759966373443604
epoch£º538	 i:3 	 global-step:10763	 l-p:0.0558018758893013
epoch£º538	 i:4 	 global-step:10764	 l-p:0.055936068296432495
epoch£º538	 i:5 	 global-step:10765	 l-p:0.05581662431359291
epoch£º538	 i:6 	 global-step:10766	 l-p:0.05579600855708122
epoch£º538	 i:7 	 global-step:10767	 l-p:0.05574968084692955
epoch£º538	 i:8 	 global-step:10768	 l-p:0.05620640516281128
epoch£º538	 i:9 	 global-step:10769	 l-p:0.05595044046640396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7429, 28.5656, 28.1208],
        [27.7429, 27.7431, 27.7429],
        [27.7429, 27.8293, 27.7526],
        [27.7429, 27.7434, 27.7429]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.055768050253391266 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055768050253391266 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5995], device='cuda:0')), ('power', tensor([-0.4331], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.055768050253391266
epoch£º539	 i:1 	 global-step:10781	 l-p:0.055758535861968994
epoch£º539	 i:2 	 global-step:10782	 l-p:0.055807966738939285
epoch£º539	 i:3 	 global-step:10783	 l-p:0.05575324967503548
epoch£º539	 i:4 	 global-step:10784	 l-p:0.055993590503931046
epoch£º539	 i:5 	 global-step:10785	 l-p:0.05583649501204491
epoch£º539	 i:6 	 global-step:10786	 l-p:0.05579322576522827
epoch£º539	 i:7 	 global-step:10787	 l-p:0.05602739378809929
epoch£º539	 i:8 	 global-step:10788	 l-p:0.05586151033639908
epoch£º539	 i:9 	 global-step:10789	 l-p:0.05583776906132698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8181, 31.0613, 31.1945],
        [27.8181, 27.8181, 27.8181],
        [27.8181, 29.0395, 28.5368],
        [27.8181, 33.9275, 36.4114]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.055917784571647644 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055917784571647644 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5095], device='cuda:0')), ('power', tensor([-0.1622], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.055917784571647644
epoch£º540	 i:1 	 global-step:10801	 l-p:0.05578093230724335
epoch£º540	 i:2 	 global-step:10802	 l-p:0.05596890300512314
epoch£º540	 i:3 	 global-step:10803	 l-p:0.055734679102897644
epoch£º540	 i:4 	 global-step:10804	 l-p:0.055759649723768234
epoch£º540	 i:5 	 global-step:10805	 l-p:0.055713146924972534
epoch£º540	 i:6 	 global-step:10806	 l-p:0.05591593682765961
epoch£º540	 i:7 	 global-step:10807	 l-p:0.05588217452168465
epoch£º540	 i:8 	 global-step:10808	 l-p:0.05585436522960663
epoch£º540	 i:9 	 global-step:10809	 l-p:0.05572665110230446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9042, 28.1775, 27.9662],
        [27.9042, 28.1145, 27.9447],
        [27.9042, 36.9613, 42.7834],
        [27.9042, 27.9042, 27.9042]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.05599634349346161 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05599634349346161 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5562], device='cuda:0')), ('power', tensor([-0.1699], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.05599634349346161
epoch£º541	 i:1 	 global-step:10821	 l-p:0.055763620883226395
epoch£º541	 i:2 	 global-step:10822	 l-p:0.05570389702916145
epoch£º541	 i:3 	 global-step:10823	 l-p:0.05606367811560631
epoch£º541	 i:4 	 global-step:10824	 l-p:0.0557267926633358
epoch£º541	 i:5 	 global-step:10825	 l-p:0.05576995387673378
epoch£º541	 i:6 	 global-step:10826	 l-p:0.05577559769153595
epoch£º541	 i:7 	 global-step:10827	 l-p:0.05583101511001587
epoch£º541	 i:8 	 global-step:10828	 l-p:0.055707234889268875
epoch£º541	 i:9 	 global-step:10829	 l-p:0.05571727454662323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9941, 27.9941, 27.9941],
        [27.9941, 30.8989, 30.8351],
        [27.9941, 38.3578, 45.8664],
        [27.9941, 31.6428, 32.0180]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.056068018078804016 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056068018078804016 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.3404e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5164], device='cuda:0')), ('power', tensor([0.0268], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.056068018078804016
epoch£º542	 i:1 	 global-step:10841	 l-p:0.05570578575134277
epoch£º542	 i:2 	 global-step:10842	 l-p:0.05580967664718628
epoch£º542	 i:3 	 global-step:10843	 l-p:0.0557321161031723
epoch£º542	 i:4 	 global-step:10844	 l-p:0.05592777952551842
epoch£º542	 i:5 	 global-step:10845	 l-p:0.055704645812511444
epoch£º542	 i:6 	 global-step:10846	 l-p:0.055662572383880615
epoch£º542	 i:7 	 global-step:10847	 l-p:0.05577559769153595
epoch£º542	 i:8 	 global-step:10848	 l-p:0.05573936924338341
epoch£º542	 i:9 	 global-step:10849	 l-p:0.055726196616888046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0853, 31.6382, 31.9438],
        [28.0853, 38.0615, 45.0304],
        [28.0853, 28.7639, 28.3588],
        [28.0853, 28.0933, 28.0856]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.05576320365071297 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576320365071297 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5951], device='cuda:0')), ('power', tensor([-0.0403], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.05576320365071297
epoch£º543	 i:1 	 global-step:10861	 l-p:0.05568131431937218
epoch£º543	 i:2 	 global-step:10862	 l-p:0.05586591735482216
epoch£º543	 i:3 	 global-step:10863	 l-p:0.05578075349330902
epoch£º543	 i:4 	 global-step:10864	 l-p:0.05589788407087326
epoch£º543	 i:5 	 global-step:10865	 l-p:0.05568094924092293
epoch£º543	 i:6 	 global-step:10866	 l-p:0.055680591613054276
epoch£º543	 i:7 	 global-step:10867	 l-p:0.0556756965816021
epoch£º543	 i:8 	 global-step:10868	 l-p:0.055972613394260406
epoch£º543	 i:9 	 global-step:10869	 l-p:0.0556478314101696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1763, 28.1831, 28.1765],
        [28.1763, 32.1869, 32.7986],
        [28.1763, 28.1763, 28.1763],
        [28.1763, 36.5711, 41.5247]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.055680517107248306 
model_pd.l_d.mean(): -2.900498827784759e-07 
model_pd.lagr.mean(): 0.055680226534605026 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.5375e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6382], device='cuda:0')), ('power', tensor([-0.0350], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.055680517107248306
epoch£º544	 i:1 	 global-step:10881	 l-p:0.0558619350194931
epoch£º544	 i:2 	 global-step:10882	 l-p:0.05585553124547005
epoch£º544	 i:3 	 global-step:10883	 l-p:0.05574466660618782
epoch£º544	 i:4 	 global-step:10884	 l-p:0.05565745383501053
epoch£º544	 i:5 	 global-step:10885	 l-p:0.05563104525208473
epoch£º544	 i:6 	 global-step:10886	 l-p:0.05572652816772461
epoch£º544	 i:7 	 global-step:10887	 l-p:0.05587092414498329
epoch£º544	 i:8 	 global-step:10888	 l-p:0.05575302988290787
epoch£º544	 i:9 	 global-step:10889	 l-p:0.055665042251348495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2648, 28.3414, 28.2727],
        [28.2648, 28.6173, 28.3580],
        [28.2648, 29.7399, 29.2304],
        [28.2648, 29.6770, 29.1647]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.05563289672136307 
model_pd.l_d.mean(): 3.4767278975778027e-06 
model_pd.lagr.mean(): 0.05563637241721153 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.5882e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6550], device='cuda:0')), ('power', tensor([0.0661], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.05563289672136307
epoch£º545	 i:1 	 global-step:10901	 l-p:0.05577678605914116
epoch£º545	 i:2 	 global-step:10902	 l-p:0.056190457195043564
epoch£º545	 i:3 	 global-step:10903	 l-p:0.055692099034786224
epoch£º545	 i:4 	 global-step:10904	 l-p:0.05561384931206703
epoch£º545	 i:5 	 global-step:10905	 l-p:0.05568240210413933
epoch£º545	 i:6 	 global-step:10906	 l-p:0.05566917359828949
epoch£º545	 i:7 	 global-step:10907	 l-p:0.05564816668629646
epoch£º545	 i:8 	 global-step:10908	 l-p:0.05568913370370865
epoch£º545	 i:9 	 global-step:10909	 l-p:0.05566846579313278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3390, 28.5294, 28.3732],
        [28.3390, 28.5201, 28.3705],
        [28.3390, 28.3399, 28.3390],
        [28.3390, 31.1579, 31.0320]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.05582651123404503 
model_pd.l_d.mean(): 2.4319075237144716e-05 
model_pd.lagr.mean(): 0.05585082992911339 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6418], device='cuda:0')), ('power', tensor([0.1759], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.05582651123404503
epoch£º546	 i:1 	 global-step:10921	 l-p:0.05563083663582802
epoch£º546	 i:2 	 global-step:10922	 l-p:0.05581684783101082
epoch£º546	 i:3 	 global-step:10923	 l-p:0.05572188273072243
epoch£º546	 i:4 	 global-step:10924	 l-p:0.05574924871325493
epoch£º546	 i:5 	 global-step:10925	 l-p:0.055651940405368805
epoch£º546	 i:6 	 global-step:10926	 l-p:0.05563316121697426
epoch£º546	 i:7 	 global-step:10927	 l-p:0.0556119866669178
epoch£º546	 i:8 	 global-step:10928	 l-p:0.05583224073052406
epoch£º546	 i:9 	 global-step:10929	 l-p:0.05565192177891731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3843, 29.3766, 28.8899],
        [28.3843, 37.7550, 43.8729],
        [28.3843, 28.3904, 28.3845],
        [28.3843, 28.3843, 28.3843]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.05563392862677574 
model_pd.l_d.mean(): 5.007525396649726e-05 
model_pd.lagr.mean(): 0.05568400397896767 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6344], device='cuda:0')), ('power', tensor([0.1966], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.05563392862677574
epoch£º547	 i:1 	 global-step:10941	 l-p:0.055806856602430344
epoch£º547	 i:2 	 global-step:10942	 l-p:0.0557992197573185
epoch£º547	 i:3 	 global-step:10943	 l-p:0.05588756501674652
epoch£º547	 i:4 	 global-step:10944	 l-p:0.055661898106336594
epoch£º547	 i:5 	 global-step:10945	 l-p:0.055666498839855194
epoch£º547	 i:6 	 global-step:10946	 l-p:0.055721741169691086
epoch£º547	 i:7 	 global-step:10947	 l-p:0.05561414733529091
epoch£º547	 i:8 	 global-step:10948	 l-p:0.055683497339487076
epoch£º547	 i:9 	 global-step:10949	 l-p:0.055591292679309845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3869, 28.3883, 28.3870],
        [28.3869, 29.2367, 28.7793],
        [28.3869, 28.6139, 28.4323],
        [28.3869, 28.7408, 28.4804]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.05581288039684296 
model_pd.l_d.mean(): 0.00012925080955028534 
model_pd.lagr.mean(): 0.05594213306903839 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5716], device='cuda:0')), ('power', tensor([0.3360], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.05581288039684296
epoch£º548	 i:1 	 global-step:10961	 l-p:0.055602774024009705
epoch£º548	 i:2 	 global-step:10962	 l-p:0.05566609278321266
epoch£º548	 i:3 	 global-step:10963	 l-p:0.055671460926532745
epoch£º548	 i:4 	 global-step:10964	 l-p:0.055688731372356415
epoch£º548	 i:5 	 global-step:10965	 l-p:0.05574337765574455
epoch£º548	 i:6 	 global-step:10966	 l-p:0.055991239845752716
epoch£º548	 i:7 	 global-step:10967	 l-p:0.05562666431069374
epoch£º548	 i:8 	 global-step:10968	 l-p:0.05565517395734787
epoch£º548	 i:9 	 global-step:10969	 l-p:0.055652812123298645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3383, 36.6501, 41.4740],
        [28.3383, 28.3696, 28.3402],
        [28.3383, 28.3884, 28.3423],
        [28.3383, 30.2034, 29.7435]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.05564543604850769 
model_pd.l_d.mean(): 6.206164107425138e-05 
model_pd.lagr.mean(): 0.055707499384880066 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6418], device='cuda:0')), ('power', tensor([0.1230], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.05564543604850769
epoch£º549	 i:1 	 global-step:10981	 l-p:0.05578875541687012
epoch£º549	 i:2 	 global-step:10982	 l-p:0.05566295608878136
epoch£º549	 i:3 	 global-step:10983	 l-p:0.05563019588589668
epoch£º549	 i:4 	 global-step:10984	 l-p:0.05571028217673302
epoch£º549	 i:5 	 global-step:10985	 l-p:0.05583980306982994
epoch£º549	 i:6 	 global-step:10986	 l-p:0.055661123245954514
epoch£º549	 i:7 	 global-step:10987	 l-p:0.055607087910175323
epoch£º549	 i:8 	 global-step:10988	 l-p:0.055653542280197144
epoch£º549	 i:9 	 global-step:10989	 l-p:0.05606918781995773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2407, 28.4795, 28.2901],
        [28.2407, 31.1949, 31.1420],
        [28.2407, 30.4958, 30.1442],
        [28.2407, 28.6680, 28.3684]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.056012436747550964 
model_pd.l_d.mean(): 0.00018421934510115534 
model_pd.lagr.mean(): 0.056196656078100204 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5317], device='cuda:0')), ('power', tensor([0.3127], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.056012436747550964
epoch£º550	 i:1 	 global-step:11001	 l-p:0.05571459233760834
epoch£º550	 i:2 	 global-step:11002	 l-p:0.05593178793787956
epoch£º550	 i:3 	 global-step:11003	 l-p:0.055664531886577606
epoch£º550	 i:4 	 global-step:11004	 l-p:0.05573393404483795
epoch£º550	 i:5 	 global-step:11005	 l-p:0.055695418268442154
epoch£º550	 i:6 	 global-step:11006	 l-p:0.05571562796831131
epoch£º550	 i:7 	 global-step:11007	 l-p:0.05565020814538002
epoch£º550	 i:8 	 global-step:11008	 l-p:0.0556497797369957
epoch£º550	 i:9 	 global-step:11009	 l-p:0.05575099214911461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1048, 29.2529, 28.7504],
        [28.1048, 34.2803, 36.7913],
        [28.1048, 38.0344, 44.9372],
        [28.1048, 28.1048, 28.1048]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.05609843134880066 
model_pd.l_d.mean(): 0.00011617896961979568 
model_pd.lagr.mean(): 0.05621461197733879 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5016], device='cuda:0')), ('power', tensor([0.1884], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.05609843134880066
epoch£º551	 i:1 	 global-step:11021	 l-p:0.055849239230155945
epoch£º551	 i:2 	 global-step:11022	 l-p:0.05567449331283569
epoch£º551	 i:3 	 global-step:11023	 l-p:0.055727213621139526
epoch£º551	 i:4 	 global-step:11024	 l-p:0.0558081679046154
epoch£º551	 i:5 	 global-step:11025	 l-p:0.055671680718660355
epoch£º551	 i:6 	 global-step:11026	 l-p:0.055740803480148315
epoch£º551	 i:7 	 global-step:11027	 l-p:0.05571551248431206
epoch£º551	 i:8 	 global-step:11028	 l-p:0.0558621808886528
epoch£º551	 i:9 	 global-step:11029	 l-p:0.05569879710674286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9498, 30.9200, 30.8926],
        [27.9498, 27.9499, 27.9498],
        [27.9498, 27.9498, 27.9497],
        [27.9498, 27.9497, 27.9497]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.055807072669267654 
model_pd.l_d.mean(): -0.00010057386680273339 
model_pd.lagr.mean(): 0.05570649728178978 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5649], device='cuda:0')), ('power', tensor([-0.1761], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.055807072669267654
epoch£º552	 i:1 	 global-step:11041	 l-p:0.055903881788253784
epoch£º552	 i:2 	 global-step:11042	 l-p:0.05575886741280556
epoch£º552	 i:3 	 global-step:11043	 l-p:0.05571212247014046
epoch£º552	 i:4 	 global-step:11044	 l-p:0.05570702999830246
epoch£º552	 i:5 	 global-step:11045	 l-p:0.05579468980431557
epoch£º552	 i:6 	 global-step:11046	 l-p:0.05587945878505707
epoch£º552	 i:7 	 global-step:11047	 l-p:0.055855587124824524
epoch£º552	 i:8 	 global-step:11048	 l-p:0.05601195991039276
epoch£º552	 i:9 	 global-step:11049	 l-p:0.05576551333069801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8043, 27.8043, 27.8043],
        [27.8043, 30.8117, 30.8128],
        [27.8043, 37.3631, 43.8484],
        [27.8043, 28.1281, 27.8863]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.05578746646642685 
model_pd.l_d.mean(): -0.00012704265827778727 
model_pd.lagr.mean(): 0.0556604228913784 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5770], device='cuda:0')), ('power', tensor([-0.2826], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.05578746646642685
epoch£º553	 i:1 	 global-step:11061	 l-p:0.05579269304871559
epoch£º553	 i:2 	 global-step:11062	 l-p:0.05617247521877289
epoch£º553	 i:3 	 global-step:11063	 l-p:0.055767908692359924
epoch£º553	 i:4 	 global-step:11064	 l-p:0.05578702688217163
epoch£º553	 i:5 	 global-step:11065	 l-p:0.05580832436680794
epoch£º553	 i:6 	 global-step:11066	 l-p:0.055721085518598557
epoch£º553	 i:7 	 global-step:11067	 l-p:0.05600068345665932
epoch£º553	 i:8 	 global-step:11068	 l-p:0.055911824107170105
epoch£º553	 i:9 	 global-step:11069	 l-p:0.055741824209690094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6978, 27.8379, 27.7189],
        [27.6978, 27.7030, 27.6979],
        [27.6978, 27.8755, 27.7288],
        [27.6978, 32.0656, 32.9909]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.05588774383068085 
model_pd.l_d.mean(): -0.0001241175486939028 
model_pd.lagr.mean(): 0.05576362460851669 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5900], device='cuda:0')), ('power', tensor([-0.4705], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.05588774383068085
epoch£º554	 i:1 	 global-step:11081	 l-p:0.055827122181653976
epoch£º554	 i:2 	 global-step:11082	 l-p:0.055781636387109756
epoch£º554	 i:3 	 global-step:11083	 l-p:0.05585449934005737
epoch£º554	 i:4 	 global-step:11084	 l-p:0.055791061371564865
epoch£º554	 i:5 	 global-step:11085	 l-p:0.05579965561628342
epoch£º554	 i:6 	 global-step:11086	 l-p:0.05585131421685219
epoch£º554	 i:7 	 global-step:11087	 l-p:0.0558125376701355
epoch£º554	 i:8 	 global-step:11088	 l-p:0.05593717843294144
epoch£º554	 i:9 	 global-step:11089	 l-p:0.05613163858652115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6569, 27.6572, 27.6569],
        [27.6569, 28.3016, 27.9110],
        [27.6569, 27.7383, 27.6658],
        [27.6569, 28.4911, 28.0444]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.055806901305913925 
model_pd.l_d.mean(): -2.047867383225821e-05 
model_pd.lagr.mean(): 0.05578642338514328 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2156e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5977], device='cuda:0')), ('power', tensor([-0.5299], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.055806901305913925
epoch£º555	 i:1 	 global-step:11101	 l-p:0.055887479335069656
epoch£º555	 i:2 	 global-step:11102	 l-p:0.056288283318281174
epoch£º555	 i:3 	 global-step:11103	 l-p:0.05583862215280533
epoch£º555	 i:4 	 global-step:11104	 l-p:0.05580640211701393
epoch£º555	 i:5 	 global-step:11105	 l-p:0.055877551436424255
epoch£º555	 i:6 	 global-step:11106	 l-p:0.055751457810401917
epoch£º555	 i:7 	 global-step:11107	 l-p:0.05584732070565224
epoch£º555	 i:8 	 global-step:11108	 l-p:0.055805500596761703
epoch£º555	 i:9 	 global-step:11109	 l-p:0.05577445030212402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6946, 30.1758, 29.9323],
        [27.6946, 27.6946, 27.6946],
        [27.6946, 33.1636, 35.0427],
        [27.6946, 27.7054, 27.6950]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.05576753616333008 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576753616333008 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6093], device='cuda:0')), ('power', tensor([-0.4820], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.05576753616333008
epoch£º556	 i:1 	 global-step:11121	 l-p:0.05576177313923836
epoch£º556	 i:2 	 global-step:11122	 l-p:0.055809956043958664
epoch£º556	 i:3 	 global-step:11123	 l-p:0.056016623973846436
epoch£º556	 i:4 	 global-step:11124	 l-p:0.05573561042547226
epoch£º556	 i:5 	 global-step:11125	 l-p:0.055729322135448456
epoch£º556	 i:6 	 global-step:11126	 l-p:0.05614403635263443
epoch£º556	 i:7 	 global-step:11127	 l-p:0.055773112922906876
epoch£º556	 i:8 	 global-step:11128	 l-p:0.056001756340265274
epoch£º556	 i:9 	 global-step:11129	 l-p:0.055808644741773605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7712, 27.8352, 27.7772],
        [27.7712, 31.3892, 31.7612],
        [27.7712, 28.2351, 27.9190],
        [27.7712, 33.3583, 35.3391]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.055830344557762146 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055830344557762146 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5613], device='cuda:0')), ('power', tensor([-0.2798], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.055830344557762146
epoch£º557	 i:1 	 global-step:11141	 l-p:0.05609583854675293
epoch£º557	 i:2 	 global-step:11142	 l-p:0.05585543438792229
epoch£º557	 i:3 	 global-step:11143	 l-p:0.055797167122364044
epoch£º557	 i:4 	 global-step:11144	 l-p:0.05575871840119362
epoch£º557	 i:5 	 global-step:11145	 l-p:0.055904321372509
epoch£º557	 i:6 	 global-step:11146	 l-p:0.055725086480379105
epoch£º557	 i:7 	 global-step:11147	 l-p:0.05585834011435509
epoch£º557	 i:8 	 global-step:11148	 l-p:0.055743806064128876
epoch£º557	 i:9 	 global-step:11149	 l-p:0.055787343531847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8629, 30.2043, 29.8988],
        [27.8629, 33.0991, 34.7441],
        [27.8629, 28.0607, 27.8996],
        [27.8629, 27.9041, 27.8659]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.05585753172636032 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05585753172636032 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5871], device='cuda:0')), ('power', tensor([-0.2507], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.05585753172636032
epoch£º558	 i:1 	 global-step:11161	 l-p:0.05575907602906227
epoch£º558	 i:2 	 global-step:11162	 l-p:0.056074827909469604
epoch£º558	 i:3 	 global-step:11163	 l-p:0.05590417608618736
epoch£º558	 i:4 	 global-step:11164	 l-p:0.05576980486512184
epoch£º558	 i:5 	 global-step:11165	 l-p:0.055740922689437866
epoch£º558	 i:6 	 global-step:11166	 l-p:0.05571933463215828
epoch£º558	 i:7 	 global-step:11167	 l-p:0.055827703326940536
epoch£º558	 i:8 	 global-step:11168	 l-p:0.0557527132332325
epoch£º558	 i:9 	 global-step:11169	 l-p:0.05573605000972748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9596, 32.3432, 33.2560],
        [27.9596, 28.0153, 27.9644],
        [27.9596, 27.9597, 27.9596],
        [27.9596, 27.9635, 27.9596]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.05579642578959465 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05579642578959465 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5513], device='cuda:0')), ('power', tensor([-0.0670], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.05579642578959465
epoch£º559	 i:1 	 global-step:11181	 l-p:0.05601813644170761
epoch£º559	 i:2 	 global-step:11182	 l-p:0.05583607032895088
epoch£º559	 i:3 	 global-step:11183	 l-p:0.055696967989206314
epoch£º559	 i:4 	 global-step:11184	 l-p:0.055708691477775574
epoch£º559	 i:5 	 global-step:11185	 l-p:0.055788200348615646
epoch£º559	 i:6 	 global-step:11186	 l-p:0.05567030608654022
epoch£º559	 i:7 	 global-step:11187	 l-p:0.05594814568758011
epoch£º559	 i:8 	 global-step:11188	 l-p:0.055759821087121964
epoch£º559	 i:9 	 global-step:11189	 l-p:0.05569867789745331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[28.0581, 38.3675, 45.7875],
        [28.0581, 31.1907, 31.2449],
        [28.0581, 38.4125, 45.8932],
        [28.0581, 33.5946, 35.4925]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.056017108261585236 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056017108261585236 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.8229e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5277], device='cuda:0')), ('power', tensor([0.0765], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.056017108261585236
epoch£º560	 i:1 	 global-step:11201	 l-p:0.05573112517595291
epoch£º560	 i:2 	 global-step:11202	 l-p:0.05572225898504257
epoch£º560	 i:3 	 global-step:11203	 l-p:0.05574411153793335
epoch£º560	 i:4 	 global-step:11204	 l-p:0.055765315890312195
epoch£º560	 i:5 	 global-step:11205	 l-p:0.05569196119904518
epoch£º560	 i:6 	 global-step:11206	 l-p:0.05568680539727211
epoch£º560	 i:7 	 global-step:11207	 l-p:0.055643051862716675
epoch£º560	 i:8 	 global-step:11208	 l-p:0.0556553453207016
epoch£º560	 i:9 	 global-step:11209	 l-p:0.05604245141148567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1571, 28.1571, 28.1571],
        [28.1571, 31.0575, 30.9819],
        [28.1571, 28.2146, 28.1621],
        [28.1571, 33.8570, 35.8966]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.055837955325841904 
model_pd.l_d.mean(): 3.67551024282875e-07 
model_pd.lagr.mean(): 0.05583832412958145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.0291e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5984], device='cuda:0')), ('power', tensor([0.0460], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.055837955325841904
epoch£º561	 i:1 	 global-step:11221	 l-p:0.056046560406684875
epoch£º561	 i:2 	 global-step:11222	 l-p:0.05566621199250221
epoch£º561	 i:3 	 global-step:11223	 l-p:0.05572183057665825
epoch£º561	 i:4 	 global-step:11224	 l-p:0.055844347923994064
epoch£º561	 i:5 	 global-step:11225	 l-p:0.05566268786787987
epoch£º561	 i:6 	 global-step:11226	 l-p:0.05566065013408661
epoch£º561	 i:7 	 global-step:11227	 l-p:0.05564406141638756
epoch£º561	 i:8 	 global-step:11228	 l-p:0.05569251999258995
epoch£º561	 i:9 	 global-step:11229	 l-p:0.05570535361766815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2535, 28.5304, 28.3164],
        [28.2535, 28.2567, 28.2535],
        [28.2535, 28.2538, 28.2535],
        [28.2535, 32.4035, 33.1107]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.055766865611076355 
model_pd.l_d.mean(): 4.1550470086804125e-06 
model_pd.lagr.mean(): 0.05577101930975914 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.9312e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6093], device='cuda:0')), ('power', tensor([0.0930], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.055766865611076355
epoch£º562	 i:1 	 global-step:11241	 l-p:0.0556284599006176
epoch£º562	 i:2 	 global-step:11242	 l-p:0.05568583682179451
epoch£º562	 i:3 	 global-step:11243	 l-p:0.05579870194196701
epoch£º562	 i:4 	 global-step:11244	 l-p:0.05589676275849342
epoch£º562	 i:5 	 global-step:11245	 l-p:0.05563531816005707
epoch£º562	 i:6 	 global-step:11246	 l-p:0.05573310703039169
epoch£º562	 i:7 	 global-step:11247	 l-p:0.05575593188405037
epoch£º562	 i:8 	 global-step:11248	 l-p:0.05569456145167351
epoch£º562	 i:9 	 global-step:11249	 l-p:0.05568356439471245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3372, 28.3807, 28.3404],
        [28.3372, 29.2719, 28.7963],
        [28.3372, 28.9113, 28.5439],
        [28.3372, 28.5653, 28.3830]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.05563665181398392 
model_pd.l_d.mean(): 2.091233000101056e-05 
model_pd.lagr.mean(): 0.05565756559371948 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6380], device='cuda:0')), ('power', tensor([0.1652], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.05563665181398392
epoch£º563	 i:1 	 global-step:11261	 l-p:0.05560734122991562
epoch£º563	 i:2 	 global-step:11262	 l-p:0.05565335601568222
epoch£º563	 i:3 	 global-step:11263	 l-p:0.05561025068163872
epoch£º563	 i:4 	 global-step:11264	 l-p:0.05604918301105499
epoch£º563	 i:5 	 global-step:11265	 l-p:0.05593893304467201
epoch£º563	 i:6 	 global-step:11266	 l-p:0.055623654276132584
epoch£º563	 i:7 	 global-step:11267	 l-p:0.05565116927027702
epoch£º563	 i:8 	 global-step:11268	 l-p:0.05561457946896553
epoch£º563	 i:9 	 global-step:11269	 l-p:0.0557350255548954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[28.3922, 32.2105, 32.6690],
        [28.3922, 33.9980, 35.9199],
        [28.3922, 30.7807, 30.4692],
        [28.3922, 29.5112, 29.0072]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.055793676525354385 
model_pd.l_d.mean(): 6.85329141560942e-05 
model_pd.lagr.mean(): 0.05586221069097519 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6123], device='cuda:0')), ('power', tensor([0.2805], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.055793676525354385
epoch£º564	 i:1 	 global-step:11281	 l-p:0.055772654712200165
epoch£º564	 i:2 	 global-step:11282	 l-p:0.05561967194080353
epoch£º564	 i:3 	 global-step:11283	 l-p:0.05572967231273651
epoch£º564	 i:4 	 global-step:11284	 l-p:0.05568269267678261
epoch£º564	 i:5 	 global-step:11285	 l-p:0.05565644055604935
epoch£º564	 i:6 	 global-step:11286	 l-p:0.05566241219639778
epoch£º564	 i:7 	 global-step:11287	 l-p:0.0556144043803215
epoch£º564	 i:8 	 global-step:11288	 l-p:0.055635061115026474
epoch£º564	 i:9 	 global-step:11289	 l-p:0.05587661638855934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4004, 28.4298, 28.4021],
        [28.4004, 29.3331, 28.8573],
        [28.4004, 38.3684, 45.2546],
        [28.4004, 38.4060, 45.3416]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.05567699298262596 
model_pd.l_d.mean(): 0.00010480431228643283 
model_pd.lagr.mean(): 0.05578179657459259 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5967], device='cuda:0')), ('power', tensor([0.2760], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.05567699298262596
epoch£º565	 i:1 	 global-step:11301	 l-p:0.05559581145644188
epoch£º565	 i:2 	 global-step:11302	 l-p:0.055768080055713654
epoch£º565	 i:3 	 global-step:11303	 l-p:0.05563592165708542
epoch£º565	 i:4 	 global-step:11304	 l-p:0.05561982840299606
epoch£º565	 i:5 	 global-step:11305	 l-p:0.055705368518829346
epoch£º565	 i:6 	 global-step:11306	 l-p:0.05575402081012726
epoch£º565	 i:7 	 global-step:11307	 l-p:0.05594570189714432
epoch£º565	 i:8 	 global-step:11308	 l-p:0.055727288126945496
epoch£º565	 i:9 	 global-step:11309	 l-p:0.05565108731389046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3531, 28.4354, 28.3620],
        [28.3531, 28.3627, 28.3534],
        [28.3531, 29.2345, 28.7700],
        [28.3531, 28.7141, 28.4498]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.0556633360683918 
model_pd.l_d.mean(): 9.468005009694025e-05 
model_pd.lagr.mean(): 0.05575801432132721 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6133], device='cuda:0')), ('power', tensor([0.1868], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.0556633360683918
epoch£º566	 i:1 	 global-step:11321	 l-p:0.055648837238550186
epoch£º566	 i:2 	 global-step:11322	 l-p:0.05561722815036774
epoch£º566	 i:3 	 global-step:11323	 l-p:0.05579618364572525
epoch£º566	 i:4 	 global-step:11324	 l-p:0.055757906287908554
epoch£º566	 i:5 	 global-step:11325	 l-p:0.05566517636179924
epoch£º566	 i:6 	 global-step:11326	 l-p:0.055663950741291046
epoch£º566	 i:7 	 global-step:11327	 l-p:0.05603446066379547
epoch£º566	 i:8 	 global-step:11328	 l-p:0.05565195530653
epoch£º566	 i:9 	 global-step:11329	 l-p:0.05574028193950653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[28.2506, 38.2880, 45.3000],
        [28.2506, 37.8327, 44.2501],
        [28.2506, 29.5000, 28.9888],
        [28.2506, 36.4871, 41.2379]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.05599743872880936 
model_pd.l_d.mean(): 0.00018995438585989177 
model_pd.lagr.mean(): 0.05618739128112793 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5147], device='cuda:0')), ('power', tensor([0.3177], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.05599743872880936
epoch£º567	 i:1 	 global-step:11341	 l-p:0.05568226799368858
epoch£º567	 i:2 	 global-step:11342	 l-p:0.05567905679345131
epoch£º567	 i:3 	 global-step:11343	 l-p:0.05565798655152321
epoch£º567	 i:4 	 global-step:11344	 l-p:0.05580306798219681
epoch£º567	 i:5 	 global-step:11345	 l-p:0.05582016706466675
epoch£º567	 i:6 	 global-step:11346	 l-p:0.055750828236341476
epoch£º567	 i:7 	 global-step:11347	 l-p:0.05565392225980759
epoch£º567	 i:8 	 global-step:11348	 l-p:0.055695485323667526
epoch£º567	 i:9 	 global-step:11349	 l-p:0.055769987404346466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1028, 31.1554, 31.1627],
        [28.1028, 28.1580, 28.1075],
        [28.1028, 35.1235, 38.5177],
        [28.1028, 31.4068, 31.5569]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.05574904382228851 
model_pd.l_d.mean(): 1.3209556527726818e-05 
model_pd.lagr.mean(): 0.05576225370168686 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5894], device='cuda:0')), ('power', tensor([0.0211], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.05574904382228851
epoch£º568	 i:1 	 global-step:11361	 l-p:0.05568646267056465
epoch£º568	 i:2 	 global-step:11362	 l-p:0.0559757761657238
epoch£º568	 i:3 	 global-step:11363	 l-p:0.05567481741309166
epoch£º568	 i:4 	 global-step:11364	 l-p:0.055703554302453995
epoch£º568	 i:5 	 global-step:11365	 l-p:0.055726535618305206
epoch£º568	 i:6 	 global-step:11366	 l-p:0.05571838095784187
epoch£º568	 i:7 	 global-step:11367	 l-p:0.05588527023792267
epoch£º568	 i:8 	 global-step:11368	 l-p:0.055977944284677505
epoch£º568	 i:9 	 global-step:11369	 l-p:0.055771902203559875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9338, 27.9339, 27.9338],
        [27.9338, 28.1465, 27.9751],
        [27.9338, 37.7308, 44.4982],
        [27.9338, 27.9338, 27.9338]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.055881328880786896 
model_pd.l_d.mean(): -8.499159594066441e-05 
model_pd.lagr.mean(): 0.05579633638262749 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5662], device='cuda:0')), ('power', tensor([-0.1471], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.055881328880786896
epoch£º569	 i:1 	 global-step:11381	 l-p:0.05591437220573425
epoch£º569	 i:2 	 global-step:11382	 l-p:0.0557643324136734
epoch£º569	 i:3 	 global-step:11383	 l-p:0.0557451993227005
epoch£º569	 i:4 	 global-step:11384	 l-p:0.05578253045678139
epoch£º569	 i:5 	 global-step:11385	 l-p:0.05603603273630142
epoch£º569	 i:6 	 global-step:11386	 l-p:0.05572148784995079
epoch£º569	 i:7 	 global-step:11387	 l-p:0.055749159306287766
epoch£º569	 i:8 	 global-step:11388	 l-p:0.05574774742126465
epoch£º569	 i:9 	 global-step:11389	 l-p:0.05590088292956352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7761, 27.8377, 27.7818],
        [27.7761, 27.7761, 27.7761],
        [27.7761, 30.5837, 30.4836],
        [27.7761, 35.5829, 39.9151]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.055829308927059174 
model_pd.l_d.mean(): -0.0001736331032589078 
model_pd.lagr.mean(): 0.05565567687153816 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5740], device='cuda:0')), ('power', tensor([-0.3896], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.055829308927059174
epoch£º570	 i:1 	 global-step:11401	 l-p:0.05573641508817673
epoch£º570	 i:2 	 global-step:11402	 l-p:0.05573562905192375
epoch£º570	 i:3 	 global-step:11403	 l-p:0.055918674916028976
epoch£º570	 i:4 	 global-step:11404	 l-p:0.055771488696336746
epoch£º570	 i:5 	 global-step:11405	 l-p:0.05577443167567253
epoch£º570	 i:6 	 global-step:11406	 l-p:0.0559365451335907
epoch£º570	 i:7 	 global-step:11407	 l-p:0.05611270293593407
epoch£º570	 i:8 	 global-step:11408	 l-p:0.05595812574028969
epoch£º570	 i:9 	 global-step:11409	 l-p:0.055793341249227524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6641, 29.6062, 29.1856],
        [27.6641, 27.6643, 27.6641],
        [27.6641, 27.8604, 27.7006],
        [27.6641, 27.6947, 27.6660]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.056037213653326035 
model_pd.l_d.mean(): -9.482711902819574e-05 
model_pd.lagr.mean(): 0.055942386388778687 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5567], device='cuda:0')), ('power', tensor([-0.3886], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.056037213653326035
epoch£º571	 i:1 	 global-step:11421	 l-p:0.055940210819244385
epoch£º571	 i:2 	 global-step:11422	 l-p:0.05605565384030342
epoch£º571	 i:3 	 global-step:11423	 l-p:0.05581093207001686
epoch£º571	 i:4 	 global-step:11424	 l-p:0.055839069187641144
epoch£º571	 i:5 	 global-step:11425	 l-p:0.055803392082452774
epoch£º571	 i:6 	 global-step:11426	 l-p:0.055803366005420685
epoch£º571	 i:7 	 global-step:11427	 l-p:0.055750664323568344
epoch£º571	 i:8 	 global-step:11428	 l-p:0.055791225284338
epoch£º571	 i:9 	 global-step:11429	 l-p:0.05591464787721634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6281, 27.6333, 27.6282],
        [27.6281, 37.7726, 45.0732],
        [27.6281, 37.1235, 43.5656],
        [27.6281, 27.7028, 27.6358]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.05595220625400543 
model_pd.l_d.mean(): -1.318535737482307e-06 
model_pd.lagr.mean(): 0.05595088750123978 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5413], device='cuda:0')), ('power', tensor([-0.4196], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.05595220625400543
epoch£º572	 i:1 	 global-step:11441	 l-p:0.055840689688920975
epoch£º572	 i:2 	 global-step:11442	 l-p:0.05581957846879959
epoch£º572	 i:3 	 global-step:11443	 l-p:0.05578174442052841
epoch£º572	 i:4 	 global-step:11444	 l-p:0.05599552020430565
epoch£º572	 i:5 	 global-step:11445	 l-p:0.05600738525390625
epoch£º572	 i:6 	 global-step:11446	 l-p:0.055766861885786057
epoch£º572	 i:7 	 global-step:11447	 l-p:0.055839069187641144
epoch£º572	 i:8 	 global-step:11448	 l-p:0.055818259716033936
epoch£º572	 i:9 	 global-step:11449	 l-p:0.0559207908809185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6736, 28.0747, 27.7903],
        [27.6736, 29.2215, 28.7315],
        [27.6736, 28.2773, 27.9016],
        [27.6736, 27.6749, 27.6736]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.05600838363170624 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05600838363170624 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5807], device='cuda:0')), ('power', tensor([-0.4246], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.05600838363170624
epoch£º573	 i:1 	 global-step:11461	 l-p:0.05598118156194687
epoch£º573	 i:2 	 global-step:11462	 l-p:0.05578533932566643
epoch£º573	 i:3 	 global-step:11463	 l-p:0.05581514537334442
epoch£º573	 i:4 	 global-step:11464	 l-p:0.05579807236790657
epoch£º573	 i:5 	 global-step:11465	 l-p:0.05597751960158348
epoch£º573	 i:6 	 global-step:11466	 l-p:0.05585209280252457
epoch£º573	 i:7 	 global-step:11467	 l-p:0.05571989342570305
epoch£º573	 i:8 	 global-step:11468	 l-p:0.055821847170591354
epoch£º573	 i:9 	 global-step:11469	 l-p:0.05582936108112335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7591, 30.3917, 30.2100],
        [27.7591, 33.6510, 35.9272],
        [27.7591, 27.7591, 27.7591],
        [27.7591, 27.7812, 27.7602]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.055803049355745316 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055803049355745316 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6014], device='cuda:0')), ('power', tensor([-0.4059], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.055803049355745316
epoch£º574	 i:1 	 global-step:11481	 l-p:0.05576249584555626
epoch£º574	 i:2 	 global-step:11482	 l-p:0.05587063357234001
epoch£º574	 i:3 	 global-step:11483	 l-p:0.05592392757534981
epoch£º574	 i:4 	 global-step:11484	 l-p:0.05599435791373253
epoch£º574	 i:5 	 global-step:11485	 l-p:0.0557498000562191
epoch£º574	 i:6 	 global-step:11486	 l-p:0.05600413307547569
epoch£º574	 i:7 	 global-step:11487	 l-p:0.05573977530002594
epoch£º574	 i:8 	 global-step:11488	 l-p:0.05574329197406769
epoch£º574	 i:9 	 global-step:11489	 l-p:0.0557841993868351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8584, 27.9293, 27.8654],
        [27.8584, 29.3058, 28.8037],
        [27.8584, 27.8585, 27.8584],
        [27.8584, 27.9392, 27.8671]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.05575980991125107 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575980991125107 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5875], device='cuda:0')), ('power', tensor([-0.2515], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.05575980991125107
epoch£º575	 i:1 	 global-step:11501	 l-p:0.055838875472545624
epoch£º575	 i:2 	 global-step:11502	 l-p:0.055714044719934464
epoch£º575	 i:3 	 global-step:11503	 l-p:0.05597476661205292
epoch£º575	 i:4 	 global-step:11504	 l-p:0.05577988550066948
epoch£º575	 i:5 	 global-step:11505	 l-p:0.055680058896541595
epoch£º575	 i:6 	 global-step:11506	 l-p:0.05569352209568024
epoch£º575	 i:7 	 global-step:11507	 l-p:0.05612315982580185
epoch£º575	 i:8 	 global-step:11508	 l-p:0.05574505776166916
epoch£º575	 i:9 	 global-step:11509	 l-p:0.05583365261554718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9627, 27.9643, 27.9627],
        [27.9627, 27.9823, 27.9636],
        [27.9627, 37.3310, 43.5359],
        [27.9627, 32.4486, 33.4423]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.05574880540370941 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574880540370941 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5936], device='cuda:0')), ('power', tensor([-0.1828], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.05574880540370941
epoch£º576	 i:1 	 global-step:11521	 l-p:0.05569539591670036
epoch£º576	 i:2 	 global-step:11522	 l-p:0.05577927455306053
epoch£º576	 i:3 	 global-step:11523	 l-p:0.055723514407873154
epoch£º576	 i:4 	 global-step:11524	 l-p:0.05603082850575447
epoch£º576	 i:5 	 global-step:11525	 l-p:0.05574209243059158
epoch£º576	 i:6 	 global-step:11526	 l-p:0.055693019181489944
epoch£º576	 i:7 	 global-step:11527	 l-p:0.055995550006628036
epoch£º576	 i:8 	 global-step:11528	 l-p:0.05576091632246971
epoch£º576	 i:9 	 global-step:11529	 l-p:0.05573587119579315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0692, 28.9241, 28.4685],
        [28.0692, 28.0691, 28.0691],
        [28.0692, 34.2434, 36.7581],
        [28.0692, 28.0692, 28.0691]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.05568121746182442 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568121746182442 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6357], device='cuda:0')), ('power', tensor([-0.1189], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.05568121746182442
epoch£º577	 i:1 	 global-step:11541	 l-p:0.05595380812883377
epoch£º577	 i:2 	 global-step:11542	 l-p:0.055675845593214035
epoch£º577	 i:3 	 global-step:11543	 l-p:0.05576128885149956
epoch£º577	 i:4 	 global-step:11544	 l-p:0.05571402609348297
epoch£º577	 i:5 	 global-step:11545	 l-p:0.05586061254143715
epoch£º577	 i:6 	 global-step:11546	 l-p:0.05573096126317978
epoch£º577	 i:7 	 global-step:11547	 l-p:0.05566737428307533
epoch£º577	 i:8 	 global-step:11548	 l-p:0.055938150733709335
epoch£º577	 i:9 	 global-step:11549	 l-p:0.05568332597613335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1763, 28.1762, 28.1763],
        [28.1763, 34.1677, 36.4866],
        [28.1763, 29.9542, 29.4826],
        [28.1763, 31.4001, 31.4984]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.05568145960569382 
model_pd.l_d.mean(): -2.8611388813715166e-08 
model_pd.lagr.mean(): 0.05568142980337143 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.9591e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6329], device='cuda:0')), ('power', tensor([-0.0055], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.05568145960569382
epoch£º578	 i:1 	 global-step:11561	 l-p:0.055638637393713
epoch£º578	 i:2 	 global-step:11562	 l-p:0.0562358982861042
epoch£º578	 i:3 	 global-step:11563	 l-p:0.05572853609919548
epoch£º578	 i:4 	 global-step:11564	 l-p:0.055643778294324875
epoch£º578	 i:5 	 global-step:11565	 l-p:0.055668506771326065
epoch£º578	 i:6 	 global-step:11566	 l-p:0.05571909621357918
epoch£º578	 i:7 	 global-step:11567	 l-p:0.055783603340387344
epoch£º578	 i:8 	 global-step:11568	 l-p:0.055655889213085175
epoch£º578	 i:9 	 global-step:11569	 l-p:0.055675651878118515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2808, 38.3358, 45.3639],
        [28.2808, 28.8536, 28.4870],
        [28.2808, 31.3538, 31.3612],
        [28.2808, 28.6665, 28.3887]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.05566810443997383 
model_pd.l_d.mean(): 5.65076607017545e-06 
model_pd.lagr.mean(): 0.0556737557053566 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.8489e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6285], device='cuda:0')), ('power', tensor([0.1063], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.05566810443997383
epoch£º579	 i:1 	 global-step:11581	 l-p:0.05566076561808586
epoch£º579	 i:2 	 global-step:11582	 l-p:0.0559217743575573
epoch£º579	 i:3 	 global-step:11583	 l-p:0.055636413395404816
epoch£º579	 i:4 	 global-step:11584	 l-p:0.05562325939536095
epoch£º579	 i:5 	 global-step:11585	 l-p:0.05568217113614082
epoch£º579	 i:6 	 global-step:11586	 l-p:0.0557774156332016
epoch£º579	 i:7 	 global-step:11587	 l-p:0.0558343380689621
epoch£º579	 i:8 	 global-step:11588	 l-p:0.05574684962630272
epoch£º579	 i:9 	 global-step:11589	 l-p:0.05566304177045822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3678, 28.4946, 28.3855],
        [28.3678, 28.4008, 28.3699],
        [28.3678, 33.0567, 34.1749],
        [28.3678, 28.5333, 28.3950]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.05559426546096802 
model_pd.l_d.mean(): 1.6390282326028682e-05 
model_pd.lagr.mean(): 0.05561065673828125 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6840], device='cuda:0')), ('power', tensor([0.1095], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.05559426546096802
epoch£º580	 i:1 	 global-step:11601	 l-p:0.055607233196496964
epoch£º580	 i:2 	 global-step:11602	 l-p:0.05562173202633858
epoch£º580	 i:3 	 global-step:11603	 l-p:0.05566875636577606
epoch£º580	 i:4 	 global-step:11604	 l-p:0.05584506690502167
epoch£º580	 i:5 	 global-step:11605	 l-p:0.05565318092703819
epoch£º580	 i:6 	 global-step:11606	 l-p:0.055873699486255646
epoch£º580	 i:7 	 global-step:11607	 l-p:0.05575961992144585
epoch£º580	 i:8 	 global-step:11608	 l-p:0.055785149335861206
epoch£º580	 i:9 	 global-step:11609	 l-p:0.05564697086811066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4188, 28.4188, 28.4188],
        [28.4188, 35.8441, 39.6326],
        [28.4188, 31.3701, 31.3054],
        [28.4188, 29.2177, 28.7732]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.05565210059285164 
model_pd.l_d.mean(): 7.902481593191624e-05 
model_pd.lagr.mean(): 0.05573112517595291 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6055], device='cuda:0')), ('power', tensor([0.2802], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.05565210059285164
epoch£º581	 i:1 	 global-step:11621	 l-p:0.05569465830922127
epoch£º581	 i:2 	 global-step:11622	 l-p:0.055608052760362625
epoch£º581	 i:3 	 global-step:11623	 l-p:0.05573689192533493
epoch£º581	 i:4 	 global-step:11624	 l-p:0.0558091439306736
epoch£º581	 i:5 	 global-step:11625	 l-p:0.05560814216732979
epoch£º581	 i:6 	 global-step:11626	 l-p:0.05562446638941765
epoch£º581	 i:7 	 global-step:11627	 l-p:0.05585081875324249
epoch£º581	 i:8 	 global-step:11628	 l-p:0.05573061481118202
epoch£º581	 i:9 	 global-step:11629	 l-p:0.055682189762592316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4134, 28.7622, 28.5047],
        [28.4134, 38.2155, 44.8819],
        [28.4134, 28.4154, 28.4134],
        [28.4134, 28.4765, 28.4192]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.05598190054297447 
model_pd.l_d.mean(): 0.0001970662415260449 
model_pd.lagr.mean(): 0.05617896839976311 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5370], device='cuda:0')), ('power', tensor([0.4606], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.05598190054297447
epoch£º582	 i:1 	 global-step:11641	 l-p:0.05577823519706726
epoch£º582	 i:2 	 global-step:11642	 l-p:0.05566718056797981
epoch£º582	 i:3 	 global-step:11643	 l-p:0.05561421439051628
epoch£º582	 i:4 	 global-step:11644	 l-p:0.05562594160437584
epoch£º582	 i:5 	 global-step:11645	 l-p:0.055652789771556854
epoch£º582	 i:6 	 global-step:11646	 l-p:0.05583450198173523
epoch£º582	 i:7 	 global-step:11647	 l-p:0.05569453909993172
epoch£º582	 i:8 	 global-step:11648	 l-p:0.05562963709235191
epoch£º582	 i:9 	 global-step:11649	 l-p:0.055593714118003845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3399, 28.5603, 28.3832],
        [28.3399, 34.9658, 37.9043],
        [28.3399, 35.1647, 38.3123],
        [28.3399, 31.7603, 31.9644]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.055648379027843475 
model_pd.l_d.mean(): 0.00011436283239163458 
model_pd.lagr.mean(): 0.05576274171471596 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6201], device='cuda:0')), ('power', tensor([0.2056], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.055648379027843475
epoch£º583	 i:1 	 global-step:11661	 l-p:0.05578354746103287
epoch£º583	 i:2 	 global-step:11662	 l-p:0.055960558354854584
epoch£º583	 i:3 	 global-step:11663	 l-p:0.055660899728536606
epoch£º583	 i:4 	 global-step:11664	 l-p:0.055662158876657486
epoch£º583	 i:5 	 global-step:11665	 l-p:0.05566694214940071
epoch£º583	 i:6 	 global-step:11666	 l-p:0.05567850545048714
epoch£º583	 i:7 	 global-step:11667	 l-p:0.05563998222351074
epoch£º583	 i:8 	 global-step:11668	 l-p:0.055680785328149796
epoch£º583	 i:9 	 global-step:11669	 l-p:0.05591987818479538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2020, 28.3280, 28.2196],
        [28.2020, 28.2020, 28.2020],
        [28.2020, 28.5811, 28.3071],
        [28.2020, 28.6736, 28.3522]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.055653855204582214 
model_pd.l_d.mean(): 2.3768394896706013e-07 
model_pd.lagr.mean(): 0.055654093623161316 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6330], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.055653855204582214
epoch£º584	 i:1 	 global-step:11681	 l-p:0.056049641221761703
epoch£º584	 i:2 	 global-step:11682	 l-p:0.05566706880927086
epoch£º584	 i:3 	 global-step:11683	 l-p:0.055795181542634964
epoch£º584	 i:4 	 global-step:11684	 l-p:0.055738408118486404
epoch£º584	 i:5 	 global-step:11685	 l-p:0.055689357221126556
epoch£º584	 i:6 	 global-step:11686	 l-p:0.055840834975242615
epoch£º584	 i:7 	 global-step:11687	 l-p:0.05576136335730553
epoch£º584	 i:8 	 global-step:11688	 l-p:0.05569056421518326
epoch£º584	 i:9 	 global-step:11689	 l-p:0.055765144526958466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0225, 28.5795, 28.2207],
        [28.0225, 28.0640, 28.0255],
        [28.0225, 28.0309, 28.0228],
        [28.0225, 28.0776, 28.0272]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.05572611838579178 
model_pd.l_d.mean(): -0.00011967481987085193 
model_pd.lagr.mean(): 0.05560644343495369 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6170], device='cuda:0')), ('power', tensor([-0.1896], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.05572611838579178
epoch£º585	 i:1 	 global-step:11701	 l-p:0.05575130507349968
epoch£º585	 i:2 	 global-step:11702	 l-p:0.05575021728873253
epoch£º585	 i:3 	 global-step:11703	 l-p:0.05582915246486664
epoch£º585	 i:4 	 global-step:11704	 l-p:0.05595289170742035
epoch£º585	 i:5 	 global-step:11705	 l-p:0.05605249106884003
epoch£º585	 i:6 	 global-step:11706	 l-p:0.05575646460056305
epoch£º585	 i:7 	 global-step:11707	 l-p:0.0557376891374588
epoch£º585	 i:8 	 global-step:11708	 l-p:0.055771149694919586
epoch£º585	 i:9 	 global-step:11709	 l-p:0.0557439848780632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8342, 27.8375, 27.8343],
        [27.8342, 37.4037, 43.8963],
        [27.8342, 29.1428, 28.6377],
        [27.8342, 28.0338, 27.8715]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.055791642516851425 
model_pd.l_d.mean(): -0.0001747359347064048 
model_pd.lagr.mean(): 0.05561690777540207 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5845], device='cuda:0')), ('power', tensor([-0.3255], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.055791642516851425
epoch£º586	 i:1 	 global-step:11721	 l-p:0.05600249022245407
epoch£º586	 i:2 	 global-step:11722	 l-p:0.05579536780714989
epoch£º586	 i:3 	 global-step:11723	 l-p:0.05579160526394844
epoch£º586	 i:4 	 global-step:11724	 l-p:0.055786799639463425
epoch£º586	 i:5 	 global-step:11725	 l-p:0.055866699665784836
epoch£º586	 i:6 	 global-step:11726	 l-p:0.05572991073131561
epoch£º586	 i:7 	 global-step:11727	 l-p:0.055942609906196594
epoch£º586	 i:8 	 global-step:11728	 l-p:0.055802006274461746
epoch£º586	 i:9 	 global-step:11729	 l-p:0.05596687272191048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6769, 30.5687, 30.5166],
        [27.6769, 33.8415, 36.4005],
        [27.6769, 33.2441, 35.2177],
        [27.6769, 28.0406, 27.7763]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.055828291922807693 
model_pd.l_d.mean(): -0.0001539411023259163 
model_pd.lagr.mean(): 0.05567435175180435 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5669], device='cuda:0')), ('power', tensor([-0.4344], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.055828291922807693
epoch£º587	 i:1 	 global-step:11741	 l-p:0.05585242062807083
epoch£º587	 i:2 	 global-step:11742	 l-p:0.05609539896249771
epoch£º587	 i:3 	 global-step:11743	 l-p:0.05580241233110428
epoch£º587	 i:4 	 global-step:11744	 l-p:0.055881574749946594
epoch£º587	 i:5 	 global-step:11745	 l-p:0.055779535323381424
epoch£º587	 i:6 	 global-step:11746	 l-p:0.05576388165354729
epoch£º587	 i:7 	 global-step:11747	 l-p:0.05613492801785469
epoch£º587	 i:8 	 global-step:11748	 l-p:0.0558650940656662
epoch£º587	 i:9 	 global-step:11749	 l-p:0.05576702952384949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5911, 27.5911, 27.5911],
        [27.5911, 28.2182, 27.8343],
        [27.5911, 29.6591, 29.2747],
        [27.5911, 28.1053, 27.7668]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.055812980979681015 
model_pd.l_d.mean(): -6.28617635811679e-05 
model_pd.lagr.mean(): 0.05575012043118477 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.9505e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5815], device='cuda:0')), ('power', tensor([-0.5795], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.055812980979681015
epoch£º588	 i:1 	 global-step:11761	 l-p:0.05575878173112869
epoch£º588	 i:2 	 global-step:11762	 l-p:0.05583710968494415
epoch£º588	 i:3 	 global-step:11763	 l-p:0.05585573986172676
epoch£º588	 i:4 	 global-step:11764	 l-p:0.055783987045288086
epoch£º588	 i:5 	 global-step:11765	 l-p:0.05580337345600128
epoch£º588	 i:6 	 global-step:11766	 l-p:0.05606872960925102
epoch£º588	 i:7 	 global-step:11767	 l-p:0.05614352598786354
epoch£º588	 i:8 	 global-step:11768	 l-p:0.055909838527441025
epoch£º588	 i:9 	 global-step:11769	 l-p:0.05589468404650688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6041, 27.6853, 27.6129],
        [27.6041, 27.6083, 27.6042],
        [27.6041, 27.8743, 27.6654],
        [27.6041, 27.6042, 27.6041]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.055839914828538895 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055839914828538895 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5595], device='cuda:0')), ('power', tensor([-0.5166], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.055839914828538895
epoch£º589	 i:1 	 global-step:11781	 l-p:0.05577372387051582
epoch£º589	 i:2 	 global-step:11782	 l-p:0.056087102741003036
epoch£º589	 i:3 	 global-step:11783	 l-p:0.055787213146686554
epoch£º589	 i:4 	 global-step:11784	 l-p:0.05580059811472893
epoch£º589	 i:5 	 global-step:11785	 l-p:0.05597444251179695
epoch£º589	 i:6 	 global-step:11786	 l-p:0.05597131699323654
epoch£º589	 i:7 	 global-step:11787	 l-p:0.05579051375389099
epoch£º589	 i:8 	 global-step:11788	 l-p:0.05579783394932747
epoch£º589	 i:9 	 global-step:11789	 l-p:0.05593487620353699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6832, 27.8918, 27.7235],
        [27.6832, 32.8105, 34.3782],
        [27.6832, 28.0583, 27.7878],
        [27.6832, 27.7376, 27.6879]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.0558084100484848 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0558084100484848 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5882], device='cuda:0')), ('power', tensor([-0.4486], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.0558084100484848
epoch£º590	 i:1 	 global-step:11801	 l-p:0.055823057889938354
epoch£º590	 i:2 	 global-step:11802	 l-p:0.05574024096131325
epoch£º590	 i:3 	 global-step:11803	 l-p:0.05593671649694443
epoch£º590	 i:4 	 global-step:11804	 l-p:0.05587085708975792
epoch£º590	 i:5 	 global-step:11805	 l-p:0.05597235634922981
epoch£º590	 i:6 	 global-step:11806	 l-p:0.05602829158306122
epoch£º590	 i:7 	 global-step:11807	 l-p:0.05578741058707237
epoch£º590	 i:8 	 global-step:11808	 l-p:0.05583947151899338
epoch£º590	 i:9 	 global-step:11809	 l-p:0.055739663541316986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7859, 34.1171, 36.8305],
        [27.7859, 30.6459, 30.5712],
        [27.7859, 27.7864, 27.7859],
        [27.7859, 27.7877, 27.7859]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.05609731003642082 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05609731003642082 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5336], device='cuda:0')), ('power', tensor([-0.2180], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.05609731003642082
epoch£º591	 i:1 	 global-step:11821	 l-p:0.055733732879161835
epoch£º591	 i:2 	 global-step:11822	 l-p:0.05570974200963974
epoch£º591	 i:3 	 global-step:11823	 l-p:0.05581560358405113
epoch£º591	 i:4 	 global-step:11824	 l-p:0.055730752646923065
epoch£º591	 i:5 	 global-step:11825	 l-p:0.0557824969291687
epoch£º591	 i:6 	 global-step:11826	 l-p:0.05585947260260582
epoch£º591	 i:7 	 global-step:11827	 l-p:0.05590786784887314
epoch£º591	 i:8 	 global-step:11828	 l-p:0.05578552559018135
epoch£º591	 i:9 	 global-step:11829	 l-p:0.05587899684906006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[27.8969, 29.6859, 29.2246],
        [27.8969, 29.6883, 29.2275],
        [27.8969, 38.1049, 45.4267],
        [27.8969, 29.9864, 29.5969]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.05581222474575043 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05581222474575043 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6086], device='cuda:0')), ('power', tensor([-0.2610], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.05581222474575043
epoch£º592	 i:1 	 global-step:11841	 l-p:0.055733613669872284
epoch£º592	 i:2 	 global-step:11842	 l-p:0.055748652666807175
epoch£º592	 i:3 	 global-step:11843	 l-p:0.055852312594652176
epoch£º592	 i:4 	 global-step:11844	 l-p:0.05594046413898468
epoch£º592	 i:5 	 global-step:11845	 l-p:0.055717162787914276
epoch£º592	 i:6 	 global-step:11846	 l-p:0.05571185424923897
epoch£º592	 i:7 	 global-step:11847	 l-p:0.05575421825051308
epoch£º592	 i:8 	 global-step:11848	 l-p:0.056098926812410355
epoch£º592	 i:9 	 global-step:11849	 l-p:0.05567619204521179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0110, 28.5078, 28.1754],
        [28.0110, 28.8964, 28.4344],
        [28.0110, 28.0112, 28.0110],
        [28.0110, 28.0110, 28.0110]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.055687494575977325 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055687494575977325 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6459], device='cuda:0')), ('power', tensor([-0.1953], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.055687494575977325
epoch£º593	 i:1 	 global-step:11861	 l-p:0.055739887058734894
epoch£º593	 i:2 	 global-step:11862	 l-p:0.05572396516799927
epoch£º593	 i:3 	 global-step:11863	 l-p:0.05571946129202843
epoch£º593	 i:4 	 global-step:11864	 l-p:0.05569589510560036
epoch£º593	 i:5 	 global-step:11865	 l-p:0.05594860762357712
epoch£º593	 i:6 	 global-step:11866	 l-p:0.055878933519124985
epoch£º593	 i:7 	 global-step:11867	 l-p:0.05576218664646149
epoch£º593	 i:8 	 global-step:11868	 l-p:0.055963873863220215
epoch£º593	 i:9 	 global-step:11869	 l-p:0.05566711351275444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1261, 28.9250, 28.4830],
        [28.1261, 28.1262, 28.1261],
        [28.1261, 28.5379, 28.2466],
        [28.1261, 36.0252, 40.4026]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.055707044899463654 
model_pd.l_d.mean(): 1.0024658081420057e-07 
model_pd.lagr.mean(): 0.05570714548230171 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.9103e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5984], device='cuda:0')), ('power', tensor([0.0289], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.055707044899463654
epoch£º594	 i:1 	 global-step:11881	 l-p:0.05566935986280441
epoch£º594	 i:2 	 global-step:11882	 l-p:0.05567777529358864
epoch£º594	 i:3 	 global-step:11883	 l-p:0.05590805783867836
epoch£º594	 i:4 	 global-step:11884	 l-p:0.05590086430311203
epoch£º594	 i:5 	 global-step:11885	 l-p:0.05572942644357681
epoch£º594	 i:6 	 global-step:11886	 l-p:0.05562583729624748
epoch£º594	 i:7 	 global-step:11887	 l-p:0.055825889110565186
epoch£º594	 i:8 	 global-step:11888	 l-p:0.05571015179157257
epoch£º594	 i:9 	 global-step:11889	 l-p:0.05577787756919861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2402, 36.2072, 40.6431],
        [28.2402, 30.0549, 29.5882],
        [28.2402, 28.2403, 28.2402],
        [28.2402, 28.2486, 28.2404]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.055656999349594116 
model_pd.l_d.mean(): 1.2467162378015928e-06 
model_pd.lagr.mean(): 0.0556582473218441 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.4004e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6372], device='cuda:0')), ('power', tensor([0.0389], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.055656999349594116
epoch£º595	 i:1 	 global-step:11901	 l-p:0.05569469928741455
epoch£º595	 i:2 	 global-step:11902	 l-p:0.05563404783606529
epoch£º595	 i:3 	 global-step:11903	 l-p:0.05565338954329491
epoch£º595	 i:4 	 global-step:11904	 l-p:0.05592712387442589
epoch£º595	 i:5 	 global-step:11905	 l-p:0.05582166090607643
epoch£º595	 i:6 	 global-step:11906	 l-p:0.05562077462673187
epoch£º595	 i:7 	 global-step:11907	 l-p:0.055945053696632385
epoch£º595	 i:8 	 global-step:11908	 l-p:0.05566459521651268
epoch£º595	 i:9 	 global-step:11909	 l-p:0.05567033216357231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3426, 28.5707, 28.3884],
        [28.3426, 28.3466, 28.3427],
        [28.3426, 32.8926, 33.9008],
        [28.3426, 28.3427, 28.3426]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.05564659461379051 
model_pd.l_d.mean(): 1.6005966244847514e-05 
model_pd.lagr.mean(): 0.055662602186203 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6397], device='cuda:0')), ('power', tensor([0.1434], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.05564659461379051
epoch£º596	 i:1 	 global-step:11921	 l-p:0.055920906364917755
epoch£º596	 i:2 	 global-step:11922	 l-p:0.055714286863803864
epoch£º596	 i:3 	 global-step:11923	 l-p:0.05570700392127037
epoch£º596	 i:4 	 global-step:11924	 l-p:0.05566610395908356
epoch£º596	 i:5 	 global-step:11925	 l-p:0.055647291243076324
epoch£º596	 i:6 	 global-step:11926	 l-p:0.055787019431591034
epoch£º596	 i:7 	 global-step:11927	 l-p:0.0556202195584774
epoch£º596	 i:8 	 global-step:11928	 l-p:0.0556224063038826
epoch£º596	 i:9 	 global-step:11929	 l-p:0.05576186999678612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4115, 29.8946, 29.3824],
        [28.4115, 28.5554, 28.4331],
        [28.4115, 28.4116, 28.4115],
        [28.4115, 28.4199, 28.4117]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.05564271658658981 
model_pd.l_d.mean(): 4.949278081767261e-05 
model_pd.lagr.mean(): 0.05569221079349518 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6265], device='cuda:0')), ('power', tensor([0.2101], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.05564271658658981
epoch£º597	 i:1 	 global-step:11941	 l-p:0.05565377697348595
epoch£º597	 i:2 	 global-step:11942	 l-p:0.055600784718990326
epoch£º597	 i:3 	 global-step:11943	 l-p:0.055810485035181046
epoch£º597	 i:4 	 global-step:11944	 l-p:0.05562897399067879
epoch£º597	 i:5 	 global-step:11945	 l-p:0.05569065362215042
epoch£º597	 i:6 	 global-step:11946	 l-p:0.055827073752880096
epoch£º597	 i:7 	 global-step:11947	 l-p:0.0558292418718338
epoch£º597	 i:8 	 global-step:11948	 l-p:0.055666062980890274
epoch£º597	 i:9 	 global-step:11949	 l-p:0.05564431473612785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.4257, 28.4297, 28.4258],
        [28.4257, 35.4841, 38.8687],
        [28.4257, 29.5743, 29.0669],
        [28.4257, 37.8414, 44.0078]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.05583096295595169 
model_pd.l_d.mean(): 0.00014247652143239975 
model_pd.lagr.mean(): 0.055973440408706665 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5669], device='cuda:0')), ('power', tensor([0.3729], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.05583096295595169
epoch£º598	 i:1 	 global-step:11961	 l-p:0.055608682334423065
epoch£º598	 i:2 	 global-step:11962	 l-p:0.05602853745222092
epoch£º598	 i:3 	 global-step:11963	 l-p:0.055590517818927765
epoch£º598	 i:4 	 global-step:11964	 l-p:0.0556449219584465
epoch£º598	 i:5 	 global-step:11965	 l-p:0.05563218891620636
epoch£º598	 i:6 	 global-step:11966	 l-p:0.055671919137239456
epoch£º598	 i:7 	 global-step:11967	 l-p:0.05563391000032425
epoch£º598	 i:8 	 global-step:11968	 l-p:0.05565197393298149
epoch£º598	 i:9 	 global-step:11969	 l-p:0.05573472008109093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3699, 28.3699, 28.3699],
        [28.3699, 29.6066, 29.0939],
        [28.3699, 36.3752, 40.8326],
        [28.3699, 28.3898, 28.3708]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.05570920184254646 
model_pd.l_d.mean(): 0.00016086666437331587 
model_pd.lagr.mean(): 0.055870067328214645 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5502], device='cuda:0')), ('power', tensor([0.3090], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.05570920184254646
epoch£º599	 i:1 	 global-step:11981	 l-p:0.055596478283405304
epoch£º599	 i:2 	 global-step:11982	 l-p:0.05595794692635536
epoch£º599	 i:3 	 global-step:11983	 l-p:0.05576935037970543
epoch£º599	 i:4 	 global-step:11984	 l-p:0.05567861348390579
epoch£º599	 i:5 	 global-step:11985	 l-p:0.05566815286874771
epoch£º599	 i:6 	 global-step:11986	 l-p:0.05561314523220062
epoch£º599	 i:7 	 global-step:11987	 l-p:0.05574943870306015
epoch£º599	 i:8 	 global-step:11988	 l-p:0.05566573143005371
epoch£º599	 i:9 	 global-step:11989	 l-p:0.05581552907824516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2420, 34.6218, 37.3206],
        [28.2420, 28.2420, 28.2420],
        [28.2420, 28.2420, 28.2420],
        [28.2420, 28.2420, 28.2420]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.05586937814950943 
model_pd.l_d.mean(): 8.65586698637344e-05 
model_pd.lagr.mean(): 0.05595593526959419 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6110], device='cuda:0')), ('power', tensor([0.1407], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.05586937814950943
epoch£º600	 i:1 	 global-step:12001	 l-p:0.05566209554672241
epoch£º600	 i:2 	 global-step:12002	 l-p:0.05569278076291084
epoch£º600	 i:3 	 global-step:12003	 l-p:0.05571558699011803
epoch£º600	 i:4 	 global-step:12004	 l-p:0.05566079169511795
epoch£º600	 i:5 	 global-step:12005	 l-p:0.05569049343466759
epoch£º600	 i:6 	 global-step:12006	 l-p:0.05591258779168129
epoch£º600	 i:7 	 global-step:12007	 l-p:0.05572204291820526
epoch£º600	 i:8 	 global-step:12008	 l-p:0.05567149817943573
epoch£º600	 i:9 	 global-step:12009	 l-p:0.05586240068078041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1695, 33.6429, 35.4687],
        [28.1695, 31.2297, 31.2370],
        [28.1695, 28.2840, 28.1845],
        [28.1695, 28.1726, 28.1695]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.05567549169063568 
model_pd.l_d.mean(): -3.823553925030865e-05 
model_pd.lagr.mean(): 0.05563725531101227 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6445], device='cuda:0')), ('power', tensor([-0.0583], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.05567549169063568
epoch£º601	 i:1 	 global-step:12021	 l-p:0.05573464184999466
epoch£º601	 i:2 	 global-step:12022	 l-p:0.05567268654704094
epoch£º601	 i:3 	 global-step:12023	 l-p:0.05582866445183754
epoch£º601	 i:4 	 global-step:12024	 l-p:0.05589684471487999
epoch£º601	 i:5 	 global-step:12025	 l-p:0.0557083897292614
epoch£º601	 i:6 	 global-step:12026	 l-p:0.05579404532909393
epoch£º601	 i:7 	 global-step:12027	 l-p:0.05574418976902962
epoch£º601	 i:8 	 global-step:12028	 l-p:0.05569489672780037
epoch£º601	 i:9 	 global-step:12029	 l-p:0.05588488280773163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0848, 28.0848, 28.0848],
        [28.0848, 34.2363, 36.7261],
        [28.0848, 28.2222, 28.1051],
        [28.0848, 33.7846, 35.8332]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.055715397000312805 
model_pd.l_d.mean(): -4.4781470933230594e-05 
model_pd.lagr.mean(): 0.055670615285634995 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6095], device='cuda:0')), ('power', tensor([-0.0680], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.055715397000312805
epoch£º602	 i:1 	 global-step:12041	 l-p:0.05594359710812569
epoch£º602	 i:2 	 global-step:12042	 l-p:0.055891335010528564
epoch£º602	 i:3 	 global-step:12043	 l-p:0.055841147899627686
epoch£º602	 i:4 	 global-step:12044	 l-p:0.0557062029838562
epoch£º602	 i:5 	 global-step:12045	 l-p:0.05572235956788063
epoch£º602	 i:6 	 global-step:12046	 l-p:0.055900271981954575
epoch£º602	 i:7 	 global-step:12047	 l-p:0.055706486105918884
epoch£º602	 i:8 	 global-step:12048	 l-p:0.05570978671312332
epoch£º602	 i:9 	 global-step:12049	 l-p:0.055690497159957886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9963, 28.9741, 28.4945],
        [27.9963, 28.0551, 28.0015],
        [27.9963, 27.9967, 27.9963],
        [27.9963, 28.8812, 28.4195]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.05569535493850708 
model_pd.l_d.mean(): -0.00014859576185699552 
model_pd.lagr.mean(): 0.05554676055908203 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6454], device='cuda:0')), ('power', tensor([-0.2405], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.05569535493850708
epoch£º603	 i:1 	 global-step:12061	 l-p:0.05571397766470909
epoch£º603	 i:2 	 global-step:12062	 l-p:0.05582725256681442
epoch£º603	 i:3 	 global-step:12063	 l-p:0.055849168449640274
epoch£º603	 i:4 	 global-step:12064	 l-p:0.0558287687599659
epoch£º603	 i:5 	 global-step:12065	 l-p:0.055727556347846985
epoch£º603	 i:6 	 global-step:12066	 l-p:0.05572020262479782
epoch£º603	 i:7 	 global-step:12067	 l-p:0.05575377494096756
epoch£º603	 i:8 	 global-step:12068	 l-p:0.05614299699664116
epoch£º603	 i:9 	 global-step:12069	 l-p:0.055766668170690536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9124, 36.0457, 40.7367],
        [27.9124, 27.9124, 27.9124],
        [27.9124, 28.3409, 28.0417],
        [27.9124, 29.8811, 29.4586]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.05573590099811554 
model_pd.l_d.mean(): -0.00015338959929067641 
model_pd.lagr.mean(): 0.05558251217007637 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6113], device='cuda:0')), ('power', tensor([-0.2874], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.05573590099811554
epoch£º604	 i:1 	 global-step:12081	 l-p:0.055848874151706696
epoch£º604	 i:2 	 global-step:12082	 l-p:0.05590471252799034
epoch£º604	 i:3 	 global-step:12083	 l-p:0.05583276227116585
epoch£º604	 i:4 	 global-step:12084	 l-p:0.055903926491737366
epoch£º604	 i:5 	 global-step:12085	 l-p:0.05578504875302315
epoch£º604	 i:6 	 global-step:12086	 l-p:0.05603744089603424
epoch£º604	 i:7 	 global-step:12087	 l-p:0.05570976436138153
epoch£º604	 i:8 	 global-step:12088	 l-p:0.05569872260093689
epoch£º604	 i:9 	 global-step:12089	 l-p:0.055743370205163956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8428, 33.0799, 34.7281],
        [27.8428, 28.6686, 28.2221],
        [27.8428, 28.1894, 27.9343],
        [27.8428, 28.1154, 27.9047]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.055765289813280106 
model_pd.l_d.mean(): -0.00014027909492142498 
model_pd.lagr.mean(): 0.055625010281801224 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6068], device='cuda:0')), ('power', tensor([-0.3412], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.055765289813280106
epoch£º605	 i:1 	 global-step:12101	 l-p:0.0557330884039402
epoch£º605	 i:2 	 global-step:12102	 l-p:0.05573658645153046
epoch£º605	 i:3 	 global-step:12103	 l-p:0.05576792731881142
epoch£º605	 i:4 	 global-step:12104	 l-p:0.055947039276361465
epoch£º605	 i:5 	 global-step:12105	 l-p:0.05589783564209938
epoch£º605	 i:6 	 global-step:12106	 l-p:0.05601206794381142
epoch£º605	 i:7 	 global-step:12107	 l-p:0.055828388780355453
epoch£º605	 i:8 	 global-step:12108	 l-p:0.05586009845137596
epoch£º605	 i:9 	 global-step:12109	 l-p:0.055791132152080536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7952, 27.7952, 27.7952],
        [27.7952, 35.6732, 40.0850],
        [27.7952, 29.7462, 29.3233],
        [27.7952, 27.7969, 27.7952]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.05574403703212738 
model_pd.l_d.mean(): -0.00010755471157608554 
model_pd.lagr.mean(): 0.055636484175920486 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6270], device='cuda:0')), ('power', tensor([-0.4163], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.05574403703212738
epoch£º606	 i:1 	 global-step:12121	 l-p:0.05579638481140137
epoch£º606	 i:2 	 global-step:12122	 l-p:0.05579853802919388
epoch£º606	 i:3 	 global-step:12123	 l-p:0.05577083304524422
epoch£º606	 i:4 	 global-step:12124	 l-p:0.055782392621040344
epoch£º606	 i:5 	 global-step:12125	 l-p:0.055927250534296036
epoch£º606	 i:6 	 global-step:12126	 l-p:0.05575481429696083
epoch£º606	 i:7 	 global-step:12127	 l-p:0.05577521026134491
epoch£º606	 i:8 	 global-step:12128	 l-p:0.05613509565591812
epoch£º606	 i:9 	 global-step:12129	 l-p:0.05593360960483551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7774, 27.8264, 27.7813],
        [27.7774, 29.9920, 29.6459],
        [27.7774, 27.7841, 27.7776],
        [27.7774, 29.5566, 29.0971]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.05577266961336136 
model_pd.l_d.mean(): -3.4465851058484986e-05 
model_pd.lagr.mean(): 0.05573820322751999 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.9006e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6004], device='cuda:0')), ('power', tensor([-0.3895], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.05577266961336136
epoch£º607	 i:1 	 global-step:12141	 l-p:0.05597573146224022
epoch£º607	 i:2 	 global-step:12142	 l-p:0.055755965411663055
epoch£º607	 i:3 	 global-step:12143	 l-p:0.05583782494068146
epoch£º607	 i:4 	 global-step:12144	 l-p:0.05578618869185448
epoch£º607	 i:5 	 global-step:12145	 l-p:0.055773790925741196
epoch£º607	 i:6 	 global-step:12146	 l-p:0.05584506690502167
epoch£º607	 i:7 	 global-step:12147	 l-p:0.05592651292681694
epoch£º607	 i:8 	 global-step:12148	 l-p:0.055775124579668045
epoch£º607	 i:9 	 global-step:12149	 l-p:0.05597485601902008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7929, 27.8000, 27.7931],
        [27.7929, 27.8216, 27.7946],
        [27.7929, 28.1958, 27.9102],
        [27.7929, 35.4826, 39.6763]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.05590168759226799 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05590168759226799 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5399], device='cuda:0')), ('power', tensor([-0.2328], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.05590168759226799
epoch£º608	 i:1 	 global-step:12161	 l-p:0.05574652552604675
epoch£º608	 i:2 	 global-step:12162	 l-p:0.05578720569610596
epoch£º608	 i:3 	 global-step:12163	 l-p:0.05578027665615082
epoch£º608	 i:4 	 global-step:12164	 l-p:0.055770594626665115
epoch£º608	 i:5 	 global-step:12165	 l-p:0.0559743233025074
epoch£º608	 i:6 	 global-step:12166	 l-p:0.05570438131690025
epoch£º608	 i:7 	 global-step:12167	 l-p:0.05590686574578285
epoch£º608	 i:8 	 global-step:12168	 l-p:0.05585154518485069
epoch£º608	 i:9 	 global-step:12169	 l-p:0.05593882501125336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8302, 27.8302, 27.8302],
        [27.8302, 27.9254, 27.8415],
        [27.8302, 35.5307, 39.7304],
        [27.8302, 28.6775, 28.2260]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.05596582219004631 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05596582219004631 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5195], device='cuda:0')), ('power', tensor([-0.1739], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.05596582219004631
epoch£º609	 i:1 	 global-step:12181	 l-p:0.055735375732183456
epoch£º609	 i:2 	 global-step:12182	 l-p:0.05578391253948212
epoch£º609	 i:3 	 global-step:12183	 l-p:0.05582178011536598
epoch£º609	 i:4 	 global-step:12184	 l-p:0.05573628470301628
epoch£º609	 i:5 	 global-step:12185	 l-p:0.055734507739543915
epoch£º609	 i:6 	 global-step:12186	 l-p:0.0560000017285347
epoch£º609	 i:7 	 global-step:12187	 l-p:0.055923402309417725
epoch£º609	 i:8 	 global-step:12188	 l-p:0.05581403523683548
epoch£º609	 i:9 	 global-step:12189	 l-p:0.05575237050652504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8758, 28.7245, 28.2722],
        [27.8758, 27.8775, 27.8759],
        [27.8758, 27.8760, 27.8758],
        [27.8758, 28.0737, 27.9126]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.05576329678297043 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05576329678297043 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5992], device='cuda:0')), ('power', tensor([-0.3003], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.05576329678297043
epoch£º610	 i:1 	 global-step:12201	 l-p:0.055693965405225754
epoch£º610	 i:2 	 global-step:12202	 l-p:0.055929187685251236
epoch£º610	 i:3 	 global-step:12203	 l-p:0.055942341685295105
epoch£º610	 i:4 	 global-step:12204	 l-p:0.055817440152168274
epoch£º610	 i:5 	 global-step:12205	 l-p:0.05571256950497627
epoch£º610	 i:6 	 global-step:12206	 l-p:0.055895958095788956
epoch£º610	 i:7 	 global-step:12207	 l-p:0.05574566125869751
epoch£º610	 i:8 	 global-step:12208	 l-p:0.05569235980510712
epoch£º610	 i:9 	 global-step:12209	 l-p:0.05596756935119629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9243, 29.9571, 29.5507],
        [27.9243, 36.1557, 40.9610],
        [27.9243, 37.7872, 44.6434],
        [27.9243, 27.9295, 27.9244]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.05573480576276779 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05573480576276779 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6206], device='cuda:0')), ('power', tensor([-0.3371], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.05573480576276779
epoch£º611	 i:1 	 global-step:12221	 l-p:0.05573569983243942
epoch£º611	 i:2 	 global-step:12222	 l-p:0.0557502917945385
epoch£º611	 i:3 	 global-step:12223	 l-p:0.055860161781311035
epoch£º611	 i:4 	 global-step:12224	 l-p:0.055856943130493164
epoch£º611	 i:5 	 global-step:12225	 l-p:0.05613641440868378
epoch£º611	 i:6 	 global-step:12226	 l-p:0.05569365993142128
epoch£º611	 i:7 	 global-step:12227	 l-p:0.055838946253061295
epoch£º611	 i:8 	 global-step:12228	 l-p:0.05573805049061775
epoch£º611	 i:9 	 global-step:12229	 l-p:0.05570457875728607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9740, 33.2322, 34.8842],
        [27.9740, 27.9742, 27.9740],
        [27.9740, 29.7130, 29.2406],
        [27.9740, 27.9785, 27.9741]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.05591800436377525 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05591800436377525 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5818], device='cuda:0')), ('power', tensor([-0.0909], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.05591800436377525
epoch£º612	 i:1 	 global-step:12241	 l-p:0.05572996661067009
epoch£º612	 i:2 	 global-step:12242	 l-p:0.05573062226176262
epoch£º612	 i:3 	 global-step:12243	 l-p:0.05567985773086548
epoch£º612	 i:4 	 global-step:12244	 l-p:0.05583421513438225
epoch£º612	 i:5 	 global-step:12245	 l-p:0.05579555407166481
epoch£º612	 i:6 	 global-step:12246	 l-p:0.05601339042186737
epoch£º612	 i:7 	 global-step:12247	 l-p:0.055698737502098083
epoch£º612	 i:8 	 global-step:12248	 l-p:0.05571228265762329
epoch£º612	 i:9 	 global-step:12249	 l-p:0.055824313312768936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0242, 28.0248, 28.0242],
        [28.0242, 28.1613, 28.0444],
        [28.0242, 28.0403, 28.0249],
        [28.0242, 28.3808, 28.1197]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.05570793151855469 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05570793151855469 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6248], device='cuda:0')), ('power', tensor([-0.1575], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.05570793151855469
epoch£º613	 i:1 	 global-step:12261	 l-p:0.05571912229061127
epoch£º613	 i:2 	 global-step:12262	 l-p:0.05580035224556923
epoch£º613	 i:3 	 global-step:12263	 l-p:0.05575837194919586
epoch£º613	 i:4 	 global-step:12264	 l-p:0.05567670613527298
epoch£º613	 i:5 	 global-step:12265	 l-p:0.05610164627432823
epoch£º613	 i:6 	 global-step:12266	 l-p:0.05570276081562042
epoch£º613	 i:7 	 global-step:12267	 l-p:0.05581333115696907
epoch£º613	 i:8 	 global-step:12268	 l-p:0.05568573996424675
epoch£º613	 i:9 	 global-step:12269	 l-p:0.05585787817835808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0746, 32.6817, 33.7624],
        [28.0746, 30.5087, 30.2287],
        [28.0746, 28.0917, 28.0753],
        [28.0746, 28.0830, 28.0748]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.05570982024073601 
model_pd.l_d.mean(): -1.6484769105318264e-08 
model_pd.lagr.mean(): 0.055709805339574814 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6071], device='cuda:0')), ('power', tensor([-0.0891], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.05570982024073601
epoch£º614	 i:1 	 global-step:12281	 l-p:0.05571272596716881
epoch£º614	 i:2 	 global-step:12282	 l-p:0.055737998336553574
epoch£º614	 i:3 	 global-step:12283	 l-p:0.05584533512592316
epoch£º614	 i:4 	 global-step:12284	 l-p:0.055755242705345154
epoch£º614	 i:5 	 global-step:12285	 l-p:0.05589915066957474
epoch£º614	 i:6 	 global-step:12286	 l-p:0.05567065253853798
epoch£º614	 i:7 	 global-step:12287	 l-p:0.05568491667509079
epoch£º614	 i:8 	 global-step:12288	 l-p:0.05596719682216644
epoch£º614	 i:9 	 global-step:12289	 l-p:0.05572796240448952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1252, 30.7842, 30.5956],
        [28.1252, 30.4899, 30.1815],
        [28.1252, 34.4526, 37.1143],
        [28.1252, 28.1252, 28.1252]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.055962368845939636 
model_pd.l_d.mean(): 7.671438311263046e-07 
model_pd.lagr.mean(): 0.05596313625574112 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.3107e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5644], device='cuda:0')), ('power', tensor([0.0882], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.055962368845939636
epoch£º615	 i:1 	 global-step:12301	 l-p:0.05570373311638832
epoch£º615	 i:2 	 global-step:12302	 l-p:0.05577603727579117
epoch£º615	 i:3 	 global-step:12303	 l-p:0.05567094311118126
epoch£º615	 i:4 	 global-step:12304	 l-p:0.05564899742603302
epoch£º615	 i:5 	 global-step:12305	 l-p:0.055648889392614365
epoch£º615	 i:6 	 global-step:12306	 l-p:0.05565877631306648
epoch£º615	 i:7 	 global-step:12307	 l-p:0.05568958818912506
epoch£º615	 i:8 	 global-step:12308	 l-p:0.056033238768577576
epoch£º615	 i:9 	 global-step:12309	 l-p:0.05580645427107811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1750, 28.1853, 28.1754],
        [28.1750, 28.3650, 28.2092],
        [28.1750, 35.5337, 39.2879],
        [28.1750, 32.1422, 32.7228]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.05595393478870392 
model_pd.l_d.mean(): 4.762977368955035e-06 
model_pd.lagr.mean(): 0.05595869943499565 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.1777e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5027], device='cuda:0')), ('power', tensor([0.2422], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.05595393478870392
epoch£º616	 i:1 	 global-step:12321	 l-p:0.055642660707235336
epoch£º616	 i:2 	 global-step:12322	 l-p:0.056025028228759766
epoch£º616	 i:3 	 global-step:12323	 l-p:0.055654678493738174
epoch£º616	 i:4 	 global-step:12324	 l-p:0.05576937645673752
epoch£º616	 i:5 	 global-step:12325	 l-p:0.055691737681627274
epoch£º616	 i:6 	 global-step:12326	 l-p:0.05573393777012825
epoch£º616	 i:7 	 global-step:12327	 l-p:0.05568230152130127
epoch£º616	 i:8 	 global-step:12328	 l-p:0.055650681257247925
epoch£º616	 i:9 	 global-step:12329	 l-p:0.05568673089146614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2223, 28.6017, 28.3275],
        [28.2223, 29.4705, 28.9598],
        [28.2223, 38.4132, 45.6354],
        [28.2223, 28.5937, 28.3239]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.055712420493364334 
model_pd.l_d.mean(): 3.656275112007279e-06 
model_pd.lagr.mean(): 0.055716075003147125 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.7504e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6120], device='cuda:0')), ('power', tensor([0.0676], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.055712420493364334
epoch£º617	 i:1 	 global-step:12341	 l-p:0.055877283215522766
epoch£º617	 i:2 	 global-step:12342	 l-p:0.05579216778278351
epoch£º617	 i:3 	 global-step:12343	 l-p:0.05577539652585983
epoch£º617	 i:4 	 global-step:12344	 l-p:0.05569649487733841
epoch£º617	 i:5 	 global-step:12345	 l-p:0.055643003433942795
epoch£º617	 i:6 	 global-step:12346	 l-p:0.055818408727645874
epoch£º617	 i:7 	 global-step:12347	 l-p:0.05565766245126724
epoch£º617	 i:8 	 global-step:12348	 l-p:0.05573441833257675
epoch£º617	 i:9 	 global-step:12349	 l-p:0.05568523332476616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2627, 30.8476, 30.6198],
        [28.2627, 28.6528, 28.3727],
        [28.2627, 38.3110, 45.3344],
        [28.2627, 30.0376, 29.5630]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.05569576099514961 
model_pd.l_d.mean(): 1.1818291568488348e-05 
model_pd.lagr.mean(): 0.055707577615976334 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6133], device='cuda:0')), ('power', tensor([0.1069], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.05569576099514961
epoch£º618	 i:1 	 global-step:12361	 l-p:0.055623263120651245
epoch£º618	 i:2 	 global-step:12362	 l-p:0.05574236810207367
epoch£º618	 i:3 	 global-step:12363	 l-p:0.05575123056769371
epoch£º618	 i:4 	 global-step:12364	 l-p:0.055759236216545105
epoch£º618	 i:5 	 global-step:12365	 l-p:0.055856749415397644
epoch£º618	 i:6 	 global-step:12366	 l-p:0.05563607066869736
epoch£º618	 i:7 	 global-step:12367	 l-p:0.05561510846018791
epoch£º618	 i:8 	 global-step:12368	 l-p:0.05600108951330185
epoch£º618	 i:9 	 global-step:12369	 l-p:0.05563217028975487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2923, 28.6647, 28.3942],
        [28.2923, 29.1717, 28.7083],
        [28.2923, 29.2171, 28.7439],
        [28.2923, 35.1384, 38.3158]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.055674199014902115 
model_pd.l_d.mean(): 3.275162453064695e-05 
model_pd.lagr.mean(): 0.05570695176720619 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6118], device='cuda:0')), ('power', tensor([0.1773], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.055674199014902115
epoch£º619	 i:1 	 global-step:12381	 l-p:0.05599767714738846
epoch£º619	 i:2 	 global-step:12382	 l-p:0.05576040968298912
epoch£º619	 i:3 	 global-step:12383	 l-p:0.05575863644480705
epoch£º619	 i:4 	 global-step:12384	 l-p:0.05561491474509239
epoch£º619	 i:5 	 global-step:12385	 l-p:0.055851154029369354
epoch£º619	 i:6 	 global-step:12386	 l-p:0.055703163146972656
epoch£º619	 i:7 	 global-step:12387	 l-p:0.05562799796462059
epoch£º619	 i:8 	 global-step:12388	 l-p:0.05562872812151909
epoch£º619	 i:9 	 global-step:12389	 l-p:0.05564538389444351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3067, 28.4332, 28.3243],
        [28.3067, 29.5214, 29.0110],
        [28.3067, 28.3067, 28.3067],
        [28.3067, 33.8947, 35.8105]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.05561418831348419 
model_pd.l_d.mean(): 2.7789659725385718e-05 
model_pd.lagr.mean(): 0.05564197897911072 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6699], device='cuda:0')), ('power', tensor([0.1027], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.05561418831348419
epoch£º620	 i:1 	 global-step:12401	 l-p:0.05566147714853287
epoch£º620	 i:2 	 global-step:12402	 l-p:0.05561237037181854
epoch£º620	 i:3 	 global-step:12403	 l-p:0.05572575330734253
epoch£º620	 i:4 	 global-step:12404	 l-p:0.05568930506706238
epoch£º620	 i:5 	 global-step:12405	 l-p:0.05587232485413551
epoch£º620	 i:6 	 global-step:12406	 l-p:0.056023500859737396
epoch£º620	 i:7 	 global-step:12407	 l-p:0.05565011873841286
epoch£º620	 i:8 	 global-step:12408	 l-p:0.055630262941122055
epoch£º620	 i:9 	 global-step:12409	 l-p:0.0557696670293808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3028, 34.7576, 37.5244],
        [28.3028, 28.3072, 28.3029],
        [28.3028, 28.3028, 28.3028],
        [28.3028, 28.3030, 28.3028]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.055619094520807266 
model_pd.l_d.mean(): 1.4168747838994022e-05 
model_pd.lagr.mean(): 0.05563326179981232 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6686], device='cuda:0')), ('power', tensor([0.0394], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.055619094520807266
epoch£º621	 i:1 	 global-step:12421	 l-p:0.05581372231245041
epoch£º621	 i:2 	 global-step:12422	 l-p:0.05567241460084915
epoch£º621	 i:3 	 global-step:12423	 l-p:0.0557435117661953
epoch£º621	 i:4 	 global-step:12424	 l-p:0.05566475912928581
epoch£º621	 i:5 	 global-step:12425	 l-p:0.055922430008649826
epoch£º621	 i:6 	 global-step:12426	 l-p:0.05577573552727699
epoch£º621	 i:7 	 global-step:12427	 l-p:0.055641498416662216
epoch£º621	 i:8 	 global-step:12428	 l-p:0.055702462792396545
epoch£º621	 i:9 	 global-step:12429	 l-p:0.055720116943120956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2798, 28.2798, 28.2798],
        [28.2798, 30.2759, 29.8476],
        [28.2798, 32.9222, 34.0114],
        [28.2798, 29.6927, 29.1802]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.055631618946790695 
model_pd.l_d.mean(): 2.538489570724778e-05 
model_pd.lagr.mean(): 0.055657003074884415 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6546], device='cuda:0')), ('power', tensor([0.0574], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.055631618946790695
epoch£º622	 i:1 	 global-step:12441	 l-p:0.056074466556310654
epoch£º622	 i:2 	 global-step:12442	 l-p:0.055667147040367126
epoch£º622	 i:3 	 global-step:12443	 l-p:0.05562691390514374
epoch£º622	 i:4 	 global-step:12444	 l-p:0.055631861090660095
epoch£º622	 i:5 	 global-step:12445	 l-p:0.055833060294389725
epoch£º622	 i:6 	 global-step:12446	 l-p:0.05567415431141853
epoch£º622	 i:7 	 global-step:12447	 l-p:0.05567865073680878
epoch£º622	 i:8 	 global-step:12448	 l-p:0.05567779764533043
epoch£º622	 i:9 	 global-step:12449	 l-p:0.055850256234407425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2372, 28.4324, 28.2729],
        [28.2372, 29.0976, 28.6391],
        [28.2372, 28.2700, 28.2393],
        [28.2372, 28.2380, 28.2372]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.05572471395134926 
model_pd.l_d.mean(): 1.7052137991413474e-05 
model_pd.lagr.mean(): 0.05574176460504532 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6348], device='cuda:0')), ('power', tensor([0.0335], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.05572471395134926
epoch£º623	 i:1 	 global-step:12461	 l-p:0.05566071718931198
epoch£º623	 i:2 	 global-step:12462	 l-p:0.05590328201651573
epoch£º623	 i:3 	 global-step:12463	 l-p:0.05575074255466461
epoch£º623	 i:4 	 global-step:12464	 l-p:0.055641163140535355
epoch£º623	 i:5 	 global-step:12465	 l-p:0.05562455952167511
epoch£º623	 i:6 	 global-step:12466	 l-p:0.05583715811371803
epoch£º623	 i:7 	 global-step:12467	 l-p:0.05594004690647125
epoch£º623	 i:8 	 global-step:12468	 l-p:0.055717334151268005
epoch£º623	 i:9 	 global-step:12469	 l-p:0.0556570328772068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1785, 28.2217, 28.1817],
        [28.1785, 37.1864, 42.8903],
        [28.1785, 28.1869, 28.1787],
        [28.1785, 28.1785, 28.1785]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.055684976279735565 
model_pd.l_d.mean(): -1.3439680515148211e-06 
model_pd.lagr.mean(): 0.05568363144993782 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6191], device='cuda:0')), ('power', tensor([-0.0024], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.055684976279735565
epoch£º624	 i:1 	 global-step:12481	 l-p:0.05566086620092392
epoch£º624	 i:2 	 global-step:12482	 l-p:0.055906448513269424
epoch£º624	 i:3 	 global-step:12483	 l-p:0.05567840114235878
epoch£º624	 i:4 	 global-step:12484	 l-p:0.05579536780714989
epoch£º624	 i:5 	 global-step:12485	 l-p:0.05572320893406868
epoch£º624	 i:6 	 global-step:12486	 l-p:0.05569063127040863
epoch£º624	 i:7 	 global-step:12487	 l-p:0.055933646857738495
epoch£º624	 i:8 	 global-step:12488	 l-p:0.055808015167713165
epoch£º624	 i:9 	 global-step:12489	 l-p:0.05571857467293739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1080, 28.1080, 28.1080],
        [28.1080, 28.1729, 28.1141],
        [28.1080, 28.1140, 28.1082],
        [28.1080, 31.2465, 31.3008]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.055720727890729904 
model_pd.l_d.mean(): -1.907354271679651e-05 
model_pd.lagr.mean(): 0.05570165440440178 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6042], device='cuda:0')), ('power', tensor([-0.0340], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.055720727890729904
epoch£º625	 i:1 	 global-step:12501	 l-p:0.05587603524327278
epoch£º625	 i:2 	 global-step:12502	 l-p:0.0557878203690052
epoch£º625	 i:3 	 global-step:12503	 l-p:0.055647194385528564
epoch£º625	 i:4 	 global-step:12504	 l-p:0.055716607719659805
epoch£º625	 i:5 	 global-step:12505	 l-p:0.05584955960512161
epoch£º625	 i:6 	 global-step:12506	 l-p:0.055953897535800934
epoch£º625	 i:7 	 global-step:12507	 l-p:0.055815547704696655
epoch£º625	 i:8 	 global-step:12508	 l-p:0.055676888674497604
epoch£º625	 i:9 	 global-step:12509	 l-p:0.05571860074996948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0326, 28.2775, 28.0844],
        [28.0326, 30.0487, 29.6341],
        [28.0326, 28.0326, 28.0326],
        [28.0326, 37.1333, 42.9836]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.05567794665694237 
model_pd.l_d.mean(): -0.00011564519081730396 
model_pd.lagr.mean(): 0.055562302470207214 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6497], device='cuda:0')), ('power', tensor([-0.2165], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.05567794665694237
epoch£º626	 i:1 	 global-step:12521	 l-p:0.05587837100028992
epoch£º626	 i:2 	 global-step:12522	 l-p:0.055707648396492004
epoch£º626	 i:3 	 global-step:12523	 l-p:0.05587269738316536
epoch£º626	 i:4 	 global-step:12524	 l-p:0.0557730570435524
epoch£º626	 i:5 	 global-step:12525	 l-p:0.055748820304870605
epoch£º626	 i:6 	 global-step:12526	 l-p:0.055985040962696075
epoch£º626	 i:7 	 global-step:12527	 l-p:0.055695485323667526
epoch£º626	 i:8 	 global-step:12528	 l-p:0.05577604100108147
epoch£º626	 i:9 	 global-step:12529	 l-p:0.055815886706113815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9600, 28.6319, 28.2299],
        [27.9600, 31.1241, 31.2023],
        [27.9600, 28.5645, 28.1869],
        [27.9600, 28.1395, 27.9913]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.05570419132709503 
model_pd.l_d.mean(): -0.00014230766100808978 
model_pd.lagr.mean(): 0.05556188523769379 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6356], device='cuda:0')), ('power', tensor([-0.3022], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.05570419132709503
epoch£º627	 i:1 	 global-step:12541	 l-p:0.05582025274634361
epoch£º627	 i:2 	 global-step:12542	 l-p:0.055710114538669586
epoch£º627	 i:3 	 global-step:12543	 l-p:0.05569644272327423
epoch£º627	 i:4 	 global-step:12544	 l-p:0.05607065185904503
epoch£º627	 i:5 	 global-step:12545	 l-p:0.05587459355592728
epoch£º627	 i:6 	 global-step:12546	 l-p:0.055783048272132874
epoch£º627	 i:7 	 global-step:12547	 l-p:0.05572722479701042
epoch£º627	 i:8 	 global-step:12548	 l-p:0.05574968829751015
epoch£º627	 i:9 	 global-step:12549	 l-p:0.055948808789253235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8988, 30.3400, 30.0708],
        [27.8988, 27.9004, 27.8988],
        [27.8988, 27.9030, 27.8989],
        [27.8988, 28.3070, 28.0183]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.05571213737130165 
model_pd.l_d.mean(): -0.00013034819858148694 
model_pd.lagr.mean(): 0.05558178946375847 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6317], device='cuda:0')), ('power', tensor([-0.3489], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.05571213737130165
epoch£º628	 i:1 	 global-step:12561	 l-p:0.05574563890695572
epoch£º628	 i:2 	 global-step:12562	 l-p:0.055743634700775146
epoch£º628	 i:3 	 global-step:12563	 l-p:0.05577762424945831
epoch£º628	 i:4 	 global-step:12564	 l-p:0.055965907871723175
epoch£º628	 i:5 	 global-step:12565	 l-p:0.05570732802152634
epoch£º628	 i:6 	 global-step:12566	 l-p:0.055828556418418884
epoch£º628	 i:7 	 global-step:12567	 l-p:0.05598979815840721
epoch£º628	 i:8 	 global-step:12568	 l-p:0.05588645860552788
epoch£º628	 i:9 	 global-step:12569	 l-p:0.055848006159067154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8575, 27.8617, 27.8576],
        [27.8575, 29.8167, 29.3936],
        [27.8575, 27.8796, 27.8586],
        [27.8575, 35.5658, 39.7698]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.055696532130241394 
model_pd.l_d.mean(): -0.00010733873205026612 
model_pd.lagr.mean(): 0.05558919161558151 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6694], device='cuda:0')), ('power', tensor([-0.4290], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.055696532130241394
epoch£º629	 i:1 	 global-step:12581	 l-p:0.05584236979484558
epoch£º629	 i:2 	 global-step:12582	 l-p:0.05588287115097046
epoch£º629	 i:3 	 global-step:12583	 l-p:0.05594048276543617
epoch£º629	 i:4 	 global-step:12584	 l-p:0.05574478209018707
epoch£º629	 i:5 	 global-step:12585	 l-p:0.055760715156793594
epoch£º629	 i:6 	 global-step:12586	 l-p:0.05578215792775154
epoch£º629	 i:7 	 global-step:12587	 l-p:0.055801790207624435
epoch£º629	 i:8 	 global-step:12588	 l-p:0.056020621210336685
epoch£º629	 i:9 	 global-step:12589	 l-p:0.05579967796802521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8427, 27.8428, 27.8427],
        [27.8427, 29.7981, 29.3746],
        [27.8427, 30.4735, 30.2867],
        [27.8427, 27.8427, 27.8427]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.05583595111966133 
model_pd.l_d.mean(): -3.0068282285355963e-05 
model_pd.lagr.mean(): 0.055805884301662445 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.8596e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5908], device='cuda:0')), ('power', tensor([-0.2684], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.05583595111966133
epoch£º630	 i:1 	 global-step:12601	 l-p:0.05581599101424217
epoch£º630	 i:2 	 global-step:12602	 l-p:0.0557505339384079
epoch£º630	 i:3 	 global-step:12603	 l-p:0.055882491171360016
epoch£º630	 i:4 	 global-step:12604	 l-p:0.05578138306736946
epoch£º630	 i:5 	 global-step:12605	 l-p:0.05575210601091385
epoch£º630	 i:6 	 global-step:12606	 l-p:0.055742137134075165
epoch£º630	 i:7 	 global-step:12607	 l-p:0.05586379021406174
epoch£º630	 i:8 	 global-step:12608	 l-p:0.05594540387392044
epoch£º630	 i:9 	 global-step:12609	 l-p:0.05590556934475899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8575, 27.9443, 27.8672],
        [27.8575, 33.0927, 34.7373],
        [27.8575, 27.8575, 27.8575],
        [27.8575, 28.8302, 28.3531]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.05570216476917267 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05570216476917267 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6596], device='cuda:0')), ('power', tensor([-0.4296], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.05570216476917267
epoch£º631	 i:1 	 global-step:12621	 l-p:0.055942337960004807
epoch£º631	 i:2 	 global-step:12622	 l-p:0.05577082559466362
epoch£º631	 i:3 	 global-step:12623	 l-p:0.055964212864637375
epoch£º631	 i:4 	 global-step:12624	 l-p:0.05580395087599754
epoch£º631	 i:5 	 global-step:12625	 l-p:0.05599775165319443
epoch£º631	 i:6 	 global-step:12626	 l-p:0.05574898421764374
epoch£º631	 i:7 	 global-step:12627	 l-p:0.05569624900817871
epoch£º631	 i:8 	 global-step:12628	 l-p:0.05582835152745247
epoch£º631	 i:9 	 global-step:12629	 l-p:0.055759020149707794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8973, 28.2222, 27.9796],
        [27.8973, 27.9854, 27.9073],
        [27.8973, 27.9228, 27.8987],
        [27.8973, 27.9174, 27.8982]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.05596480146050453 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05596480146050453 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5717], device='cuda:0')), ('power', tensor([-0.1801], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.05596480146050453
epoch£º632	 i:1 	 global-step:12641	 l-p:0.0559353232383728
epoch£º632	 i:2 	 global-step:12642	 l-p:0.055719126015901566
epoch£º632	 i:3 	 global-step:12643	 l-p:0.0557023324072361
epoch£º632	 i:4 	 global-step:12644	 l-p:0.05580128729343414
epoch£º632	 i:5 	 global-step:12645	 l-p:0.05572137609124184
epoch£º632	 i:6 	 global-step:12646	 l-p:0.0557667538523674
epoch£º632	 i:7 	 global-step:12647	 l-p:0.05582398921251297
epoch£º632	 i:8 	 global-step:12648	 l-p:0.0557243749499321
epoch£º632	 i:9 	 global-step:12649	 l-p:0.055952053517103195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9471, 28.0209, 27.9546],
        [27.9471, 30.8577, 30.7995],
        [27.9471, 28.7407, 28.3016],
        [27.9471, 38.2140, 45.6032]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.05584821477532387 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05584821477532387 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5846], device='cuda:0')), ('power', tensor([-0.1456], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.05584821477532387
epoch£º633	 i:1 	 global-step:12661	 l-p:0.05574683099985123
epoch£º633	 i:2 	 global-step:12662	 l-p:0.055726297199726105
epoch£º633	 i:3 	 global-step:12663	 l-p:0.05595478042960167
epoch£º633	 i:4 	 global-step:12664	 l-p:0.0557849295437336
epoch£º633	 i:5 	 global-step:12665	 l-p:0.05572767183184624
epoch£º633	 i:6 	 global-step:12666	 l-p:0.05571348965167999
epoch£º633	 i:7 	 global-step:12667	 l-p:0.055792465806007385
epoch£º633	 i:8 	 global-step:12668	 l-p:0.05569206923246384
epoch£º633	 i:9 	 global-step:12669	 l-p:0.05600764974951744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0005, 28.0005, 28.0005],
        [28.0005, 28.0192, 28.0014],
        [28.0005, 28.7128, 28.2972],
        [28.0005, 28.6240, 28.2390]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.05577963590621948 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05577963590621948 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5769], device='cuda:0')), ('power', tensor([-0.1406], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.05577963590621948
epoch£º634	 i:1 	 global-step:12681	 l-p:0.05570312216877937
epoch£º634	 i:2 	 global-step:12682	 l-p:0.055801257491111755
epoch£º634	 i:3 	 global-step:12683	 l-p:0.05574103817343712
epoch£º634	 i:4 	 global-step:12684	 l-p:0.05578572303056717
epoch£º634	 i:5 	 global-step:12685	 l-p:0.055674295872449875
epoch£º634	 i:6 	 global-step:12686	 l-p:0.05586228892207146
epoch£º634	 i:7 	 global-step:12687	 l-p:0.055719852447509766
epoch£º634	 i:8 	 global-step:12688	 l-p:0.0561085119843483
epoch£º634	 i:9 	 global-step:12689	 l-p:0.055696677416563034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0553, 29.2956, 28.7881],
        [28.0553, 29.1601, 28.6625],
        [28.0553, 28.0596, 28.0554],
        [28.0553, 32.3472, 33.1797]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.05569220334291458 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05569220334291458 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6257], device='cuda:0')), ('power', tensor([-0.1204], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.05569220334291458
epoch£º635	 i:1 	 global-step:12701	 l-p:0.05617254227399826
epoch£º635	 i:2 	 global-step:12702	 l-p:0.0557384192943573
epoch£º635	 i:3 	 global-step:12703	 l-p:0.055778928101062775
epoch£º635	 i:4 	 global-step:12704	 l-p:0.05574117228388786
epoch£º635	 i:5 	 global-step:12705	 l-p:0.05583463981747627
epoch£º635	 i:6 	 global-step:12706	 l-p:0.0556766577064991
epoch£º635	 i:7 	 global-step:12707	 l-p:0.055722832679748535
epoch£º635	 i:8 	 global-step:12708	 l-p:0.05568581447005272
epoch£º635	 i:9 	 global-step:12709	 l-p:0.055707044899463654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1107, 28.4805, 28.2119],
        [28.1107, 28.2043, 28.1217],
        [28.1107, 33.4008, 35.0659],
        [28.1107, 28.1112, 28.1107]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.05570260062813759 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05570260062813759 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6225], device='cuda:0')), ('power', tensor([-0.1234], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.05570260062813759
epoch£º636	 i:1 	 global-step:12721	 l-p:0.05566516891121864
epoch£º636	 i:2 	 global-step:12722	 l-p:0.0557759553194046
epoch£º636	 i:3 	 global-step:12723	 l-p:0.055729031562805176
epoch£º636	 i:4 	 global-step:12724	 l-p:0.0556960254907608
epoch£º636	 i:5 	 global-step:12725	 l-p:0.055882230401039124
epoch£º636	 i:6 	 global-step:12726	 l-p:0.05584605410695076
epoch£º636	 i:7 	 global-step:12727	 l-p:0.055940449237823486
epoch£º636	 i:8 	 global-step:12728	 l-p:0.05572781339287758
epoch£º636	 i:9 	 global-step:12729	 l-p:0.05566055327653885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1659, 29.9369, 29.4644],
        [28.1659, 38.4766, 45.8726],
        [28.1659, 29.1657, 28.6804],
        [28.1659, 28.2549, 28.1760]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.05587111413478851 
model_pd.l_d.mean(): 1.5083386415426503e-06 
model_pd.lagr.mean(): 0.05587262287735939 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.1318e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5864], device='cuda:0')), ('power', tensor([0.0896], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.05587111413478851
epoch£º637	 i:1 	 global-step:12741	 l-p:0.05598083510994911
epoch£º637	 i:2 	 global-step:12742	 l-p:0.05573602393269539
epoch£º637	 i:3 	 global-step:12743	 l-p:0.055660899728536606
epoch£º637	 i:4 	 global-step:12744	 l-p:0.05577364191412926
epoch£º637	 i:5 	 global-step:12745	 l-p:0.055651091039180756
epoch£º637	 i:6 	 global-step:12746	 l-p:0.055808428674936295
epoch£º637	 i:7 	 global-step:12747	 l-p:0.05567113682627678
epoch£º637	 i:8 	 global-step:12748	 l-p:0.05566951259970665
epoch£º637	 i:9 	 global-step:12749	 l-p:0.05568350851535797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2184, 28.2198, 28.2184],
        [28.2184, 28.2184, 28.2184],
        [28.2184, 36.0301, 40.2901],
        [28.2184, 37.5404, 43.6316]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.05567290261387825 
model_pd.l_d.mean(): 6.482627213699743e-06 
model_pd.lagr.mean(): 0.055679384618997574 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.4691e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6085], device='cuda:0')), ('power', tensor([0.1353], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.05567290261387825
epoch£º638	 i:1 	 global-step:12761	 l-p:0.05568742752075195
epoch£º638	 i:2 	 global-step:12762	 l-p:0.05567101761698723
epoch£º638	 i:3 	 global-step:12763	 l-p:0.05589645355939865
epoch£º638	 i:4 	 global-step:12764	 l-p:0.05583678558468819
epoch£º638	 i:5 	 global-step:12765	 l-p:0.055768344551324844
epoch£º638	 i:6 	 global-step:12766	 l-p:0.05564366281032562
epoch£º638	 i:7 	 global-step:12767	 l-p:0.05574006959795952
epoch£º638	 i:8 	 global-step:12768	 l-p:0.05575883761048317
epoch£º638	 i:9 	 global-step:12769	 l-p:0.055720143020153046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2641, 37.8510, 44.2716],
        [28.2641, 28.2830, 28.2650],
        [28.2641, 30.3854, 29.9914],
        [28.2641, 30.1240, 29.6654]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.05597380921244621 
model_pd.l_d.mean(): 3.962218397646211e-05 
model_pd.lagr.mean(): 0.056013431400060654 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4720], device='cuda:0')), ('power', tensor([0.3826], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.05597380921244621
epoch£º639	 i:1 	 global-step:12781	 l-p:0.055656448006629944
epoch£º639	 i:2 	 global-step:12782	 l-p:0.055698174983263016
epoch£º639	 i:3 	 global-step:12783	 l-p:0.05562678351998329
epoch£º639	 i:4 	 global-step:12784	 l-p:0.05563012883067131
epoch£º639	 i:5 	 global-step:12785	 l-p:0.05569758266210556
epoch£º639	 i:6 	 global-step:12786	 l-p:0.055654630064964294
epoch£º639	 i:7 	 global-step:12787	 l-p:0.055609606206417084
epoch£º639	 i:8 	 global-step:12788	 l-p:0.056032709777355194
epoch£º639	 i:9 	 global-step:12789	 l-p:0.05572555586695671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2981, 30.2956, 29.8670],
        [28.2981, 31.2351, 31.1701],
        [28.2981, 28.2981, 28.2981],
        [28.2981, 28.3923, 28.3091]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.055700987577438354 
model_pd.l_d.mean(): 4.189684841549024e-05 
model_pd.lagr.mean(): 0.055742885917425156 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5734], device='cuda:0')), ('power', tensor([0.2334], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.055700987577438354
epoch£º640	 i:1 	 global-step:12801	 l-p:0.05568885803222656
epoch£º640	 i:2 	 global-step:12802	 l-p:0.05569363385438919
epoch£º640	 i:3 	 global-step:12803	 l-p:0.05561912804841995
epoch£º640	 i:4 	 global-step:12804	 l-p:0.05565296858549118
epoch£º640	 i:5 	 global-step:12805	 l-p:0.055748872458934784
epoch£º640	 i:6 	 global-step:12806	 l-p:0.05587300285696983
epoch£º640	 i:7 	 global-step:12807	 l-p:0.05563481152057648
epoch£º640	 i:8 	 global-step:12808	 l-p:0.05575798079371452
epoch£º640	 i:9 	 global-step:12809	 l-p:0.05587587133049965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3157, 28.3225, 28.3159],
        [28.3157, 33.8743, 35.7615],
        [28.3157, 28.3158, 28.3157],
        [28.3157, 28.6884, 28.4176]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.05563245713710785 
model_pd.l_d.mean(): 3.9794351323507726e-05 
model_pd.lagr.mean(): 0.05567225068807602 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6399], device='cuda:0')), ('power', tensor([0.1480], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.05563245713710785
epoch£º641	 i:1 	 global-step:12821	 l-p:0.05585567653179169
epoch£º641	 i:2 	 global-step:12822	 l-p:0.055736977607011795
epoch£º641	 i:3 	 global-step:12823	 l-p:0.055670201778411865
epoch£º641	 i:4 	 global-step:12824	 l-p:0.05591987818479538
epoch£º641	 i:5 	 global-step:12825	 l-p:0.05564567446708679
epoch£º641	 i:6 	 global-step:12826	 l-p:0.055620696395635605
epoch£º641	 i:7 	 global-step:12827	 l-p:0.05588936060667038
epoch£º641	 i:8 	 global-step:12828	 l-p:0.055617716163396835
epoch£º641	 i:9 	 global-step:12829	 l-p:0.05563836917281151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3127, 28.3138, 28.3127],
        [28.3127, 30.1760, 29.7165],
        [28.3127, 28.3127, 28.3127],
        [28.3127, 38.0785, 44.7200]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.05585764721035957 
model_pd.l_d.mean(): 0.0001251639041583985 
model_pd.lagr.mean(): 0.0559828095138073 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5398], device='cuda:0')), ('power', tensor([0.3453], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.05585764721035957
epoch£º642	 i:1 	 global-step:12841	 l-p:0.0556299202144146
epoch£º642	 i:2 	 global-step:12842	 l-p:0.055624835193157196
epoch£º642	 i:3 	 global-step:12843	 l-p:0.055616989731788635
epoch£º642	 i:4 	 global-step:12844	 l-p:0.055641163140535355
epoch£º642	 i:5 	 global-step:12845	 l-p:0.05572887882590294
epoch£º642	 i:6 	 global-step:12846	 l-p:0.05579448491334915
epoch£º642	 i:7 	 global-step:12847	 l-p:0.05597808584570885
epoch£º642	 i:8 	 global-step:12848	 l-p:0.055671319365501404
epoch£º642	 i:9 	 global-step:12849	 l-p:0.05571400374174118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2862, 28.7005, 28.4075],
        [28.2862, 28.2862, 28.2862],
        [28.2862, 32.8327, 33.8437],
        [28.2862, 28.2863, 28.2862]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.05592372640967369 
model_pd.l_d.mean(): 0.0001318208669545129 
model_pd.lagr.mean(): 0.056055545806884766 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5437], device='cuda:0')), ('power', tensor([0.2934], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.05592372640967369
epoch£º643	 i:1 	 global-step:12861	 l-p:0.0557648204267025
epoch£º643	 i:2 	 global-step:12862	 l-p:0.0556187629699707
epoch£º643	 i:3 	 global-step:12863	 l-p:0.05570213124155998
epoch£º643	 i:4 	 global-step:12864	 l-p:0.055631112307310104
epoch£º643	 i:5 	 global-step:12865	 l-p:0.05609286203980446
epoch£º643	 i:6 	 global-step:12866	 l-p:0.05562965199351311
epoch£º643	 i:7 	 global-step:12867	 l-p:0.05562898516654968
epoch£º643	 i:8 	 global-step:12868	 l-p:0.05569083243608475
epoch£º643	 i:9 	 global-step:12869	 l-p:0.05565369129180908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[28.2374, 30.6864, 30.4047],
        [28.2374, 32.7757, 33.7847],
        [28.2374, 30.1361, 29.6865],
        [28.2374, 28.9178, 28.5110]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.05569661036133766 
model_pd.l_d.mean(): 6.316691724350676e-05 
model_pd.lagr.mean(): 0.05575977638363838 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6054], device='cuda:0')), ('power', tensor([0.1219], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.05569661036133766
epoch£º644	 i:1 	 global-step:12881	 l-p:0.05574607849121094
epoch£º644	 i:2 	 global-step:12882	 l-p:0.055937595665454865
epoch£º644	 i:3 	 global-step:12883	 l-p:0.05565366521477699
epoch£º644	 i:4 	 global-step:12884	 l-p:0.05564885586500168
epoch£º644	 i:5 	 global-step:12885	 l-p:0.05581410229206085
epoch£º644	 i:6 	 global-step:12886	 l-p:0.05565958470106125
epoch£º644	 i:7 	 global-step:12887	 l-p:0.055874209851026535
epoch£º644	 i:8 	 global-step:12888	 l-p:0.05572894960641861
epoch£º644	 i:9 	 global-step:12889	 l-p:0.055705975741147995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1692, 28.5201, 28.2619],
        [28.1692, 29.7685, 29.2716],
        [28.1692, 33.8869, 35.9420],
        [28.1692, 28.1850, 28.1698]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.05573756620287895 
model_pd.l_d.mean(): 3.7229841836960986e-05 
model_pd.lagr.mean(): 0.05577479675412178 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5833], device='cuda:0')), ('power', tensor([0.0667], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.05573756620287895
epoch£º645	 i:1 	 global-step:12901	 l-p:0.05564839392900467
epoch£º645	 i:2 	 global-step:12902	 l-p:0.05587853863835335
epoch£º645	 i:3 	 global-step:12903	 l-p:0.055735886096954346
epoch£º645	 i:4 	 global-step:12904	 l-p:0.05610106512904167
epoch£º645	 i:5 	 global-step:12905	 l-p:0.05574005842208862
epoch£º645	 i:6 	 global-step:12906	 l-p:0.05569039285182953
epoch£º645	 i:7 	 global-step:12907	 l-p:0.05573420226573944
epoch£º645	 i:8 	 global-step:12908	 l-p:0.055703725665807724
epoch£º645	 i:9 	 global-step:12909	 l-p:0.0556594543159008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[28.0891, 33.0973, 34.5148],
        [28.0891, 37.7445, 44.2922],
        [28.0891, 38.0127, 44.9115],
        [28.0891, 29.8524, 29.3809]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.055855657905340195 
model_pd.l_d.mean(): 3.831214417004958e-05 
model_pd.lagr.mean(): 0.05589396879076958 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5371], device='cuda:0')), ('power', tensor([0.0683], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.055855657905340195
epoch£º646	 i:1 	 global-step:12921	 l-p:0.05568554624915123
epoch£º646	 i:2 	 global-step:12922	 l-p:0.0557422898709774
epoch£º646	 i:3 	 global-step:12923	 l-p:0.05571809038519859
epoch£º646	 i:4 	 global-step:12924	 l-p:0.055909283459186554
epoch£º646	 i:5 	 global-step:12925	 l-p:0.05570630356669426
epoch£º646	 i:6 	 global-step:12926	 l-p:0.055688533931970596
epoch£º646	 i:7 	 global-step:12927	 l-p:0.055724963545799255
epoch£º646	 i:8 	 global-step:12928	 l-p:0.05568094551563263
epoch£º646	 i:9 	 global-step:12929	 l-p:0.05610443651676178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0042, 28.0042, 28.0042],
        [28.0042, 28.0056, 28.0042],
        [28.0042, 32.4229, 33.3591],
        [28.0042, 28.0058, 28.0042]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.055763695389032364 
model_pd.l_d.mean(): -4.020732740173116e-05 
model_pd.lagr.mean(): 0.055723488330841064 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5646], device='cuda:0')), ('power', tensor([-0.0768], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.055763695389032364
epoch£º647	 i:1 	 global-step:12941	 l-p:0.05569220334291458
epoch£º647	 i:2 	 global-step:12942	 l-p:0.055883459746837616
epoch£º647	 i:3 	 global-step:12943	 l-p:0.055948302149772644
epoch£º647	 i:4 	 global-step:12944	 l-p:0.05569170042872429
epoch£º647	 i:5 	 global-step:12945	 l-p:0.05569043010473251
epoch£º647	 i:6 	 global-step:12946	 l-p:0.05575419217348099
epoch£º647	 i:7 	 global-step:12947	 l-p:0.05574093759059906
epoch£º647	 i:8 	 global-step:12948	 l-p:0.055878303945064545
epoch£º647	 i:9 	 global-step:12949	 l-p:0.05595828592777252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9261, 33.6469, 35.7354],
        [27.9261, 27.9263, 27.9261],
        [27.9261, 28.1367, 27.9667],
        [27.9261, 27.9483, 27.9272]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.055703304708004 
model_pd.l_d.mean(): -0.00012031365622533485 
model_pd.lagr.mean(): 0.05558299273252487 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6392], device='cuda:0')), ('power', tensor([-0.2707], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.055703304708004
epoch£º648	 i:1 	 global-step:12961	 l-p:0.05572991818189621
epoch£º648	 i:2 	 global-step:12962	 l-p:0.05582026392221451
epoch£º648	 i:3 	 global-step:12963	 l-p:0.05589129030704498
epoch£º648	 i:4 	 global-step:12964	 l-p:0.05569993332028389
epoch£º648	 i:5 	 global-step:12965	 l-p:0.05583181232213974
epoch£º648	 i:6 	 global-step:12966	 l-p:0.05595152825117111
epoch£º648	 i:7 	 global-step:12967	 l-p:0.05573407933115959
epoch£º648	 i:8 	 global-step:12968	 l-p:0.05575649440288544
epoch£º648	 i:9 	 global-step:12969	 l-p:0.0560445599257946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8649, 28.0703, 27.9039],
        [27.8649, 27.8791, 27.8655],
        [27.8649, 27.8758, 27.8653],
        [27.8649, 29.7372, 29.2937]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.055829763412475586 
model_pd.l_d.mean(): -9.039317956194282e-05 
model_pd.lagr.mean(): 0.05573936924338341 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5948], device='cuda:0')), ('power', tensor([-0.2737], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.055829763412475586
epoch£º649	 i:1 	 global-step:12981	 l-p:0.05583975091576576
epoch£º649	 i:2 	 global-step:12982	 l-p:0.05592944100499153
epoch£º649	 i:3 	 global-step:12983	 l-p:0.055762965232133865
epoch£º649	 i:4 	 global-step:12984	 l-p:0.055812444537878036
epoch£º649	 i:5 	 global-step:12985	 l-p:0.055711448192596436
epoch£º649	 i:6 	 global-step:12986	 l-p:0.055972568690776825
epoch£º649	 i:7 	 global-step:12987	 l-p:0.05574220418930054
epoch£º649	 i:8 	 global-step:12988	 l-p:0.05588725954294205
epoch£º649	 i:9 	 global-step:12989	 l-p:0.05578816309571266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8294, 28.4489, 28.0664],
        [27.8294, 28.1033, 27.8918],
        [27.8294, 31.7454, 32.3182],
        [27.8294, 29.0513, 28.5484]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.05573316290974617 
model_pd.l_d.mean(): -7.175561040639877e-05 
model_pd.lagr.mean(): 0.0556614063680172 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6235], device='cuda:0')), ('power', tensor([-0.3754], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.05573316290974617
epoch£º650	 i:1 	 global-step:13001	 l-p:0.056177813559770584
epoch£º650	 i:2 	 global-step:13002	 l-p:0.05577118694782257
epoch£º650	 i:3 	 global-step:13003	 l-p:0.05587850511074066
epoch£º650	 i:4 	 global-step:13004	 l-p:0.0558345690369606
epoch£º650	 i:5 	 global-step:13005	 l-p:0.055732760578393936
epoch£º650	 i:6 	 global-step:13006	 l-p:0.05581064149737358
epoch£º650	 i:7 	 global-step:13007	 l-p:0.05571168288588524
epoch£º650	 i:8 	 global-step:13008	 l-p:0.05570810288190842
epoch£º650	 i:9 	 global-step:13009	 l-p:0.05596713349223137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8251, 27.8250, 27.8250],
        [27.8251, 37.2830, 43.6330],
        [27.8251, 27.8251, 27.8250],
        [27.8251, 29.4037, 28.9132]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.05578240379691124 
model_pd.l_d.mean(): -1.2304393749218434e-05 
model_pd.lagr.mean(): 0.05577009916305542 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6403e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5828], device='cuda:0')), ('power', tensor([-0.2979], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.05578240379691124
epoch£º651	 i:1 	 global-step:13021	 l-p:0.05580887198448181
epoch£º651	 i:2 	 global-step:13022	 l-p:0.05576668679714203
epoch£º651	 i:3 	 global-step:13023	 l-p:0.05576016381382942
epoch£º651	 i:4 	 global-step:13024	 l-p:0.05584438145160675
epoch£º651	 i:5 	 global-step:13025	 l-p:0.0557967834174633
epoch£º651	 i:6 	 global-step:13026	 l-p:0.056023500859737396
epoch£º651	 i:7 	 global-step:13027	 l-p:0.05578109622001648
epoch£º651	 i:8 	 global-step:13028	 l-p:0.05573437735438347
epoch£º651	 i:9 	 global-step:13029	 l-p:0.056002043187618256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8547, 28.6367, 28.2016],
        [27.8547, 27.9132, 27.8599],
        [27.8547, 27.8547, 27.8547],
        [27.8547, 27.8607, 27.8548]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.05575425922870636 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05575425922870636 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6141], device='cuda:0')), ('power', tensor([-0.3680], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.05575425922870636
epoch£º652	 i:1 	 global-step:13041	 l-p:0.05573637783527374
epoch£º652	 i:2 	 global-step:13042	 l-p:0.05576992779970169
epoch£º652	 i:3 	 global-step:13043	 l-p:0.05577000603079796
epoch£º652	 i:4 	 global-step:13044	 l-p:0.05583421140909195
epoch£º652	 i:5 	 global-step:13045	 l-p:0.055960193276405334
epoch£º652	 i:6 	 global-step:13046	 l-p:0.05591563135385513
epoch£º652	 i:7 	 global-step:13047	 l-p:0.05587787553668022
epoch£º652	 i:8 	 global-step:13048	 l-p:0.05577806755900383
epoch£º652	 i:9 	 global-step:13049	 l-p:0.055812280625104904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9043, 35.0024, 38.5124],
        [27.9043, 32.7410, 34.0307],
        [27.9043, 35.0670, 38.6481],
        [27.9043, 28.3126, 28.0238]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.05578380823135376 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05578380823135376 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5799], device='cuda:0')), ('power', tensor([-0.2432], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.05578380823135376
epoch£º653	 i:1 	 global-step:13061	 l-p:0.05595177039504051
epoch£º653	 i:2 	 global-step:13062	 l-p:0.05582559481263161
epoch£º653	 i:3 	 global-step:13063	 l-p:0.05569560080766678
epoch£º653	 i:4 	 global-step:13064	 l-p:0.055757105350494385
epoch£º653	 i:5 	 global-step:13065	 l-p:0.055690716952085495
epoch£º653	 i:6 	 global-step:13066	 l-p:0.055926378816366196
epoch£º653	 i:7 	 global-step:13067	 l-p:0.055742744356393814
epoch£º653	 i:8 	 global-step:13068	 l-p:0.0557321198284626
epoch£º653	 i:9 	 global-step:13069	 l-p:0.05598200857639313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9611, 34.8987, 38.2250],
        [27.9611, 29.3406, 28.8337],
        [27.9611, 27.9612, 27.9611],
        [27.9611, 29.6973, 29.2248]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.05588962510228157 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05588962510228157 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5777], device='cuda:0')), ('power', tensor([-0.0995], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.05588962510228157
epoch£º654	 i:1 	 global-step:13081	 l-p:0.05583695322275162
epoch£º654	 i:2 	 global-step:13082	 l-p:0.055791109800338745
epoch£º654	 i:3 	 global-step:13083	 l-p:0.05571453273296356
epoch£º654	 i:4 	 global-step:13084	 l-p:0.05574656277894974
epoch£º654	 i:5 	 global-step:13085	 l-p:0.05597307160496712
epoch£º654	 i:6 	 global-step:13086	 l-p:0.05583943426609039
epoch£º654	 i:7 	 global-step:13087	 l-p:0.0557483471930027
epoch£º654	 i:8 	 global-step:13088	 l-p:0.05567111819982529
epoch£º654	 i:9 	 global-step:13089	 l-p:0.05574623495340347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[28.0208, 30.9508, 30.8982],
        [28.0208, 31.8386, 32.3266],
        [28.0208, 29.8960, 29.4481],
        [28.0208, 28.9994, 28.5194]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.055713098496198654 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055713098496198654 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6297], device='cuda:0')), ('power', tensor([-0.1531], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.055713098496198654
epoch£º655	 i:1 	 global-step:13101	 l-p:0.055673226714134216
epoch£º655	 i:2 	 global-step:13102	 l-p:0.055740296840667725
epoch£º655	 i:3 	 global-step:13103	 l-p:0.05594073608517647
epoch£º655	 i:4 	 global-step:13104	 l-p:0.05579475685954094
epoch£º655	 i:5 	 global-step:13105	 l-p:0.05573299899697304
epoch£º655	 i:6 	 global-step:13106	 l-p:0.055883150547742844
epoch£º655	 i:7 	 global-step:13107	 l-p:0.055688560009002686
epoch£º655	 i:8 	 global-step:13108	 l-p:0.05569572001695633
epoch£º655	 i:9 	 global-step:13109	 l-p:0.05595884099602699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0810, 28.1890, 28.0948],
        [28.0810, 28.2603, 28.1122],
        [28.0810, 28.1571, 28.0889],
        [28.0810, 32.8119, 33.9940]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.05569365248084068 
model_pd.l_d.mean(): -4.480138784401788e-07 
model_pd.lagr.mean(): 0.05569320544600487 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6272], device='cuda:0')), ('power', tensor([-0.1339], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.05569365248084068
epoch£º656	 i:1 	 global-step:13121	 l-p:0.055696502327919006
epoch£º656	 i:2 	 global-step:13122	 l-p:0.05585069954395294
epoch£º656	 i:3 	 global-step:13123	 l-p:0.05608194321393967
epoch£º656	 i:4 	 global-step:13124	 l-p:0.055677883327007294
epoch£º656	 i:5 	 global-step:13125	 l-p:0.055687710642814636
epoch£º656	 i:6 	 global-step:13126	 l-p:0.05570990964770317
epoch£º656	 i:7 	 global-step:13127	 l-p:0.055655498057603836
epoch£º656	 i:8 	 global-step:13128	 l-p:0.05566762760281563
epoch£º656	 i:9 	 global-step:13129	 l-p:0.055965226143598557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[28.1418, 29.9086, 29.4361],
        [28.1418, 30.8973, 30.7520],
        [28.1418, 34.0771, 36.3455],
        [28.1418, 33.1140, 34.4946]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.055698029696941376 
model_pd.l_d.mean(): 1.6423851434410608e-07 
model_pd.lagr.mean(): 0.05569819360971451 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.1901e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6194], device='cuda:0')), ('power', tensor([0.0147], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.055698029696941376
epoch£º657	 i:1 	 global-step:13141	 l-p:0.05568666011095047
epoch£º657	 i:2 	 global-step:13142	 l-p:0.05567260459065437
epoch£º657	 i:3 	 global-step:13143	 l-p:0.055689118802547455
epoch£º657	 i:4 	 global-step:13144	 l-p:0.055946461856365204
epoch£º657	 i:5 	 global-step:13145	 l-p:0.05591952055692673
epoch£º657	 i:6 	 global-step:13146	 l-p:0.05589155852794647
epoch£º657	 i:7 	 global-step:13147	 l-p:0.05573759973049164
epoch£º657	 i:8 	 global-step:13148	 l-p:0.05562938004732132
epoch£º657	 i:9 	 global-step:13149	 l-p:0.055681467056274414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2012, 28.2013, 28.2012],
        [28.2012, 31.2538, 31.2551],
        [28.2012, 30.2553, 29.8447],
        [28.2012, 30.4515, 30.1000]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.05570857971906662 
model_pd.l_d.mean(): 1.5500150993830175e-06 
model_pd.lagr.mean(): 0.05571012943983078 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.4122e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5884], device='cuda:0')), ('power', tensor([0.0489], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.05570857971906662
epoch£º658	 i:1 	 global-step:13161	 l-p:0.055642422288656235
epoch£º658	 i:2 	 global-step:13162	 l-p:0.05571381375193596
epoch£º658	 i:3 	 global-step:13163	 l-p:0.055803459137678146
epoch£º658	 i:4 	 global-step:13164	 l-p:0.05579274892807007
epoch£º658	 i:5 	 global-step:13165	 l-p:0.05567224323749542
epoch£º658	 i:6 	 global-step:13166	 l-p:0.055685605853796005
epoch£º658	 i:7 	 global-step:13167	 l-p:0.05587032064795494
epoch£º658	 i:8 	 global-step:13168	 l-p:0.05561921000480652
epoch£º658	 i:9 	 global-step:13169	 l-p:0.055916573852300644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2554, 34.0470, 36.1617],
        [28.2554, 28.2569, 28.2555],
        [28.2554, 30.9378, 30.7529],
        [28.2554, 28.2598, 28.2555]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.05568235367536545 
model_pd.l_d.mean(): 2.6122895633307053e-06 
model_pd.lagr.mean(): 0.05568496510386467 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.2243e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6412], device='cuda:0')), ('power', tensor([0.0324], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.05568235367536545
epoch£º659	 i:1 	 global-step:13181	 l-p:0.05571088567376137
epoch£º659	 i:2 	 global-step:13182	 l-p:0.05566328018903732
epoch£º659	 i:3 	 global-step:13183	 l-p:0.055633530020713806
epoch£º659	 i:4 	 global-step:13184	 l-p:0.05575818940997124
epoch£º659	 i:5 	 global-step:13185	 l-p:0.05577336996793747
epoch£º659	 i:6 	 global-step:13186	 l-p:0.05585967376828194
epoch£º659	 i:7 	 global-step:13187	 l-p:0.05572241544723511
epoch£º659	 i:8 	 global-step:13188	 l-p:0.05565117672085762
epoch£º659	 i:9 	 global-step:13189	 l-p:0.05585920065641403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2997, 36.1278, 40.3926],
        [28.2997, 29.1545, 28.6967],
        [28.2997, 34.3121, 36.6353],
        [28.2997, 28.3002, 28.2997]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.055768709629774094 
model_pd.l_d.mean(): 4.525644544628449e-05 
model_pd.lagr.mean(): 0.05581396445631981 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5618], device='cuda:0')), ('power', tensor([0.2929], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.055768709629774094
epoch£º660	 i:1 	 global-step:13201	 l-p:0.05574248358607292
epoch£º660	 i:2 	 global-step:13202	 l-p:0.05569697916507721
epoch£º660	 i:3 	 global-step:13203	 l-p:0.0556684285402298
epoch£º660	 i:4 	 global-step:13204	 l-p:0.05575897917151451
epoch£º660	 i:5 	 global-step:13205	 l-p:0.05595161393284798
epoch£º660	 i:6 	 global-step:13206	 l-p:0.0556221567094326
epoch£º660	 i:7 	 global-step:13207	 l-p:0.05562850832939148
epoch£º660	 i:8 	 global-step:13208	 l-p:0.05576224625110626
epoch£º660	 i:9 	 global-step:13209	 l-p:0.05563376471400261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3254, 28.3297, 28.3254],
        [28.3254, 29.0100, 28.6012],
        [28.3254, 28.4688, 28.3469],
        [28.3254, 29.4992, 28.9912]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.05575253814458847 
model_pd.l_d.mean(): 5.30791912751738e-05 
model_pd.lagr.mean(): 0.055805616080760956 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6166], device='cuda:0')), ('power', tensor([0.2152], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.05575253814458847
epoch£º661	 i:1 	 global-step:13221	 l-p:0.05565604194998741
epoch£º661	 i:2 	 global-step:13222	 l-p:0.05561841279268265
epoch£º661	 i:3 	 global-step:13223	 l-p:0.055643271654844284
epoch£º661	 i:4 	 global-step:13224	 l-p:0.055791012942790985
epoch£º661	 i:5 	 global-step:13225	 l-p:0.05590486526489258
epoch£º661	 i:6 	 global-step:13226	 l-p:0.05561244860291481
epoch£º661	 i:7 	 global-step:13227	 l-p:0.055673401802778244
epoch£º661	 i:8 	 global-step:13228	 l-p:0.05567457154393196
epoch£º661	 i:9 	 global-step:13229	 l-p:0.055874522775411606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3272, 28.3272, 28.3272],
        [28.3272, 28.3272, 28.3272],
        [28.3272, 28.4424, 28.3424],
        [28.3272, 29.9360, 29.4362]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.05561161786317825 
model_pd.l_d.mean(): 1.2984564818907529e-05 
model_pd.lagr.mean(): 0.05562460422515869 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6680], device='cuda:0')), ('power', tensor([0.0375], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.05561161786317825
epoch£º662	 i:1 	 global-step:13241	 l-p:0.0557970367372036
epoch£º662	 i:2 	 global-step:13242	 l-p:0.056084077805280685
epoch£º662	 i:3 	 global-step:13243	 l-p:0.055629584938287735
epoch£º662	 i:4 	 global-step:13244	 l-p:0.055698465555906296
epoch£º662	 i:5 	 global-step:13245	 l-p:0.05568833649158478
epoch£º662	 i:6 	 global-step:13246	 l-p:0.05563947185873985
epoch£º662	 i:7 	 global-step:13247	 l-p:0.05564339458942413
epoch£º662	 i:8 	 global-step:13248	 l-p:0.05567818135023117
epoch£º662	 i:9 	 global-step:13249	 l-p:0.05575108528137207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3036, 35.5737, 39.2088],
        [28.3036, 28.6639, 28.4001],
        [28.3036, 31.0818, 30.9385],
        [28.3036, 28.3689, 28.3097]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.0556531697511673 
model_pd.l_d.mean(): 2.8314712835708633e-05 
model_pd.lagr.mean(): 0.055681485682725906 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6404], device='cuda:0')), ('power', tensor([0.0642], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.0556531697511673
epoch£º663	 i:1 	 global-step:13261	 l-p:0.055862292647361755
epoch£º663	 i:2 	 global-step:13262	 l-p:0.05564156547188759
epoch£º663	 i:3 	 global-step:13263	 l-p:0.05571224167943001
epoch£º663	 i:4 	 global-step:13264	 l-p:0.055653564631938934
epoch£º663	 i:5 	 global-step:13265	 l-p:0.05573447421193123
epoch£º663	 i:6 	 global-step:13266	 l-p:0.05575178563594818
epoch£º663	 i:7 	 global-step:13267	 l-p:0.05576040595769882
epoch£º663	 i:8 	 global-step:13268	 l-p:0.055878713726997375
epoch£º663	 i:9 	 global-step:13269	 l-p:0.055651817470788956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2537, 28.2538, 28.2537],
        [28.2537, 31.3123, 31.3137],
        [28.2537, 28.2620, 28.2539],
        [28.2537, 29.7228, 29.2132]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.05568067356944084 
model_pd.l_d.mean(): 4.5894234062870964e-05 
model_pd.lagr.mean(): 0.05572656914591789 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6105], device='cuda:0')), ('power', tensor([0.0885], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.05568067356944084
epoch£º664	 i:1 	 global-step:13281	 l-p:0.055876292288303375
epoch£º664	 i:2 	 global-step:13282	 l-p:0.055653031915426254
epoch£º664	 i:3 	 global-step:13283	 l-p:0.055752191692590714
epoch£º664	 i:4 	 global-step:13284	 l-p:0.05591114982962608
epoch£º664	 i:5 	 global-step:13285	 l-p:0.05563205108046532
epoch£º664	 i:6 	 global-step:13286	 l-p:0.05570113658905029
epoch£º664	 i:7 	 global-step:13287	 l-p:0.055763792246580124
epoch£º664	 i:8 	 global-step:13288	 l-p:0.055795539170503616
epoch£º664	 i:9 	 global-step:13289	 l-p:0.055666640400886536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1815, 28.1925, 28.1819],
        [28.1815, 28.1928, 28.1819],
        [28.1815, 31.1290, 31.0762],
        [28.1815, 28.6363, 28.3231]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.055691517889499664 
model_pd.l_d.mean(): 3.5682569432538e-05 
model_pd.lagr.mean(): 0.05572719871997833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6108], device='cuda:0')), ('power', tensor([0.0631], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.055691517889499664
epoch£º665	 i:1 	 global-step:13301	 l-p:0.05575989559292793
epoch£º665	 i:2 	 global-step:13302	 l-p:0.05586571246385574
epoch£º665	 i:3 	 global-step:13303	 l-p:0.05571059137582779
epoch£º665	 i:4 	 global-step:13304	 l-p:0.055895134806632996
epoch£º665	 i:5 	 global-step:13305	 l-p:0.0557529516518116
epoch£º665	 i:6 	 global-step:13306	 l-p:0.05579899251461029
epoch£º665	 i:7 	 global-step:13307	 l-p:0.055697955191135406
epoch£º665	 i:8 	 global-step:13308	 l-p:0.0557001493871212
epoch£º665	 i:9 	 global-step:13309	 l-p:0.05573698878288269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0930, 28.0930, 28.0930],
        [28.0930, 33.3553, 34.9973],
        [28.0930, 33.5509, 35.3715],
        [28.0930, 29.4520, 28.9421]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.055985692888498306 
model_pd.l_d.mean(): 6.411286449292675e-05 
model_pd.lagr.mean(): 0.056049805134534836 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5419], device='cuda:0')), ('power', tensor([0.1118], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.055985692888498306
epoch£º666	 i:1 	 global-step:13321	 l-p:0.05574049800634384
epoch£º666	 i:2 	 global-step:13322	 l-p:0.05568268150091171
epoch£º666	 i:3 	 global-step:13323	 l-p:0.055928587913513184
epoch£º666	 i:4 	 global-step:13324	 l-p:0.05587253347039223
epoch£º666	 i:5 	 global-step:13325	 l-p:0.05572928488254547
epoch£º666	 i:6 	 global-step:13326	 l-p:0.05573653429746628
epoch£º666	 i:7 	 global-step:13327	 l-p:0.05570916086435318
epoch£º666	 i:8 	 global-step:13328	 l-p:0.0557326078414917
epoch£º666	 i:9 	 global-step:13329	 l-p:0.05569678544998169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[27.9984, 37.9485, 44.9029],
        [27.9984, 30.1415, 29.7639],
        [27.9984, 36.2526, 41.0713],
        [27.9984, 31.1240, 31.1780]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.05574991926550865 
model_pd.l_d.mean(): -5.0711965741356835e-05 
model_pd.lagr.mean(): 0.05569920688867569 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5938], device='cuda:0')), ('power', tensor([-0.0948], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.05574991926550865
epoch£º667	 i:1 	 global-step:13341	 l-p:0.05580661818385124
epoch£º667	 i:2 	 global-step:13342	 l-p:0.05590595677495003
epoch£º667	 i:3 	 global-step:13343	 l-p:0.05572863295674324
epoch£º667	 i:4 	 global-step:13344	 l-p:0.05596882849931717
epoch£º667	 i:5 	 global-step:13345	 l-p:0.055828724056482315
epoch£º667	 i:6 	 global-step:13346	 l-p:0.05570480599999428
epoch£º667	 i:7 	 global-step:13347	 l-p:0.055833347141742706
epoch£º667	 i:8 	 global-step:13348	 l-p:0.0557464100420475
epoch£º667	 i:9 	 global-step:13349	 l-p:0.05575180426239967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9093, 28.0650, 27.9342],
        [27.9093, 27.9094, 27.9093],
        [27.9093, 29.6974, 29.2356],
        [27.9093, 32.8839, 34.2917]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.05581037327647209 
model_pd.l_d.mean(): -9.213418525177985e-05 
model_pd.lagr.mean(): 0.05571823939681053 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5572], device='cuda:0')), ('power', tensor([-0.2043], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.05581037327647209
epoch£º668	 i:1 	 global-step:13361	 l-p:0.055796172469854355
epoch£º668	 i:2 	 global-step:13362	 l-p:0.0558796189725399
epoch£º668	 i:3 	 global-step:13363	 l-p:0.05586525797843933
epoch£º668	 i:4 	 global-step:13364	 l-p:0.055776119232177734
epoch£º668	 i:5 	 global-step:13365	 l-p:0.055752649903297424
epoch£º668	 i:6 	 global-step:13366	 l-p:0.055724162608385086
epoch£º668	 i:7 	 global-step:13367	 l-p:0.05584311857819557
epoch£º668	 i:8 	 global-step:13368	 l-p:0.0560264065861702
epoch£º668	 i:9 	 global-step:13369	 l-p:0.055736444890499115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8390, 29.0766, 28.5729],
        [27.8390, 27.8394, 27.8390],
        [27.8390, 27.8390, 27.8390],
        [27.8390, 31.1103, 31.2589]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.05587993189692497 
model_pd.l_d.mean(): -0.00011804476525867358 
model_pd.lagr.mean(): 0.05576188862323761 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6224], device='cuda:0')), ('power', tensor([-0.3619], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.05587993189692497
epoch£º669	 i:1 	 global-step:13381	 l-p:0.05575026571750641
epoch£º669	 i:2 	 global-step:13382	 l-p:0.055816665291786194
epoch£º669	 i:3 	 global-step:13383	 l-p:0.05584944784641266
epoch£º669	 i:4 	 global-step:13384	 l-p:0.05574949085712433
epoch£º669	 i:5 	 global-step:13385	 l-p:0.05585172399878502
epoch£º669	 i:6 	 global-step:13386	 l-p:0.05571281164884567
epoch£º669	 i:7 	 global-step:13387	 l-p:0.05597502738237381
epoch£º669	 i:8 	 global-step:13388	 l-p:0.055921971797943115
epoch£º669	 i:9 	 global-step:13389	 l-p:0.05583338439464569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7995, 27.7995, 27.7995],
        [27.7995, 28.2168, 27.9237],
        [27.7995, 27.8062, 27.7997],
        [27.7995, 27.7995, 27.7995]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.055747970938682556 
model_pd.l_d.mean(): -5.950219929218292e-05 
model_pd.lagr.mean(): 0.05568847060203552 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6090], device='cuda:0')), ('power', tensor([-0.3437], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.055747970938682556
epoch£º670	 i:1 	 global-step:13401	 l-p:0.05587153136730194
epoch£º670	 i:2 	 global-step:13402	 l-p:0.0557888001203537
epoch£º670	 i:3 	 global-step:13403	 l-p:0.05589620769023895
epoch£º670	 i:4 	 global-step:13404	 l-p:0.05610847473144531
epoch£º670	 i:5 	 global-step:13405	 l-p:0.05595013499259949
epoch£º670	 i:6 	 global-step:13406	 l-p:0.05576358363032341
epoch£º670	 i:7 	 global-step:13407	 l-p:0.05574331060051918
epoch£º670	 i:8 	 global-step:13408	 l-p:0.05579844489693642
epoch£º670	 i:9 	 global-step:13409	 l-p:0.05572319030761719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7990, 27.8511, 27.8033],
        [27.7990, 27.8159, 27.7998],
        [27.7990, 29.7593, 29.3386],
        [27.7990, 27.8165, 27.7998]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.055747054517269135 
model_pd.l_d.mean(): -3.8481316551042255e-06 
model_pd.lagr.mean(): 0.05574320629239082 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6361], device='cuda:0')), ('power', tensor([-0.4314], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.055747054517269135
epoch£º671	 i:1 	 global-step:13421	 l-p:0.055770572274923325
epoch£º671	 i:2 	 global-step:13422	 l-p:0.055782657116651535
epoch£º671	 i:3 	 global-step:13423	 l-p:0.05610409751534462
epoch£º671	 i:4 	 global-step:13424	 l-p:0.05581314116716385
epoch£º671	 i:5 	 global-step:13425	 l-p:0.05574769899249077
epoch£º671	 i:6 	 global-step:13426	 l-p:0.05582645907998085
epoch£º671	 i:7 	 global-step:13427	 l-p:0.055763453245162964
epoch£º671	 i:8 	 global-step:13428	 l-p:0.055953819304704666
epoch£º671	 i:9 	 global-step:13429	 l-p:0.05584276095032692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8362, 27.9431, 27.8498],
        [27.8362, 32.4021, 33.4730],
        [27.8362, 27.8362, 27.8362],
        [27.8362, 29.5828, 29.1157]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.0558115653693676 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0558115653693676 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5747], device='cuda:0')), ('power', tensor([-0.2753], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.0558115653693676
epoch£º672	 i:1 	 global-step:13441	 l-p:0.056054987013339996
epoch£º672	 i:2 	 global-step:13442	 l-p:0.05588454008102417
epoch£º672	 i:3 	 global-step:13443	 l-p:0.05571412295103073
epoch£º672	 i:4 	 global-step:13444	 l-p:0.05571793392300606
epoch£º672	 i:5 	 global-step:13445	 l-p:0.05579870194196701
epoch£º672	 i:6 	 global-step:13446	 l-p:0.05574166029691696
epoch£º672	 i:7 	 global-step:13447	 l-p:0.05576787516474724
epoch£º672	 i:8 	 global-step:13448	 l-p:0.055957481265068054
epoch£º672	 i:9 	 global-step:13449	 l-p:0.055795736610889435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8922, 27.8934, 27.8922],
        [27.8922, 27.8921, 27.8922],
        [27.8922, 34.0256, 36.5235],
        [27.8922, 27.9429, 27.8963]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.05580861121416092 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05580861121416092 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6128], device='cuda:0')), ('power', tensor([-0.2782], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.05580861121416092
epoch£º673	 i:1 	 global-step:13461	 l-p:0.05572795867919922
epoch£º673	 i:2 	 global-step:13462	 l-p:0.055898863822221756
epoch£º673	 i:3 	 global-step:13463	 l-p:0.05571885406970978
epoch£º673	 i:4 	 global-step:13464	 l-p:0.05573435127735138
epoch£º673	 i:5 	 global-step:13465	 l-p:0.055749159306287766
epoch£º673	 i:6 	 global-step:13466	 l-p:0.055976416915655136
epoch£º673	 i:7 	 global-step:13467	 l-p:0.05597316101193428
epoch£º673	 i:8 	 global-step:13468	 l-p:0.05576985329389572
epoch£º673	 i:9 	 global-step:13469	 l-p:0.05575232207775116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9546, 28.7020, 28.2760],
        [27.9546, 28.5770, 28.1927],
        [27.9546, 29.0830, 28.5845],
        [27.9546, 30.6250, 30.4502]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.05587847903370857 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05587847903370857 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5459], device='cuda:0')), ('power', tensor([-0.0631], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.05587847903370857
epoch£º674	 i:1 	 global-step:13481	 l-p:0.055703043937683105
epoch£º674	 i:2 	 global-step:13482	 l-p:0.05575139820575714
epoch£º674	 i:3 	 global-step:13483	 l-p:0.055977027863264084
epoch£º674	 i:4 	 global-step:13484	 l-p:0.055745143443346024
epoch£º674	 i:5 	 global-step:13485	 l-p:0.0557263158261776
epoch£º674	 i:6 	 global-step:13486	 l-p:0.055719807744026184
epoch£º674	 i:7 	 global-step:13487	 l-p:0.05579276755452156
epoch£º674	 i:8 	 global-step:13488	 l-p:0.055718209594488144
epoch£º674	 i:9 	 global-step:13489	 l-p:0.055953577160835266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0195, 29.4325, 28.9254],
        [28.0195, 30.3629, 30.0513],
        [28.0195, 28.6254, 28.2470],
        [28.0195, 28.4526, 28.1507]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.055773038417100906 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055773038417100906 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5971], device='cuda:0')), ('power', tensor([-0.1508], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.055773038417100906
epoch£º675	 i:1 	 global-step:13501	 l-p:0.055773522704839706
epoch£º675	 i:2 	 global-step:13502	 l-p:0.055696457624435425
epoch£º675	 i:3 	 global-step:13503	 l-p:0.05581231787800789
epoch£º675	 i:4 	 global-step:13504	 l-p:0.05576600506901741
epoch£º675	 i:5 	 global-step:13505	 l-p:0.05579042807221413
epoch£º675	 i:6 	 global-step:13506	 l-p:0.055705174803733826
epoch£º675	 i:7 	 global-step:13507	 l-p:0.0559527687728405
epoch£º675	 i:8 	 global-step:13508	 l-p:0.055856987833976746
epoch£º675	 i:9 	 global-step:13509	 l-p:0.05569208040833473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0854, 28.0856, 28.0854],
        [28.0854, 31.4862, 31.6961],
        [28.0854, 28.1444, 28.0907],
        [28.0854, 28.1042, 28.0863]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.055711258202791214 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055711258202791214 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6105], device='cuda:0')), ('power', tensor([-0.1303], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.055711258202791214
epoch£º676	 i:1 	 global-step:13521	 l-p:0.055655114352703094
epoch£º676	 i:2 	 global-step:13522	 l-p:0.05570016801357269
epoch£º676	 i:3 	 global-step:13523	 l-p:0.05588601157069206
epoch£º676	 i:4 	 global-step:13524	 l-p:0.05567815154790878
epoch£º676	 i:5 	 global-step:13525	 l-p:0.055852729827165604
epoch£º676	 i:6 	 global-step:13526	 l-p:0.055754825472831726
epoch£º676	 i:7 	 global-step:13527	 l-p:0.05585412308573723
epoch£º676	 i:8 	 global-step:13528	 l-p:0.05589255318045616
epoch£º676	 i:9 	 global-step:13529	 l-p:0.055686019361019135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1514, 28.1514, 28.1514],
        [28.1514, 28.7921, 28.3999],
        [28.1514, 30.0288, 29.5773],
        [28.1514, 28.2331, 28.1602]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.055681608617305756 
model_pd.l_d.mean(): -3.83561200578697e-07 
model_pd.lagr.mean(): 0.055681224912405014 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.2098e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6304], device='cuda:0')), ('power', tensor([-0.0379], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.055681608617305756
epoch£º677	 i:1 	 global-step:13541	 l-p:0.05574478581547737
epoch£º677	 i:2 	 global-step:13542	 l-p:0.05565550550818443
epoch£º677	 i:3 	 global-step:13543	 l-p:0.05590905249118805
epoch£º677	 i:4 	 global-step:13544	 l-p:0.055647481232881546
epoch£º677	 i:5 	 global-step:13545	 l-p:0.05576736852526665
epoch£º677	 i:6 	 global-step:13546	 l-p:0.055818572640419006
epoch£º677	 i:7 	 global-step:13547	 l-p:0.05569127947092056
epoch£º677	 i:8 	 global-step:13548	 l-p:0.05580587312579155
epoch£º677	 i:9 	 global-step:13549	 l-p:0.055804088711738586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2162, 28.3807, 28.2432],
        [28.2162, 28.2235, 28.2164],
        [28.2162, 28.3591, 28.2377],
        [28.2162, 28.4291, 28.2572]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.055742472410202026 
model_pd.l_d.mean(): 5.594848516921047e-06 
model_pd.lagr.mean(): 0.055748067796230316 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.4191e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5569], device='cuda:0')), ('power', tensor([0.1531], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:0.055742472410202026
epoch£º678	 i:1 	 global-step:13561	 l-p:0.05599427595734596
epoch£º678	 i:2 	 global-step:13562	 l-p:0.05574050918221474
epoch£º678	 i:3 	 global-step:13563	 l-p:0.055732741951942444
epoch£º678	 i:4 	 global-step:13564	 l-p:0.05566069483757019
epoch£º678	 i:5 	 global-step:13565	 l-p:0.05565568432211876
epoch£º678	 i:6 	 global-step:13566	 l-p:0.0558173842728138
epoch£º678	 i:7 	 global-step:13567	 l-p:0.05562840402126312
epoch£º678	 i:8 	 global-step:13568	 l-p:0.055610161274671555
epoch£º678	 i:9 	 global-step:13569	 l-p:0.05580584332346916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2740, 28.2743, 28.2740],
        [28.2740, 34.2806, 36.6016],
        [28.2740, 28.4117, 28.2942],
        [28.2740, 28.5511, 28.3369]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.055675312876701355 
model_pd.l_d.mean(): 9.289508852816653e-06 
model_pd.lagr.mean(): 0.05568460375070572 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.8956e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6183], device='cuda:0')), ('power', tensor([0.0988], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.055675312876701355
epoch£º679	 i:1 	 global-step:13581	 l-p:0.05567241460084915
epoch£º679	 i:2 	 global-step:13582	 l-p:0.05566399544477463
epoch£º679	 i:3 	 global-step:13583	 l-p:0.05586427450180054
epoch£º679	 i:4 	 global-step:13584	 l-p:0.055673711001873016
epoch£º679	 i:5 	 global-step:13585	 l-p:0.05571701005101204
epoch£º679	 i:6 	 global-step:13586	 l-p:0.055887602269649506
epoch£º679	 i:7 	 global-step:13587	 l-p:0.05563665181398392
epoch£º679	 i:8 	 global-step:13588	 l-p:0.05567528307437897
epoch£º679	 i:9 	 global-step:13589	 l-p:0.05580759793519974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3174, 28.3499, 28.3194],
        [28.3174, 28.3288, 28.3178],
        [28.3174, 28.5186, 28.3548],
        [28.3174, 36.3504, 40.8495]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.05560772120952606 
model_pd.l_d.mean(): 1.3333926290215459e-05 
model_pd.lagr.mean(): 0.05562105402350426 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6763], device='cuda:0')), ('power', tensor([0.0753], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.05560772120952606
epoch£º680	 i:1 	 global-step:13601	 l-p:0.055608466267585754
epoch£º680	 i:2 	 global-step:13602	 l-p:0.055867455899715424
epoch£º680	 i:3 	 global-step:13603	 l-p:0.05568133294582367
epoch£º680	 i:4 	 global-step:13604	 l-p:0.05568448826670647
epoch£º680	 i:5 	 global-step:13605	 l-p:0.05567960441112518
epoch£º680	 i:6 	 global-step:13606	 l-p:0.05603849142789841
epoch£º680	 i:7 	 global-step:13607	 l-p:0.05572393909096718
epoch£º680	 i:8 	 global-step:13608	 l-p:0.055659882724285126
epoch£º680	 i:9 	 global-step:13609	 l-p:0.05564621835947037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3404, 29.9580, 29.4588],
        [28.3404, 37.7265, 43.8734],
        [28.3404, 28.3444, 28.3405],
        [28.3404, 28.3448, 28.3405]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.055666711181402206 
model_pd.l_d.mean(): 4.023089786642231e-05 
model_pd.lagr.mean(): 0.0557069405913353 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6141], device='cuda:0')), ('power', tensor([0.1451], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.055666711181402206
epoch£º681	 i:1 	 global-step:13621	 l-p:0.055665530264377594
epoch£º681	 i:2 	 global-step:13622	 l-p:0.055884428322315216
epoch£º681	 i:3 	 global-step:13623	 l-p:0.055710162967443466
epoch£º681	 i:4 	 global-step:13624	 l-p:0.05567819997668266
epoch£º681	 i:5 	 global-step:13625	 l-p:0.05565294995903969
epoch£º681	 i:6 	 global-step:13626	 l-p:0.056070588529109955
epoch£º681	 i:7 	 global-step:13627	 l-p:0.05559908598661423
epoch£º681	 i:8 	 global-step:13628	 l-p:0.05561771243810654
epoch£º681	 i:9 	 global-step:13629	 l-p:0.05562825873494148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3354, 28.9485, 28.5656],
        [28.3354, 29.2011, 28.7404],
        [28.3354, 28.3468, 28.3358],
        [28.3354, 28.4103, 28.3430]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.055645331740379333 
model_pd.l_d.mean(): 5.796459663542919e-05 
model_pd.lagr.mean(): 0.0557032972574234 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6299], device='cuda:0')), ('power', tensor([0.1513], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.055645331740379333
epoch£º682	 i:1 	 global-step:13641	 l-p:0.055616721510887146
epoch£º682	 i:2 	 global-step:13642	 l-p:0.05569693073630333
epoch£º682	 i:3 	 global-step:13643	 l-p:0.05593297258019447
epoch£º682	 i:4 	 global-step:13644	 l-p:0.05568719282746315
epoch£º682	 i:5 	 global-step:13645	 l-p:0.055642832070589066
epoch£º682	 i:6 	 global-step:13646	 l-p:0.055619027465581894
epoch£º682	 i:7 	 global-step:13647	 l-p:0.05568632110953331
epoch£º682	 i:8 	 global-step:13648	 l-p:0.05603627860546112
epoch£º682	 i:9 	 global-step:13649	 l-p:0.05565199255943298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2995, 29.1037, 28.6589],
        [28.2995, 28.4903, 28.3338],
        [28.2995, 28.2996, 28.2995],
        [28.2995, 29.4560, 28.9498]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.05562130734324455 
model_pd.l_d.mean(): 4.625981455319561e-05 
model_pd.lagr.mean(): 0.05566756799817085 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6654], device='cuda:0')), ('power', tensor([0.0965], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.05562130734324455
epoch£º683	 i:1 	 global-step:13661	 l-p:0.05568385496735573
epoch£º683	 i:2 	 global-step:13662	 l-p:0.055735375732183456
epoch£º683	 i:3 	 global-step:13663	 l-p:0.055812571197748184
epoch£º683	 i:4 	 global-step:13664	 l-p:0.055892884731292725
epoch£º683	 i:5 	 global-step:13665	 l-p:0.05585147440433502
epoch£º683	 i:6 	 global-step:13666	 l-p:0.05564277619123459
epoch£º683	 i:7 	 global-step:13667	 l-p:0.0557185560464859
epoch£º683	 i:8 	 global-step:13668	 l-p:0.05566873773932457
epoch£º683	 i:9 	 global-step:13669	 l-p:0.05569504573941231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2350, 32.7728, 33.7818],
        [28.2350, 28.2350, 28.2350],
        [28.2350, 36.6480, 41.6123],
        [28.2350, 28.2452, 28.2353]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.05569489672780037 
model_pd.l_d.mean(): 3.695890336530283e-05 
model_pd.lagr.mean(): 0.05573185533285141 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6083], device='cuda:0')), ('power', tensor([0.0670], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.05569489672780037
epoch£º684	 i:1 	 global-step:13681	 l-p:0.05570858716964722
epoch£º684	 i:2 	 global-step:13682	 l-p:0.0556526854634285
epoch£º684	 i:3 	 global-step:13683	 l-p:0.05568976327776909
epoch£º684	 i:4 	 global-step:13684	 l-p:0.056002575904130936
epoch£º684	 i:5 	 global-step:13685	 l-p:0.05579365789890289
epoch£º684	 i:6 	 global-step:13686	 l-p:0.05563626065850258
epoch£º684	 i:7 	 global-step:13687	 l-p:0.05566926673054695
epoch£º684	 i:8 	 global-step:13688	 l-p:0.055860478430986404
epoch£º684	 i:9 	 global-step:13689	 l-p:0.055783066898584366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1465, 28.6171, 28.2964],
        [28.1465, 37.4350, 43.4990],
        [28.1465, 30.9084, 30.7659],
        [28.1465, 28.1468, 28.1465]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.05567660182714462 
model_pd.l_d.mean(): -1.534167313366197e-05 
model_pd.lagr.mean(): 0.05566126108169556 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6271], device='cuda:0')), ('power', tensor([-0.0262], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.05567660182714462
epoch£º685	 i:1 	 global-step:13701	 l-p:0.05565744638442993
epoch£º685	 i:2 	 global-step:13702	 l-p:0.0557260736823082
epoch£º685	 i:3 	 global-step:13703	 l-p:0.055647749453783035
epoch£º685	 i:4 	 global-step:13704	 l-p:0.05598287284374237
epoch£º685	 i:5 	 global-step:13705	 l-p:0.05595897510647774
epoch£º685	 i:6 	 global-step:13706	 l-p:0.055722057819366455
epoch£º685	 i:7 	 global-step:13707	 l-p:0.055743854492902756
epoch£º685	 i:8 	 global-step:13708	 l-p:0.055727604776620865
epoch£º685	 i:9 	 global-step:13709	 l-p:0.05586015060544014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0449, 28.0464, 28.0449],
        [28.0449, 28.0474, 28.0450],
        [28.0449, 28.2053, 28.0710],
        [28.0449, 36.2187, 40.9331]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.055921390652656555 
model_pd.l_d.mean(): 4.7645658924011514e-05 
model_pd.lagr.mean(): 0.05596903711557388 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5022], device='cuda:0')), ('power', tensor([0.0832], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.055921390652656555
epoch£º686	 i:1 	 global-step:13721	 l-p:0.055704087018966675
epoch£º686	 i:2 	 global-step:13722	 l-p:0.05582120642066002
epoch£º686	 i:3 	 global-step:13723	 l-p:0.05584242567420006
epoch£º686	 i:4 	 global-step:13724	 l-p:0.055716365575790405
epoch£º686	 i:5 	 global-step:13725	 l-p:0.05575712025165558
epoch£º686	 i:6 	 global-step:13726	 l-p:0.05602172389626503
epoch£º686	 i:7 	 global-step:13727	 l-p:0.05571982264518738
epoch£º686	 i:8 	 global-step:13728	 l-p:0.055712975561618805
epoch£º686	 i:9 	 global-step:13729	 l-p:0.0557151734828949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[27.9420, 35.7976, 40.1570],
        [27.9420, 31.7654, 32.2637],
        [27.9420, 29.8115, 29.3650],
        [27.9420, 34.8745, 38.1984]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.05582016706466675 
model_pd.l_d.mean(): -4.4580225221579894e-05 
model_pd.lagr.mean(): 0.055775586515665054 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5283], device='cuda:0')), ('power', tensor([-0.0876], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.05582016706466675
epoch£º687	 i:1 	 global-step:13741	 l-p:0.05594119802117348
epoch£º687	 i:2 	 global-step:13742	 l-p:0.05579766258597374
epoch£º687	 i:3 	 global-step:13743	 l-p:0.055837344378232956
epoch£º687	 i:4 	 global-step:13744	 l-p:0.055690325796604156
epoch£º687	 i:5 	 global-step:13745	 l-p:0.05582171306014061
epoch£º687	 i:6 	 global-step:13746	 l-p:0.05588671565055847
epoch£º687	 i:7 	 global-step:13747	 l-p:0.055717747658491135
epoch£º687	 i:8 	 global-step:13748	 l-p:0.05572507902979851
epoch£º687	 i:9 	 global-step:13749	 l-p:0.05591774359345436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8511, 27.8511, 27.8511],
        [27.8511, 31.1239, 31.2725],
        [27.8511, 29.0823, 28.5786],
        [27.8511, 27.8514, 27.8511]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.055936459451913834 
model_pd.l_d.mean(): -6.770878098905087e-05 
model_pd.lagr.mean(): 0.05586875230073929 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5456], device='cuda:0')), ('power', tensor([-0.1710], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.055936459451913834
epoch£º688	 i:1 	 global-step:13761	 l-p:0.055768124759197235
epoch£º688	 i:2 	 global-step:13762	 l-p:0.05604618787765503
epoch£º688	 i:3 	 global-step:13763	 l-p:0.055717792361974716
epoch£º688	 i:4 	 global-step:13764	 l-p:0.05587035045027733
epoch£º688	 i:5 	 global-step:13765	 l-p:0.0557263046503067
epoch£º688	 i:6 	 global-step:13766	 l-p:0.05577404797077179
epoch£º688	 i:7 	 global-step:13767	 l-p:0.05578049644827843
epoch£º688	 i:8 	 global-step:13768	 l-p:0.05587140470743179
epoch£º688	 i:9 	 global-step:13769	 l-p:0.05584476515650749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7885, 28.1617, 27.8920],
        [27.7885, 27.8219, 27.7906],
        [27.7885, 27.7886, 27.7885],
        [27.7885, 27.7885, 27.7885]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.055746253579854965 
model_pd.l_d.mean(): -0.00010035962623078376 
model_pd.lagr.mean(): 0.0556458942592144 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6146], device='cuda:0')), ('power', tensor([-0.4117], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.055746253579854965
epoch£º689	 i:1 	 global-step:13781	 l-p:0.05583525449037552
epoch£º689	 i:2 	 global-step:13782	 l-p:0.05585009604692459
epoch£º689	 i:3 	 global-step:13783	 l-p:0.055784910917282104
epoch£º689	 i:4 	 global-step:13784	 l-p:0.05577433109283447
epoch£º689	 i:5 	 global-step:13785	 l-p:0.0557834655046463
epoch£º689	 i:6 	 global-step:13786	 l-p:0.05588579922914505
epoch£º689	 i:7 	 global-step:13787	 l-p:0.05602113530039787
epoch£º689	 i:8 	 global-step:13788	 l-p:0.055739421397447586
epoch£º689	 i:9 	 global-step:13789	 l-p:0.056019190698862076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7670, 27.7673, 27.7670],
        [27.7670, 37.3068, 43.7757],
        [27.7670, 27.7695, 27.7670],
        [27.7670, 27.7670, 27.7670]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.05576246976852417 
model_pd.l_d.mean(): -2.7623245841823518e-05 
model_pd.lagr.mean(): 0.055734846740961075 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.9226e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6137], device='cuda:0')), ('power', tensor([-0.3992], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.05576246976852417
epoch£º690	 i:1 	 global-step:13801	 l-p:0.056000661104917526
epoch£º690	 i:2 	 global-step:13802	 l-p:0.05615568906068802
epoch£º690	 i:3 	 global-step:13803	 l-p:0.05578327178955078
epoch£º690	 i:4 	 global-step:13804	 l-p:0.05586160719394684
epoch£º690	 i:5 	 global-step:13805	 l-p:0.05589786544442177
epoch£º690	 i:6 	 global-step:13806	 l-p:0.05576835200190544
epoch£º690	 i:7 	 global-step:13807	 l-p:0.05573264881968498
epoch£º690	 i:8 	 global-step:13808	 l-p:0.055729836225509644
epoch£º690	 i:9 	 global-step:13809	 l-p:0.055746711790561676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7934, 27.7935, 27.7934],
        [27.7934, 28.4611, 28.0616],
        [27.7934, 32.9967, 34.6201],
        [27.7934, 27.8035, 27.7937]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.055947329849004745 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055947329849004745 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5665], device='cuda:0')), ('power', tensor([-0.2572], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.055947329849004745
epoch£º691	 i:1 	 global-step:13821	 l-p:0.0560159869492054
epoch£º691	 i:2 	 global-step:13822	 l-p:0.05594410002231598
epoch£º691	 i:3 	 global-step:13823	 l-p:0.055774934589862823
epoch£º691	 i:4 	 global-step:13824	 l-p:0.05573703721165657
epoch£º691	 i:5 	 global-step:13825	 l-p:0.05573318898677826
epoch£º691	 i:6 	 global-step:13826	 l-p:0.05577421188354492
epoch£º691	 i:7 	 global-step:13827	 l-p:0.05581672117114067
epoch£º691	 i:8 	 global-step:13828	 l-p:0.05576890707015991
epoch£º691	 i:9 	 global-step:13829	 l-p:0.0558321475982666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8485, 27.8498, 27.8485],
        [27.8485, 27.8510, 27.8486],
        [27.8485, 28.2056, 27.9446],
        [27.8485, 27.8485, 27.8485]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.05571869760751724 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05571869760751724 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6417], device='cuda:0')), ('power', tensor([-0.4158], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.05571869760751724
epoch£º692	 i:1 	 global-step:13841	 l-p:0.05576386675238609
epoch£º692	 i:2 	 global-step:13842	 l-p:0.055790260434150696
epoch£º692	 i:3 	 global-step:13843	 l-p:0.05589795112609863
epoch£º692	 i:4 	 global-step:13844	 l-p:0.055820997804403305
epoch£º692	 i:5 	 global-step:13845	 l-p:0.05579955875873566
epoch£º692	 i:6 	 global-step:13846	 l-p:0.0557108111679554
epoch£º692	 i:7 	 global-step:13847	 l-p:0.05582111328840256
epoch£º692	 i:8 	 global-step:13848	 l-p:0.05599261075258255
epoch£º692	 i:9 	 global-step:13849	 l-p:0.05588947981595993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9138, 27.9308, 27.9145],
        [27.9138, 35.2010, 38.9185],
        [27.9138, 37.1221, 43.1334],
        [27.9138, 37.4032, 43.7746]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.05587054044008255 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05587054044008255 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5307], device='cuda:0')), ('power', tensor([-0.0593], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.05587054044008255
epoch£º693	 i:1 	 global-step:13861	 l-p:0.05578043684363365
epoch£º693	 i:2 	 global-step:13862	 l-p:0.05570472776889801
epoch£º693	 i:3 	 global-step:13863	 l-p:0.05595541000366211
epoch£º693	 i:4 	 global-step:13864	 l-p:0.05606955289840698
epoch£º693	 i:5 	 global-step:13865	 l-p:0.055691976100206375
epoch£º693	 i:6 	 global-step:13866	 l-p:0.05578111484646797
epoch£º693	 i:7 	 global-step:13867	 l-p:0.055704813450574875
epoch£º693	 i:8 	 global-step:13868	 l-p:0.05577829107642174
epoch£º693	 i:9 	 global-step:13869	 l-p:0.05571673437952995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9838, 28.3444, 28.0811],
        [27.9838, 31.1851, 31.2830],
        [27.9838, 27.9839, 27.9838],
        [27.9838, 27.9838, 27.9838]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.05593132972717285 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05593132972717285 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5304], device='cuda:0')), ('power', tensor([-0.0351], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.05593132972717285
epoch£º694	 i:1 	 global-step:13881	 l-p:0.05569474399089813
epoch£º694	 i:2 	 global-step:13882	 l-p:0.05570586398243904
epoch£º694	 i:3 	 global-step:13883	 l-p:0.055958207696676254
epoch£º694	 i:4 	 global-step:13884	 l-p:0.055716291069984436
epoch£º694	 i:5 	 global-step:13885	 l-p:0.05581571161746979
epoch£º694	 i:6 	 global-step:13886	 l-p:0.05571456626057625
epoch£º694	 i:7 	 global-step:13887	 l-p:0.05590188875794411
epoch£º694	 i:8 	 global-step:13888	 l-p:0.05574057623744011
epoch£º694	 i:9 	 global-step:13889	 l-p:0.055715128779411316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0550, 30.4015, 30.0895],
        [28.0550, 32.5564, 33.5536],
        [28.0550, 32.6920, 33.7992],
        [28.0550, 28.0563, 28.0550]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.055923942476511 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055923942476511 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5897], device='cuda:0')), ('power', tensor([-0.0056], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.055923942476511
epoch£º695	 i:1 	 global-step:13901	 l-p:0.05567130818963051
epoch£º695	 i:2 	 global-step:13902	 l-p:0.055812641978263855
epoch£º695	 i:3 	 global-step:13903	 l-p:0.05578930675983429
epoch£º695	 i:4 	 global-step:13904	 l-p:0.0556999109685421
epoch£º695	 i:5 	 global-step:13905	 l-p:0.05574037507176399
epoch£º695	 i:6 	 global-step:13906	 l-p:0.05584732070565224
epoch£º695	 i:7 	 global-step:13907	 l-p:0.05593932047486305
epoch£º695	 i:8 	 global-step:13908	 l-p:0.05563821643590927
epoch£º695	 i:9 	 global-step:13909	 l-p:0.055672019720077515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1267, 28.1683, 28.1297],
        [28.1267, 32.6402, 33.6402],
        [28.1267, 28.3644, 28.1759],
        [28.1267, 37.2592, 43.1301]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.055660538375377655 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055660538375377655 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6509], device='cuda:0')), ('power', tensor([-0.0946], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.055660538375377655
epoch£º696	 i:1 	 global-step:13921	 l-p:0.05569853633642197
epoch£º696	 i:2 	 global-step:13922	 l-p:0.05566263198852539
epoch£º696	 i:3 	 global-step:13923	 l-p:0.05593521520495415
epoch£º696	 i:4 	 global-step:13924	 l-p:0.055870816111564636
epoch£º696	 i:5 	 global-step:13925	 l-p:0.05582424998283386
epoch£º696	 i:6 	 global-step:13926	 l-p:0.05583749711513519
epoch£º696	 i:7 	 global-step:13927	 l-p:0.055688660591840744
epoch£º696	 i:8 	 global-step:13928	 l-p:0.05565968528389931
epoch£º696	 i:9 	 global-step:13929	 l-p:0.05573684722185135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1977, 28.1977, 28.1977],
        [28.1977, 28.1979, 28.1977],
        [28.1977, 35.4393, 39.0601],
        [28.1977, 28.2016, 28.1978]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.05576905608177185 
model_pd.l_d.mean(): 4.794003984898154e-07 
model_pd.lagr.mean(): 0.05576953664422035 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6350e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6207], device='cuda:0')), ('power', tensor([0.0189], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.05576905608177185
epoch£º697	 i:1 	 global-step:13941	 l-p:0.05591520294547081
epoch£º697	 i:2 	 global-step:13942	 l-p:0.05567379668354988
epoch£º697	 i:3 	 global-step:13943	 l-p:0.05567643418908119
epoch£º697	 i:4 	 global-step:13944	 l-p:0.05567004159092903
epoch£º697	 i:5 	 global-step:13945	 l-p:0.055679868906736374
epoch£º697	 i:6 	 global-step:13946	 l-p:0.055652447044849396
epoch£º697	 i:7 	 global-step:13947	 l-p:0.05594754219055176
epoch£º697	 i:8 	 global-step:13948	 l-p:0.05572858825325966
epoch£º697	 i:9 	 global-step:13949	 l-p:0.05570843443274498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2636, 28.4534, 28.2976],
        [28.2636, 28.2749, 28.2640],
        [28.2636, 29.8686, 29.3700],
        [28.2636, 30.9639, 30.7867]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.05566218122839928 
model_pd.l_d.mean(): 4.672547220252454e-06 
model_pd.lagr.mean(): 0.05566685274243355 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.8418e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6353], device='cuda:0')), ('power', tensor([0.0620], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.05566218122839928
epoch£º698	 i:1 	 global-step:13961	 l-p:0.055631354451179504
epoch£º698	 i:2 	 global-step:13962	 l-p:0.055625542998313904
epoch£º698	 i:3 	 global-step:13963	 l-p:0.055855654180049896
epoch£º698	 i:4 	 global-step:13964	 l-p:0.05585696920752525
epoch£º698	 i:5 	 global-step:13965	 l-p:0.05563947558403015
epoch£º698	 i:6 	 global-step:13966	 l-p:0.05568576604127884
epoch£º698	 i:7 	 global-step:13967	 l-p:0.05576120316982269
epoch£º698	 i:8 	 global-step:13968	 l-p:0.0557028204202652
epoch£º698	 i:9 	 global-step:13969	 l-p:0.05586545169353485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3174, 28.3502, 28.3194],
        [28.3174, 28.6779, 28.4139],
        [28.3174, 31.2565, 31.1915],
        [28.3174, 28.3173, 28.3173]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.055662818253040314 
model_pd.l_d.mean(): 2.4168886739062145e-05 
model_pd.lagr.mean(): 0.055686987936496735 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6199], device='cuda:0')), ('power', tensor([0.1555], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.055662818253040314
epoch£º699	 i:1 	 global-step:13981	 l-p:0.05582069233059883
epoch£º699	 i:2 	 global-step:13982	 l-p:0.05585139989852905
epoch£º699	 i:3 	 global-step:13983	 l-p:0.055628757923841476
epoch£º699	 i:4 	 global-step:13984	 l-p:0.055956725031137466
epoch£º699	 i:5 	 global-step:13985	 l-p:0.05563706159591675
epoch£º699	 i:6 	 global-step:13986	 l-p:0.05564376339316368
epoch£º699	 i:7 	 global-step:13987	 l-p:0.055681899189949036
epoch£º699	 i:8 	 global-step:13988	 l-p:0.05563961714506149
epoch£º699	 i:9 	 global-step:13989	 l-p:0.055665988475084305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3488, 28.3720, 28.3500],
        [28.3488, 30.6130, 30.2600],
        [28.3488, 30.1319, 29.6562],
        [28.3488, 28.7220, 28.4509]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.05566802993416786 
model_pd.l_d.mean(): 3.451634256634861e-05 
model_pd.lagr.mean(): 0.05570254474878311 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6391], device='cuda:0')), ('power', tensor([0.1339], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.05566802993416786
epoch£º700	 i:1 	 global-step:14001	 l-p:0.055853381752967834
epoch£º700	 i:2 	 global-step:14002	 l-p:0.055686719715595245
epoch£º700	 i:3 	 global-step:14003	 l-p:0.05563850700855255
epoch£º700	 i:4 	 global-step:14004	 l-p:0.05577467009425163
epoch£º700	 i:5 	 global-step:14005	 l-p:0.055619869381189346
epoch£º700	 i:6 	 global-step:14006	 l-p:0.055647123605012894
epoch£º700	 i:7 	 global-step:14007	 l-p:0.05560706928372383
epoch£º700	 i:8 	 global-step:14008	 l-p:0.05564635619521141
epoch£º700	 i:9 	 global-step:14009	 l-p:0.05600826069712639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3486, 28.3827, 28.3508],
        [28.3486, 28.8228, 28.4997],
        [28.3486, 28.3486, 28.3486],
        [28.3486, 29.6827, 29.1678]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.05569359287619591 
model_pd.l_d.mean(): 7.662949792575091e-05 
model_pd.lagr.mean(): 0.05577022209763527 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5932], device='cuda:0')), ('power', tensor([0.2077], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.05569359287619591
epoch£º701	 i:1 	 global-step:14021	 l-p:0.05565090849995613
epoch£º701	 i:2 	 global-step:14022	 l-p:0.05563298985362053
epoch£º701	 i:3 	 global-step:14023	 l-p:0.05571112409234047
epoch£º701	 i:4 	 global-step:14024	 l-p:0.05564827471971512
epoch£º701	 i:5 	 global-step:14025	 l-p:0.05576217174530029
epoch£º701	 i:6 	 global-step:14026	 l-p:0.05579734593629837
epoch£º701	 i:7 	 global-step:14027	 l-p:0.05574661120772362
epoch£º701	 i:8 	 global-step:14028	 l-p:0.05563080683350563
epoch£º701	 i:9 	 global-step:14029	 l-p:0.055910155177116394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3148, 28.6753, 28.4114],
        [28.3148, 36.1557, 40.4324],
        [28.3148, 28.6624, 28.4058],
        [28.3148, 28.3148, 28.3148]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.055678870528936386 
model_pd.l_d.mean(): 7.705230382271111e-05 
model_pd.lagr.mean(): 0.05575592443346977 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6055], device='cuda:0')), ('power', tensor([0.1631], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.055678870528936386
epoch£º702	 i:1 	 global-step:14041	 l-p:0.055687811225652695
epoch£º702	 i:2 	 global-step:14042	 l-p:0.05571818724274635
epoch£º702	 i:3 	 global-step:14043	 l-p:0.055672015994787216
epoch£º702	 i:4 	 global-step:14044	 l-p:0.05573311820626259
epoch£º702	 i:5 	 global-step:14045	 l-p:0.055696651339530945
epoch£º702	 i:6 	 global-step:14046	 l-p:0.05564779043197632
epoch£º702	 i:7 	 global-step:14047	 l-p:0.055620089173316956
epoch£º702	 i:8 	 global-step:14048	 l-p:0.05586826056241989
epoch£º702	 i:9 	 global-step:14049	 l-p:0.05596911907196045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2478, 28.2609, 28.2483],
        [28.2478, 36.6648, 41.6315],
        [28.2478, 28.3072, 28.2531],
        [28.2478, 36.8491, 42.0384]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.05566803738474846 
model_pd.l_d.mean(): 5.97758153162431e-05 
model_pd.lagr.mean(): 0.05572781339287758 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6097], device='cuda:0')), ('power', tensor([0.1084], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.05566803738474846
epoch£º703	 i:1 	 global-step:14061	 l-p:0.05580330640077591
epoch£º703	 i:2 	 global-step:14062	 l-p:0.055720195174217224
epoch£º703	 i:3 	 global-step:14063	 l-p:0.05563531816005707
epoch£º703	 i:4 	 global-step:14064	 l-p:0.05566556751728058
epoch£º703	 i:5 	 global-step:14065	 l-p:0.05567310377955437
epoch£º703	 i:6 	 global-step:14066	 l-p:0.05585679039359093
epoch£º703	 i:7 	 global-step:14067	 l-p:0.055742427706718445
epoch£º703	 i:8 	 global-step:14068	 l-p:0.05582456290721893
epoch£º703	 i:9 	 global-step:14069	 l-p:0.05587918683886528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1534, 28.1533, 28.1533],
        [28.1534, 28.3913, 28.2026],
        [28.1534, 28.6303, 28.3066],
        [28.1534, 32.3419, 33.0868]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.055631235241889954 
model_pd.l_d.mean(): -9.070129453903064e-05 
model_pd.lagr.mean(): 0.0555405355989933 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6860], device='cuda:0')), ('power', tensor([-0.1534], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.055631235241889954
epoch£º704	 i:1 	 global-step:14081	 l-p:0.05571872740983963
epoch£º704	 i:2 	 global-step:14082	 l-p:0.055815190076828
epoch£º704	 i:3 	 global-step:14083	 l-p:0.055693645030260086
epoch£º704	 i:4 	 global-step:14084	 l-p:0.05594475194811821
epoch£º704	 i:5 	 global-step:14085	 l-p:0.05573704093694687
epoch£º704	 i:6 	 global-step:14086	 l-p:0.055926837027072906
epoch£º704	 i:7 	 global-step:14087	 l-p:0.05567184463143349
epoch£º704	 i:8 	 global-step:14088	 l-p:0.05583536997437477
epoch£º704	 i:9 	 global-step:14089	 l-p:0.05572148784995079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0428, 28.0428, 28.0428],
        [28.0428, 28.0630, 28.0438],
        [28.0428, 32.3868, 33.2608],
        [28.0428, 35.1778, 38.7062]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.05574969947338104 
model_pd.l_d.mean(): -1.9762537704082206e-05 
model_pd.lagr.mean(): 0.0557299368083477 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5777], device='cuda:0')), ('power', tensor([-0.0341], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:0.05574969947338104
epoch£º705	 i:1 	 global-step:14101	 l-p:0.05592275410890579
epoch£º705	 i:2 	 global-step:14102	 l-p:0.05577058345079422
epoch£º705	 i:3 	 global-step:14103	 l-p:0.055751703679561615
epoch£º705	 i:4 	 global-step:14104	 l-p:0.05567634478211403
epoch£º705	 i:5 	 global-step:14105	 l-p:0.055887214839458466
epoch£º705	 i:6 	 global-step:14106	 l-p:0.055716656148433685
epoch£º705	 i:7 	 global-step:14107	 l-p:0.055988483130931854
epoch£º705	 i:8 	 global-step:14108	 l-p:0.05574101209640503
epoch£º705	 i:9 	 global-step:14109	 l-p:0.05574377253651619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9297, 28.3613, 28.0605],
        [27.9297, 35.7711, 40.1162],
        [27.9297, 28.1171, 27.9633],
        [27.9297, 30.6975, 30.5696]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.05573184788227081 
model_pd.l_d.mean(): -0.00011759609333239496 
model_pd.lagr.mean(): 0.055614251643419266 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6043], device='cuda:0')), ('power', tensor([-0.2294], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.05573184788227081
epoch£º706	 i:1 	 global-step:14121	 l-p:0.05572453513741493
epoch£º706	 i:2 	 global-step:14122	 l-p:0.05598159506917
epoch£º706	 i:3 	 global-step:14123	 l-p:0.05573948845267296
epoch£º706	 i:4 	 global-step:14124	 l-p:0.05583411082625389
epoch£º706	 i:5 	 global-step:14125	 l-p:0.05577431619167328
epoch£º706	 i:6 	 global-step:14126	 l-p:0.05593143776059151
epoch£º706	 i:7 	 global-step:14127	 l-p:0.05599861592054367
epoch£º706	 i:8 	 global-step:14128	 l-p:0.05575146526098251
epoch£º706	 i:9 	 global-step:14129	 l-p:0.05572500452399254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8318, 27.8331, 27.8318],
        [27.8318, 27.8319, 27.8318],
        [27.8318, 27.8319, 27.8318],
        [27.8318, 28.5018, 28.1012]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.05571484565734863 
model_pd.l_d.mean(): -0.00015589030226692557 
model_pd.lagr.mean(): 0.055558957159519196 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6478], device='cuda:0')), ('power', tensor([-0.3979], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.05571484565734863
epoch£º707	 i:1 	 global-step:14141	 l-p:0.055761292576789856
epoch£º707	 i:2 	 global-step:14142	 l-p:0.055780597031116486
epoch£º707	 i:3 	 global-step:14143	 l-p:0.05584900826215744
epoch£º707	 i:4 	 global-step:14144	 l-p:0.055858444422483444
epoch£º707	 i:5 	 global-step:14145	 l-p:0.055835578590631485
epoch£º707	 i:6 	 global-step:14146	 l-p:0.05580437183380127
epoch£º707	 i:7 	 global-step:14147	 l-p:0.05598873645067215
epoch£º707	 i:8 	 global-step:14148	 l-p:0.05573384091258049
epoch£º707	 i:9 	 global-step:14149	 l-p:0.05605926737189293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7665, 28.1806, 27.8892],
        [27.7665, 29.0717, 28.5679],
        [27.7665, 33.6770, 35.9705],
        [27.7665, 28.1862, 27.8919]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.055792149156332016 
model_pd.l_d.mean(): -8.450014138361439e-05 
model_pd.lagr.mean(): 0.055707648396492004 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5911], device='cuda:0')), ('power', tensor([-0.3688], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.055792149156332016
epoch£º708	 i:1 	 global-step:14161	 l-p:0.055782198905944824
epoch£º708	 i:2 	 global-step:14162	 l-p:0.055766671895980835
epoch£º708	 i:3 	 global-step:14163	 l-p:0.055823273956775665
epoch£º708	 i:4 	 global-step:14164	 l-p:0.055990926921367645
epoch£º708	 i:5 	 global-step:14165	 l-p:0.05593077465891838
epoch£º708	 i:6 	 global-step:14166	 l-p:0.055811863392591476
epoch£º708	 i:7 	 global-step:14167	 l-p:0.055755265057086945
epoch£º708	 i:8 	 global-step:14168	 l-p:0.05602620169520378
epoch£º708	 i:9 	 global-step:14169	 l-p:0.055807970464229584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7488, 29.7918, 29.3945],
        [27.7488, 27.9404, 27.7837],
        [27.7488, 37.6001, 44.4813],
        [27.7488, 31.9632, 32.7643]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.05583110824227333 
model_pd.l_d.mean(): -1.7455817214795388e-05 
model_pd.lagr.mean(): 0.05581365153193474 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.4515e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5878], device='cuda:0')), ('power', tensor([-0.3945], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.05583110824227333
epoch£º709	 i:1 	 global-step:14181	 l-p:0.05576789751648903
epoch£º709	 i:2 	 global-step:14182	 l-p:0.05580450966954231
epoch£º709	 i:3 	 global-step:14183	 l-p:0.055811021476984024
epoch£º709	 i:4 	 global-step:14184	 l-p:0.055779121816158295
epoch£º709	 i:5 	 global-step:14185	 l-p:0.055955514311790466
epoch£º709	 i:6 	 global-step:14186	 l-p:0.05574393272399902
epoch£º709	 i:7 	 global-step:14187	 l-p:0.056160345673561096
epoch£º709	 i:8 	 global-step:14188	 l-p:0.05589313805103302
epoch£º709	 i:9 	 global-step:14189	 l-p:0.05572621524333954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7825, 27.8643, 27.7914],
        [27.7825, 29.7797, 29.3688],
        [27.7825, 29.1527, 28.6492],
        [27.7825, 27.9177, 27.8024]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.0557749941945076 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0557749941945076 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6046], device='cuda:0')), ('power', tensor([-0.4458], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.0557749941945076
epoch£º710	 i:1 	 global-step:14201	 l-p:0.05580819025635719
epoch£º710	 i:2 	 global-step:14202	 l-p:0.05592275410890579
epoch£º710	 i:3 	 global-step:14203	 l-p:0.05581890046596527
epoch£º710	 i:4 	 global-step:14204	 l-p:0.055824458599090576
epoch£º710	 i:5 	 global-step:14205	 l-p:0.05584070086479187
epoch£º710	 i:6 	 global-step:14206	 l-p:0.05571584403514862
epoch£º710	 i:7 	 global-step:14207	 l-p:0.056011419743299484
epoch£º710	 i:8 	 global-step:14208	 l-p:0.05574841797351837
epoch£º710	 i:9 	 global-step:14209	 l-p:0.055895619094371796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8444, 27.8950, 27.8485],
        [27.8444, 34.7975, 38.1588],
        [27.8444, 34.1301, 36.7886],
        [27.8444, 27.8444, 27.8444]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.05585172399878502 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05585172399878502 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5314], device='cuda:0')), ('power', tensor([-0.2280], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.05585172399878502
epoch£º711	 i:1 	 global-step:14221	 l-p:0.055740050971508026
epoch£º711	 i:2 	 global-step:14222	 l-p:0.05577925592660904
epoch£º711	 i:3 	 global-step:14223	 l-p:0.05597628653049469
epoch£º711	 i:4 	 global-step:14224	 l-p:0.055844515562057495
epoch£º711	 i:5 	 global-step:14225	 l-p:0.05587948113679886
epoch£º711	 i:6 	 global-step:14226	 l-p:0.0559161938726902
epoch£º711	 i:7 	 global-step:14227	 l-p:0.05572107061743736
epoch£º711	 i:8 	 global-step:14228	 l-p:0.055705420672893524
epoch£º711	 i:9 	 global-step:14229	 l-p:0.05579482764005661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9165, 33.6351, 35.7229],
        [27.9165, 35.0178, 38.5294],
        [27.9165, 28.3329, 28.0399],
        [27.9165, 36.0511, 40.7427]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.055763110518455505 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055763110518455505 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5744], device='cuda:0')), ('power', tensor([-0.1347], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:0.055763110518455505
epoch£º712	 i:1 	 global-step:14241	 l-p:0.056085795164108276
epoch£º712	 i:2 	 global-step:14242	 l-p:0.05573268607258797
epoch£º712	 i:3 	 global-step:14243	 l-p:0.05570478364825249
epoch£º712	 i:4 	 global-step:14244	 l-p:0.05585017055273056
epoch£º712	 i:5 	 global-step:14245	 l-p:0.055716827511787415
epoch£º712	 i:6 	 global-step:14246	 l-p:0.05580424144864082
epoch£º712	 i:7 	 global-step:14247	 l-p:0.05573052167892456
epoch£º712	 i:8 	 global-step:14248	 l-p:0.0557137206196785
epoch£º712	 i:9 	 global-step:14249	 l-p:0.05593953654170036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9920, 28.0240, 27.9939],
        [27.9920, 30.9187, 30.8662],
        [27.9920, 27.9920, 27.9919],
        [27.9920, 32.0459, 32.7049]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.05568058416247368 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05568058416247368 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6544], device='cuda:0')), ('power', tensor([-0.2825], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.05568058416247368
epoch£º713	 i:1 	 global-step:14261	 l-p:0.05611998587846756
epoch£º713	 i:2 	 global-step:14262	 l-p:0.055750150233507156
epoch£º713	 i:3 	 global-step:14263	 l-p:0.055731985718011856
epoch£º713	 i:4 	 global-step:14264	 l-p:0.05570526048541069
epoch£º713	 i:5 	 global-step:14265	 l-p:0.05569477006793022
epoch£º713	 i:6 	 global-step:14266	 l-p:0.055879201740026474
epoch£º713	 i:7 	 global-step:14267	 l-p:0.055716436356306076
epoch£º713	 i:8 	 global-step:14268	 l-p:0.05572104454040527
epoch£º713	 i:9 	 global-step:14269	 l-p:0.05587064102292061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0687, 28.0687, 28.0687],
        [28.0687, 33.8201, 35.9200],
        [28.0687, 32.4169, 33.2917],
        [28.0687, 28.0874, 28.0696]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.05569705367088318 
model_pd.l_d.mean(): -3.918393076673965e-08 
model_pd.lagr.mean(): 0.055697012692689896 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6308], device='cuda:0')), ('power', tensor([-0.1094], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.05569705367088318
epoch£º714	 i:1 	 global-step:14281	 l-p:0.05570172518491745
epoch£º714	 i:2 	 global-step:14282	 l-p:0.05576648563146591
epoch£º714	 i:3 	 global-step:14283	 l-p:0.05590212717652321
epoch£º714	 i:4 	 global-step:14284	 l-p:0.055937543511390686
epoch£º714	 i:5 	 global-step:14285	 l-p:0.05573463439941406
epoch£º714	 i:6 	 global-step:14286	 l-p:0.05570315197110176
epoch£º714	 i:7 	 global-step:14287	 l-p:0.05571914464235306
epoch£º714	 i:8 	 global-step:14288	 l-p:0.05588298663496971
epoch£º714	 i:9 	 global-step:14289	 l-p:0.05565296858549118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1458, 28.2019, 28.1506],
        [28.1458, 37.5781, 43.8258],
        [28.1458, 31.0449, 30.9694],
        [28.1458, 28.1458, 28.1458]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.055708255618810654 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055708255618810654 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6068], device='cuda:0')), ('power', tensor([-0.0209], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.055708255618810654
epoch£º715	 i:1 	 global-step:14301	 l-p:0.055884115397930145
epoch£º715	 i:2 	 global-step:14302	 l-p:0.05568647384643555
epoch£º715	 i:3 	 global-step:14303	 l-p:0.05564761906862259
epoch£º715	 i:4 	 global-step:14304	 l-p:0.05567622929811478
epoch£º715	 i:5 	 global-step:14305	 l-p:0.055765245109796524
epoch£º715	 i:6 	 global-step:14306	 l-p:0.05567992106080055
epoch£º715	 i:7 	 global-step:14307	 l-p:0.055785927921533585
epoch£º715	 i:8 	 global-step:14308	 l-p:0.05589069798588753
epoch£º715	 i:9 	 global-step:14309	 l-p:0.05580183491110802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2223, 33.5065, 35.1533],
        [28.2223, 28.2336, 28.2227],
        [28.2223, 28.2223, 28.2223],
        [28.2223, 37.5456, 43.6377]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.055691175162792206 
model_pd.l_d.mean(): 2.3287036583496956e-06 
model_pd.lagr.mean(): 0.055693503469228745 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.1543e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6021], device='cuda:0')), ('power', tensor([0.0854], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.055691175162792206
epoch£º716	 i:1 	 global-step:14321	 l-p:0.05582146346569061
epoch£º716	 i:2 	 global-step:14322	 l-p:0.05586572363972664
epoch£º716	 i:3 	 global-step:14323	 l-p:0.05566900223493576
epoch£º716	 i:4 	 global-step:14324	 l-p:0.05577610805630684
epoch£º716	 i:5 	 global-step:14325	 l-p:0.055709049105644226
epoch£º716	 i:6 	 global-step:14326	 l-p:0.055739566683769226
epoch£º716	 i:7 	 global-step:14327	 l-p:0.05563376471400261
epoch£º716	 i:8 	 global-step:14328	 l-p:0.055668529123067856
epoch£º716	 i:9 	 global-step:14329	 l-p:0.05578746274113655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2931, 28.9111, 28.5266],
        [28.2931, 29.1476, 28.6900],
        [28.2931, 28.3170, 28.2943],
        [28.2931, 35.8462, 39.7986]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.055620864033699036 
model_pd.l_d.mean(): 3.89698607250466e-06 
model_pd.lagr.mean(): 0.05562476068735123 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.2775e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6630], device='cuda:0')), ('power', tensor([0.0430], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.055620864033699036
epoch£º717	 i:1 	 global-step:14341	 l-p:0.05587923899292946
epoch£º717	 i:2 	 global-step:14342	 l-p:0.05587603896856308
epoch£º717	 i:3 	 global-step:14343	 l-p:0.055629730224609375
epoch£º717	 i:4 	 global-step:14344	 l-p:0.05562765151262283
epoch£º717	 i:5 	 global-step:14345	 l-p:0.05563843250274658
epoch£º717	 i:6 	 global-step:14346	 l-p:0.055744271725416183
epoch£º717	 i:7 	 global-step:14347	 l-p:0.05569583177566528
epoch£º717	 i:8 	 global-step:14348	 l-p:0.05583389103412628
epoch£º717	 i:9 	 global-step:14349	 l-p:0.05567542836070061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[28.3463, 30.1641, 29.6947],
        [28.3463, 29.6002, 29.0872],
        [28.3463, 38.0998, 44.7178],
        [28.3463, 29.2926, 28.8146]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.055612802505493164 
model_pd.l_d.mean(): 1.6541969671379775e-05 
model_pd.lagr.mean(): 0.055629342794418335 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6733], device='cuda:0')), ('power', tensor([0.0891], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.055612802505493164
epoch£º718	 i:1 	 global-step:14361	 l-p:0.055648691952228546
epoch£º718	 i:2 	 global-step:14362	 l-p:0.055849943310022354
epoch£º718	 i:3 	 global-step:14363	 l-p:0.0556299090385437
epoch£º718	 i:4 	 global-step:14364	 l-p:0.05564244091510773
epoch£º718	 i:5 	 global-step:14365	 l-p:0.05572015792131424
epoch£º718	 i:6 	 global-step:14366	 l-p:0.055677641183137894
epoch£º718	 i:7 	 global-step:14367	 l-p:0.055968448519706726
epoch£º718	 i:8 	 global-step:14368	 l-p:0.05565397068858147
epoch£º718	 i:9 	 global-step:14369	 l-p:0.055726680904626846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3714, 29.4257, 28.9298],
        [28.3714, 33.9729, 35.8932],
        [28.3714, 35.6597, 39.3041],
        [28.3714, 28.4806, 28.3853]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.0559452585875988 
model_pd.l_d.mean(): 0.0001397978630848229 
model_pd.lagr.mean(): 0.05608505755662918 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5210], device='cuda:0')), ('power', tensor([0.4645], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.0559452585875988
epoch£º719	 i:1 	 global-step:14381	 l-p:0.05567380413413048
epoch£º719	 i:2 	 global-step:14382	 l-p:0.05565652623772621
epoch£º719	 i:3 	 global-step:14383	 l-p:0.0556197315454483
epoch£º719	 i:4 	 global-step:14384	 l-p:0.055847376585006714
epoch£º719	 i:5 	 global-step:14385	 l-p:0.05566614493727684
epoch£º719	 i:6 	 global-step:14386	 l-p:0.05560299754142761
epoch£º719	 i:7 	 global-step:14387	 l-p:0.05566844716668129
epoch£º719	 i:8 	 global-step:14388	 l-p:0.05576462671160698
epoch£º719	 i:9 	 global-step:14389	 l-p:0.055665865540504456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3592, 28.7507, 28.4695],
        [28.3592, 30.1820, 29.7132],
        [28.3592, 32.6712, 33.4913],
        [28.3592, 30.8429, 30.5692]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.05567135661840439 
model_pd.l_d.mean(): 8.376856567338109e-05 
model_pd.lagr.mean(): 0.0557551234960556 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6281], device='cuda:0')), ('power', tensor([0.1990], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.05567135661840439
epoch£º720	 i:1 	 global-step:14401	 l-p:0.055747464299201965
epoch£º720	 i:2 	 global-step:14402	 l-p:0.05580965802073479
epoch£º720	 i:3 	 global-step:14403	 l-p:0.055880483239889145
epoch£º720	 i:4 	 global-step:14404	 l-p:0.055731434375047684
epoch£º720	 i:5 	 global-step:14405	 l-p:0.0556640625
epoch£º720	 i:6 	 global-step:14406	 l-p:0.05559854209423065
epoch£º720	 i:7 	 global-step:14407	 l-p:0.055655431002378464
epoch£º720	 i:8 	 global-step:14408	 l-p:0.05575655773282051
epoch£º720	 i:9 	 global-step:14409	 l-p:0.05566326528787613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[28.3052, 33.2421, 34.5748],
        [28.3052, 38.0382, 44.6389],
        [28.3052, 30.9822, 30.7923],
        [28.3052, 31.5453, 31.6446]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.05582055822014809 
model_pd.l_d.mean(): 0.00013290095375850797 
model_pd.lagr.mean(): 0.055953457951545715 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5879], device='cuda:0')), ('power', tensor([0.2529], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.05582055822014809
epoch£º721	 i:1 	 global-step:14421	 l-p:0.055673591792583466
epoch£º721	 i:2 	 global-step:14422	 l-p:0.05563768371939659
epoch£º721	 i:3 	 global-step:14423	 l-p:0.05575939640402794
epoch£º721	 i:4 	 global-step:14424	 l-p:0.05563647300004959
epoch£º721	 i:5 	 global-step:14425	 l-p:0.05578947067260742
epoch£º721	 i:6 	 global-step:14426	 l-p:0.05576931685209274
epoch£º721	 i:7 	 global-step:14427	 l-p:0.055627498775720596
epoch£º721	 i:8 	 global-step:14428	 l-p:0.05588075891137123
epoch£º721	 i:9 	 global-step:14429	 l-p:0.05574174225330353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2135, 28.2135, 28.2135],
        [28.2135, 28.2245, 28.2139],
        [28.2135, 28.2135, 28.2135],
        [28.2135, 28.4586, 28.2651]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.055708903819322586 
model_pd.l_d.mean(): 4.6670604206155986e-05 
model_pd.lagr.mean(): 0.05575557425618172 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5902], device='cuda:0')), ('power', tensor([0.0785], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.055708903819322586
epoch£º722	 i:1 	 global-step:14441	 l-p:0.05594434589147568
epoch£º722	 i:2 	 global-step:14442	 l-p:0.055643677711486816
epoch£º722	 i:3 	 global-step:14443	 l-p:0.05577157810330391
epoch£º722	 i:4 	 global-step:14444	 l-p:0.0558541938662529
epoch£º722	 i:5 	 global-step:14445	 l-p:0.05579470470547676
epoch£º722	 i:6 	 global-step:14446	 l-p:0.05579446628689766
epoch£º722	 i:7 	 global-step:14447	 l-p:0.05565892532467842
epoch£º722	 i:8 	 global-step:14448	 l-p:0.0557003878057003
epoch£º722	 i:9 	 global-step:14449	 l-p:0.055695388466119766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0948, 28.4450, 28.1873],
        [28.0948, 33.7160, 35.6889],
        [28.0948, 28.0955, 28.0948],
        [28.0948, 28.0947, 28.0947]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.05570079758763313 
model_pd.l_d.mean(): -4.549077129922807e-05 
model_pd.lagr.mean(): 0.055655308067798615 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6159], device='cuda:0')), ('power', tensor([-0.0744], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.05570079758763313
epoch£º723	 i:1 	 global-step:14461	 l-p:0.055880624800920486
epoch£º723	 i:2 	 global-step:14462	 l-p:0.05587171018123627
epoch£º723	 i:3 	 global-step:14463	 l-p:0.05580517649650574
epoch£º723	 i:4 	 global-step:14464	 l-p:0.05573593080043793
epoch£º723	 i:5 	 global-step:14465	 l-p:0.055715158581733704
epoch£º723	 i:6 	 global-step:14466	 l-p:0.0557890385389328
epoch£º723	 i:7 	 global-step:14467	 l-p:0.05591568723320961
epoch£º723	 i:8 	 global-step:14468	 l-p:0.05570144206285477
epoch£º723	 i:9 	 global-step:14469	 l-p:0.05573178827762604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9645, 27.9765, 27.9649],
        [27.9645, 33.9192, 36.2300],
        [27.9645, 28.0188, 27.9691],
        [27.9645, 30.0243, 29.6238]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.05585115775465965 
model_pd.l_d.mean(): -7.563084363937378e-05 
model_pd.lagr.mean(): 0.05577552691102028 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5742], device='cuda:0')), ('power', tensor([-0.1335], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.05585115775465965
epoch£º724	 i:1 	 global-step:14481	 l-p:0.055740512907505035
epoch£º724	 i:2 	 global-step:14482	 l-p:0.055782850831747055
epoch£º724	 i:3 	 global-step:14483	 l-p:0.05579940974712372
epoch£º724	 i:4 	 global-step:14484	 l-p:0.05584672838449478
epoch£º724	 i:5 	 global-step:14485	 l-p:0.05573311820626259
epoch£º724	 i:6 	 global-step:14486	 l-p:0.05570301041007042
epoch£º724	 i:7 	 global-step:14487	 l-p:0.055757250636816025
epoch£º724	 i:8 	 global-step:14488	 l-p:0.05603039264678955
epoch£º724	 i:9 	 global-step:14489	 l-p:0.055892858654260635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8431, 27.8617, 27.8440],
        [27.8431, 32.6954, 34.0050],
        [27.8431, 32.1009, 32.9267],
        [27.8431, 28.6858, 28.2352]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.05575789511203766 
model_pd.l_d.mean(): -0.00018182101484853774 
model_pd.lagr.mean(): 0.05557607486844063 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6171], device='cuda:0')), ('power', tensor([-0.3970], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.05575789511203766
epoch£º725	 i:1 	 global-step:14501	 l-p:0.055985406041145325
epoch£º725	 i:2 	 global-step:14502	 l-p:0.05577685683965683
epoch£º725	 i:3 	 global-step:14503	 l-p:0.055740103125572205
epoch£º725	 i:4 	 global-step:14504	 l-p:0.055872686207294464
epoch£º725	 i:5 	 global-step:14505	 l-p:0.055760324001312256
epoch£º725	 i:6 	 global-step:14506	 l-p:0.05574900656938553
epoch£º725	 i:7 	 global-step:14507	 l-p:0.05599058046936989
epoch£º725	 i:8 	 global-step:14508	 l-p:0.05588366836309433
epoch£º725	 i:9 	 global-step:14509	 l-p:0.05586826056241989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[27.7527, 28.5317, 28.0982],
        [27.7527, 30.5020, 30.3748],
        [27.7527, 33.8470, 36.3247],
        [27.7527, 35.9311, 40.7054]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.05599553510546684 
model_pd.l_d.mean(): -7.334585097851232e-05 
model_pd.lagr.mean(): 0.055922187864780426 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5135], device='cuda:0')), ('power', tensor([-0.2483], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.05599553510546684
epoch£º726	 i:1 	 global-step:14521	 l-p:0.055811185389757156
epoch£º726	 i:2 	 global-step:14522	 l-p:0.055814266204833984
epoch£º726	 i:3 	 global-step:14523	 l-p:0.05574600398540497
epoch£º726	 i:4 	 global-step:14524	 l-p:0.05577607825398445
epoch£º726	 i:5 	 global-step:14525	 l-p:0.05587327480316162
epoch£º726	 i:6 	 global-step:14526	 l-p:0.05603263899683952
epoch£º726	 i:7 	 global-step:14527	 l-p:0.05586427450180054
epoch£º726	 i:8 	 global-step:14528	 l-p:0.05575910583138466
epoch£º726	 i:9 	 global-step:14529	 l-p:0.05587242916226387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7101, 27.7101, 27.7101],
        [27.7101, 35.3746, 39.5538],
        [27.7101, 37.2589, 43.7520],
        [27.7101, 27.7101, 27.7101]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.05578247830271721 
model_pd.l_d.mean(): -4.324304609326646e-05 
model_pd.lagr.mean(): 0.05573923513293266 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.6004e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6011], device='cuda:0')), ('power', tensor([-0.4410], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.05578247830271721
epoch£º727	 i:1 	 global-step:14541	 l-p:0.055756986141204834
epoch£º727	 i:2 	 global-step:14542	 l-p:0.055759746581315994
epoch£º727	 i:3 	 global-step:14543	 l-p:0.055814407765865326
epoch£º727	 i:4 	 global-step:14544	 l-p:0.055802009999752045
epoch£º727	 i:5 	 global-step:14545	 l-p:0.05603470280766487
epoch£º727	 i:6 	 global-step:14546	 l-p:0.055940382182598114
epoch£º727	 i:7 	 global-step:14547	 l-p:0.0558353029191494
epoch£º727	 i:8 	 global-step:14548	 l-p:0.055999286472797394
epoch£º727	 i:9 	 global-step:14549	 l-p:0.055855732411146164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7284, 27.7899, 27.7340],
        [27.7284, 27.7317, 27.7284],
        [27.7284, 31.9396, 32.7400],
        [27.7284, 37.4868, 44.2502]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.055785711854696274 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055785711854696274 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5927], device='cuda:0')), ('power', tensor([-0.4442], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.055785711854696274
epoch£º728	 i:1 	 global-step:14561	 l-p:0.055968549102544785
epoch£º728	 i:2 	 global-step:14562	 l-p:0.05585647374391556
epoch£º728	 i:3 	 global-step:14563	 l-p:0.05579540133476257
epoch£º728	 i:4 	 global-step:14564	 l-p:0.05612723156809807
epoch£º728	 i:5 	 global-step:14565	 l-p:0.05579129606485367
epoch£º728	 i:6 	 global-step:14566	 l-p:0.05589987710118294
epoch£º728	 i:7 	 global-step:14567	 l-p:0.05578186362981796
epoch£º728	 i:8 	 global-step:14568	 l-p:0.05575326830148697
epoch£º728	 i:9 	 global-step:14569	 l-p:0.055729568004608154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7886, 28.7000, 28.2350],
        [27.7886, 28.1951, 27.9076],
        [27.7886, 29.0086, 28.5065],
        [27.7886, 27.7886, 27.7886]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.05594002455472946 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05594002455472946 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5061], device='cuda:0')), ('power', tensor([-0.1589], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.05594002455472946
epoch£º729	 i:1 	 global-step:14581	 l-p:0.055813759565353394
epoch£º729	 i:2 	 global-step:14582	 l-p:0.05625408887863159
epoch£º729	 i:3 	 global-step:14583	 l-p:0.05572372302412987
epoch£º729	 i:4 	 global-step:14584	 l-p:0.055801019072532654
epoch£º729	 i:5 	 global-step:14585	 l-p:0.055737122893333435
epoch£º729	 i:6 	 global-step:14586	 l-p:0.0557517372071743
epoch£º729	 i:7 	 global-step:14587	 l-p:0.0557677298784256
epoch£º729	 i:8 	 global-step:14588	 l-p:0.05578698590397835
epoch£º729	 i:9 	 global-step:14589	 l-p:0.055757950991392136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[27.8636, 33.5000, 35.5159],
        [27.8636, 35.9821, 40.6643],
        [27.8636, 32.1783, 33.0463],
        [27.8636, 32.3327, 33.3227]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.05595973879098892 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05595973879098892 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5377], device='cuda:0')), ('power', tensor([-0.1031], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.05595973879098892
epoch£º730	 i:1 	 global-step:14601	 l-p:0.055729422718286514
epoch£º730	 i:2 	 global-step:14602	 l-p:0.05587790533900261
epoch£º730	 i:3 	 global-step:14603	 l-p:0.05572538077831268
epoch£º730	 i:4 	 global-step:14604	 l-p:0.055993955582380295
epoch£º730	 i:5 	 global-step:14605	 l-p:0.05579594150185585
epoch£º730	 i:6 	 global-step:14606	 l-p:0.055861152708530426
epoch£º730	 i:7 	 global-step:14607	 l-p:0.05572250112891197
epoch£º730	 i:8 	 global-step:14608	 l-p:0.055755142122507095
epoch£º730	 i:9 	 global-step:14609	 l-p:0.05573601648211479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9436, 36.9883, 42.7865],
        [27.9436, 29.3388, 28.8327],
        [27.9436, 30.8208, 30.7458],
        [27.9436, 27.9930, 27.9476]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.055761802941560745 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055761802941560745 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5787], device='cuda:0')), ('power', tensor([-0.1808], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.055761802941560745
epoch£º731	 i:1 	 global-step:14621	 l-p:0.05568547546863556
epoch£º731	 i:2 	 global-step:14622	 l-p:0.05573716014623642
epoch£º731	 i:3 	 global-step:14623	 l-p:0.055840764194726944
epoch£º731	 i:4 	 global-step:14624	 l-p:0.055931661278009415
epoch£º731	 i:5 	 global-step:14625	 l-p:0.05580328032374382
epoch£º731	 i:6 	 global-step:14626	 l-p:0.055810850113630295
epoch£º731	 i:7 	 global-step:14627	 l-p:0.05579175055027008
epoch£º731	 i:8 	 global-step:14628	 l-p:0.05593203380703926
epoch£º731	 i:9 	 global-step:14629	 l-p:0.05567847192287445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0254, 33.9832, 36.2890],
        [28.0254, 28.0356, 28.0258],
        [28.0254, 28.0254, 28.0254],
        [28.0254, 28.0415, 28.0261]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.05589457228779793 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05589457228779793 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5582], device='cuda:0')), ('power', tensor([-0.0138], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.05589457228779793
epoch£º732	 i:1 	 global-step:14641	 l-p:0.05576968938112259
epoch£º732	 i:2 	 global-step:14642	 l-p:0.05574924498796463
epoch£º732	 i:3 	 global-step:14643	 l-p:0.05577569454908371
epoch£º732	 i:4 	 global-step:14644	 l-p:0.055972784757614136
epoch£º732	 i:5 	 global-step:14645	 l-p:0.05572709068655968
epoch£º732	 i:6 	 global-step:14646	 l-p:0.05570119246840477
epoch£º732	 i:7 	 global-step:14647	 l-p:0.055691011250019073
epoch£º732	 i:8 	 global-step:14648	 l-p:0.055696554481983185
epoch£º732	 i:9 	 global-step:14649	 l-p:0.05581122264266014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1085, 28.1085, 28.1085],
        [28.1085, 28.1632, 28.1132],
        [28.1085, 28.5431, 28.2402],
        [28.1085, 28.2692, 28.1346]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.05581330135464668 
model_pd.l_d.mean(): 6.133819852038869e-08 
model_pd.lagr.mean(): 0.05581336095929146 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.5112e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5880], device='cuda:0')), ('power', tensor([0.0376], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.05581330135464668
epoch£º733	 i:1 	 global-step:14661	 l-p:0.05572706088423729
epoch£º733	 i:2 	 global-step:14662	 l-p:0.05570931360125542
epoch£º733	 i:3 	 global-step:14663	 l-p:0.05568823590874672
epoch£º733	 i:4 	 global-step:14664	 l-p:0.05574535205960274
epoch£º733	 i:5 	 global-step:14665	 l-p:0.055663831532001495
epoch£º733	 i:6 	 global-step:14666	 l-p:0.05567263439297676
epoch£º733	 i:7 	 global-step:14667	 l-p:0.055956363677978516
epoch£º733	 i:8 	 global-step:14668	 l-p:0.055878281593322754
epoch£º733	 i:9 	 global-step:14669	 l-p:0.055748481303453445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1912, 28.1965, 28.1913],
        [28.1912, 35.7158, 39.6533],
        [28.1912, 37.3456, 43.2307],
        [28.1912, 32.0506, 32.5537]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.05569028854370117 
model_pd.l_d.mean(): 2.3654490632907255e-07 
model_pd.lagr.mean(): 0.055690523236989975 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.6777e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6161], device='cuda:0')), ('power', tensor([0.0147], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.05569028854370117
epoch£º734	 i:1 	 global-step:14681	 l-p:0.05569685995578766
epoch£º734	 i:2 	 global-step:14682	 l-p:0.055648718029260635
epoch£º734	 i:3 	 global-step:14683	 l-p:0.05570121482014656
epoch£º734	 i:4 	 global-step:14684	 l-p:0.055660419166088104
epoch£º734	 i:5 	 global-step:14685	 l-p:0.05582749843597412
epoch£º734	 i:6 	 global-step:14686	 l-p:0.05573541671037674
epoch£º734	 i:7 	 global-step:14687	 l-p:0.056047577410936356
epoch£º734	 i:8 	 global-step:14688	 l-p:0.055608585476875305
epoch£º734	 i:9 	 global-step:14689	 l-p:0.05580519512295723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2703, 30.7459, 30.4730],
        [28.2703, 28.7432, 28.4210],
        [28.2703, 33.9285, 35.9145],
        [28.2703, 33.5640, 35.2138]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.0557585135102272 
model_pd.l_d.mean(): 9.718007277115248e-06 
model_pd.lagr.mean(): 0.05576823279261589 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.2999e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6138], device='cuda:0')), ('power', tensor([0.1482], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.0557585135102272
epoch£º735	 i:1 	 global-step:14701	 l-p:0.05564459040760994
epoch£º735	 i:2 	 global-step:14702	 l-p:0.055691175162792206
epoch£º735	 i:3 	 global-step:14703	 l-p:0.055714767426252365
epoch£º735	 i:4 	 global-step:14704	 l-p:0.05585837736725807
epoch£º735	 i:5 	 global-step:14705	 l-p:0.05586536228656769
epoch£º735	 i:6 	 global-step:14706	 l-p:0.05566184222698212
epoch£º735	 i:7 	 global-step:14707	 l-p:0.055726308375597
epoch£º735	 i:8 	 global-step:14708	 l-p:0.05562568083405495
epoch£º735	 i:9 	 global-step:14709	 l-p:0.05571316182613373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3357, 37.6117, 43.6200],
        [28.3357, 28.6964, 28.4323],
        [28.3357, 31.3492, 31.3216],
        [28.3357, 31.2554, 31.1794]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.055658433586359024 
model_pd.l_d.mean(): 3.2581046980340034e-05 
model_pd.lagr.mean(): 0.05569101497530937 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6280], device='cuda:0')), ('power', tensor([0.2145], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.055658433586359024
epoch£º736	 i:1 	 global-step:14721	 l-p:0.05561784654855728
epoch£º736	 i:2 	 global-step:14722	 l-p:0.05564490333199501
epoch£º736	 i:3 	 global-step:14723	 l-p:0.055895183235406876
epoch£º736	 i:4 	 global-step:14724	 l-p:0.05593656376004219
epoch£º736	 i:5 	 global-step:14725	 l-p:0.05563764274120331
epoch£º736	 i:6 	 global-step:14726	 l-p:0.05562726780772209
epoch£º736	 i:7 	 global-step:14727	 l-p:0.055644482374191284
epoch£º736	 i:8 	 global-step:14728	 l-p:0.0558168925344944
epoch£º736	 i:9 	 global-step:14729	 l-p:0.05566231533885002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3733, 28.5388, 28.4005],
        [28.3733, 38.3313, 45.2106],
        [28.3733, 33.8709, 35.6947],
        [28.3733, 37.8649, 44.1394]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.05562841147184372 
model_pd.l_d.mean(): 3.3678610634524375e-05 
model_pd.lagr.mean(): 0.055662091821432114 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6561], device='cuda:0')), ('power', tensor([0.1272], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.05562841147184372
epoch£º737	 i:1 	 global-step:14741	 l-p:0.05572468787431717
epoch£º737	 i:2 	 global-step:14742	 l-p:0.05559920147061348
epoch£º737	 i:3 	 global-step:14743	 l-p:0.05581340566277504
epoch£º737	 i:4 	 global-step:14744	 l-p:0.055636778473854065
epoch£º737	 i:5 	 global-step:14745	 l-p:0.05582910776138306
epoch£º737	 i:6 	 global-step:14746	 l-p:0.055658236145973206
epoch£º737	 i:7 	 global-step:14747	 l-p:0.055923689156770706
epoch£º737	 i:8 	 global-step:14748	 l-p:0.055635347962379456
epoch£º737	 i:9 	 global-step:14749	 l-p:0.05564592406153679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3734, 28.4300, 28.3783],
        [28.3734, 28.3845, 28.3738],
        [28.3734, 28.3766, 28.3735],
        [28.3734, 28.3734, 28.3734]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.055669382214546204 
model_pd.l_d.mean(): 8.947342575993389e-05 
model_pd.lagr.mean(): 0.05575885623693466 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6044], device='cuda:0')), ('power', tensor([0.2303], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.055669382214546204
epoch£º738	 i:1 	 global-step:14761	 l-p:0.055693771690130234
epoch£º738	 i:2 	 global-step:14762	 l-p:0.05604708939790726
epoch£º738	 i:3 	 global-step:14763	 l-p:0.055734556168317795
epoch£º738	 i:4 	 global-step:14764	 l-p:0.05572880804538727
epoch£º738	 i:5 	 global-step:14765	 l-p:0.05569171905517578
epoch£º738	 i:6 	 global-step:14766	 l-p:0.055666401982307434
epoch£º738	 i:7 	 global-step:14767	 l-p:0.05560120940208435
epoch£º738	 i:8 	 global-step:14768	 l-p:0.055636465549468994
epoch£º738	 i:9 	 global-step:14769	 l-p:0.05566735193133354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3287, 28.3287, 28.3287],
        [28.3287, 28.3287, 28.3287],
        [28.3287, 34.3096, 36.5981],
        [28.3287, 29.1941, 28.7336]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.055775392800569534 
model_pd.l_d.mean(): 0.00014397829363588244 
model_pd.lagr.mean(): 0.05591937154531479 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5801], device='cuda:0')), ('power', tensor([0.2866], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.055775392800569534
epoch£º739	 i:1 	 global-step:14781	 l-p:0.05579395219683647
epoch£º739	 i:2 	 global-step:14782	 l-p:0.05564548075199127
epoch£º739	 i:3 	 global-step:14783	 l-p:0.05575455725193024
epoch£º739	 i:4 	 global-step:14784	 l-p:0.05573941767215729
epoch£º739	 i:5 	 global-step:14785	 l-p:0.0556853748857975
epoch£º739	 i:6 	 global-step:14786	 l-p:0.05570833012461662
epoch£º739	 i:7 	 global-step:14787	 l-p:0.05564101040363312
epoch£º739	 i:8 	 global-step:14788	 l-p:0.05587374418973923
epoch£º739	 i:9 	 global-step:14789	 l-p:0.055662851780653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2398, 28.2398, 28.2398],
        [28.2398, 28.2912, 28.2440],
        [28.2398, 30.2330, 29.8052],
        [28.2398, 32.0890, 32.5811]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.05591768026351929 
model_pd.l_d.mean(): 0.0001426054077455774 
model_pd.lagr.mean(): 0.05606028437614441 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5633], device='cuda:0')), ('power', tensor([0.2442], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.05591768026351929
epoch£º740	 i:1 	 global-step:14801	 l-p:0.055656202137470245
epoch£º740	 i:2 	 global-step:14802	 l-p:0.05577129125595093
epoch£º740	 i:3 	 global-step:14803	 l-p:0.05572952702641487
epoch£º740	 i:4 	 global-step:14804	 l-p:0.05583765357732773
epoch£º740	 i:5 	 global-step:14805	 l-p:0.055648624897003174
epoch£º740	 i:6 	 global-step:14806	 l-p:0.055913157761096954
epoch£º740	 i:7 	 global-step:14807	 l-p:0.05567292869091034
epoch£º740	 i:8 	 global-step:14808	 l-p:0.055698007345199585
epoch£º740	 i:9 	 global-step:14809	 l-p:0.05566560477018356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1175, 32.7320, 33.8145],
        [28.1175, 37.7888, 44.3508],
        [28.1175, 32.8548, 34.0386],
        [28.1175, 28.1175, 28.1175]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.05571315065026283 
model_pd.l_d.mean(): -8.269705176644493e-06 
model_pd.lagr.mean(): 0.05570488050580025 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5982], device='cuda:0')), ('power', tensor([-0.0135], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.05571315065026283
epoch£º741	 i:1 	 global-step:14821	 l-p:0.05573077127337456
epoch£º741	 i:2 	 global-step:14822	 l-p:0.0559527687728405
epoch£º741	 i:3 	 global-step:14823	 l-p:0.055670127272605896
epoch£º741	 i:4 	 global-step:14824	 l-p:0.05587884783744812
epoch£º741	 i:5 	 global-step:14825	 l-p:0.05571456626057625
epoch£º741	 i:6 	 global-step:14826	 l-p:0.05576677992939949
epoch£º741	 i:7 	 global-step:14827	 l-p:0.055696725845336914
epoch£º741	 i:8 	 global-step:14828	 l-p:0.05576294660568237
epoch£º741	 i:9 	 global-step:14829	 l-p:0.055917996913194656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9794, 37.6251, 44.1846],
        [27.9794, 28.7740, 28.3344],
        [27.9794, 27.9970, 27.9802],
        [27.9794, 30.3312, 30.0244]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.05580434203147888 
model_pd.l_d.mean(): -4.351429743110202e-05 
model_pd.lagr.mean(): 0.05576082691550255 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5402], device='cuda:0')), ('power', tensor([-0.0753], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.05580434203147888
epoch£º742	 i:1 	 global-step:14841	 l-p:0.05570971220731735
epoch£º742	 i:2 	 global-step:14842	 l-p:0.0557713657617569
epoch£º742	 i:3 	 global-step:14843	 l-p:0.05608562380075455
epoch£º742	 i:4 	 global-step:14844	 l-p:0.05571310967206955
epoch£º742	 i:5 	 global-step:14845	 l-p:0.05599247291684151
epoch£º742	 i:6 	 global-step:14846	 l-p:0.05578161031007767
epoch£º742	 i:7 	 global-step:14847	 l-p:0.055718183517456055
epoch£º742	 i:8 	 global-step:14848	 l-p:0.05571258068084717
epoch£º742	 i:9 	 global-step:14849	 l-p:0.05582289397716522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8475, 27.8476, 27.8474],
        [27.8475, 28.1889, 27.9368],
        [27.8475, 33.9636, 36.4503],
        [27.8475, 32.3139, 33.3032]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.05583838373422623 
model_pd.l_d.mean(): -0.00011159892892464995 
model_pd.lagr.mean(): 0.0557267852127552 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5501], device='cuda:0')), ('power', tensor([-0.2351], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.05583838373422623
epoch£º743	 i:1 	 global-step:14861	 l-p:0.055806227028369904
epoch£º743	 i:2 	 global-step:14862	 l-p:0.05576634407043457
epoch£º743	 i:3 	 global-step:14863	 l-p:0.05575094744563103
epoch£º743	 i:4 	 global-step:14864	 l-p:0.056036725640296936
epoch£º743	 i:5 	 global-step:14865	 l-p:0.05586377531290054
epoch£º743	 i:6 	 global-step:14866	 l-p:0.05572504922747612
epoch£º743	 i:7 	 global-step:14867	 l-p:0.055795978754758835
epoch£º743	 i:8 	 global-step:14868	 l-p:0.05597682669758797
epoch£º743	 i:9 	 global-step:14869	 l-p:0.05582797899842262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7446, 28.2142, 27.8954],
        [27.7446, 28.6059, 28.1520],
        [27.7446, 28.2046, 27.8904],
        [27.7446, 27.9314, 27.7782]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.055735863745212555 
model_pd.l_d.mean(): -0.0001685033057583496 
model_pd.lagr.mean(): 0.055567361414432526 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6473], device='cuda:0')), ('power', tensor([-0.5412], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.055735863745212555
epoch£º744	 i:1 	 global-step:14881	 l-p:0.055874623358249664
epoch£º744	 i:2 	 global-step:14882	 l-p:0.05579410493373871
epoch£º744	 i:3 	 global-step:14883	 l-p:0.05594056844711304
epoch£º744	 i:4 	 global-step:14884	 l-p:0.05577956885099411
epoch£º744	 i:5 	 global-step:14885	 l-p:0.05585946887731552
epoch£º744	 i:6 	 global-step:14886	 l-p:0.05574807524681091
epoch£º744	 i:7 	 global-step:14887	 l-p:0.05579179525375366
epoch£º744	 i:8 	 global-step:14888	 l-p:0.05581296980381012
epoch£º744	 i:9 	 global-step:14889	 l-p:0.05623622611165047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6947, 27.8529, 27.7204],
        [27.6947, 27.6949, 27.6947],
        [27.6947, 37.4042, 44.1108],
        [27.6947, 32.6291, 34.0255]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.055916573852300644 
model_pd.l_d.mean(): -3.832186484942213e-05 
model_pd.lagr.mean(): 0.055878251791000366 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.0322e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5385], device='cuda:0')), ('power', tensor([-0.3547], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.055916573852300644
epoch£º745	 i:1 	 global-step:14901	 l-p:0.05581769347190857
epoch£º745	 i:2 	 global-step:14902	 l-p:0.05576635152101517
epoch£º745	 i:3 	 global-step:14903	 l-p:0.055788688361644745
epoch£º745	 i:4 	 global-step:14904	 l-p:0.05579277500510216
epoch£º745	 i:5 	 global-step:14905	 l-p:0.056191787123680115
epoch£º745	 i:6 	 global-step:14906	 l-p:0.05597909912467003
epoch£º745	 i:7 	 global-step:14907	 l-p:0.055856578052043915
epoch£º745	 i:8 	 global-step:14908	 l-p:0.05574403703212738
epoch£º745	 i:9 	 global-step:14909	 l-p:0.055764537304639816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7123, 27.7409, 27.7140],
        [27.7123, 27.7547, 27.7154],
        [27.7123, 34.6308, 37.9753],
        [27.7123, 27.7123, 27.7123]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.05603712797164917 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05603712797164917 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5347], device='cuda:0')), ('power', tensor([-0.2606], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.05603712797164917
epoch£º746	 i:1 	 global-step:14921	 l-p:0.05574993044137955
epoch£º746	 i:2 	 global-step:14922	 l-p:0.05596420168876648
epoch£º746	 i:3 	 global-step:14923	 l-p:0.055761851370334625
epoch£º746	 i:4 	 global-step:14924	 l-p:0.055770985782146454
epoch£º746	 i:5 	 global-step:14925	 l-p:0.05575175955891609
epoch£º746	 i:6 	 global-step:14926	 l-p:0.055861975997686386
epoch£º746	 i:7 	 global-step:14927	 l-p:0.055841319262981415
epoch£º746	 i:8 	 global-step:14928	 l-p:0.055895328521728516
epoch£º746	 i:9 	 global-step:14929	 l-p:0.05588888004422188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7750, 28.0470, 27.8368],
        [27.7750, 27.9621, 27.8087],
        [27.7750, 34.2728, 37.1601],
        [27.7750, 34.4573, 37.5387]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.05586416274309158 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05586416274309158 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5727], device='cuda:0')), ('power', tensor([-0.2877], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.05586416274309158
epoch£º747	 i:1 	 global-step:14941	 l-p:0.05577586963772774
epoch£º747	 i:2 	 global-step:14942	 l-p:0.05577436462044716
epoch£º747	 i:3 	 global-step:14943	 l-p:0.05592155456542969
epoch£º747	 i:4 	 global-step:14944	 l-p:0.05572470650076866
epoch£º747	 i:5 	 global-step:14945	 l-p:0.05600966513156891
epoch£º747	 i:6 	 global-step:14946	 l-p:0.05577211081981659
epoch£º747	 i:7 	 global-step:14947	 l-p:0.05592293664813042
epoch£º747	 i:8 	 global-step:14948	 l-p:0.05585624650120735
epoch£º747	 i:9 	 global-step:14949	 l-p:0.055737897753715515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8545, 29.3653, 28.8676],
        [27.8545, 37.4552, 43.9839],
        [27.8545, 28.5041, 28.1105],
        [27.8545, 29.6109, 29.1450]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.05573054775595665 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05573054775595665 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6319], device='cuda:0')), ('power', tensor([-0.3579], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.05573054775595665
epoch£º748	 i:1 	 global-step:14961	 l-p:0.05579739436507225
epoch£º748	 i:2 	 global-step:14962	 l-p:0.05585396662354469
epoch£º748	 i:3 	 global-step:14963	 l-p:0.055706873536109924
epoch£º748	 i:4 	 global-step:14964	 l-p:0.056052617728710175
epoch£º748	 i:5 	 global-step:14965	 l-p:0.05583354830741882
epoch£º748	 i:6 	 global-step:14966	 l-p:0.05579041317105293
epoch£º748	 i:7 	 global-step:14967	 l-p:0.05577529966831207
epoch£º748	 i:8 	 global-step:14968	 l-p:0.05574554204940796
epoch£º748	 i:9 	 global-step:14969	 l-p:0.055885154753923416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9399, 37.3002, 43.4998],
        [27.9399, 28.0472, 27.9536],
        [27.9399, 28.0683, 27.9581],
        [27.9399, 31.4733, 31.7772]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.05578349158167839 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05578349158167839 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5598], device='cuda:0')), ('power', tensor([-0.1569], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.05578349158167839
epoch£º749	 i:1 	 global-step:14981	 l-p:0.055704373866319656
epoch£º749	 i:2 	 global-step:14982	 l-p:0.05575442314147949
epoch£º749	 i:3 	 global-step:14983	 l-p:0.05574185773730278
epoch£º749	 i:4 	 global-step:14984	 l-p:0.05568879842758179
epoch£º749	 i:5 	 global-step:14985	 l-p:0.05616723746061325
epoch£º749	 i:6 	 global-step:14986	 l-p:0.05573510006070137
epoch£º749	 i:7 	 global-step:14987	 l-p:0.05568103492259979
epoch£º749	 i:8 	 global-step:14988	 l-p:0.05579804256558418
epoch£º749	 i:9 	 global-step:14989	 l-p:0.055921029299497604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0273, 33.6996, 35.7292],
        [28.0273, 37.1262, 42.9753],
        [28.0273, 28.0463, 28.0282],
        [28.0273, 28.0510, 28.0285]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.05572860687971115 
model_pd.l_d.mean(): -2.2683509826038062e-07 
model_pd.lagr.mean(): 0.055728379637002945 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5995], device='cuda:0')), ('power', tensor([-0.1469], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.05572860687971115
epoch£º750	 i:1 	 global-step:15001	 l-p:0.055837392807006836
epoch£º750	 i:2 	 global-step:15002	 l-p:0.05570882558822632
epoch£º750	 i:3 	 global-step:15003	 l-p:0.05580311268568039
epoch£º750	 i:4 	 global-step:15004	 l-p:0.05573257431387901
epoch£º750	 i:5 	 global-step:15005	 l-p:0.05588407441973686
epoch£º750	 i:6 	 global-step:15006	 l-p:0.05578303709626198
epoch£º750	 i:7 	 global-step:15007	 l-p:0.05591113120317459
epoch£º750	 i:8 	 global-step:15008	 l-p:0.05569464713335037
epoch£º750	 i:9 	 global-step:15009	 l-p:0.055695317685604095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1160, 28.7953, 28.3897],
        [28.1160, 34.0497, 36.3200],
        [28.1160, 28.1160, 28.1160],
        [28.1160, 29.3591, 28.8504]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.055672239512205124 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055672239512205124 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6310], device='cuda:0')), ('power', tensor([-0.0477], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.055672239512205124
epoch£º751	 i:1 	 global-step:15021	 l-p:0.0556509867310524
epoch£º751	 i:2 	 global-step:15022	 l-p:0.05571822077035904
epoch£º751	 i:3 	 global-step:15023	 l-p:0.0557040236890316
epoch£º751	 i:4 	 global-step:15024	 l-p:0.05581522732973099
epoch£º751	 i:5 	 global-step:15025	 l-p:0.055937618017196655
epoch£º751	 i:6 	 global-step:15026	 l-p:0.05587080866098404
epoch£º751	 i:7 	 global-step:15027	 l-p:0.05569193884730339
epoch£º751	 i:8 	 global-step:15028	 l-p:0.055776309221982956
epoch£º751	 i:9 	 global-step:15029	 l-p:0.05574323609471321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2040, 28.2040, 28.2040],
        [28.2040, 28.4065, 28.2419],
        [28.2040, 29.0054, 28.5621],
        [28.2040, 28.2229, 28.2049]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.05583702027797699 
model_pd.l_d.mean(): 4.1599214455345646e-06 
model_pd.lagr.mean(): 0.05584118142724037 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.2794e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5485], device='cuda:0')), ('power', tensor([0.1719], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.05583702027797699
epoch£º752	 i:1 	 global-step:15041	 l-p:0.05571312829852104
epoch£º752	 i:2 	 global-step:15042	 l-p:0.05584258958697319
epoch£º752	 i:3 	 global-step:15043	 l-p:0.05568358674645424
epoch£º752	 i:4 	 global-step:15044	 l-p:0.05566386133432388
epoch£º752	 i:5 	 global-step:15045	 l-p:0.05563674867153168
epoch£º752	 i:6 	 global-step:15046	 l-p:0.055883970111608505
epoch£º752	 i:7 	 global-step:15047	 l-p:0.055746957659721375
epoch£º752	 i:8 	 global-step:15048	 l-p:0.05560535192489624
epoch£º752	 i:9 	 global-step:15049	 l-p:0.05577879771590233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2847, 28.2930, 28.2849],
        [28.2847, 28.2848, 28.2847],
        [28.2847, 29.1389, 28.6815],
        [28.2847, 30.3179, 29.8989]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.055648211389780045 
model_pd.l_d.mean(): 1.0755743460322265e-05 
model_pd.lagr.mean(): 0.055658966302871704 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.7417e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6360], device='cuda:0')), ('power', tensor([0.1332], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.055648211389780045
epoch£º753	 i:1 	 global-step:15061	 l-p:0.05560798570513725
epoch£º753	 i:2 	 global-step:15062	 l-p:0.055742401629686356
epoch£º753	 i:3 	 global-step:15063	 l-p:0.055663470178842545
epoch£º753	 i:4 	 global-step:15064	 l-p:0.05562064051628113
epoch£º753	 i:5 	 global-step:15065	 l-p:0.055952318012714386
epoch£º753	 i:6 	 global-step:15066	 l-p:0.05590742826461792
epoch£º753	 i:7 	 global-step:15067	 l-p:0.055650290101766586
epoch£º753	 i:8 	 global-step:15068	 l-p:0.05562952905893326
epoch£º753	 i:9 	 global-step:15069	 l-p:0.0558074451982975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3477, 28.3716, 28.3489],
        [28.3477, 28.3655, 28.3485],
        [28.3477, 35.8806, 39.8011],
        [28.3477, 28.3483, 28.3477]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.05565030500292778 
model_pd.l_d.mean(): 2.6538402380538173e-05 
model_pd.lagr.mean(): 0.05567684397101402 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6310], device='cuda:0')), ('power', tensor([0.1528], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.05565030500292778
epoch£º754	 i:1 	 global-step:15081	 l-p:0.0556718185544014
epoch£º754	 i:2 	 global-step:15082	 l-p:0.05585270747542381
epoch£º754	 i:3 	 global-step:15083	 l-p:0.055633239448070526
epoch£º754	 i:4 	 global-step:15084	 l-p:0.05560349300503731
epoch£º754	 i:5 	 global-step:15085	 l-p:0.055750925093889236
epoch£º754	 i:6 	 global-step:15086	 l-p:0.05580028519034386
epoch£º754	 i:7 	 global-step:15087	 l-p:0.0556844063103199
epoch£º754	 i:8 	 global-step:15088	 l-p:0.05565518140792847
epoch£º754	 i:9 	 global-step:15089	 l-p:0.05581706762313843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3801, 29.5267, 29.0202],
        [28.3801, 28.5894, 28.4198],
        [28.3801, 34.0612, 36.0554],
        [28.3801, 30.6454, 30.2916]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.05569102242588997 
model_pd.l_d.mean(): 6.859311542939395e-05 
model_pd.lagr.mean(): 0.05575961619615555 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5912], device='cuda:0')), ('power', tensor([0.2352], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.05569102242588997
epoch£º755	 i:1 	 global-step:15101	 l-p:0.0558365136384964
epoch£º755	 i:2 	 global-step:15102	 l-p:0.05561595782637596
epoch£º755	 i:3 	 global-step:15103	 l-p:0.05568588152527809
epoch£º755	 i:4 	 global-step:15104	 l-p:0.055779263377189636
epoch£º755	 i:5 	 global-step:15105	 l-p:0.055709805339574814
epoch£º755	 i:6 	 global-step:15106	 l-p:0.05560361593961716
epoch£º755	 i:7 	 global-step:15107	 l-p:0.055666904896497726
epoch£º755	 i:8 	 global-step:15108	 l-p:0.055908203125
epoch£º755	 i:9 	 global-step:15109	 l-p:0.05559225007891655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3694, 33.7321, 35.4330],
        [28.3694, 28.3694, 28.3694],
        [28.3694, 31.7063, 31.8581],
        [28.3694, 30.7436, 30.4281]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.05563528835773468 
model_pd.l_d.mean(): 6.7400440457277e-05 
model_pd.lagr.mean(): 0.05570269003510475 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6491], device='cuda:0')), ('power', tensor([0.1619], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.05563528835773468
epoch£º756	 i:1 	 global-step:15121	 l-p:0.05565090849995613
epoch£º756	 i:2 	 global-step:15122	 l-p:0.05577382072806358
epoch£º756	 i:3 	 global-step:15123	 l-p:0.055727310478687286
epoch£º756	 i:4 	 global-step:15124	 l-p:0.05563970282673836
epoch£º756	 i:5 	 global-step:15125	 l-p:0.05560601130127907
epoch£º756	 i:6 	 global-step:15126	 l-p:0.05599706619977951
epoch£º756	 i:7 	 global-step:15127	 l-p:0.055830828845500946
epoch£º756	 i:8 	 global-step:15128	 l-p:0.05567588657140732
epoch£º756	 i:9 	 global-step:15129	 l-p:0.05562344565987587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3111, 32.6154, 33.4340],
        [28.3111, 33.9778, 35.9668],
        [28.3111, 29.1662, 28.7083],
        [28.3111, 28.3111, 28.3111]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.055983565747737885 
model_pd.l_d.mean(): 0.00017980806296691298 
model_pd.lagr.mean(): 0.056163374334573746 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5362], device='cuda:0')), ('power', tensor([0.3423], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.055983565747737885
epoch£º757	 i:1 	 global-step:15141	 l-p:0.05562117323279381
epoch£º757	 i:2 	 global-step:15142	 l-p:0.0559973306953907
epoch£º757	 i:3 	 global-step:15143	 l-p:0.05567442253232002
epoch£º757	 i:4 	 global-step:15144	 l-p:0.05567853897809982
epoch£º757	 i:5 	 global-step:15145	 l-p:0.05566922947764397
epoch£º757	 i:6 	 global-step:15146	 l-p:0.0556747242808342
epoch£º757	 i:7 	 global-step:15147	 l-p:0.055676478892564774
epoch£º757	 i:8 	 global-step:15148	 l-p:0.05563441291451454
epoch£º757	 i:9 	 global-step:15149	 l-p:0.05572056770324707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2083, 29.6803, 29.1719],
        [28.2083, 29.3476, 28.8443],
        [28.2083, 28.5909, 28.3150],
        [28.2083, 30.6781, 30.4059]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.055648334324359894 
model_pd.l_d.mean(): -1.567957406223286e-05 
model_pd.lagr.mean(): 0.05563265457749367 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6522], device='cuda:0')), ('power', tensor([-0.0263], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.055648334324359894
epoch£º758	 i:1 	 global-step:15161	 l-p:0.05596669390797615
epoch£º758	 i:2 	 global-step:15162	 l-p:0.05569133535027504
epoch£º758	 i:3 	 global-step:15163	 l-p:0.055728502571582794
epoch£º758	 i:4 	 global-step:15164	 l-p:0.05568036064505577
epoch£º758	 i:5 	 global-step:15165	 l-p:0.05572879686951637
epoch£º758	 i:6 	 global-step:15166	 l-p:0.05591845512390137
epoch£º758	 i:7 	 global-step:15167	 l-p:0.055714596062898636
epoch£º758	 i:8 	 global-step:15168	 l-p:0.055645741522312164
epoch£º758	 i:9 	 global-step:15169	 l-p:0.05587248504161835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0735, 28.0737, 28.0735],
        [28.0735, 28.0735, 28.0735],
        [28.0735, 33.5102, 35.3134],
        [28.0735, 28.0735, 28.0735]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.05582663416862488 
model_pd.l_d.mean(): -4.271210855222307e-05 
model_pd.lagr.mean(): 0.05578392371535301 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5900], device='cuda:0')), ('power', tensor([-0.0705], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.05582663416862488
epoch£º759	 i:1 	 global-step:15181	 l-p:0.05581796169281006
epoch£º759	 i:2 	 global-step:15182	 l-p:0.05573258176445961
epoch£º759	 i:3 	 global-step:15183	 l-p:0.055877283215522766
epoch£º759	 i:4 	 global-step:15184	 l-p:0.055721696466207504
epoch£º759	 i:5 	 global-step:15185	 l-p:0.055690646171569824
epoch£º759	 i:6 	 global-step:15186	 l-p:0.055747684091329575
epoch£º759	 i:7 	 global-step:15187	 l-p:0.05569053441286087
epoch£º759	 i:8 	 global-step:15188	 l-p:0.05583946406841278
epoch£º759	 i:9 	 global-step:15189	 l-p:0.055967312306165695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9279, 28.0837, 27.9528],
        [27.9279, 29.3791, 28.8757],
        [27.9279, 31.1458, 31.2569],
        [27.9279, 28.2951, 28.0283]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.05594504624605179 
model_pd.l_d.mean(): -2.1799201931571588e-05 
model_pd.lagr.mean(): 0.05592324584722519 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5323], device='cuda:0')), ('power', tensor([-0.0399], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.05594504624605179
epoch£º760	 i:1 	 global-step:15201	 l-p:0.05611048638820648
epoch£º760	 i:2 	 global-step:15202	 l-p:0.05587194487452507
epoch£º760	 i:3 	 global-step:15203	 l-p:0.055787913501262665
epoch£º760	 i:4 	 global-step:15204	 l-p:0.05572959780693054
epoch£º760	 i:5 	 global-step:15205	 l-p:0.055748529732227325
epoch£º760	 i:6 	 global-step:15206	 l-p:0.055715542286634445
epoch£º760	 i:7 	 global-step:15207	 l-p:0.05578775331377983
epoch£º760	 i:8 	 global-step:15208	 l-p:0.05577664077281952
epoch£º760	 i:9 	 global-step:15209	 l-p:0.05575370788574219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7961, 27.8709, 27.8038],
        [27.7961, 32.0463, 32.8706],
        [27.7961, 29.5783, 29.1187],
        [27.7961, 33.9074, 36.3962]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.056125469505786896 
model_pd.l_d.mean(): -2.0832454538322054e-05 
model_pd.lagr.mean(): 0.0561046376824379 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4879], device='cuda:0')), ('power', tensor([-0.0499], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.056125469505786896
epoch£º761	 i:1 	 global-step:15221	 l-p:0.05574730783700943
epoch£º761	 i:2 	 global-step:15222	 l-p:0.05577321723103523
epoch£º761	 i:3 	 global-step:15223	 l-p:0.05573748052120209
epoch£º761	 i:4 	 global-step:15224	 l-p:0.05575285851955414
epoch£º761	 i:5 	 global-step:15225	 l-p:0.05604545399546623
epoch£º761	 i:6 	 global-step:15226	 l-p:0.05581780895590782
epoch£º761	 i:7 	 global-step:15227	 l-p:0.055872924625873566
epoch£º761	 i:8 	 global-step:15228	 l-p:0.05580221861600876
epoch£º761	 i:9 	 global-step:15229	 l-p:0.05582045391201973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7043, 29.2541, 28.7635],
        [27.7043, 29.4507, 28.9874],
        [27.7043, 31.4251, 31.8716],
        [27.7043, 27.7043, 27.7043]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.05576799437403679 
model_pd.l_d.mean(): -0.00011230982636334375 
model_pd.lagr.mean(): 0.05565568432211876 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6235], device='cuda:0')), ('power', tensor([-0.4861], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.05576799437403679
epoch£º762	 i:1 	 global-step:15241	 l-p:0.05578310042619705
epoch£º762	 i:2 	 global-step:15242	 l-p:0.05577537417411804
epoch£º762	 i:3 	 global-step:15243	 l-p:0.05580572038888931
epoch£º762	 i:4 	 global-step:15244	 l-p:0.055787067860364914
epoch£º762	 i:5 	 global-step:15245	 l-p:0.056070536375045776
epoch£º762	 i:6 	 global-step:15246	 l-p:0.055855732411146164
epoch£º762	 i:7 	 global-step:15247	 l-p:0.0558745376765728
epoch£º762	 i:8 	 global-step:15248	 l-p:0.05584124103188515
epoch£º762	 i:9 	 global-step:15249	 l-p:0.05608268454670906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6783, 30.4199, 30.2931],
        [27.6783, 27.7009, 27.6795],
        [27.6783, 27.8767, 27.7154],
        [27.6783, 35.9039, 40.7484]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.05599197372794151 
model_pd.l_d.mean(): -4.62626303487923e-06 
model_pd.lagr.mean(): 0.05598734691739082 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5415], device='cuda:0')), ('power', tensor([-0.3707], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.05599197372794151
epoch£º763	 i:1 	 global-step:15261	 l-p:0.05590549856424332
epoch£º763	 i:2 	 global-step:15262	 l-p:0.055858857929706573
epoch£º763	 i:3 	 global-step:15263	 l-p:0.05575614795088768
epoch£º763	 i:4 	 global-step:15264	 l-p:0.05577138811349869
epoch£º763	 i:5 	 global-step:15265	 l-p:0.05596262216567993
epoch£º763	 i:6 	 global-step:15266	 l-p:0.05577193573117256
epoch£º763	 i:7 	 global-step:15267	 l-p:0.05600951984524727
epoch£º763	 i:8 	 global-step:15268	 l-p:0.05576617270708084
epoch£º763	 i:9 	 global-step:15269	 l-p:0.05583086982369423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7229, 28.1247, 27.8398],
        [27.7229, 27.7384, 27.7235],
        [27.7229, 31.4980, 31.9803],
        [27.7229, 29.4984, 29.0398]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.05574783682823181 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574783682823181 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6280], device='cuda:0')), ('power', tensor([-0.4806], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.05574783682823181
epoch£º764	 i:1 	 global-step:15281	 l-p:0.05609169229865074
epoch£º764	 i:2 	 global-step:15282	 l-p:0.05574123561382294
epoch£º764	 i:3 	 global-step:15283	 l-p:0.055784378200769424
epoch£º764	 i:4 	 global-step:15284	 l-p:0.0557684600353241
epoch£º764	 i:5 	 global-step:15285	 l-p:0.05594412237405777
epoch£º764	 i:6 	 global-step:15286	 l-p:0.055898673832416534
epoch£º764	 i:7 	 global-step:15287	 l-p:0.055740613490343094
epoch£º764	 i:8 	 global-step:15288	 l-p:0.055920276790857315
epoch£º764	 i:9 	 global-step:15289	 l-p:0.05584586039185524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7995, 28.4004, 28.0251],
        [27.7995, 27.8250, 27.8009],
        [27.7995, 27.8001, 27.7995],
        [27.7995, 28.5064, 28.0939]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.05612741410732269 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05612741410732269 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5006], device='cuda:0')), ('power', tensor([-0.0996], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.05612741410732269
epoch£º765	 i:1 	 global-step:15301	 l-p:0.055828846991062164
epoch£º765	 i:2 	 global-step:15302	 l-p:0.05574692785739899
epoch£º765	 i:3 	 global-step:15303	 l-p:0.05576787889003754
epoch£º765	 i:4 	 global-step:15304	 l-p:0.05580243840813637
epoch£º765	 i:5 	 global-step:15305	 l-p:0.055739521980285645
epoch£º765	 i:6 	 global-step:15306	 l-p:0.0557062141597271
epoch£º765	 i:7 	 global-step:15307	 l-p:0.05579696595668793
epoch£º765	 i:8 	 global-step:15308	 l-p:0.055876631289720535
epoch£º765	 i:9 	 global-step:15309	 l-p:0.0559014268219471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8878, 37.0872, 43.0927],
        [27.8878, 30.5515, 30.3772],
        [27.8878, 28.1234, 27.9366],
        [27.8878, 35.3277, 39.2205]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.05574043095111847 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05574043095111847 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6095], device='cuda:0')), ('power', tensor([-0.2904], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.05574043095111847
epoch£º766	 i:1 	 global-step:15321	 l-p:0.05583012476563454
epoch£º766	 i:2 	 global-step:15322	 l-p:0.05572821572422981
epoch£º766	 i:3 	 global-step:15323	 l-p:0.055966176092624664
epoch£º766	 i:4 	 global-step:15324	 l-p:0.05595851689577103
epoch£º766	 i:5 	 global-step:15325	 l-p:0.055691227316856384
epoch£º766	 i:6 	 global-step:15326	 l-p:0.055807292461395264
epoch£º766	 i:7 	 global-step:15327	 l-p:0.05574868991971016
epoch£º766	 i:8 	 global-step:15328	 l-p:0.05571393668651581
epoch£º766	 i:9 	 global-step:15329	 l-p:0.05590425431728363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9801, 28.3060, 28.0627],
        [27.9801, 27.9804, 27.9801],
        [27.9801, 27.9802, 27.9801],
        [27.9801, 36.4962, 41.6338]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.055690716952085495 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055690716952085495 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6446], device='cuda:0')), ('power', tensor([-0.2612], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.055690716952085495
epoch£º767	 i:1 	 global-step:15341	 l-p:0.0557938851416111
epoch£º767	 i:2 	 global-step:15342	 l-p:0.055878255516290665
epoch£º767	 i:3 	 global-step:15343	 l-p:0.05567755550146103
epoch£º767	 i:4 	 global-step:15344	 l-p:0.055904410779476166
epoch£º767	 i:5 	 global-step:15345	 l-p:0.05567890405654907
epoch£º767	 i:6 	 global-step:15346	 l-p:0.055856890976428986
epoch£º767	 i:7 	 global-step:15347	 l-p:0.055928561836481094
epoch£º767	 i:8 	 global-step:15348	 l-p:0.05575524643063545
epoch£º767	 i:9 	 global-step:15349	 l-p:0.055714648216962814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0740, 30.8284, 30.6862],
        [28.0740, 38.3495, 45.7202],
        [28.0740, 28.3112, 28.1231],
        [28.0740, 33.9984, 36.2651]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.05589187517762184 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05589187517762184 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.8291e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5850], device='cuda:0')), ('power', tensor([0.0366], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.05589187517762184
epoch£º768	 i:1 	 global-step:15361	 l-p:0.05570382624864578
epoch£º768	 i:2 	 global-step:15362	 l-p:0.05589795485138893
epoch£º768	 i:3 	 global-step:15363	 l-p:0.05568362772464752
epoch£º768	 i:4 	 global-step:15364	 l-p:0.05566295608878136
epoch£º768	 i:5 	 global-step:15365	 l-p:0.05596350505948067
epoch£º768	 i:6 	 global-step:15366	 l-p:0.055670663714408875
epoch£º768	 i:7 	 global-step:15367	 l-p:0.05571417137980461
epoch£º768	 i:8 	 global-step:15368	 l-p:0.05573363974690437
epoch£º768	 i:9 	 global-step:15369	 l-p:0.05574711784720421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1682, 31.5667, 31.7695],
        [28.1682, 34.5908, 37.3437],
        [28.1682, 34.5103, 37.1809],
        [28.1682, 28.2307, 28.1740]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.05571868643164635 
model_pd.l_d.mean(): 3.548129257069377e-07 
model_pd.lagr.mean(): 0.0557190403342247 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.3388e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6035], device='cuda:0')), ('power', tensor([0.0298], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.05571868643164635
epoch£º769	 i:1 	 global-step:15381	 l-p:0.055659808218479156
epoch£º769	 i:2 	 global-step:15382	 l-p:0.05569157376885414
epoch£º769	 i:3 	 global-step:15383	 l-p:0.05573887750506401
epoch£º769	 i:4 	 global-step:15384	 l-p:0.0557190403342247
epoch£º769	 i:5 	 global-step:15385	 l-p:0.05568018928170204
epoch£º769	 i:6 	 global-step:15386	 l-p:0.055992528796195984
epoch£º769	 i:7 	 global-step:15387	 l-p:0.055643633008003235
epoch£º769	 i:8 	 global-step:15388	 l-p:0.055819518864154816
epoch£º769	 i:9 	 global-step:15389	 l-p:0.055797602981328964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2587, 28.2588, 28.2587],
        [28.2587, 34.4695, 36.9951],
        [28.2587, 31.9437, 32.3227],
        [28.2587, 28.9383, 28.5317]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.0560227632522583 
model_pd.l_d.mean(): 1.9996487026219256e-05 
model_pd.lagr.mean(): 0.056042760610580444 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.1569e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4940], device='cuda:0')), ('power', tensor([0.3806], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:0.0560227632522583
epoch£º770	 i:1 	 global-step:15401	 l-p:0.05564286559820175
epoch£º770	 i:2 	 global-step:15402	 l-p:0.055671460926532745
epoch£º770	 i:3 	 global-step:15403	 l-p:0.055712826550006866
epoch£º770	 i:4 	 global-step:15404	 l-p:0.055628105998039246
epoch£º770	 i:5 	 global-step:15405	 l-p:0.055626317858695984
epoch£º770	 i:6 	 global-step:15406	 l-p:0.05572260916233063
epoch£º770	 i:7 	 global-step:15407	 l-p:0.05564562603831291
epoch£º770	 i:8 	 global-step:15408	 l-p:0.05597114562988281
epoch£º770	 i:9 	 global-step:15409	 l-p:0.05563059076666832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3350, 28.3354, 28.3350],
        [28.3350, 28.5441, 28.3748],
        [28.3350, 28.3549, 28.3360],
        [28.3350, 31.5779, 31.6769]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.055624399334192276 
model_pd.l_d.mean(): 8.920877007767558e-06 
model_pd.lagr.mean(): 0.05563332140445709 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6579], device='cuda:0')), ('power', tensor([0.0658], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.055624399334192276
epoch£º771	 i:1 	 global-step:15421	 l-p:0.055672988295555115
epoch£º771	 i:2 	 global-step:15422	 l-p:0.055706243962049484
epoch£º771	 i:3 	 global-step:15423	 l-p:0.05560080334544182
epoch£º771	 i:4 	 global-step:15424	 l-p:0.0556119903922081
epoch£º771	 i:5 	 global-step:15425	 l-p:0.055656153708696365
epoch£º771	 i:6 	 global-step:15426	 l-p:0.055944837629795074
epoch£º771	 i:7 	 global-step:15427	 l-p:0.05585343390703201
epoch£º771	 i:8 	 global-step:15428	 l-p:0.05583949014544487
epoch£º771	 i:9 	 global-step:15429	 l-p:0.055621273815631866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3836, 28.5853, 28.4210],
        [28.3836, 28.7687, 28.4910],
        [28.3836, 37.5770, 43.4711],
        [28.3836, 28.5930, 28.4234]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.05560806021094322 
model_pd.l_d.mean(): 3.805294545600191e-05 
model_pd.lagr.mean(): 0.05564611405134201 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6580], device='cuda:0')), ('power', tensor([0.1518], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.05560806021094322
epoch£º772	 i:1 	 global-step:15441	 l-p:0.055694568902254105
epoch£º772	 i:2 	 global-step:15442	 l-p:0.055827848613262177
epoch£º772	 i:3 	 global-step:15443	 l-p:0.055606383830308914
epoch£º772	 i:4 	 global-step:15444	 l-p:0.05561458319425583
epoch£º772	 i:5 	 global-step:15445	 l-p:0.05587763711810112
epoch£º772	 i:6 	 global-step:15446	 l-p:0.05586571618914604
epoch£º772	 i:7 	 global-step:15447	 l-p:0.05562087520956993
epoch£º772	 i:8 	 global-step:15448	 l-p:0.05567055195569992
epoch£º772	 i:9 	 global-step:15449	 l-p:0.05567874759435654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3899, 28.3899, 28.3899],
        [28.3899, 28.5713, 28.4214],
        [28.3899, 30.6575, 30.3040],
        [28.3899, 30.5646, 30.1816]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.055584605783224106 
model_pd.l_d.mean(): 4.3603296944638714e-05 
model_pd.lagr.mean(): 0.0556282103061676 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6930], device='cuda:0')), ('power', tensor([0.1144], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.055584605783224106
epoch£º773	 i:1 	 global-step:15461	 l-p:0.05601804703474045
epoch£º773	 i:2 	 global-step:15462	 l-p:0.0558815561234951
epoch£º773	 i:3 	 global-step:15463	 l-p:0.05571287125349045
epoch£º773	 i:4 	 global-step:15464	 l-p:0.055593643337488174
epoch£º773	 i:5 	 global-step:15465	 l-p:0.055701758712530136
epoch£º773	 i:6 	 global-step:15466	 l-p:0.05560582876205444
epoch£º773	 i:7 	 global-step:15467	 l-p:0.05562301352620125
epoch£º773	 i:8 	 global-step:15468	 l-p:0.0557420440018177
epoch£º773	 i:9 	 global-step:15469	 l-p:0.05563778802752495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3428, 28.9065, 28.5434],
        [28.3428, 28.5388, 28.3786],
        [28.3428, 31.0512, 30.8735],
        [28.3428, 28.4312, 28.3528]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.055619221180677414 
model_pd.l_d.mean(): 6.965809006942436e-05 
model_pd.lagr.mean(): 0.05568888038396835 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6561], device='cuda:0')), ('power', tensor([0.1385], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.055619221180677414
epoch£º774	 i:1 	 global-step:15481	 l-p:0.055888790637254715
epoch£º774	 i:2 	 global-step:15482	 l-p:0.055689599364995956
epoch£º774	 i:3 	 global-step:15483	 l-p:0.05564316734671593
epoch£º774	 i:4 	 global-step:15484	 l-p:0.05566369742155075
epoch£º774	 i:5 	 global-step:15485	 l-p:0.05590161681175232
epoch£º774	 i:6 	 global-step:15486	 l-p:0.055800482630729675
epoch£º774	 i:7 	 global-step:15487	 l-p:0.055680446326732635
epoch£º774	 i:8 	 global-step:15488	 l-p:0.05566922202706337
epoch£º774	 i:9 	 global-step:15489	 l-p:0.05570211261510849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2435, 30.6474, 30.3483],
        [28.2435, 30.1343, 29.6828],
        [28.2435, 28.2435, 28.2435],
        [28.2435, 35.7827, 39.7278]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.05588875710964203 
model_pd.l_d.mean(): 0.0002026266447501257 
model_pd.lagr.mean(): 0.05609138309955597 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4715], device='cuda:0')), ('power', tensor([0.3437], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.05588875710964203
epoch£º775	 i:1 	 global-step:15501	 l-p:0.05575008690357208
epoch£º775	 i:2 	 global-step:15502	 l-p:0.05562375858426094
epoch£º775	 i:3 	 global-step:15503	 l-p:0.05597002059221268
epoch£º775	 i:4 	 global-step:15504	 l-p:0.05564557760953903
epoch£º775	 i:5 	 global-step:15505	 l-p:0.055812057107686996
epoch£º775	 i:6 	 global-step:15506	 l-p:0.055735163390636444
epoch£º775	 i:7 	 global-step:15507	 l-p:0.05566568300127983
epoch£º775	 i:8 	 global-step:15508	 l-p:0.055667780339717865
epoch£º775	 i:9 	 global-step:15509	 l-p:0.05575941875576973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[28.1044, 29.3391, 28.8310],
        [28.1044, 30.1240, 29.7077],
        [28.1044, 29.9855, 29.5363],
        [28.1044, 37.3871, 43.4525]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.05568086355924606 
model_pd.l_d.mean(): -3.944601121474989e-05 
model_pd.lagr.mean(): 0.05564141646027565 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6320], device='cuda:0')), ('power', tensor([-0.0639], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.05568086355924606
epoch£º776	 i:1 	 global-step:15521	 l-p:0.05580494925379753
epoch£º776	 i:2 	 global-step:15522	 l-p:0.055955056101083755
epoch£º776	 i:3 	 global-step:15523	 l-p:0.055759649723768234
epoch£º776	 i:4 	 global-step:15524	 l-p:0.05575372651219368
epoch£º776	 i:5 	 global-step:15525	 l-p:0.05570105090737343
epoch£º776	 i:6 	 global-step:15526	 l-p:0.055793579667806625
epoch£º776	 i:7 	 global-step:15527	 l-p:0.055719055235385895
epoch£º776	 i:8 	 global-step:15528	 l-p:0.05575115978717804
epoch£º776	 i:9 	 global-step:15529	 l-p:0.05593340843915939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9480, 29.6833, 29.2110],
        [27.9480, 28.3987, 28.0883],
        [27.9480, 29.9103, 29.4850],
        [27.9480, 30.8587, 30.8005]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.055721137672662735 
model_pd.l_d.mean(): -0.00014198913413565606 
model_pd.lagr.mean(): 0.05557914823293686 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6220], device='cuda:0')), ('power', tensor([-0.2485], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.055721137672662735
epoch£º777	 i:1 	 global-step:15541	 l-p:0.05570238083600998
epoch£º777	 i:2 	 global-step:15542	 l-p:0.0562032014131546
epoch£º777	 i:3 	 global-step:15543	 l-p:0.0558914840221405
epoch£º777	 i:4 	 global-step:15544	 l-p:0.05569954589009285
epoch£º777	 i:5 	 global-step:15545	 l-p:0.05573417618870735
epoch£º777	 i:6 	 global-step:15546	 l-p:0.055789537727832794
epoch£º777	 i:7 	 global-step:15547	 l-p:0.05591277778148651
epoch£º777	 i:8 	 global-step:15548	 l-p:0.05577607825398445
epoch£º777	 i:9 	 global-step:15549	 l-p:0.05576783046126366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8027, 27.8066, 27.8028],
        [27.8027, 28.2114, 27.9227],
        [27.8027, 27.8774, 27.8104],
        [27.8027, 27.8314, 27.8044]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.055976685136556625 
model_pd.l_d.mean(): -6.126520747784525e-05 
model_pd.lagr.mean(): 0.05591541901230812 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.4791], device='cuda:0')), ('power', tensor([-0.1364], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.055976685136556625
epoch£º778	 i:1 	 global-step:15561	 l-p:0.055752094835042953
epoch£º778	 i:2 	 global-step:15562	 l-p:0.056104905903339386
epoch£º778	 i:3 	 global-step:15563	 l-p:0.0557401180267334
epoch£º778	 i:4 	 global-step:15564	 l-p:0.05591733753681183
epoch£º778	 i:5 	 global-step:15565	 l-p:0.0558333657681942
epoch£º778	 i:6 	 global-step:15566	 l-p:0.05576904118061066
epoch£º778	 i:7 	 global-step:15567	 l-p:0.055779166519641876
epoch£º778	 i:8 	 global-step:15568	 l-p:0.055866193026304245
epoch£º778	 i:9 	 global-step:15569	 l-p:0.05575527623295784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6958, 28.7236, 28.2401],
        [27.6958, 30.9240, 31.0566],
        [27.6958, 27.6963, 27.6959],
        [27.6958, 28.1029, 27.8154]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.05579131841659546 
model_pd.l_d.mean(): -0.000128506391774863 
model_pd.lagr.mean(): 0.05566281080245972 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5959], device='cuda:0')), ('power', tensor([-0.4900], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.05579131841659546
epoch£º779	 i:1 	 global-step:15581	 l-p:0.05592579394578934
epoch£º779	 i:2 	 global-step:15582	 l-p:0.05596734955906868
epoch£º779	 i:3 	 global-step:15583	 l-p:0.05582254379987717
epoch£º779	 i:4 	 global-step:15584	 l-p:0.05576745793223381
epoch£º779	 i:5 	 global-step:15585	 l-p:0.05583524703979492
epoch£º779	 i:6 	 global-step:15586	 l-p:0.05576873570680618
epoch£º779	 i:7 	 global-step:15587	 l-p:0.05583241209387779
epoch£º779	 i:8 	 global-step:15588	 l-p:0.0560465045273304
epoch£º779	 i:9 	 global-step:15589	 l-p:0.05592111125588417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6554, 27.6774, 27.6565],
        [27.6554, 27.7368, 27.6642],
        [27.6554, 35.4671, 39.8266],
        [27.6554, 35.4162, 39.7165]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.056167878210544586 
model_pd.l_d.mean(): -8.970565431809518e-06 
model_pd.lagr.mean(): 0.05615890771150589 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.3822e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5070], device='cuda:0')), ('power', tensor([-0.2477], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.056167878210544586
epoch£º780	 i:1 	 global-step:15601	 l-p:0.05577188730239868
epoch£º780	 i:2 	 global-step:15602	 l-p:0.05574321746826172
epoch£º780	 i:3 	 global-step:15603	 l-p:0.05605708435177803
epoch£º780	 i:4 	 global-step:15604	 l-p:0.05578640475869179
epoch£º780	 i:5 	 global-step:15605	 l-p:0.05576098710298538
epoch£º780	 i:6 	 global-step:15606	 l-p:0.055844374001026154
epoch£º780	 i:7 	 global-step:15607	 l-p:0.05594964325428009
epoch£º780	 i:8 	 global-step:15608	 l-p:0.0558726042509079
epoch£º780	 i:9 	 global-step:15609	 l-p:0.055732592940330505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6937, 35.4994, 39.8450],
        [27.6937, 27.6937, 27.6937],
        [27.6937, 27.7191, 27.6951],
        [27.6937, 27.7048, 27.6941]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.055785320699214935 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055785320699214935 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5983], device='cuda:0')), ('power', tensor([-0.4573], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.055785320699214935
epoch£º781	 i:1 	 global-step:15621	 l-p:0.05574299767613411
epoch£º781	 i:2 	 global-step:15622	 l-p:0.05620139464735985
epoch£º781	 i:3 	 global-step:15623	 l-p:0.05577583983540535
epoch£º781	 i:4 	 global-step:15624	 l-p:0.05575184524059296
epoch£º781	 i:5 	 global-step:15625	 l-p:0.056023042649030685
epoch£º781	 i:6 	 global-step:15626	 l-p:0.055920183658599854
epoch£º781	 i:7 	 global-step:15627	 l-p:0.055752504616975784
epoch£º781	 i:8 	 global-step:15628	 l-p:0.05585300177335739
epoch£º781	 i:9 	 global-step:15629	 l-p:0.05574376881122589
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7719, 27.8524, 27.7805],
        [27.7719, 34.7985, 38.2513],
        [27.7719, 28.0952, 27.8538],
        [27.7719, 28.1178, 27.8633]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.055860552936792374 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055860552936792374 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6022], device='cuda:0')), ('power', tensor([-0.4085], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.055860552936792374
epoch£º782	 i:1 	 global-step:15641	 l-p:0.05573171377182007
epoch£º782	 i:2 	 global-step:15642	 l-p:0.05578412115573883
epoch£º782	 i:3 	 global-step:15643	 l-p:0.055797331035137177
epoch£º782	 i:4 	 global-step:15644	 l-p:0.05588163435459137
epoch£º782	 i:5 	 global-step:15645	 l-p:0.05584084242582321
epoch£º782	 i:6 	 global-step:15646	 l-p:0.05594240874052048
epoch£º782	 i:7 	 global-step:15647	 l-p:0.05576586723327637
epoch£º782	 i:8 	 global-step:15648	 l-p:0.055801570415496826
epoch£º782	 i:9 	 global-step:15649	 l-p:0.05594673007726669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8639, 34.0477, 36.6003],
        [27.8639, 29.8921, 29.4866],
        [27.8639, 33.7067, 35.9212],
        [27.8639, 28.0761, 27.9050]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.0557398796081543 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0557398796081543 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6334], device='cuda:0')), ('power', tensor([-0.3626], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.0557398796081543
epoch£º783	 i:1 	 global-step:15661	 l-p:0.055810365825891495
epoch£º783	 i:2 	 global-step:15662	 l-p:0.056117311120033264
epoch£º783	 i:3 	 global-step:15663	 l-p:0.05571245029568672
epoch£º783	 i:4 	 global-step:15664	 l-p:0.055914197117090225
epoch£º783	 i:5 	 global-step:15665	 l-p:0.055755145847797394
epoch£º783	 i:6 	 global-step:15666	 l-p:0.05572912469506264
epoch£º783	 i:7 	 global-step:15667	 l-p:0.05571198835968971
epoch£º783	 i:8 	 global-step:15668	 l-p:0.05593777447938919
epoch£º783	 i:9 	 global-step:15669	 l-p:0.05570979788899422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9617, 34.1448, 36.6832],
        [27.9617, 27.9617, 27.9617],
        [27.9617, 28.5277, 28.1654],
        [27.9617, 28.5843, 28.1999]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.055758293718099594 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055758293718099594 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5706], device='cuda:0')), ('power', tensor([-0.1405], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.055758293718099594
epoch£º784	 i:1 	 global-step:15681	 l-p:0.05626162886619568
epoch£º784	 i:2 	 global-step:15682	 l-p:0.05571679025888443
epoch£º784	 i:3 	 global-step:15683	 l-p:0.05569412186741829
epoch£º784	 i:4 	 global-step:15684	 l-p:0.05577385798096657
epoch£º784	 i:5 	 global-step:15685	 l-p:0.05566475912928581
epoch£º784	 i:6 	 global-step:15686	 l-p:0.05575941130518913
epoch£º784	 i:7 	 global-step:15687	 l-p:0.055692438036203384
epoch£º784	 i:8 	 global-step:15688	 l-p:0.0557602196931839
epoch£º784	 i:9 	 global-step:15689	 l-p:0.055834293365478516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0613, 28.0613, 28.0613],
        [28.0613, 28.0800, 28.0621],
        [28.0613, 32.1815, 32.8835],
        [28.0613, 29.2074, 28.7057]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.055842358618974686 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055842358618974686 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6127], device='cuda:0')), ('power', tensor([-0.0710], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.055842358618974686
epoch£º785	 i:1 	 global-step:15701	 l-p:0.05566466227173805
epoch£º785	 i:2 	 global-step:15702	 l-p:0.05569778382778168
epoch£º785	 i:3 	 global-step:15703	 l-p:0.055818524211645126
epoch£º785	 i:4 	 global-step:15704	 l-p:0.055721454322338104
epoch£º785	 i:5 	 global-step:15705	 l-p:0.05566847324371338
epoch£º785	 i:6 	 global-step:15706	 l-p:0.056040287017822266
epoch£º785	 i:7 	 global-step:15707	 l-p:0.05570544674992561
epoch£º785	 i:8 	 global-step:15708	 l-p:0.05574169009923935
epoch£º785	 i:9 	 global-step:15709	 l-p:0.05579076707363129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1608, 28.1608, 28.1608],
        [28.1608, 37.8476, 44.4202],
        [28.1608, 34.7428, 37.6617],
        [28.1608, 28.4890, 28.2439]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.05566462501883507 
model_pd.l_d.mean(): -9.371751730213873e-07 
model_pd.lagr.mean(): 0.055663686245679855 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.5155e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6433], device='cuda:0')), ('power', tensor([-0.0716], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.05566462501883507
epoch£º786	 i:1 	 global-step:15721	 l-p:0.055656805634498596
epoch£º786	 i:2 	 global-step:15722	 l-p:0.05570312961935997
epoch£º786	 i:3 	 global-step:15723	 l-p:0.055789731442928314
epoch£º786	 i:4 	 global-step:15724	 l-p:0.0559198260307312
epoch£º786	 i:5 	 global-step:15725	 l-p:0.05579250305891037
epoch£º786	 i:6 	 global-step:15726	 l-p:0.05570421740412712
epoch£º786	 i:7 	 global-step:15727	 l-p:0.05565476790070534
epoch£º786	 i:8 	 global-step:15728	 l-p:0.05563058704137802
epoch£º786	 i:9 	 global-step:15729	 l-p:0.05595578998327255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2572, 34.4482, 36.9542],
        [28.2572, 28.4725, 28.2989],
        [28.2572, 28.3392, 28.2660],
        [28.2572, 28.6790, 28.3822]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.05581573396921158 
model_pd.l_d.mean(): 1.3218405911175068e-05 
model_pd.lagr.mean(): 0.05582895129919052 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.4277e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5334], device='cuda:0')), ('power', tensor([0.2570], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.05581573396921158
epoch£º787	 i:1 	 global-step:15741	 l-p:0.05571957305073738
epoch£º787	 i:2 	 global-step:15742	 l-p:0.0556555911898613
epoch£º787	 i:3 	 global-step:15743	 l-p:0.05565149337053299
epoch£º787	 i:4 	 global-step:15744	 l-p:0.05576996132731438
epoch£º787	 i:5 	 global-step:15745	 l-p:0.0559019036591053
epoch£º787	 i:6 	 global-step:15746	 l-p:0.055671051144599915
epoch£º787	 i:7 	 global-step:15747	 l-p:0.055625200271606445
epoch£º787	 i:8 	 global-step:15748	 l-p:0.055627889931201935
epoch£º787	 i:9 	 global-step:15749	 l-p:0.05583346635103226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3389, 30.7105, 30.3953],
        [28.3389, 28.3390, 28.3389],
        [28.3389, 31.0192, 30.8292],
        [28.3389, 35.8549, 39.7579]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.05563005432486534 
model_pd.l_d.mean(): 1.2344264177954756e-05 
model_pd.lagr.mean(): 0.055642399936914444 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6499], device='cuda:0')), ('power', tensor([0.0915], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.05563005432486534
epoch£º788	 i:1 	 global-step:15761	 l-p:0.05565455183386803
epoch£º788	 i:2 	 global-step:15762	 l-p:0.055950477719306946
epoch£º788	 i:3 	 global-step:15763	 l-p:0.055697184056043625
epoch£º788	 i:4 	 global-step:15764	 l-p:0.05562664940953255
epoch£º788	 i:5 	 global-step:15765	 l-p:0.05583447217941284
epoch£º788	 i:6 	 global-step:15766	 l-p:0.055603742599487305
epoch£º788	 i:7 	 global-step:15767	 l-p:0.05562816560268402
epoch£º788	 i:8 	 global-step:15768	 l-p:0.05560798570513725
epoch£º788	 i:9 	 global-step:15769	 l-p:0.055886972695589066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3896, 28.6129, 28.4337],
        [28.3896, 30.5208, 30.1249],
        [28.3896, 28.3895, 28.3896],
        [28.3896, 28.3896, 28.3896]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.05565192177891731 
model_pd.l_d.mean(): 5.078956746729091e-05 
model_pd.lagr.mean(): 0.05570271238684654 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6208], device='cuda:0')), ('power', tensor([0.2009], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.05565192177891731
epoch£º789	 i:1 	 global-step:15781	 l-p:0.05578400567173958
epoch£º789	 i:2 	 global-step:15782	 l-p:0.05561010167002678
epoch£º789	 i:3 	 global-step:15783	 l-p:0.055818598717451096
epoch£º789	 i:4 	 global-step:15784	 l-p:0.05562232807278633
epoch£º789	 i:5 	 global-step:15785	 l-p:0.0558282695710659
epoch£º789	 i:6 	 global-step:15786	 l-p:0.05579439923167229
epoch£º789	 i:7 	 global-step:15787	 l-p:0.05564042925834656
epoch£º789	 i:8 	 global-step:15788	 l-p:0.05564771965146065
epoch£º789	 i:9 	 global-step:15789	 l-p:0.05565547198057175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3935, 28.7753, 28.4993],
        [28.3935, 28.3934, 28.3934],
        [28.3935, 34.7887, 37.4819],
        [28.3935, 30.5250, 30.1291]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.055591881275177 
model_pd.l_d.mean(): 5.574292299570516e-05 
model_pd.lagr.mean(): 0.05564762279391289 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6792], device='cuda:0')), ('power', tensor([0.1445], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.055591881275177
epoch£º790	 i:1 	 global-step:15801	 l-p:0.055625006556510925
epoch£º790	 i:2 	 global-step:15802	 l-p:0.05561332032084465
epoch£º790	 i:3 	 global-step:15803	 l-p:0.055645622313022614
epoch£º790	 i:4 	 global-step:15804	 l-p:0.05562792718410492
epoch£º790	 i:5 	 global-step:15805	 l-p:0.05566379800438881
epoch£º790	 i:6 	 global-step:15806	 l-p:0.05604841187596321
epoch£º790	 i:7 	 global-step:15807	 l-p:0.05579476058483124
epoch£º790	 i:8 	 global-step:15808	 l-p:0.05571873486042023
epoch£º790	 i:9 	 global-step:15809	 l-p:0.055770713835954666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.3426, 30.2404, 29.7873],
        [28.3426, 38.4204, 45.4645],
        [28.3426, 28.9558, 28.5728],
        [28.3426, 29.7696, 29.2563]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.05563920736312866 
model_pd.l_d.mean(): 4.616891601472162e-05 
model_pd.lagr.mean(): 0.0556853748857975 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6503], device='cuda:0')), ('power', tensor([0.0908], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.05563920736312866
epoch£º791	 i:1 	 global-step:15821	 l-p:0.055692851543426514
epoch£º791	 i:2 	 global-step:15822	 l-p:0.05572017654776573
epoch£º791	 i:3 	 global-step:15823	 l-p:0.055819835513830185
epoch£º791	 i:4 	 global-step:15824	 l-p:0.05582491680979729
epoch£º791	 i:5 	 global-step:15825	 l-p:0.055641334503889084
epoch£º791	 i:6 	 global-step:15826	 l-p:0.05566802993416786
epoch£º791	 i:7 	 global-step:15827	 l-p:0.05563504248857498
epoch£º791	 i:8 	 global-step:15828	 l-p:0.05598972737789154
epoch£º791	 i:9 	 global-step:15829	 l-p:0.055632028728723526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2383, 31.6457, 31.8490],
        [28.2383, 33.7084, 35.5229],
        [28.2383, 28.5905, 28.3314],
        [28.2383, 29.0911, 28.6344]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.055639080703258514 
model_pd.l_d.mean(): -8.157260822372336e-07 
model_pd.lagr.mean(): 0.05563826486468315 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6567], device='cuda:0')), ('power', tensor([-0.0014], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.055639080703258514
epoch£º792	 i:1 	 global-step:15841	 l-p:0.05577898770570755
epoch£º792	 i:2 	 global-step:15842	 l-p:0.05565871298313141
epoch£º792	 i:3 	 global-step:15843	 l-p:0.05575396120548248
epoch£º792	 i:4 	 global-step:15844	 l-p:0.055713851004838943
epoch£º792	 i:5 	 global-step:15845	 l-p:0.05565563216805458
epoch£º792	 i:6 	 global-step:15846	 l-p:0.0559646412730217
epoch£º792	 i:7 	 global-step:15847	 l-p:0.055980004370212555
epoch£º792	 i:8 	 global-step:15848	 l-p:0.05571872740983963
epoch£º792	 i:9 	 global-step:15849	 l-p:0.055676862597465515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0918, 28.0919, 28.0918],
        [28.0918, 28.1030, 28.0921],
        [28.0918, 28.0919, 28.0918],
        [28.0918, 28.3162, 28.1366]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.05568034201860428 
model_pd.l_d.mean(): -0.00011012374307028949 
model_pd.lagr.mean(): 0.055570218712091446 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6531], device='cuda:0')), ('power', tensor([-0.1784], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.05568034201860428
epoch£º793	 i:1 	 global-step:15861	 l-p:0.05615204572677612
epoch£º793	 i:2 	 global-step:15862	 l-p:0.05570262297987938
epoch£º793	 i:3 	 global-step:15863	 l-p:0.0556771494448185
epoch£º793	 i:4 	 global-step:15864	 l-p:0.0558847114443779
epoch£º793	 i:5 	 global-step:15865	 l-p:0.055685918778181076
epoch£º793	 i:6 	 global-step:15866	 l-p:0.05574044957756996
epoch£º793	 i:7 	 global-step:15867	 l-p:0.05573553591966629
epoch£º793	 i:8 	 global-step:15868	 l-p:0.055778536945581436
epoch£º793	 i:9 	 global-step:15869	 l-p:0.05584965646266937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9276, 30.8030, 30.7280],
        [27.9276, 28.0231, 27.9390],
        [27.9276, 30.5774, 30.3947],
        [27.9276, 27.9276, 27.9276]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.055769555270671844 
model_pd.l_d.mean(): -0.00011336294119246304 
model_pd.lagr.mean(): 0.05565619096159935 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5786], device='cuda:0')), ('power', tensor([-0.2012], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.055769555270671844
epoch£º794	 i:1 	 global-step:15881	 l-p:0.055855635553598404
epoch£º794	 i:2 	 global-step:15882	 l-p:0.05571439489722252
epoch£º794	 i:3 	 global-step:15883	 l-p:0.055743731558322906
epoch£º794	 i:4 	 global-step:15884	 l-p:0.05602617189288139
epoch£º794	 i:5 	 global-step:15885	 l-p:0.05572916939854622
epoch£º794	 i:6 	 global-step:15886	 l-p:0.05576606094837189
epoch£º794	 i:7 	 global-step:15887	 l-p:0.05588836595416069
epoch£º794	 i:8 	 global-step:15888	 l-p:0.055788516998291016
epoch£º794	 i:9 	 global-step:15889	 l-p:0.055972203612327576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7763, 27.7763, 27.7763],
        [27.7763, 32.1572, 33.0852],
        [27.7763, 27.9735, 27.8129],
        [27.7763, 27.9634, 27.8100]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.05579045042395592 
model_pd.l_d.mean(): -0.00017598077829461545 
model_pd.lagr.mean(): 0.055614471435546875 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5884], device='cuda:0')), ('power', tensor([-0.4098], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.05579045042395592
epoch£º795	 i:1 	 global-step:15901	 l-p:0.05581555888056755
epoch£º795	 i:2 	 global-step:15902	 l-p:0.05576266720890999
epoch£º795	 i:3 	 global-step:15903	 l-p:0.05591726303100586
epoch£º795	 i:4 	 global-step:15904	 l-p:0.05583428591489792
epoch£º795	 i:5 	 global-step:15905	 l-p:0.055926594883203506
epoch£º795	 i:6 	 global-step:15906	 l-p:0.05594897270202637
epoch£º795	 i:7 	 global-step:15907	 l-p:0.05579797178506851
epoch£º795	 i:8 	 global-step:15908	 l-p:0.056006669998168945
epoch£º795	 i:9 	 global-step:15909	 l-p:0.05575596168637276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6720, 27.9136, 27.7230],
        [27.6720, 33.8926, 36.5090],
        [27.6720, 28.4366, 28.0078],
        [27.6720, 27.6722, 27.6720]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.05588727816939354 
model_pd.l_d.mean(): -9.826925816014409e-05 
model_pd.lagr.mean(): 0.05578900873661041 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5685], device='cuda:0')), ('power', tensor([-0.4276], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.05588727816939354
epoch£º796	 i:1 	 global-step:15921	 l-p:0.05600356310606003
epoch£º796	 i:2 	 global-step:15922	 l-p:0.05599232390522957
epoch£º796	 i:3 	 global-step:15923	 l-p:0.0558212473988533
epoch£º796	 i:4 	 global-step:15924	 l-p:0.055959317833185196
epoch£º796	 i:5 	 global-step:15925	 l-p:0.055776771157979965
epoch£º796	 i:6 	 global-step:15926	 l-p:0.05585309863090515
epoch£º796	 i:7 	 global-step:15927	 l-p:0.055809058248996735
epoch£º796	 i:8 	 global-step:15928	 l-p:0.05583201348781586
epoch£º796	 i:9 	 global-step:15929	 l-p:0.05578768253326416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6425, 28.1081, 27.7916],
        [27.6425, 27.7127, 27.6494],
        [27.6425, 27.6449, 27.6425],
        [27.6425, 27.6424, 27.6424]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.05579937994480133 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05579937994480133 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5965], device='cuda:0')), ('power', tensor([-0.5328], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.05579937994480133
epoch£º797	 i:1 	 global-step:15941	 l-p:0.05576358735561371
epoch£º797	 i:2 	 global-step:15942	 l-p:0.05574440956115723
epoch£º797	 i:3 	 global-step:15943	 l-p:0.055860985070466995
epoch£º797	 i:4 	 global-step:15944	 l-p:0.055999912321567535
epoch£º797	 i:5 	 global-step:15945	 l-p:0.05586469545960426
epoch£º797	 i:6 	 global-step:15946	 l-p:0.055905140936374664
epoch£º797	 i:7 	 global-step:15947	 l-p:0.05579940974712372
epoch£º797	 i:8 	 global-step:15948	 l-p:0.05588773638010025
epoch£º797	 i:9 	 global-step:15949	 l-p:0.0560787096619606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6912, 34.7325, 38.2142],
        [27.6912, 27.7099, 27.6920],
        [27.6912, 28.4684, 28.0359],
        [27.6912, 36.5363, 42.1366]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.05593378469347954 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05593378469347954 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.5888], device='cuda:0')), ('power', tensor([-0.4200], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.05593378469347954
epoch£º798	 i:1 	 global-step:15961	 l-p:0.05593791976571083
epoch£º798	 i:2 	 global-step:15962	 l-p:0.05576278641819954
epoch£º798	 i:3 	 global-step:15963	 l-p:0.05576341971755028
epoch£º798	 i:4 	 global-step:15964	 l-p:0.05613322556018829
epoch£º798	 i:5 	 global-step:15965	 l-p:0.05574404448270798
epoch£º798	 i:6 	 global-step:15966	 l-p:0.05586066097021103
epoch£º798	 i:7 	 global-step:15967	 l-p:0.05575762689113617
epoch£º798	 i:8 	 global-step:15968	 l-p:0.05585770308971405
epoch£º798	 i:9 	 global-step:15969	 l-p:0.055795542895793915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7777, 27.9749, 27.8143],
        [27.7777, 27.7830, 27.7779],
        [27.7777, 27.7777, 27.7777],
        [27.7777, 27.7886, 27.7781]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.055718593299388885 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.055718593299388885 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-2.6572], device='cuda:0')), ('power', tensor([-0.5105], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.055718593299388885
epoch£º799	 i:1 	 global-step:15981	 l-p:0.05578932538628578
epoch£º799	 i:2 	 global-step:15982	 l-p:0.05580849573016167
epoch£º799	 i:3 	 global-step:15983	 l-p:0.05584361031651497
epoch£º799	 i:4 	 global-step:15984	 l-p:0.05575133115053177
epoch£º799	 i:5 	 global-step:15985	 l-p:0.05602053552865982
epoch£º799	 i:6 	 global-step:15986	 l-p:0.05572572723031044
epoch£º799	 i:7 	 global-step:15987	 l-p:0.05582088232040405
epoch£º799	 i:8 	 global-step:15988	 l-p:0.055733416229486465
epoch£º799	 i:9 	 global-step:15989	 l-p:0.05612002685666084
