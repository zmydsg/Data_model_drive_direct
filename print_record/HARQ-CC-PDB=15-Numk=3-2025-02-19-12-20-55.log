
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.22045108675956726 
model_pd.l_d.mean(): -22.75084114074707 
model_pd.lagr.mean(): -22.5303897857666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6202], device='cuda:0')), ('power', tensor([-23.3711], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.22045108675956726
epoch£º0	 i:1 	 global-step:1	 l-p:0.16397282481193542
epoch£º0	 i:2 	 global-step:2	 l-p:0.20958468317985535
epoch£º0	 i:3 	 global-step:3	 l-p:0.18547755479812622
epoch£º0	 i:4 	 global-step:4	 l-p:0.1604577600955963
epoch£º0	 i:5 	 global-step:5	 l-p:0.13323137164115906
epoch£º0	 i:6 	 global-step:6	 l-p:0.07205234467983246
epoch£º0	 i:7 	 global-step:7	 l-p:0.15227164328098297
epoch£º0	 i:8 	 global-step:8	 l-p:0.12444312125444412
epoch£º0	 i:9 	 global-step:9	 l-p:0.15250356495380402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5290, 3.5294, 3.5290],
        [3.5290, 3.7409, 3.6980],
        [3.5290, 3.5896, 3.5508],
        [3.5290, 4.5221, 5.2100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.0351058654487133 
model_pd.l_d.mean(): -22.22926139831543 
model_pd.lagr.mean(): -22.194154739379883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1471], device='cuda:0')), ('power', tensor([-22.3764], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.0351058654487133
epoch£º1	 i:1 	 global-step:21	 l-p:0.0948038101196289
epoch£º1	 i:2 	 global-step:22	 l-p:0.1295902580022812
epoch£º1	 i:3 	 global-step:23	 l-p:0.1265886425971985
epoch£º1	 i:4 	 global-step:24	 l-p:0.08247099816799164
epoch£º1	 i:5 	 global-step:25	 l-p:0.10980381071567535
epoch£º1	 i:6 	 global-step:26	 l-p:0.14614053070545197
epoch£º1	 i:7 	 global-step:27	 l-p:0.1311836540699005
epoch£º1	 i:8 	 global-step:28	 l-p:0.11177261173725128
epoch£º1	 i:9 	 global-step:29	 l-p:0.09871819615364075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0404, 4.8941, 5.3155],
        [4.0404, 4.1189, 4.0708],
        [4.0404, 4.6872, 4.8975],
        [4.0404, 4.1062, 4.0631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.1317572444677353 
model_pd.l_d.mean(): -22.885700225830078 
model_pd.lagr.mean(): -22.753942489624023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0763], device='cuda:0')), ('power', tensor([-22.8094], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.1317572444677353
epoch£º2	 i:1 	 global-step:41	 l-p:0.08083644509315491
epoch£º2	 i:2 	 global-step:42	 l-p:0.13022765517234802
epoch£º2	 i:3 	 global-step:43	 l-p:0.11213386058807373
epoch£º2	 i:4 	 global-step:44	 l-p:0.12493523955345154
epoch£º2	 i:5 	 global-step:45	 l-p:0.1048060953617096
epoch£º2	 i:6 	 global-step:46	 l-p:0.10844957828521729
epoch£º2	 i:7 	 global-step:47	 l-p:0.11355183273553848
epoch£º2	 i:8 	 global-step:48	 l-p:0.10386304557323456
epoch£º2	 i:9 	 global-step:49	 l-p:0.12123554199934006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6315, 4.3173, 4.6195],
        [3.6315, 3.6643, 3.6394],
        [3.6315, 3.6995, 3.6575],
        [3.6315, 4.6156, 5.2709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.2710484564304352 
model_pd.l_d.mean(): -21.906286239624023 
model_pd.lagr.mean(): -21.635238647460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1621], device='cuda:0')), ('power', tensor([-22.0684], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.2710484564304352
epoch£º3	 i:1 	 global-step:61	 l-p:0.2700667977333069
epoch£º3	 i:2 	 global-step:62	 l-p:0.1937008500099182
epoch£º3	 i:3 	 global-step:63	 l-p:0.12462922930717468
epoch£º3	 i:4 	 global-step:64	 l-p:0.11662868410348892
epoch£º3	 i:5 	 global-step:65	 l-p:0.11948096752166748
epoch£º3	 i:6 	 global-step:66	 l-p:0.14479830861091614
epoch£º3	 i:7 	 global-step:67	 l-p:0.15526233613491058
epoch£º3	 i:8 	 global-step:68	 l-p:0.110715351998806
epoch£º3	 i:9 	 global-step:69	 l-p:0.11260285973548889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5472, 4.0326, 4.1544],
        [3.5472, 4.1388, 4.3576],
        [3.5472, 4.5717, 5.3009],
        [3.5472, 3.6072, 3.5688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.1594347357749939 
model_pd.l_d.mean(): -22.146488189697266 
model_pd.lagr.mean(): -21.98705291748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1523], device='cuda:0')), ('power', tensor([-22.2988], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.1594347357749939
epoch£º4	 i:1 	 global-step:81	 l-p:0.12336206436157227
epoch£º4	 i:2 	 global-step:82	 l-p:0.4048042893409729
epoch£º4	 i:3 	 global-step:83	 l-p:0.11930130422115326
epoch£º4	 i:4 	 global-step:84	 l-p:0.12040647864341736
epoch£º4	 i:5 	 global-step:85	 l-p:0.14234118163585663
epoch£º4	 i:6 	 global-step:86	 l-p:0.1375739872455597
epoch£º4	 i:7 	 global-step:87	 l-p:0.3229498565196991
epoch£º4	 i:8 	 global-step:88	 l-p:0.13615930080413818
epoch£º4	 i:9 	 global-step:89	 l-p:0.09223654866218567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8067, 3.8069, 3.8067],
        [3.8067, 3.8116, 3.8071],
        [3.8067, 3.8805, 3.8356],
        [3.8067, 3.9124, 3.8589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.12029201537370682 
model_pd.l_d.mean(): -22.832006454467773 
model_pd.lagr.mean(): -22.711713790893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0265], device='cuda:0')), ('power', tensor([-22.8585], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.12029201537370682
epoch£º5	 i:1 	 global-step:101	 l-p:6.5456132888793945
epoch£º5	 i:2 	 global-step:102	 l-p:0.11233145743608475
epoch£º5	 i:3 	 global-step:103	 l-p:0.12942193448543549
epoch£º5	 i:4 	 global-step:104	 l-p:0.07733571529388428
epoch£º5	 i:5 	 global-step:105	 l-p:0.12809385359287262
epoch£º5	 i:6 	 global-step:106	 l-p:-1.7804425954818726
epoch£º5	 i:7 	 global-step:107	 l-p:0.11401429772377014
epoch£º5	 i:8 	 global-step:108	 l-p:0.12316782772541046
epoch£º5	 i:9 	 global-step:109	 l-p:0.11050175130367279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8387, 3.8597, 3.8424],
        [3.8387, 3.8387, 3.8387],
        [3.8387, 4.8962, 5.6053],
        [3.8387, 3.8423, 3.8389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.14074261486530304 
model_pd.l_d.mean(): -23.183931350708008 
model_pd.lagr.mean(): -23.043188095092773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0408], device='cuda:0')), ('power', tensor([-23.1431], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.14074261486530304
epoch£º6	 i:1 	 global-step:121	 l-p:0.11582385748624802
epoch£º6	 i:2 	 global-step:122	 l-p:0.10622210055589676
epoch£º6	 i:3 	 global-step:123	 l-p:0.13054071366786957
epoch£º6	 i:4 	 global-step:124	 l-p:0.10712093114852905
epoch£º6	 i:5 	 global-step:125	 l-p:0.00932423584163189
epoch£º6	 i:6 	 global-step:126	 l-p:0.12978659570217133
epoch£º6	 i:7 	 global-step:127	 l-p:0.674193799495697
epoch£º6	 i:8 	 global-step:128	 l-p:0.09854269027709961
epoch£º6	 i:9 	 global-step:129	 l-p:0.11275803297758102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6214, 4.0770, 4.1697],
        [3.6214, 3.6655, 3.6344],
        [3.6214, 3.6214, 3.6214],
        [3.6214, 3.6364, 3.6236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.6807875633239746 
model_pd.l_d.mean(): -23.140625 
model_pd.lagr.mean(): -22.459836959838867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0511], device='cuda:0')), ('power', tensor([-23.1917], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.6807875633239746
epoch£º7	 i:1 	 global-step:141	 l-p:0.12045933306217194
epoch£º7	 i:2 	 global-step:142	 l-p:0.10574361681938171
epoch£º7	 i:3 	 global-step:143	 l-p:0.1348562389612198
epoch£º7	 i:4 	 global-step:144	 l-p:0.14500756561756134
epoch£º7	 i:5 	 global-step:145	 l-p:0.10494811832904816
epoch£º7	 i:6 	 global-step:146	 l-p:0.11308520287275314
epoch£º7	 i:7 	 global-step:147	 l-p:0.11201997846364975
epoch£º7	 i:8 	 global-step:148	 l-p:0.10918056219816208
epoch£º7	 i:9 	 global-step:149	 l-p:0.11247792840003967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7490, 3.7490, 3.7490],
        [3.7490, 3.7491, 3.7490],
        [3.7490, 4.3568, 4.5700],
        [3.7490, 3.8584, 3.8052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.15561474859714508 
model_pd.l_d.mean(): -21.972936630249023 
model_pd.lagr.mean(): -21.81732177734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0890], device='cuda:0')), ('power', tensor([-22.0620], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.15561474859714508
epoch£º8	 i:1 	 global-step:161	 l-p:0.12010690569877625
epoch£º8	 i:2 	 global-step:162	 l-p:0.08678356558084488
epoch£º8	 i:3 	 global-step:163	 l-p:0.13845525681972504
epoch£º8	 i:4 	 global-step:164	 l-p:0.10763386636972427
epoch£º8	 i:5 	 global-step:165	 l-p:0.12447143346071243
epoch£º8	 i:6 	 global-step:166	 l-p:0.1498439610004425
epoch£º8	 i:7 	 global-step:167	 l-p:0.13415364921092987
epoch£º8	 i:8 	 global-step:168	 l-p:0.08105462789535522
epoch£º8	 i:9 	 global-step:169	 l-p:0.1400148719549179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7623, 3.7637, 3.7624],
        [3.7623, 3.7638, 3.7624],
        [3.7623, 3.8714, 3.8183],
        [3.7623, 3.7777, 3.7646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.09200488775968552 
model_pd.l_d.mean(): -21.241451263427734 
model_pd.lagr.mean(): -21.149446487426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1146], device='cuda:0')), ('power', tensor([-21.3561], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.09200488775968552
epoch£º9	 i:1 	 global-step:181	 l-p:0.11670836806297302
epoch£º9	 i:2 	 global-step:182	 l-p:0.11232365667819977
epoch£º9	 i:3 	 global-step:183	 l-p:0.08288081735372543
epoch£º9	 i:4 	 global-step:184	 l-p:0.1212141364812851
epoch£º9	 i:5 	 global-step:185	 l-p:0.11359872668981552
epoch£º9	 i:6 	 global-step:186	 l-p:0.1159854382276535
epoch£º9	 i:7 	 global-step:187	 l-p:0.1259697824716568
epoch£º9	 i:8 	 global-step:188	 l-p:0.06590736657381058
epoch£º9	 i:9 	 global-step:189	 l-p:0.12519222497940063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7771, 4.6898, 5.2332],
        [3.7771, 4.4007, 4.6276],
        [3.7771, 3.7809, 3.7774],
        [3.7771, 3.7771, 3.7771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.1788804531097412 
model_pd.l_d.mean(): -22.927595138549805 
model_pd.lagr.mean(): -22.748714447021484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0218], device='cuda:0')), ('power', tensor([-22.9494], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.1788804531097412
epoch£º10	 i:1 	 global-step:201	 l-p:0.09280982613563538
epoch£º10	 i:2 	 global-step:202	 l-p:0.06468681991100311
epoch£º10	 i:3 	 global-step:203	 l-p:0.12416326254606247
epoch£º10	 i:4 	 global-step:204	 l-p:-0.49287569522857666
epoch£º10	 i:5 	 global-step:205	 l-p:0.12360355257987976
epoch£º10	 i:6 	 global-step:206	 l-p:0.09681139141321182
epoch£º10	 i:7 	 global-step:207	 l-p:0.11734691262245178
epoch£º10	 i:8 	 global-step:208	 l-p:0.1265064924955368
epoch£º10	 i:9 	 global-step:209	 l-p:0.12664128839969635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8724, 4.2737, 4.3100],
        [3.8724, 4.8797, 5.5226],
        [3.8724, 4.4213, 4.5703],
        [3.8724, 3.8903, 3.8753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11920811980962753 
model_pd.l_d.mean(): -22.82210350036621 
model_pd.lagr.mean(): -22.702896118164062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0043], device='cuda:0')), ('power', tensor([-22.8178], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11920811980962753
epoch£º11	 i:1 	 global-step:221	 l-p:0.11746802926063538
epoch£º11	 i:2 	 global-step:222	 l-p:0.11574054509401321
epoch£º11	 i:3 	 global-step:223	 l-p:0.10371654480695724
epoch£º11	 i:4 	 global-step:224	 l-p:0.1426636129617691
epoch£º11	 i:5 	 global-step:225	 l-p:0.11525001376867294
epoch£º11	 i:6 	 global-step:226	 l-p:0.08512259274721146
epoch£º11	 i:7 	 global-step:227	 l-p:0.11964728683233261
epoch£º11	 i:8 	 global-step:228	 l-p:0.20507600903511047
epoch£º11	 i:9 	 global-step:229	 l-p:0.1198464184999466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7241, 4.0126, 3.9949],
        [3.7241, 3.7505, 3.7297],
        [3.7241, 3.7465, 3.7284],
        [3.7241, 3.8464, 3.7927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.2427380383014679 
model_pd.l_d.mean(): -22.336225509643555 
model_pd.lagr.mean(): -22.093486785888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1062], device='cuda:0')), ('power', tensor([-22.4424], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.2427380383014679
epoch£º12	 i:1 	 global-step:241	 l-p:0.11948978155851364
epoch£º12	 i:2 	 global-step:242	 l-p:0.12386117875576019
epoch£º12	 i:3 	 global-step:243	 l-p:0.14190706610679626
epoch£º12	 i:4 	 global-step:244	 l-p:0.16068139672279358
epoch£º12	 i:5 	 global-step:245	 l-p:0.09152314811944962
epoch£º12	 i:6 	 global-step:246	 l-p:0.11633598059415817
epoch£º12	 i:7 	 global-step:247	 l-p:0.11325565725564957
epoch£º12	 i:8 	 global-step:248	 l-p:0.1330377161502838
epoch£º12	 i:9 	 global-step:249	 l-p:0.04745473340153694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7689, 4.6085, 5.0698],
        [3.7689, 3.7689, 3.7689],
        [3.7689, 3.7713, 3.7690],
        [3.7689, 4.0600, 4.0418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.11455335468053818 
model_pd.l_d.mean(): -21.925798416137695 
model_pd.lagr.mean(): -21.81124496459961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0802], device='cuda:0')), ('power', tensor([-22.0060], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.11455335468053818
epoch£º13	 i:1 	 global-step:261	 l-p:0.14189614355564117
epoch£º13	 i:2 	 global-step:262	 l-p:0.11534204334020615
epoch£º13	 i:3 	 global-step:263	 l-p:0.1134125217795372
epoch£º13	 i:4 	 global-step:264	 l-p:0.11746612936258316
epoch£º13	 i:5 	 global-step:265	 l-p:0.14185132086277008
epoch£º13	 i:6 	 global-step:266	 l-p:0.07032310962677002
epoch£º13	 i:7 	 global-step:267	 l-p:0.20943450927734375
epoch£º13	 i:8 	 global-step:268	 l-p:0.12021090090274811
epoch£º13	 i:9 	 global-step:269	 l-p:0.10982891917228699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7609, 3.7609, 3.7609],
        [3.7609, 4.4219, 4.6915],
        [3.7609, 3.7861, 3.7660],
        [3.7609, 4.3735, 4.5955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.11441037803888321 
model_pd.l_d.mean(): -21.86316680908203 
model_pd.lagr.mean(): -21.748756408691406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0713], device='cuda:0')), ('power', tensor([-21.9345], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.11441037803888321
epoch£º14	 i:1 	 global-step:281	 l-p:0.11519262194633484
epoch£º14	 i:2 	 global-step:282	 l-p:0.1181008517742157
epoch£º14	 i:3 	 global-step:283	 l-p:0.1034037172794342
epoch£º14	 i:4 	 global-step:284	 l-p:0.09407634288072586
epoch£º14	 i:5 	 global-step:285	 l-p:0.07596331089735031
epoch£º14	 i:6 	 global-step:286	 l-p:0.27592572569847107
epoch£º14	 i:7 	 global-step:287	 l-p:0.18174511194229126
epoch£º14	 i:8 	 global-step:288	 l-p:0.154464453458786
epoch£º14	 i:9 	 global-step:289	 l-p:0.12279869616031647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8352, 3.8591, 3.8399],
        [3.8352, 3.8380, 3.8353],
        [3.8352, 3.9672, 3.9118],
        [3.8352, 3.8433, 3.8360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.1317308247089386 
model_pd.l_d.mean(): -23.132570266723633 
model_pd.lagr.mean(): -23.000839233398438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0307], device='cuda:0')), ('power', tensor([-23.1019], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.1317308247089386
epoch£º15	 i:1 	 global-step:301	 l-p:0.12631694972515106
epoch£º15	 i:2 	 global-step:302	 l-p:0.13736744225025177
epoch£º15	 i:3 	 global-step:303	 l-p:0.11725364625453949
epoch£º15	 i:4 	 global-step:304	 l-p:0.12032560259103775
epoch£º15	 i:5 	 global-step:305	 l-p:0.12893430888652802
epoch£º15	 i:6 	 global-step:306	 l-p:0.0949343740940094
epoch£º15	 i:7 	 global-step:307	 l-p:1.0174343585968018
epoch£º15	 i:8 	 global-step:308	 l-p:0.10419000685214996
epoch£º15	 i:9 	 global-step:309	 l-p:0.04232519119977951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8626, 3.8640, 3.8627],
        [3.8626, 3.8824, 3.8660],
        [3.8626, 3.8629, 3.8626],
        [3.8626, 4.1328, 4.1029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.10998698323965073 
model_pd.l_d.mean(): -22.141252517700195 
model_pd.lagr.mean(): -22.031265258789062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0100], device='cuda:0')), ('power', tensor([-22.1513], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.10998698323965073
epoch£º16	 i:1 	 global-step:321	 l-p:0.32816213369369507
epoch£º16	 i:2 	 global-step:322	 l-p:0.11119919270277023
epoch£º16	 i:3 	 global-step:323	 l-p:0.10360869020223618
epoch£º16	 i:4 	 global-step:324	 l-p:0.14509359002113342
epoch£º16	 i:5 	 global-step:325	 l-p:0.3996686041355133
epoch£º16	 i:6 	 global-step:326	 l-p:0.1100514680147171
epoch£º16	 i:7 	 global-step:327	 l-p:0.09145969152450562
epoch£º16	 i:8 	 global-step:328	 l-p:0.11583076417446136
epoch£º16	 i:9 	 global-step:329	 l-p:0.09215151518583298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9204, 4.6263, 4.9230],
        [3.9204, 4.1597, 4.1172],
        [3.9204, 3.9204, 3.9204],
        [3.9204, 4.6170, 4.9044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.11162634938955307 
model_pd.l_d.mean(): -22.87198829650879 
model_pd.lagr.mean(): -22.76036262512207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0310], device='cuda:0')), ('power', tensor([-22.8410], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.11162634938955307
epoch£º17	 i:1 	 global-step:341	 l-p:0.2632170021533966
epoch£º17	 i:2 	 global-step:342	 l-p:0.1087547242641449
epoch£º17	 i:3 	 global-step:343	 l-p:0.11330132186412811
epoch£º17	 i:4 	 global-step:344	 l-p:0.0905986949801445
epoch£º17	 i:5 	 global-step:345	 l-p:0.12896424531936646
epoch£º17	 i:6 	 global-step:346	 l-p:0.16306686401367188
epoch£º17	 i:7 	 global-step:347	 l-p:0.10663881152868271
epoch£º17	 i:8 	 global-step:348	 l-p:0.11521043628454208
epoch£º17	 i:9 	 global-step:349	 l-p:0.11583168059587479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7934, 3.8300, 3.8029],
        [3.7934, 3.7936, 3.7934],
        [3.7934, 3.8782, 3.8312],
        [3.7934, 3.8028, 3.7944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.0865333154797554 
model_pd.l_d.mean(): -22.798385620117188 
model_pd.lagr.mean(): -22.71185302734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0313], device='cuda:0')), ('power', tensor([-22.8297], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.0865333154797554
epoch£º18	 i:1 	 global-step:361	 l-p:0.11233288794755936
epoch£º18	 i:2 	 global-step:362	 l-p:0.16528145968914032
epoch£º18	 i:3 	 global-step:363	 l-p:0.12352412194013596
epoch£º18	 i:4 	 global-step:364	 l-p:0.10213082283735275
epoch£º18	 i:5 	 global-step:365	 l-p:0.11971526592969894
epoch£º18	 i:6 	 global-step:366	 l-p:0.12316489964723587
epoch£º18	 i:7 	 global-step:367	 l-p:0.10506780445575714
epoch£º18	 i:8 	 global-step:368	 l-p:0.14713431894779205
epoch£º18	 i:9 	 global-step:369	 l-p:0.08811240643262863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7012, 3.8280, 3.7756],
        [3.7012, 3.7229, 3.7054],
        [3.7012, 3.7012, 3.7012],
        [3.7012, 3.8518, 3.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.11581718921661377 
model_pd.l_d.mean(): -23.166126251220703 
model_pd.lagr.mean(): -23.050308227539062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0085], device='cuda:0')), ('power', tensor([-23.1746], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.11581718921661377
epoch£º19	 i:1 	 global-step:381	 l-p:0.14557048678398132
epoch£º19	 i:2 	 global-step:382	 l-p:0.14351299405097961
epoch£º19	 i:3 	 global-step:383	 l-p:-0.032025158405303955
epoch£º19	 i:4 	 global-step:384	 l-p:0.12570887804031372
epoch£º19	 i:5 	 global-step:385	 l-p:0.1252789944410324
epoch£º19	 i:6 	 global-step:386	 l-p:0.13288146257400513
epoch£º19	 i:7 	 global-step:387	 l-p:0.1556006222963333
epoch£º19	 i:8 	 global-step:388	 l-p:0.07486942410469055
epoch£º19	 i:9 	 global-step:389	 l-p:0.11703821271657944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8219, 3.8239, 3.8220],
        [3.8219, 4.1326, 4.1248],
        [3.8219, 3.8531, 3.8293],
        [3.8219, 3.8669, 3.8353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.150783509016037 
model_pd.l_d.mean(): -23.117788314819336 
model_pd.lagr.mean(): -22.967004776000977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0226], device='cuda:0')), ('power', tensor([-23.0952], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.150783509016037
epoch£º20	 i:1 	 global-step:401	 l-p:0.13313643634319305
epoch£º20	 i:2 	 global-step:402	 l-p:0.1031574010848999
epoch£º20	 i:3 	 global-step:403	 l-p:0.11281022429466248
epoch£º20	 i:4 	 global-step:404	 l-p:0.11339651793241501
epoch£º20	 i:5 	 global-step:405	 l-p:0.034690011292696
epoch£º20	 i:6 	 global-step:406	 l-p:0.11853238940238953
epoch£º20	 i:7 	 global-step:407	 l-p:-0.5709853172302246
epoch£º20	 i:8 	 global-step:408	 l-p:0.1128518283367157
epoch£º20	 i:9 	 global-step:409	 l-p:0.12351880967617035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8516, 4.8987, 5.6068],
        [3.8516, 3.8974, 3.8653],
        [3.8516, 3.8516, 3.8516],
        [3.8516, 3.9260, 3.8821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.11030696332454681 
model_pd.l_d.mean(): -22.99588394165039 
model_pd.lagr.mean(): -22.885576248168945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0155], device='cuda:0')), ('power', tensor([-22.9804], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.11030696332454681
epoch£º21	 i:1 	 global-step:421	 l-p:0.029301118105649948
epoch£º21	 i:2 	 global-step:422	 l-p:0.10432832688093185
epoch£º21	 i:3 	 global-step:423	 l-p:0.08593259006738663
epoch£º21	 i:4 	 global-step:424	 l-p:0.13496236503124237
epoch£º21	 i:5 	 global-step:425	 l-p:0.17801734805107117
epoch£º21	 i:6 	 global-step:426	 l-p:0.1081925556063652
epoch£º21	 i:7 	 global-step:427	 l-p:0.14280155301094055
epoch£º21	 i:8 	 global-step:428	 l-p:0.14503155648708344
epoch£º21	 i:9 	 global-step:429	 l-p:0.10710394382476807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8264, 3.9014, 3.8575],
        [3.8264, 4.0448, 4.0013],
        [3.8264, 3.8281, 3.8265],
        [3.8264, 3.8499, 3.8311]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.13284404575824738 
model_pd.l_d.mean(): -23.111663818359375 
model_pd.lagr.mean(): -22.978818893432617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0248], device='cuda:0')), ('power', tensor([-23.0869], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.13284404575824738
epoch£º22	 i:1 	 global-step:441	 l-p:0.09535230696201324
epoch£º22	 i:2 	 global-step:442	 l-p:0.11970231682062149
epoch£º22	 i:3 	 global-step:443	 l-p:0.11250708997249603
epoch£º22	 i:4 	 global-step:444	 l-p:0.11690182238817215
epoch£º22	 i:5 	 global-step:445	 l-p:0.1420506238937378
epoch£º22	 i:6 	 global-step:446	 l-p:0.12790372967720032
epoch£º22	 i:7 	 global-step:447	 l-p:0.09478235989809036
epoch£º22	 i:8 	 global-step:448	 l-p:0.13422928750514984
epoch£º22	 i:9 	 global-step:449	 l-p:0.14300915598869324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7791, 4.7956, 5.4815],
        [3.7791, 4.1736, 4.2199],
        [3.7791, 3.7795, 3.7791],
        [3.7791, 4.4408, 4.7168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.10782180726528168 
model_pd.l_d.mean(): -22.796955108642578 
model_pd.lagr.mean(): -22.689132690429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0335], device='cuda:0')), ('power', tensor([-22.8304], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.10782180726528168
epoch£º23	 i:1 	 global-step:461	 l-p:0.06418613344430923
epoch£º23	 i:2 	 global-step:462	 l-p:0.15171977877616882
epoch£º23	 i:3 	 global-step:463	 l-p:0.1308538317680359
epoch£º23	 i:4 	 global-step:464	 l-p:0.10266975313425064
epoch£º23	 i:5 	 global-step:465	 l-p:0.13040299713611603
epoch£º23	 i:6 	 global-step:466	 l-p:-0.3031691610813141
epoch£º23	 i:7 	 global-step:467	 l-p:0.1045546606183052
epoch£º23	 i:8 	 global-step:468	 l-p:0.10430852323770523
epoch£º23	 i:9 	 global-step:469	 l-p:0.10917922109365463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9055, 4.2512, 4.2594],
        [3.9055, 4.1244, 4.0792],
        [3.9055, 3.9675, 3.9280],
        [3.9055, 3.9055, 3.9055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.11318067461252213 
model_pd.l_d.mean(): -23.413278579711914 
model_pd.lagr.mean(): -23.300098419189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0989], device='cuda:0')), ('power', tensor([-23.3144], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.11318067461252213
epoch£º24	 i:1 	 global-step:481	 l-p:0.125020369887352
epoch£º24	 i:2 	 global-step:482	 l-p:0.1117277666926384
epoch£º24	 i:3 	 global-step:483	 l-p:0.13888674974441528
epoch£º24	 i:4 	 global-step:484	 l-p:0.055436424911022186
epoch£º24	 i:5 	 global-step:485	 l-p:0.5427646636962891
epoch£º24	 i:6 	 global-step:486	 l-p:0.1204608678817749
epoch£º24	 i:7 	 global-step:487	 l-p:0.11295974254608154
epoch£º24	 i:8 	 global-step:488	 l-p:0.11951186507940292
epoch£º24	 i:9 	 global-step:489	 l-p:0.12337253987789154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8888, 3.8888, 3.8888],
        [3.8888, 3.8888, 3.8888],
        [3.8888, 3.8888, 3.8888],
        [3.8888, 4.7421, 5.2103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.08396527916193008 
model_pd.l_d.mean(): -21.67487335205078 
model_pd.lagr.mean(): -21.59090805053711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0988], device='cuda:0')), ('power', tensor([-21.7737], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.08396527916193008
epoch£º25	 i:1 	 global-step:501	 l-p:0.11542730033397675
epoch£º25	 i:2 	 global-step:502	 l-p:0.12104310095310211
epoch£º25	 i:3 	 global-step:503	 l-p:0.20850583910942078
epoch£º25	 i:4 	 global-step:504	 l-p:0.08302442729473114
epoch£º25	 i:5 	 global-step:505	 l-p:0.15853789448738098
epoch£º25	 i:6 	 global-step:506	 l-p:0.12133230268955231
epoch£º25	 i:7 	 global-step:507	 l-p:0.10520610213279724
epoch£º25	 i:8 	 global-step:508	 l-p:0.13596326112747192
epoch£º25	 i:9 	 global-step:509	 l-p:0.11568349599838257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7649, 3.9050, 3.8526],
        [3.7649, 4.2053, 4.2875],
        [3.7649, 3.8085, 3.7780],
        [3.7649, 4.1448, 4.1840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.13035181164741516 
model_pd.l_d.mean(): -23.182910919189453 
model_pd.lagr.mean(): -23.05255889892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0053], device='cuda:0')), ('power', tensor([-23.1776], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.13035181164741516
epoch£º26	 i:1 	 global-step:521	 l-p:0.1127692386507988
epoch£º26	 i:2 	 global-step:522	 l-p:0.13189518451690674
epoch£º26	 i:3 	 global-step:523	 l-p:-0.32744306325912476
epoch£º26	 i:4 	 global-step:524	 l-p:0.08744321763515472
epoch£º26	 i:5 	 global-step:525	 l-p:0.11349992454051971
epoch£º26	 i:6 	 global-step:526	 l-p:0.09335698932409286
epoch£º26	 i:7 	 global-step:527	 l-p:0.13619057834148407
epoch£º26	 i:8 	 global-step:528	 l-p:0.11190711706876755
epoch£º26	 i:9 	 global-step:529	 l-p:0.15822193026542664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8244, 3.9108, 3.8641],
        [3.8244, 4.0707, 4.0379],
        [3.8244, 3.8244, 3.8244],
        [3.8244, 3.8263, 3.8245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.06126954033970833 
model_pd.l_d.mean(): -21.645605087280273 
model_pd.lagr.mean(): -21.584335327148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0769], device='cuda:0')), ('power', tensor([-21.7225], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.06126954033970833
epoch£º27	 i:1 	 global-step:541	 l-p:0.1414199322462082
epoch£º27	 i:2 	 global-step:542	 l-p:0.12238093465566635
epoch£º27	 i:3 	 global-step:543	 l-p:0.12276653945446014
epoch£º27	 i:4 	 global-step:544	 l-p:0.12213047593832016
epoch£º27	 i:5 	 global-step:545	 l-p:0.07949177920818329
epoch£º27	 i:6 	 global-step:546	 l-p:0.12185625731945038
epoch£º27	 i:7 	 global-step:547	 l-p:0.12261320650577545
epoch£º27	 i:8 	 global-step:548	 l-p:-0.017776088789105415
epoch£º27	 i:9 	 global-step:549	 l-p:0.11871644109487534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8125, 3.9606, 3.9081],
        [3.8125, 4.2561, 4.3379],
        [3.8125, 3.8484, 3.8220],
        [3.8125, 4.7859, 5.4145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.14631804823875427 
model_pd.l_d.mean(): -22.71399688720703 
model_pd.lagr.mean(): -22.567678451538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0332], device='cuda:0')), ('power', tensor([-22.7472], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.14631804823875427
epoch£º28	 i:1 	 global-step:561	 l-p:0.11901567131280899
epoch£º28	 i:2 	 global-step:562	 l-p:0.10430900007486343
epoch£º28	 i:3 	 global-step:563	 l-p:0.26307159662246704
epoch£º28	 i:4 	 global-step:564	 l-p:0.08319030702114105
epoch£º28	 i:5 	 global-step:565	 l-p:0.11559731513261795
epoch£º28	 i:6 	 global-step:566	 l-p:0.1475946456193924
epoch£º28	 i:7 	 global-step:567	 l-p:0.13099540770053864
epoch£º28	 i:8 	 global-step:568	 l-p:0.08372494578361511
epoch£º28	 i:9 	 global-step:569	 l-p:0.11788737028837204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8309, 3.8309, 3.8309],
        [3.8309, 3.9781, 3.9253],
        [3.8309, 4.2974, 4.3960],
        [3.8309, 3.8339, 3.8310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.12106289714574814 
model_pd.l_d.mean(): -22.579269409179688 
model_pd.lagr.mean(): -22.458206176757812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0230], device='cuda:0')), ('power', tensor([-22.6023], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.12106289714574814
epoch£º29	 i:1 	 global-step:581	 l-p:0.12872843444347382
epoch£º29	 i:2 	 global-step:582	 l-p:0.11865052580833435
epoch£º29	 i:3 	 global-step:583	 l-p:0.057872697710990906
epoch£º29	 i:4 	 global-step:584	 l-p:0.16666145622730255
epoch£º29	 i:5 	 global-step:585	 l-p:0.1135219931602478
epoch£º29	 i:6 	 global-step:586	 l-p:0.08371292054653168
epoch£º29	 i:7 	 global-step:587	 l-p:0.1189638301730156
epoch£º29	 i:8 	 global-step:588	 l-p:0.09421849250793457
epoch£º29	 i:9 	 global-step:589	 l-p:0.1074385941028595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9097, 3.9109, 3.9098],
        [3.9097, 4.9010, 5.5344],
        [3.9097, 4.0728, 4.0196],
        [3.9097, 4.8098, 5.3329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.22506041824817657 
model_pd.l_d.mean(): -22.054780960083008 
model_pd.lagr.mean(): -21.829721450805664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0631], device='cuda:0')), ('power', tensor([-22.1178], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.22506041824817657
epoch£º30	 i:1 	 global-step:601	 l-p:-0.13526800274848938
epoch£º30	 i:2 	 global-step:602	 l-p:0.0946684330701828
epoch£º30	 i:3 	 global-step:603	 l-p:0.1222430169582367
epoch£º30	 i:4 	 global-step:604	 l-p:0.11623933166265488
epoch£º30	 i:5 	 global-step:605	 l-p:0.12858642637729645
epoch£º30	 i:6 	 global-step:606	 l-p:0.11233630776405334
epoch£º30	 i:7 	 global-step:607	 l-p:0.1301596611738205
epoch£º30	 i:8 	 global-step:608	 l-p:0.12818148732185364
epoch£º30	 i:9 	 global-step:609	 l-p:0.1026611477136612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7536, 3.7536, 3.7536],
        [3.7536, 3.7595, 3.7541],
        [3.7536, 4.7448, 5.4102],
        [3.7536, 3.7941, 3.7654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.12072732299566269 
model_pd.l_d.mean(): -21.506969451904297 
model_pd.lagr.mean(): -21.386241912841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1577], device='cuda:0')), ('power', tensor([-21.6647], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.12072732299566269
epoch£º31	 i:1 	 global-step:621	 l-p:0.1181817576289177
epoch£º31	 i:2 	 global-step:622	 l-p:0.15119020640850067
epoch£º31	 i:3 	 global-step:623	 l-p:0.11457700282335281
epoch£º31	 i:4 	 global-step:624	 l-p:0.12782219052314758
epoch£º31	 i:5 	 global-step:625	 l-p:0.1300579309463501
epoch£º31	 i:6 	 global-step:626	 l-p:0.1142263114452362
epoch£º31	 i:7 	 global-step:627	 l-p:-0.9490215182304382
epoch£º31	 i:8 	 global-step:628	 l-p:0.12521500885486603
epoch£º31	 i:9 	 global-step:629	 l-p:0.13826781511306763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5207, 4.4885, 5.1745],
        [3.5207, 4.1494, 4.4335],
        [3.5207, 3.5589, 3.5320],
        [3.5207, 3.5217, 3.5207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.1606912612915039 
model_pd.l_d.mean(): -22.784683227539062 
model_pd.lagr.mean(): -22.623992919921875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1105], device='cuda:0')), ('power', tensor([-22.8951], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.1606912612915039
epoch£º32	 i:1 	 global-step:641	 l-p:0.1293693333864212
epoch£º32	 i:2 	 global-step:642	 l-p:0.2086695283651352
epoch£º32	 i:3 	 global-step:643	 l-p:0.3446274399757385
epoch£º32	 i:4 	 global-step:644	 l-p:0.10323859006166458
epoch£º32	 i:5 	 global-step:645	 l-p:0.11943995952606201
epoch£º32	 i:6 	 global-step:646	 l-p:0.12306152284145355
epoch£º32	 i:7 	 global-step:647	 l-p:0.12167150527238846
epoch£º32	 i:8 	 global-step:648	 l-p:-0.05474179983139038
epoch£º32	 i:9 	 global-step:649	 l-p:0.20301836729049683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7209, 3.7211, 3.7209],
        [3.7209, 3.8943, 3.8474],
        [3.7209, 3.7214, 3.7209],
        [3.7209, 3.8945, 3.8475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): -0.2601531445980072 
model_pd.l_d.mean(): -23.05179786682129 
model_pd.lagr.mean(): -23.31195068359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0309], device='cuda:0')), ('power', tensor([-23.0827], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:-0.2601531445980072
epoch£º33	 i:1 	 global-step:661	 l-p:0.11726973950862885
epoch£º33	 i:2 	 global-step:662	 l-p:0.11145032942295074
epoch£º33	 i:3 	 global-step:663	 l-p:0.10965076088905334
epoch£º33	 i:4 	 global-step:664	 l-p:0.1062401756644249
epoch£º33	 i:5 	 global-step:665	 l-p:0.12351610511541367
epoch£º33	 i:6 	 global-step:666	 l-p:0.11971017718315125
epoch£º33	 i:7 	 global-step:667	 l-p:0.11743805557489395
epoch£º33	 i:8 	 global-step:668	 l-p:-0.05081119015812874
epoch£º33	 i:9 	 global-step:669	 l-p:0.124356210231781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8476, 4.2929, 4.3763],
        [3.8476, 3.8478, 3.8476],
        [3.8476, 4.0555, 4.0122],
        [3.8476, 3.8888, 3.8596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.11409066617488861 
model_pd.l_d.mean(): -23.04611587524414 
model_pd.lagr.mean(): -22.932025909423828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0165], device='cuda:0')), ('power', tensor([-23.0296], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.11409066617488861
epoch£º34	 i:1 	 global-step:681	 l-p:0.10812783986330032
epoch£º34	 i:2 	 global-step:682	 l-p:0.14481982588768005
epoch£º34	 i:3 	 global-step:683	 l-p:0.14252199232578278
epoch£º34	 i:4 	 global-step:684	 l-p:0.10835044831037521
epoch£º34	 i:5 	 global-step:685	 l-p:0.13005155324935913
epoch£º34	 i:6 	 global-step:686	 l-p:0.15122836828231812
epoch£º34	 i:7 	 global-step:687	 l-p:0.1077546551823616
epoch£º34	 i:8 	 global-step:688	 l-p:0.12234277278184891
epoch£º34	 i:9 	 global-step:689	 l-p:0.15532925724983215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7232, 4.0434, 4.0530],
        [3.7232, 3.7232, 3.7232],
        [3.7232, 3.8159, 3.7695],
        [3.7232, 4.6790, 5.3086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): -0.24687594175338745 
model_pd.l_d.mean(): -22.73963737487793 
model_pd.lagr.mean(): -22.986513137817383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0696], device='cuda:0')), ('power', tensor([-22.8093], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:-0.24687594175338745
epoch£º35	 i:1 	 global-step:701	 l-p:0.10261870175600052
epoch£º35	 i:2 	 global-step:702	 l-p:0.1837376356124878
epoch£º35	 i:3 	 global-step:703	 l-p:0.12559746205806732
epoch£º35	 i:4 	 global-step:704	 l-p:0.11742273718118668
epoch£º35	 i:5 	 global-step:705	 l-p:0.11477993428707123
epoch£º35	 i:6 	 global-step:706	 l-p:0.11720804125070572
epoch£º35	 i:7 	 global-step:707	 l-p:0.13229088485240936
epoch£º35	 i:8 	 global-step:708	 l-p:0.07798758149147034
epoch£º35	 i:9 	 global-step:709	 l-p:0.11868617683649063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7253, 3.7253, 3.7253],
        [3.7253, 3.7253, 3.7253],
        [3.7253, 4.1784, 4.2795],
        [3.7253, 3.7578, 3.7337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.15492233633995056 
model_pd.l_d.mean(): -22.156057357788086 
model_pd.lagr.mean(): -22.001134872436523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0909], device='cuda:0')), ('power', tensor([-22.2469], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.15492233633995056
epoch£º36	 i:1 	 global-step:721	 l-p:0.1348421573638916
epoch£º36	 i:2 	 global-step:722	 l-p:0.11047101020812988
epoch£º36	 i:3 	 global-step:723	 l-p:0.17624421417713165
epoch£º36	 i:4 	 global-step:724	 l-p:0.3602812886238098
epoch£º36	 i:5 	 global-step:725	 l-p:0.07968929409980774
epoch£º36	 i:6 	 global-step:726	 l-p:0.07121531665325165
epoch£º36	 i:7 	 global-step:727	 l-p:0.10512128472328186
epoch£º36	 i:8 	 global-step:728	 l-p:0.12607669830322266
epoch£º36	 i:9 	 global-step:729	 l-p:0.12072409689426422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9075, 3.9121, 3.9078],
        [3.9075, 4.8953, 5.5294],
        [3.9075, 4.2071, 4.1965],
        [3.9075, 4.2835, 4.3156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): -0.0015724467812106013 
model_pd.l_d.mean(): -21.811899185180664 
model_pd.lagr.mean(): -21.8134708404541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0701], device='cuda:0')), ('power', tensor([-21.8820], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:-0.0015724467812106013
epoch£º37	 i:1 	 global-step:741	 l-p:0.11195948719978333
epoch£º37	 i:2 	 global-step:742	 l-p:0.11492282897233963
epoch£º37	 i:3 	 global-step:743	 l-p:0.12536261975765228
epoch£º37	 i:4 	 global-step:744	 l-p:0.1428239941596985
epoch£º37	 i:5 	 global-step:745	 l-p:0.11150550842285156
epoch£º37	 i:6 	 global-step:746	 l-p:0.04163084924221039
epoch£º37	 i:7 	 global-step:747	 l-p:0.13050305843353271
epoch£º37	 i:8 	 global-step:748	 l-p:0.11133807897567749
epoch£º37	 i:9 	 global-step:749	 l-p:0.1428813487291336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7668, 3.8453, 3.8020],
        [3.7668, 3.8134, 3.7819],
        [3.7668, 3.7848, 3.7700],
        [3.7668, 3.8113, 3.7808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.1421009600162506 
model_pd.l_d.mean(): -22.71839714050293 
model_pd.lagr.mean(): -22.576295852661133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0538], device='cuda:0')), ('power', tensor([-22.7722], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.1421009600162506
epoch£º38	 i:1 	 global-step:761	 l-p:0.1055116131901741
epoch£º38	 i:2 	 global-step:762	 l-p:0.09144416451454163
epoch£º38	 i:3 	 global-step:763	 l-p:0.18325339257717133
epoch£º38	 i:4 	 global-step:764	 l-p:0.11348982900381088
epoch£º38	 i:5 	 global-step:765	 l-p:0.14165854454040527
epoch£º38	 i:6 	 global-step:766	 l-p:0.11542602628469467
epoch£º38	 i:7 	 global-step:767	 l-p:0.20300178229808807
epoch£º38	 i:8 	 global-step:768	 l-p:0.10499802976846695
epoch£º38	 i:9 	 global-step:769	 l-p:0.12951229512691498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8366, 3.8366, 3.8366],
        [3.8366, 4.6339, 5.0573],
        [3.8366, 3.8366, 3.8366],
        [3.8366, 3.8381, 3.8367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.12102517485618591 
model_pd.l_d.mean(): -23.303003311157227 
model_pd.lagr.mean(): -23.181978225708008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0512], device='cuda:0')), ('power', tensor([-23.2518], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.12102517485618591
epoch£º39	 i:1 	 global-step:781	 l-p:0.12058225274085999
epoch£º39	 i:2 	 global-step:782	 l-p:0.11495733261108398
epoch£º39	 i:3 	 global-step:783	 l-p:0.15336839854717255
epoch£º39	 i:4 	 global-step:784	 l-p:0.11790284514427185
epoch£º39	 i:5 	 global-step:785	 l-p:0.12010353803634644
epoch£º39	 i:6 	 global-step:786	 l-p:0.08843166381120682
epoch£º39	 i:7 	 global-step:787	 l-p:0.19958852231502533
epoch£º39	 i:8 	 global-step:788	 l-p:0.16205495595932007
epoch£º39	 i:9 	 global-step:789	 l-p:0.005766320042312145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7402, 3.7404, 3.7402],
        [3.7402, 3.7531, 3.7421],
        [3.7402, 3.8186, 3.7757],
        [3.7402, 3.7454, 3.7407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.19148452579975128 
model_pd.l_d.mean(): -23.21372413635254 
model_pd.lagr.mean(): -23.022239685058594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0037], device='cuda:0')), ('power', tensor([-23.2174], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.19148452579975128
epoch£º40	 i:1 	 global-step:801	 l-p:0.13156658411026
epoch£º40	 i:2 	 global-step:802	 l-p:0.04753144085407257
epoch£º40	 i:3 	 global-step:803	 l-p:0.10202884674072266
epoch£º40	 i:4 	 global-step:804	 l-p:-0.15411755442619324
epoch£º40	 i:5 	 global-step:805	 l-p:0.12506003677845
epoch£º40	 i:6 	 global-step:806	 l-p:0.09734542667865753
epoch£º40	 i:7 	 global-step:807	 l-p:0.11132422089576721
epoch£º40	 i:8 	 global-step:808	 l-p:0.07735875993967056
epoch£º40	 i:9 	 global-step:809	 l-p:0.10063304752111435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0686, 4.0707, 4.0687],
        [4.0686, 4.0689, 4.0686],
        [4.0686, 4.2129, 4.1576],
        [4.0686, 4.1586, 4.1101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.10376778990030289 
model_pd.l_d.mean(): -23.05397605895996 
model_pd.lagr.mean(): -22.95020866394043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1045], device='cuda:0')), ('power', tensor([-22.9495], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.10376778990030289
epoch£º41	 i:1 	 global-step:821	 l-p:0.09601066261529922
epoch£º41	 i:2 	 global-step:822	 l-p:0.10072754323482513
epoch£º41	 i:3 	 global-step:823	 l-p:0.11080394685268402
epoch£º41	 i:4 	 global-step:824	 l-p:-1.6980788707733154
epoch£º41	 i:5 	 global-step:825	 l-p:0.12507057189941406
epoch£º41	 i:6 	 global-step:826	 l-p:0.10846728086471558
epoch£º41	 i:7 	 global-step:827	 l-p:15.48177433013916
epoch£º41	 i:8 	 global-step:828	 l-p:0.11642638593912125
epoch£º41	 i:9 	 global-step:829	 l-p:0.11794537305831909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8171, 4.0243, 3.9836],
        [3.8171, 3.8171, 3.8171],
        [3.8171, 4.8333, 5.5240],
        [3.8171, 3.8287, 3.8187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.08384133130311966 
model_pd.l_d.mean(): -22.126876831054688 
model_pd.lagr.mean(): -22.04303550720215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0560], device='cuda:0')), ('power', tensor([-22.1828], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.08384133130311966
epoch£º42	 i:1 	 global-step:841	 l-p:0.18224097788333893
epoch£º42	 i:2 	 global-step:842	 l-p:0.11273939162492752
epoch£º42	 i:3 	 global-step:843	 l-p:0.10488639771938324
epoch£º42	 i:4 	 global-step:844	 l-p:0.10099834203720093
epoch£º42	 i:5 	 global-step:845	 l-p:0.18178187310695648
epoch£º42	 i:6 	 global-step:846	 l-p:0.11873839795589447
epoch£º42	 i:7 	 global-step:847	 l-p:0.18738201260566711
epoch£º42	 i:8 	 global-step:848	 l-p:0.16544872522354126
epoch£º42	 i:9 	 global-step:849	 l-p:0.09371025115251541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[3.7835, 4.6866, 5.2442],
        [3.7835, 4.1896, 4.2534],
        [3.7835, 4.0071, 3.9727],
        [3.7835, 4.6417, 5.1458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.1313989758491516 
model_pd.l_d.mean(): -22.7299747467041 
model_pd.lagr.mean(): -22.598575592041016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0448], device='cuda:0')), ('power', tensor([-22.7748], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.1313989758491516
epoch£º43	 i:1 	 global-step:861	 l-p:0.17105552554130554
epoch£º43	 i:2 	 global-step:862	 l-p:0.11308402568101883
epoch£º43	 i:3 	 global-step:863	 l-p:0.12290641665458679
epoch£º43	 i:4 	 global-step:864	 l-p:0.11126058548688889
epoch£º43	 i:5 	 global-step:865	 l-p:0.10947088152170181
epoch£º43	 i:6 	 global-step:866	 l-p:0.1002216786146164
epoch£º43	 i:7 	 global-step:867	 l-p:0.12813687324523926
epoch£º43	 i:8 	 global-step:868	 l-p:-0.023156050592660904
epoch£º43	 i:9 	 global-step:869	 l-p:0.07132407277822495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8882, 3.9825, 3.9349],
        [3.8882, 3.8934, 3.8886],
        [3.8882, 3.8899, 3.8882],
        [3.8882, 3.8882, 3.8882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.11000140011310577 
model_pd.l_d.mean(): -23.073284149169922 
model_pd.lagr.mean(): -22.96328353881836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0450], device='cuda:0')), ('power', tensor([-23.0283], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.11000140011310577
epoch£º44	 i:1 	 global-step:881	 l-p:-0.3346986770629883
epoch£º44	 i:2 	 global-step:882	 l-p:0.11299224942922592
epoch£º44	 i:3 	 global-step:883	 l-p:0.10209430754184723
epoch£º44	 i:4 	 global-step:884	 l-p:0.15907244384288788
epoch£º44	 i:5 	 global-step:885	 l-p:0.09092290699481964
epoch£º44	 i:6 	 global-step:886	 l-p:0.12280043959617615
epoch£º44	 i:7 	 global-step:887	 l-p:0.11172185838222504
epoch£º44	 i:8 	 global-step:888	 l-p:0.11702681332826614
epoch£º44	 i:9 	 global-step:889	 l-p:0.0986960306763649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8422, 3.8422, 3.8422],
        [3.8422, 3.8429, 3.8422],
        [3.8422, 4.0147, 3.9667],
        [3.8422, 3.8422, 3.8422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.04469916224479675 
model_pd.l_d.mean(): -23.363056182861328 
model_pd.lagr.mean(): -23.318357467651367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0587], device='cuda:0')), ('power', tensor([-23.3044], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.04469916224479675
epoch£º45	 i:1 	 global-step:901	 l-p:0.13416488468647003
epoch£º45	 i:2 	 global-step:902	 l-p:0.09294959157705307
epoch£º45	 i:3 	 global-step:903	 l-p:0.1220894530415535
epoch£º45	 i:4 	 global-step:904	 l-p:0.0995771586894989
epoch£º45	 i:5 	 global-step:905	 l-p:0.12766845524311066
epoch£º45	 i:6 	 global-step:906	 l-p:0.10010601580142975
epoch£º45	 i:7 	 global-step:907	 l-p:0.1017606258392334
epoch£º45	 i:8 	 global-step:908	 l-p:0.10965076088905334
epoch£º45	 i:9 	 global-step:909	 l-p:0.14283476769924164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7962, 3.7992, 3.7964],
        [3.7962, 3.8153, 3.7998],
        [3.7962, 3.7962, 3.7962],
        [3.7962, 3.8149, 3.7997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.16943299770355225 
model_pd.l_d.mean(): -22.58794403076172 
model_pd.lagr.mean(): -22.41851043701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0454], device='cuda:0')), ('power', tensor([-22.6333], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.16943299770355225
epoch£º46	 i:1 	 global-step:921	 l-p:0.14203712344169617
epoch£º46	 i:2 	 global-step:922	 l-p:0.12390657514333725
epoch£º46	 i:3 	 global-step:923	 l-p:0.08980519324541092
epoch£º46	 i:4 	 global-step:924	 l-p:0.09120174497365952
epoch£º46	 i:5 	 global-step:925	 l-p:0.10180917382240295
epoch£º46	 i:6 	 global-step:926	 l-p:0.11855348199605942
epoch£º46	 i:7 	 global-step:927	 l-p:0.13569265604019165
epoch£º46	 i:8 	 global-step:928	 l-p:0.1435209959745407
epoch£º46	 i:9 	 global-step:929	 l-p:0.11398977041244507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7894, 3.8302, 3.8017],
        [3.7894, 3.8674, 3.8246],
        [3.7894, 3.7907, 3.7894],
        [3.7894, 4.0407, 4.0172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.2090681791305542 
model_pd.l_d.mean(): -21.779796600341797 
model_pd.lagr.mean(): -21.570728302001953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0723], device='cuda:0')), ('power', tensor([-21.8521], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.2090681791305542
epoch£º47	 i:1 	 global-step:941	 l-p:0.08582448959350586
epoch£º47	 i:2 	 global-step:942	 l-p:0.11831153929233551
epoch£º47	 i:3 	 global-step:943	 l-p:0.1464482545852661
epoch£º47	 i:4 	 global-step:944	 l-p:0.1046125739812851
epoch£º47	 i:5 	 global-step:945	 l-p:0.14741599559783936
epoch£º47	 i:6 	 global-step:946	 l-p:0.12251207232475281
epoch£º47	 i:7 	 global-step:947	 l-p:0.08189518004655838
epoch£º47	 i:8 	 global-step:948	 l-p:0.11358888447284698
epoch£º47	 i:9 	 global-step:949	 l-p:0.14250589907169342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7932, 4.0455, 4.0225],
        [3.7932, 3.9276, 3.8782],
        [3.7932, 3.7932, 3.7932],
        [3.7932, 3.9646, 3.9183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.11857284605503082 
model_pd.l_d.mean(): -23.372051239013672 
model_pd.lagr.mean(): -23.25347900390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0387], device='cuda:0')), ('power', tensor([-23.3333], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.11857284605503082
epoch£º48	 i:1 	 global-step:961	 l-p:0.11271405965089798
epoch£º48	 i:2 	 global-step:962	 l-p:0.10798356682062149
epoch£º48	 i:3 	 global-step:963	 l-p:-0.08623460680246353
epoch£º48	 i:4 	 global-step:964	 l-p:0.08909501880407333
epoch£º48	 i:5 	 global-step:965	 l-p:0.15329334139823914
epoch£º48	 i:6 	 global-step:966	 l-p:0.1272207796573639
epoch£º48	 i:7 	 global-step:967	 l-p:0.11596819013357162
epoch£º48	 i:8 	 global-step:968	 l-p:0.10683919489383698
epoch£º48	 i:9 	 global-step:969	 l-p:0.1321413516998291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7387, 3.7626, 3.7440],
        [3.7387, 3.7396, 3.7387],
        [3.7387, 3.8496, 3.8020],
        [3.7387, 3.7429, 3.7390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12840573489665985 
model_pd.l_d.mean(): -23.414897918701172 
model_pd.lagr.mean(): -23.28649139404297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0257], device='cuda:0')), ('power', tensor([-23.3892], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.12840573489665985
epoch£º49	 i:1 	 global-step:981	 l-p:0.115110844373703
epoch£º49	 i:2 	 global-step:982	 l-p:0.10643667727708817
epoch£º49	 i:3 	 global-step:983	 l-p:0.1236269623041153
epoch£º49	 i:4 	 global-step:984	 l-p:0.11067838221788406
epoch£º49	 i:5 	 global-step:985	 l-p:-0.11603090912103653
epoch£º49	 i:6 	 global-step:986	 l-p:0.15530088543891907
epoch£º49	 i:7 	 global-step:987	 l-p:0.12757501006126404
epoch£º49	 i:8 	 global-step:988	 l-p:0.017896952107548714
epoch£º49	 i:9 	 global-step:989	 l-p:0.09462215006351471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4581, 4.0017, 4.2186],
        [3.4581, 3.4582, 3.4581],
        [3.4581, 3.6645, 3.6386],
        [3.4581, 3.4593, 3.4582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.10187207907438278 
model_pd.l_d.mean(): -22.903209686279297 
model_pd.lagr.mean(): -22.80133819580078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1509], device='cuda:0')), ('power', tensor([-23.0541], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.10187207907438278
epoch£º50	 i:1 	 global-step:1001	 l-p:-0.06383880227804184
epoch£º50	 i:2 	 global-step:1002	 l-p:0.09116872400045395
epoch£º50	 i:3 	 global-step:1003	 l-p:0.08424262702465057
epoch£º50	 i:4 	 global-step:1004	 l-p:0.1033417135477066
epoch£º50	 i:5 	 global-step:1005	 l-p:0.11412212252616882
epoch£º50	 i:6 	 global-step:1006	 l-p:0.21485546231269836
epoch£º50	 i:7 	 global-step:1007	 l-p:0.1255318820476532
epoch£º50	 i:8 	 global-step:1008	 l-p:0.22202147543430328
epoch£º50	 i:9 	 global-step:1009	 l-p:0.10828079283237457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0597, 4.0597, 4.0597],
        [4.0597, 4.5324, 4.6255],
        [4.0597, 5.0986, 5.7735],
        [4.0597, 4.0598, 4.0597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.17797701060771942 
model_pd.l_d.mean(): -21.904199600219727 
model_pd.lagr.mean(): -21.72622299194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0035], device='cuda:0')), ('power', tensor([-21.9007], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.17797701060771942
epoch£º51	 i:1 	 global-step:1021	 l-p:0.0961446613073349
epoch£º51	 i:2 	 global-step:1022	 l-p:-0.031068293377757072
epoch£º51	 i:3 	 global-step:1023	 l-p:0.11083868145942688
epoch£º51	 i:4 	 global-step:1024	 l-p:0.09953866899013519
epoch£º51	 i:5 	 global-step:1025	 l-p:0.10178022086620331
epoch£º51	 i:6 	 global-step:1026	 l-p:0.16796576976776123
epoch£º51	 i:7 	 global-step:1027	 l-p:0.13071109354496002
epoch£º51	 i:8 	 global-step:1028	 l-p:0.10761227458715439
epoch£º51	 i:9 	 global-step:1029	 l-p:0.1064462810754776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3190, 4.8481, 4.9616],
        [4.3190, 4.7000, 4.7131],
        [4.3190, 4.3636, 4.3318],
        [4.3190, 4.3190, 4.3190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.0999874398112297 
model_pd.l_d.mean(): -23.005634307861328 
model_pd.lagr.mean(): -22.90564727783203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1976], device='cuda:0')), ('power', tensor([-22.8080], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.0999874398112297
epoch£º52	 i:1 	 global-step:1041	 l-p:0.11120330542325974
epoch£º52	 i:2 	 global-step:1042	 l-p:-0.24360297620296478
epoch£º52	 i:3 	 global-step:1043	 l-p:0.11900471150875092
epoch£º52	 i:4 	 global-step:1044	 l-p:0.1001088097691536
epoch£º52	 i:5 	 global-step:1045	 l-p:0.15980015695095062
epoch£º52	 i:6 	 global-step:1046	 l-p:0.09236948937177658
epoch£º52	 i:7 	 global-step:1047	 l-p:0.17571887373924255
epoch£º52	 i:8 	 global-step:1048	 l-p:0.11822868138551712
epoch£º52	 i:9 	 global-step:1049	 l-p:0.13573351502418518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9180, 3.9245, 3.9186],
        [3.9180, 3.9180, 3.9180],
        [3.9180, 3.9185, 3.9180],
        [3.9180, 3.9183, 3.9180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.12864674627780914 
model_pd.l_d.mean(): -22.196334838867188 
model_pd.lagr.mean(): -22.06768798828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0068], device='cuda:0')), ('power', tensor([-22.2031], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.12864674627780914
epoch£º53	 i:1 	 global-step:1061	 l-p:0.1020144447684288
epoch£º53	 i:2 	 global-step:1062	 l-p:0.04397890344262123
epoch£º53	 i:3 	 global-step:1063	 l-p:0.09638389199972153
epoch£º53	 i:4 	 global-step:1064	 l-p:0.12693217396736145
epoch£º53	 i:5 	 global-step:1065	 l-p:0.41664770245552063
epoch£º53	 i:6 	 global-step:1066	 l-p:0.1442514806985855
epoch£º53	 i:7 	 global-step:1067	 l-p:0.11256890743970871
epoch£º53	 i:8 	 global-step:1068	 l-p:0.11495128273963928
epoch£º53	 i:9 	 global-step:1069	 l-p:0.11105344444513321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8570, 3.8570, 3.8570],
        [3.8570, 4.3732, 4.5211],
        [3.8570, 4.4108, 4.5916],
        [3.8570, 3.9338, 3.8912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.03789348155260086 
model_pd.l_d.mean(): -23.212995529174805 
model_pd.lagr.mean(): -23.17510223388672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0439], device='cuda:0')), ('power', tensor([-23.1691], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.03789348155260086
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12669990956783295
epoch£º54	 i:2 	 global-step:1082	 l-p:0.10356953740119934
epoch£º54	 i:3 	 global-step:1083	 l-p:0.1357579231262207
epoch£º54	 i:4 	 global-step:1084	 l-p:0.12945985794067383
epoch£º54	 i:5 	 global-step:1085	 l-p:0.10692355781793594
epoch£º54	 i:6 	 global-step:1086	 l-p:0.14397931098937988
epoch£º54	 i:7 	 global-step:1087	 l-p:0.1173357143998146
epoch£º54	 i:8 	 global-step:1088	 l-p:0.3278201222419739
epoch£º54	 i:9 	 global-step:1089	 l-p:0.11791088432073593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[3.7846, 4.3228, 4.4983],
        [3.7846, 4.5439, 4.9415],
        [3.7846, 4.1015, 4.1119],
        [3.7846, 4.3295, 4.5109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.08423519134521484 
model_pd.l_d.mean(): -22.81218719482422 
model_pd.lagr.mean(): -22.727951049804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0353], device='cuda:0')), ('power', tensor([-22.8475], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.08423519134521484
epoch£º55	 i:1 	 global-step:1101	 l-p:0.11853199452161789
epoch£º55	 i:2 	 global-step:1102	 l-p:0.11204905062913895
epoch£º55	 i:3 	 global-step:1103	 l-p:0.11541591584682465
epoch£º55	 i:4 	 global-step:1104	 l-p:0.18059806525707245
epoch£º55	 i:5 	 global-step:1105	 l-p:0.15377651154994965
epoch£º55	 i:6 	 global-step:1106	 l-p:0.12152928858995438
epoch£º55	 i:7 	 global-step:1107	 l-p:0.19899970293045044
epoch£º55	 i:8 	 global-step:1108	 l-p:0.13703973591327667
epoch£º55	 i:9 	 global-step:1109	 l-p:0.11297505348920822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8462, 4.8100, 5.4363],
        [3.8462, 3.8462, 3.8462],
        [3.8462, 4.0210, 3.9749],
        [3.8462, 3.9022, 3.8668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.06316489726305008 
model_pd.l_d.mean(): -22.782812118530273 
model_pd.lagr.mean(): -22.719646453857422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0223], device='cuda:0')), ('power', tensor([-22.8052], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.06316489726305008
epoch£º56	 i:1 	 global-step:1121	 l-p:0.02433970384299755
epoch£º56	 i:2 	 global-step:1122	 l-p:0.11694087088108063
epoch£º56	 i:3 	 global-step:1123	 l-p:0.1220489889383316
epoch£º56	 i:4 	 global-step:1124	 l-p:0.14505593478679657
epoch£º56	 i:5 	 global-step:1125	 l-p:0.11663540452718735
epoch£º56	 i:6 	 global-step:1126	 l-p:0.13516762852668762
epoch£º56	 i:7 	 global-step:1127	 l-p:0.10056965798139572
epoch£º56	 i:8 	 global-step:1128	 l-p:0.13194124400615692
epoch£º56	 i:9 	 global-step:1129	 l-p:0.1154940128326416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8224, 4.5559, 4.9207],
        [3.8224, 4.6421, 5.1022],
        [3.8224, 3.8224, 3.8224],
        [3.8224, 3.8716, 3.8393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.13993018865585327 
model_pd.l_d.mean(): -22.999351501464844 
model_pd.lagr.mean(): -22.859420776367188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0028], device='cuda:0')), ('power', tensor([-23.0022], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.13993018865585327
epoch£º57	 i:1 	 global-step:1141	 l-p:0.07549376040697098
epoch£º57	 i:2 	 global-step:1142	 l-p:0.12238160520792007
epoch£º57	 i:3 	 global-step:1143	 l-p:0.14422503113746643
epoch£º57	 i:4 	 global-step:1144	 l-p:0.09374920278787613
epoch£º57	 i:5 	 global-step:1145	 l-p:0.06214031204581261
epoch£º57	 i:6 	 global-step:1146	 l-p:0.5636112689971924
epoch£º57	 i:7 	 global-step:1147	 l-p:0.12961164116859436
epoch£º57	 i:8 	 global-step:1148	 l-p:0.18203867971897125
epoch£º57	 i:9 	 global-step:1149	 l-p:0.09932132810354233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7513, 3.9715, 3.9404],
        [3.7513, 3.9052, 3.8590],
        [3.7513, 4.6421, 5.1988],
        [3.7513, 3.7513, 3.7513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.10059395432472229 
model_pd.l_d.mean(): -23.249481201171875 
model_pd.lagr.mean(): -23.148887634277344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0045], device='cuda:0')), ('power', tensor([-23.2450], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.10059395432472229
epoch£º58	 i:1 	 global-step:1161	 l-p:0.14521671831607819
epoch£º58	 i:2 	 global-step:1162	 l-p:0.09206295013427734
epoch£º58	 i:3 	 global-step:1163	 l-p:0.14176081120967865
epoch£º58	 i:4 	 global-step:1164	 l-p:0.16088657081127167
epoch£º58	 i:5 	 global-step:1165	 l-p:0.10178985446691513
epoch£º58	 i:6 	 global-step:1166	 l-p:0.09293941408395767
epoch£º58	 i:7 	 global-step:1167	 l-p:0.10854045301675797
epoch£º58	 i:8 	 global-step:1168	 l-p:0.11263392865657806
epoch£º58	 i:9 	 global-step:1169	 l-p:0.134917214512825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9367, 4.2014, 4.1793],
        [3.9367, 3.9384, 3.9368],
        [3.9367, 3.9368, 3.9367],
        [3.9367, 4.0639, 4.0131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.12281281501054764 
model_pd.l_d.mean(): -22.057344436645508 
model_pd.lagr.mean(): -21.934532165527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0099], device='cuda:0')), ('power', tensor([-22.0673], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.12281281501054764
epoch£º59	 i:1 	 global-step:1181	 l-p:0.07728727161884308
epoch£º59	 i:2 	 global-step:1182	 l-p:0.11491121351718903
epoch£º59	 i:3 	 global-step:1183	 l-p:-0.31884756684303284
epoch£º59	 i:4 	 global-step:1184	 l-p:0.10969990491867065
epoch£º59	 i:5 	 global-step:1185	 l-p:0.10559415817260742
epoch£º59	 i:6 	 global-step:1186	 l-p:0.11268113553524017
epoch£º59	 i:7 	 global-step:1187	 l-p:0.18567420542240143
epoch£º59	 i:8 	 global-step:1188	 l-p:0.1913449764251709
epoch£º59	 i:9 	 global-step:1189	 l-p:0.11049087345600128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8051, 3.8813, 3.8397],
        [3.8051, 3.8052, 3.8051],
        [3.8051, 3.8065, 3.8052],
        [3.8051, 4.0499, 4.0259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.08115388453006744 
model_pd.l_d.mean(): -21.5677547454834 
model_pd.lagr.mean(): -21.486600875854492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1420], device='cuda:0')), ('power', tensor([-21.7098], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.08115388453006744
epoch£º60	 i:1 	 global-step:1201	 l-p:0.11714828014373779
epoch£º60	 i:2 	 global-step:1202	 l-p:0.18173332512378693
epoch£º60	 i:3 	 global-step:1203	 l-p:0.10950826108455658
epoch£º60	 i:4 	 global-step:1204	 l-p:0.13693442940711975
epoch£º60	 i:5 	 global-step:1205	 l-p:0.10386838763952255
epoch£º60	 i:6 	 global-step:1206	 l-p:0.12396268546581268
epoch£º60	 i:7 	 global-step:1207	 l-p:0.12577730417251587
epoch£º60	 i:8 	 global-step:1208	 l-p:0.11200955510139465
epoch£º60	 i:9 	 global-step:1209	 l-p:0.061214689165353775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6624, 3.6624, 3.6624],
        [3.6624, 4.0992, 4.2036],
        [3.6624, 3.6669, 3.6627],
        [3.6624, 4.1610, 4.3173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.11633622646331787 
model_pd.l_d.mean(): -22.297348022460938 
model_pd.lagr.mean(): -22.181011199951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0964], device='cuda:0')), ('power', tensor([-22.3937], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.11633622646331787
epoch£º61	 i:1 	 global-step:1221	 l-p:0.1209602802991867
epoch£º61	 i:2 	 global-step:1222	 l-p:0.138065367937088
epoch£º61	 i:3 	 global-step:1223	 l-p:0.18056030571460724
epoch£º61	 i:4 	 global-step:1224	 l-p:0.09166591614484787
epoch£º61	 i:5 	 global-step:1225	 l-p:0.015878940001130104
epoch£º61	 i:6 	 global-step:1226	 l-p:0.13370370864868164
epoch£º61	 i:7 	 global-step:1227	 l-p:0.09418386220932007
epoch£º61	 i:8 	 global-step:1228	 l-p:0.10398992151021957
epoch£º61	 i:9 	 global-step:1229	 l-p:0.1347041130065918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7970, 3.8201, 3.8020],
        [3.7970, 4.4832, 4.8043],
        [3.7970, 3.8930, 3.8474],
        [3.7970, 3.9537, 3.9073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.09816047549247742 
model_pd.l_d.mean(): -22.7551326751709 
model_pd.lagr.mean(): -22.656972885131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0449], device='cuda:0')), ('power', tensor([-22.8000], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.09816047549247742
epoch£º62	 i:1 	 global-step:1241	 l-p:0.11947242170572281
epoch£º62	 i:2 	 global-step:1242	 l-p:0.15797743201255798
epoch£º62	 i:3 	 global-step:1243	 l-p:0.09387174248695374
epoch£º62	 i:4 	 global-step:1244	 l-p:0.1134130135178566
epoch£º62	 i:5 	 global-step:1245	 l-p:0.09573116153478622
epoch£º62	 i:6 	 global-step:1246	 l-p:0.010179166682064533
epoch£º62	 i:7 	 global-step:1247	 l-p:0.11351577192544937
epoch£º62	 i:8 	 global-step:1248	 l-p:0.10369385778903961
epoch£º62	 i:9 	 global-step:1249	 l-p:0.1500317007303238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8316, 3.8335, 3.8317],
        [3.8316, 4.8138, 5.4698],
        [3.8316, 4.8173, 5.4776],
        [3.8316, 3.8316, 3.8316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.10748277604579926 
model_pd.l_d.mean(): -22.821331024169922 
model_pd.lagr.mean(): -22.713848114013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0139], device='cuda:0')), ('power', tensor([-22.8352], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.10748277604579926
epoch£º63	 i:1 	 global-step:1261	 l-p:0.14728233218193054
epoch£º63	 i:2 	 global-step:1262	 l-p:0.12215263396501541
epoch£º63	 i:3 	 global-step:1263	 l-p:0.11828400939702988
epoch£º63	 i:4 	 global-step:1264	 l-p:-0.16281497478485107
epoch£º63	 i:5 	 global-step:1265	 l-p:0.11767718195915222
epoch£º63	 i:6 	 global-step:1266	 l-p:0.11738791316747665
epoch£º63	 i:7 	 global-step:1267	 l-p:0.3876507878303528
epoch£º63	 i:8 	 global-step:1268	 l-p:0.3107011616230011
epoch£º63	 i:9 	 global-step:1269	 l-p:0.09905913472175598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5042, 3.5053, 3.5042],
        [3.5042, 3.5042, 3.5042],
        [3.5042, 3.5221, 3.5078],
        [3.5042, 3.5042, 3.5042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.09796927869319916 
model_pd.l_d.mean(): -22.689491271972656 
model_pd.lagr.mean(): -22.591522216796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1460], device='cuda:0')), ('power', tensor([-22.8355], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.09796927869319916
epoch£º64	 i:1 	 global-step:1281	 l-p:0.12263944000005722
epoch£º64	 i:2 	 global-step:1282	 l-p:0.1479646861553192
epoch£º64	 i:3 	 global-step:1283	 l-p:0.12382485717535019
epoch£º64	 i:4 	 global-step:1284	 l-p:0.17149360477924347
epoch£º64	 i:5 	 global-step:1285	 l-p:0.12173280119895935
epoch£º64	 i:6 	 global-step:1286	 l-p:0.12320748716592789
epoch£º64	 i:7 	 global-step:1287	 l-p:0.10793334990739822
epoch£º64	 i:8 	 global-step:1288	 l-p:0.12385255843400955
epoch£º64	 i:9 	 global-step:1289	 l-p:-0.01628582924604416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6544, 3.6547, 3.6544],
        [3.6544, 3.6544, 3.6544],
        [3.6544, 4.3845, 4.7755],
        [3.6544, 3.7349, 3.6938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.11809192597866058 
model_pd.l_d.mean(): -22.833274841308594 
model_pd.lagr.mean(): -22.71518325805664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0842], device='cuda:0')), ('power', tensor([-22.9175], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.11809192597866058
epoch£º65	 i:1 	 global-step:1301	 l-p:0.02597838267683983
epoch£º65	 i:2 	 global-step:1302	 l-p:0.11239172518253326
epoch£º65	 i:3 	 global-step:1303	 l-p:0.1003868579864502
epoch£º65	 i:4 	 global-step:1304	 l-p:0.0776284784078598
epoch£º65	 i:5 	 global-step:1305	 l-p:0.13282400369644165
epoch£º65	 i:6 	 global-step:1306	 l-p:0.16189074516296387
epoch£º65	 i:7 	 global-step:1307	 l-p:0.10224214196205139
epoch£º65	 i:8 	 global-step:1308	 l-p:0.13377174735069275
epoch£º65	 i:9 	 global-step:1309	 l-p:0.11374358832836151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8691, 3.8690, 3.8690],
        [3.8691, 4.6478, 5.0589],
        [3.8691, 4.2856, 4.3581],
        [3.8691, 3.8878, 3.8726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.1394449919462204 
model_pd.l_d.mean(): -23.072650909423828 
model_pd.lagr.mean(): -22.93320655822754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0296], device='cuda:0')), ('power', tensor([-23.0431], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.1394449919462204
epoch£º66	 i:1 	 global-step:1321	 l-p:0.05914820358157158
epoch£º66	 i:2 	 global-step:1322	 l-p:0.12000005692243576
epoch£º66	 i:3 	 global-step:1323	 l-p:0.12031438201665878
epoch£º66	 i:4 	 global-step:1324	 l-p:0.11235780268907547
epoch£º66	 i:5 	 global-step:1325	 l-p:0.10638009756803513
epoch£º66	 i:6 	 global-step:1326	 l-p:0.12017255276441574
epoch£º66	 i:7 	 global-step:1327	 l-p:0.10954710096120834
epoch£º66	 i:8 	 global-step:1328	 l-p:0.10487939417362213
epoch£º66	 i:9 	 global-step:1329	 l-p:0.0036156128626316786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8743, 3.8743, 3.8743],
        [3.8743, 3.8743, 3.8743],
        [3.8743, 4.6054, 4.9644],
        [3.8743, 4.4504, 4.6551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.11506345123052597 
model_pd.l_d.mean(): -22.762380599975586 
model_pd.lagr.mean(): -22.64731788635254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0107], device='cuda:0')), ('power', tensor([-22.7731], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.11506345123052597
epoch£º67	 i:1 	 global-step:1341	 l-p:0.08150491863489151
epoch£º67	 i:2 	 global-step:1342	 l-p:0.1705545336008072
epoch£º67	 i:3 	 global-step:1343	 l-p:0.11209587007761002
epoch£º67	 i:4 	 global-step:1344	 l-p:0.12370546162128448
epoch£º67	 i:5 	 global-step:1345	 l-p:0.11113153398036957
epoch£º67	 i:6 	 global-step:1346	 l-p:0.10382652282714844
epoch£º67	 i:7 	 global-step:1347	 l-p:0.2325705885887146
epoch£º67	 i:8 	 global-step:1348	 l-p:0.19971312582492828
epoch£º67	 i:9 	 global-step:1349	 l-p:0.2274823933839798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7294, 4.6312, 5.2104],
        [3.7294, 4.6720, 5.3014],
        [3.7294, 3.7362, 3.7301],
        [3.7294, 4.4763, 4.8754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.11043614149093628 
model_pd.l_d.mean(): -22.180326461791992 
model_pd.lagr.mean(): -22.06989097595215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1196], device='cuda:0')), ('power', tensor([-22.2999], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.11043614149093628
epoch£º68	 i:1 	 global-step:1361	 l-p:0.10146757960319519
epoch£º68	 i:2 	 global-step:1362	 l-p:2.3344085216522217
epoch£º68	 i:3 	 global-step:1363	 l-p:0.12789098918437958
epoch£º68	 i:4 	 global-step:1364	 l-p:0.10559430718421936
epoch£º68	 i:5 	 global-step:1365	 l-p:0.11440012603998184
epoch£º68	 i:6 	 global-step:1366	 l-p:0.10114427655935287
epoch£º68	 i:7 	 global-step:1367	 l-p:0.11751438677310944
epoch£º68	 i:8 	 global-step:1368	 l-p:0.15710632503032684
epoch£º68	 i:9 	 global-step:1369	 l-p:0.11410821974277496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8542, 3.9783, 3.9300],
        [3.8542, 4.6651, 5.1157],
        [3.8542, 4.0218, 3.9763],
        [3.8542, 3.8542, 3.8542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.11088583618402481 
model_pd.l_d.mean(): -21.859769821166992 
model_pd.lagr.mean(): -21.748884201049805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0913], device='cuda:0')), ('power', tensor([-21.9510], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.11088583618402481
epoch£º69	 i:1 	 global-step:1381	 l-p:0.15062734484672546
epoch£º69	 i:2 	 global-step:1382	 l-p:0.1233273446559906
epoch£º69	 i:3 	 global-step:1383	 l-p:0.11228135973215103
epoch£º69	 i:4 	 global-step:1384	 l-p:0.04219139367341995
epoch£º69	 i:5 	 global-step:1385	 l-p:0.12788932025432587
epoch£º69	 i:6 	 global-step:1386	 l-p:0.10919671505689621
epoch£º69	 i:7 	 global-step:1387	 l-p:0.11163105070590973
epoch£º69	 i:8 	 global-step:1388	 l-p:0.13823458552360535
epoch£º69	 i:9 	 global-step:1389	 l-p:0.13902629911899567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7399, 4.7196, 5.3946],
        [3.7399, 3.7591, 3.7438],
        [3.7399, 3.7399, 3.7399],
        [3.7399, 4.6560, 5.2516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.14222073554992676 
model_pd.l_d.mean(): -21.440847396850586 
model_pd.lagr.mean(): -21.298625946044922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1618], device='cuda:0')), ('power', tensor([-21.6027], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.14222073554992676
epoch£º70	 i:1 	 global-step:1401	 l-p:0.07688385993242264
epoch£º70	 i:2 	 global-step:1402	 l-p:0.7754522562026978
epoch£º70	 i:3 	 global-step:1403	 l-p:0.11335151642560959
epoch£º70	 i:4 	 global-step:1404	 l-p:0.10040794312953949
epoch£º70	 i:5 	 global-step:1405	 l-p:0.11660851538181305
epoch£º70	 i:6 	 global-step:1406	 l-p:0.15063057839870453
epoch£º70	 i:7 	 global-step:1407	 l-p:0.6734028458595276
epoch£º70	 i:8 	 global-step:1408	 l-p:0.12018775939941406
epoch£º70	 i:9 	 global-step:1409	 l-p:0.12278158217668533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6351, 3.7678, 3.7239],
        [3.6351, 3.6373, 3.6353],
        [3.6351, 3.6351, 3.6351],
        [3.6351, 4.0562, 4.1542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.12841160595417023 
model_pd.l_d.mean(): -23.040557861328125 
model_pd.lagr.mean(): -22.912145614624023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0714], device='cuda:0')), ('power', tensor([-23.1119], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.12841160595417023
epoch£º71	 i:1 	 global-step:1421	 l-p:0.11342450976371765
epoch£º71	 i:2 	 global-step:1422	 l-p:0.15452638268470764
epoch£º71	 i:3 	 global-step:1423	 l-p:0.19857144355773926
epoch£º71	 i:4 	 global-step:1424	 l-p:0.11939135938882828
epoch£º71	 i:5 	 global-step:1425	 l-p:0.1095198318362236
epoch£º71	 i:6 	 global-step:1426	 l-p:0.08341114223003387
epoch£º71	 i:7 	 global-step:1427	 l-p:0.03573736548423767
epoch£º71	 i:8 	 global-step:1428	 l-p:0.12054584920406342
epoch£º71	 i:9 	 global-step:1429	 l-p:0.21014584600925446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7626, 3.7626, 3.7626],
        [3.7626, 3.7626, 3.7626],
        [3.7626, 3.7658, 3.7628],
        [3.7626, 3.7626, 3.7626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.11072511225938797 
model_pd.l_d.mean(): -22.290231704711914 
model_pd.lagr.mean(): -22.179506301879883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0460], device='cuda:0')), ('power', tensor([-22.3362], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.11072511225938797
epoch£º72	 i:1 	 global-step:1441	 l-p:0.1501331627368927
epoch£º72	 i:2 	 global-step:1442	 l-p:0.17734897136688232
epoch£º72	 i:3 	 global-step:1443	 l-p:0.040760669857263565
epoch£º72	 i:4 	 global-step:1444	 l-p:0.10534726828336716
epoch£º72	 i:5 	 global-step:1445	 l-p:0.11927209794521332
epoch£º72	 i:6 	 global-step:1446	 l-p:0.11107036471366882
epoch£º72	 i:7 	 global-step:1447	 l-p:0.11607713252305984
epoch£º72	 i:8 	 global-step:1448	 l-p:0.10620564967393875
epoch£º72	 i:9 	 global-step:1449	 l-p:0.10983672738075256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0744, 4.0744, 4.0744],
        [4.0744, 4.2023, 4.1505],
        [4.0744, 4.8571, 5.2440],
        [4.0744, 4.2539, 4.2049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.11471523344516754 
model_pd.l_d.mean(): -22.328601837158203 
model_pd.lagr.mean(): -22.213886260986328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0695], device='cuda:0')), ('power', tensor([-22.2591], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.11471523344516754
epoch£º73	 i:1 	 global-step:1461	 l-p:0.0947510302066803
epoch£º73	 i:2 	 global-step:1462	 l-p:0.10933434963226318
epoch£º73	 i:3 	 global-step:1463	 l-p:0.13255652785301208
epoch£º73	 i:4 	 global-step:1464	 l-p:0.11828210204839706
epoch£º73	 i:5 	 global-step:1465	 l-p:0.10145420581102371
epoch£º73	 i:6 	 global-step:1466	 l-p:0.1123310998082161
epoch£º73	 i:7 	 global-step:1467	 l-p:0.15129663050174713
epoch£º73	 i:8 	 global-step:1468	 l-p:0.08929036557674408
epoch£º73	 i:9 	 global-step:1469	 l-p:0.11557286232709885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6917, 3.7040, 3.6936],
        [3.6917, 3.9489, 3.9385],
        [3.6917, 4.2014, 4.3694],
        [3.6917, 3.8448, 3.8020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.5145483016967773 
model_pd.l_d.mean(): -22.64354133605957 
model_pd.lagr.mean(): -22.12899398803711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0968], device='cuda:0')), ('power', tensor([-22.7403], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.5145483016967773
epoch£º74	 i:1 	 global-step:1481	 l-p:0.10996966809034348
epoch£º74	 i:2 	 global-step:1482	 l-p:0.11526046693325043
epoch£º74	 i:3 	 global-step:1483	 l-p:0.10334935784339905
epoch£º74	 i:4 	 global-step:1484	 l-p:-0.04406201094388962
epoch£º74	 i:5 	 global-step:1485	 l-p:0.12796032428741455
epoch£º74	 i:6 	 global-step:1486	 l-p:0.19075095653533936
epoch£º74	 i:7 	 global-step:1487	 l-p:-0.034637317061424255
epoch£º74	 i:8 	 global-step:1488	 l-p:0.11356059461832047
epoch£º74	 i:9 	 global-step:1489	 l-p:0.11407109349966049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8594, 4.1085, 4.0869],
        [3.8594, 4.0165, 3.9702],
        [3.8594, 3.8594, 3.8594],
        [3.8594, 4.4448, 4.6630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.10105809569358826 
model_pd.l_d.mean(): -22.844202041625977 
model_pd.lagr.mean(): -22.74314308166504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0017], device='cuda:0')), ('power', tensor([-22.8459], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.10105809569358826
epoch£º75	 i:1 	 global-step:1501	 l-p:0.12377191334962845
epoch£º75	 i:2 	 global-step:1502	 l-p:0.1516796499490738
epoch£º75	 i:3 	 global-step:1503	 l-p:0.11654021590948105
epoch£º75	 i:4 	 global-step:1504	 l-p:0.11643127351999283
epoch£º75	 i:5 	 global-step:1505	 l-p:0.11223769932985306
epoch£º75	 i:6 	 global-step:1506	 l-p:0.1109863892197609
epoch£º75	 i:7 	 global-step:1507	 l-p:0.3554871380329132
epoch£º75	 i:8 	 global-step:1508	 l-p:0.028960207477211952
epoch£º75	 i:9 	 global-step:1509	 l-p:0.07630091905593872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6721, 3.6724, 3.6721],
        [3.6721, 3.6721, 3.6721],
        [3.6721, 4.1914, 4.3712],
        [3.6721, 3.8322, 3.7912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.01650574617087841 
model_pd.l_d.mean(): -22.406782150268555 
model_pd.lagr.mean(): -22.390275955200195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1127], device='cuda:0')), ('power', tensor([-22.5195], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.01650574617087841
epoch£º76	 i:1 	 global-step:1521	 l-p:0.1158917024731636
epoch£º76	 i:2 	 global-step:1522	 l-p:0.10926992446184158
epoch£º76	 i:3 	 global-step:1523	 l-p:0.09348346292972565
epoch£º76	 i:4 	 global-step:1524	 l-p:0.12143317610025406
epoch£º76	 i:5 	 global-step:1525	 l-p:1.0432507991790771
epoch£º76	 i:6 	 global-step:1526	 l-p:0.1445184201002121
epoch£º76	 i:7 	 global-step:1527	 l-p:0.15518328547477722
epoch£º76	 i:8 	 global-step:1528	 l-p:0.10976951569318771
epoch£º76	 i:9 	 global-step:1529	 l-p:0.11328090727329254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8754, 3.8881, 3.8773],
        [3.8754, 3.8757, 3.8754],
        [3.8754, 3.9065, 3.8836],
        [3.8754, 3.9504, 3.9093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.11972081661224365 
model_pd.l_d.mean(): -23.097999572753906 
model_pd.lagr.mean(): -22.97827911376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0295], device='cuda:0')), ('power', tensor([-23.0685], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.11972081661224365
epoch£º77	 i:1 	 global-step:1541	 l-p:0.05797870457172394
epoch£º77	 i:2 	 global-step:1542	 l-p:0.09163122624158859
epoch£º77	 i:3 	 global-step:1543	 l-p:0.1382322609424591
epoch£º77	 i:4 	 global-step:1544	 l-p:0.11779629439115524
epoch£º77	 i:5 	 global-step:1545	 l-p:0.1488843560218811
epoch£º77	 i:6 	 global-step:1546	 l-p:0.14841710031032562
epoch£º77	 i:7 	 global-step:1547	 l-p:0.10795562714338303
epoch£º77	 i:8 	 global-step:1548	 l-p:0.09848581999540329
epoch£º77	 i:9 	 global-step:1549	 l-p:0.11210715770721436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8533, 4.3048, 4.4090],
        [3.8533, 4.5156, 4.8109],
        [3.8533, 3.9299, 3.8886],
        [3.8533, 3.8921, 3.8651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.09053020924329758 
model_pd.l_d.mean(): -22.714359283447266 
model_pd.lagr.mean(): -22.623828887939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0264], device='cuda:0')), ('power', tensor([-22.7408], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.09053020924329758
epoch£º78	 i:1 	 global-step:1561	 l-p:0.13669714331626892
epoch£º78	 i:2 	 global-step:1562	 l-p:0.11846324801445007
epoch£º78	 i:3 	 global-step:1563	 l-p:0.16213753819465637
epoch£º78	 i:4 	 global-step:1564	 l-p:0.11886046826839447
epoch£º78	 i:5 	 global-step:1565	 l-p:0.4699085056781769
epoch£º78	 i:6 	 global-step:1566	 l-p:0.09165196865797043
epoch£º78	 i:7 	 global-step:1567	 l-p:-0.09262074530124664
epoch£º78	 i:8 	 global-step:1568	 l-p:0.1235545426607132
epoch£º78	 i:9 	 global-step:1569	 l-p:0.12907905876636505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8396, 4.3261, 4.4604],
        [3.8396, 3.8397, 3.8396],
        [3.8396, 3.8409, 3.8397],
        [3.8396, 3.8429, 3.8399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.18810321390628815 
model_pd.l_d.mean(): -23.086118698120117 
model_pd.lagr.mean(): -22.89801597595215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0100], device='cuda:0')), ('power', tensor([-23.0762], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.18810321390628815
epoch£º79	 i:1 	 global-step:1581	 l-p:0.056859925389289856
epoch£º79	 i:2 	 global-step:1582	 l-p:0.10765913128852844
epoch£º79	 i:3 	 global-step:1583	 l-p:0.10773579031229019
epoch£º79	 i:4 	 global-step:1584	 l-p:0.14924584329128265
epoch£º79	 i:5 	 global-step:1585	 l-p:0.09643576294183731
epoch£º79	 i:6 	 global-step:1586	 l-p:0.09851694107055664
epoch£º79	 i:7 	 global-step:1587	 l-p:0.09914389997720718
epoch£º79	 i:8 	 global-step:1588	 l-p:0.06929490715265274
epoch£º79	 i:9 	 global-step:1589	 l-p:0.11955773830413818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9132, 4.9553, 5.6775],
        [3.9132, 3.9259, 3.9151],
        [3.9132, 3.9132, 3.9132],
        [3.9132, 4.6431, 5.0008]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.11874525994062424 
model_pd.l_d.mean(): -22.17983055114746 
model_pd.lagr.mean(): -22.061084747314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0137], device='cuda:0')), ('power', tensor([-22.1936], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.11874525994062424
epoch£º80	 i:1 	 global-step:1601	 l-p:0.10714638233184814
epoch£º80	 i:2 	 global-step:1602	 l-p:0.10204064846038818
epoch£º80	 i:3 	 global-step:1603	 l-p:0.11095918715000153
epoch£º80	 i:4 	 global-step:1604	 l-p:0.11076083779335022
epoch£º80	 i:5 	 global-step:1605	 l-p:0.12833359837532043
epoch£º80	 i:6 	 global-step:1606	 l-p:0.11665543913841248
epoch£º80	 i:7 	 global-step:1607	 l-p:0.1241033747792244
epoch£º80	 i:8 	 global-step:1608	 l-p:0.1293047070503235
epoch£º80	 i:9 	 global-step:1609	 l-p:0.5641008615493774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6766, 3.6806, 3.6769],
        [3.6766, 3.6766, 3.6766],
        [3.6766, 4.3440, 4.6713],
        [3.6766, 3.7245, 3.6940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.11279398947954178 
model_pd.l_d.mean(): -22.975061416625977 
model_pd.lagr.mean(): -22.862266540527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0637], device='cuda:0')), ('power', tensor([-23.0388], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.11279398947954178
epoch£º81	 i:1 	 global-step:1621	 l-p:0.1307430863380432
epoch£º81	 i:2 	 global-step:1622	 l-p:0.1138976663351059
epoch£º81	 i:3 	 global-step:1623	 l-p:0.1805136501789093
epoch£º81	 i:4 	 global-step:1624	 l-p:0.1435338258743286
epoch£º81	 i:5 	 global-step:1625	 l-p:0.10404661297798157
epoch£º81	 i:6 	 global-step:1626	 l-p:0.7607515454292297
epoch£º81	 i:7 	 global-step:1627	 l-p:0.11040996015071869
epoch£º81	 i:8 	 global-step:1628	 l-p:0.04553595557808876
epoch£º81	 i:9 	 global-step:1629	 l-p:0.09437678754329681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6507, 3.6978, 3.6678],
        [3.6507, 4.1140, 4.2493],
        [3.6507, 3.7944, 3.7524],
        [3.6507, 4.3927, 4.8056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.12220307439565659 
model_pd.l_d.mean(): -23.002782821655273 
model_pd.lagr.mean(): -22.880578994750977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0706], device='cuda:0')), ('power', tensor([-23.0734], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.12220307439565659
epoch£º82	 i:1 	 global-step:1641	 l-p:0.12302560359239578
epoch£º82	 i:2 	 global-step:1642	 l-p:0.12147266417741776
epoch£º82	 i:3 	 global-step:1643	 l-p:0.09328256547451019
epoch£º82	 i:4 	 global-step:1644	 l-p:0.10831447690725327
epoch£º82	 i:5 	 global-step:1645	 l-p:0.0709228590130806
epoch£º82	 i:6 	 global-step:1646	 l-p:0.7440273761749268
epoch£º82	 i:7 	 global-step:1647	 l-p:0.2725211977958679
epoch£º82	 i:8 	 global-step:1648	 l-p:0.11244171857833862
epoch£º82	 i:9 	 global-step:1649	 l-p:0.08329790085554123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8145, 4.4778, 4.7819],
        [3.8145, 3.9834, 3.9411],
        [3.8145, 3.8150, 3.8146],
        [3.8145, 4.1785, 4.2239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.2438521832227707 
model_pd.l_d.mean(): -22.301197052001953 
model_pd.lagr.mean(): -22.057344436645508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0620], device='cuda:0')), ('power', tensor([-22.3632], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.2438521832227707
epoch£º83	 i:1 	 global-step:1661	 l-p:0.1408086121082306
epoch£º83	 i:2 	 global-step:1662	 l-p:0.11449950933456421
epoch£º83	 i:3 	 global-step:1663	 l-p:0.12014133483171463
epoch£º83	 i:4 	 global-step:1664	 l-p:0.1177489161491394
epoch£º83	 i:5 	 global-step:1665	 l-p:0.057559382170438766
epoch£º83	 i:6 	 global-step:1666	 l-p:0.10929527878761292
epoch£º83	 i:7 	 global-step:1667	 l-p:0.12282685190439224
epoch£º83	 i:8 	 global-step:1668	 l-p:0.10782497376203537
epoch£º83	 i:9 	 global-step:1669	 l-p:-0.07577063143253326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9253, 3.9254, 3.9253],
        [3.9253, 3.9317, 3.9260],
        [3.9253, 3.9815, 3.9466],
        [3.9253, 4.1018, 4.0578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.137843519449234 
model_pd.l_d.mean(): -22.954282760620117 
model_pd.lagr.mean(): -22.816438674926758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0188], device='cuda:0')), ('power', tensor([-22.9355], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.137843519449234
epoch£º84	 i:1 	 global-step:1681	 l-p:-0.10532873868942261
epoch£º84	 i:2 	 global-step:1682	 l-p:0.10834217816591263
epoch£º84	 i:3 	 global-step:1683	 l-p:0.11038833111524582
epoch£º84	 i:4 	 global-step:1684	 l-p:0.10757113248109818
epoch£º84	 i:5 	 global-step:1685	 l-p:0.13324548304080963
epoch£º84	 i:6 	 global-step:1686	 l-p:0.11751952767372131
epoch£º84	 i:7 	 global-step:1687	 l-p:0.18126535415649414
epoch£º84	 i:8 	 global-step:1688	 l-p:0.10839993506669998
epoch£º84	 i:9 	 global-step:1689	 l-p:0.018758201971650124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[3.7225, 4.2469, 4.4291],
        [3.7225, 4.0558, 4.0885],
        [3.7225, 4.3225, 4.5763],
        [3.7225, 3.9554, 3.9355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 1.5058364868164062 
model_pd.l_d.mean(): -22.80066680908203 
model_pd.lagr.mean(): -21.294830322265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0714], device='cuda:0')), ('power', tensor([-22.8721], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:1.5058364868164062
epoch£º85	 i:1 	 global-step:1701	 l-p:0.1078353300690651
epoch£º85	 i:2 	 global-step:1702	 l-p:0.19120556116104126
epoch£º85	 i:3 	 global-step:1703	 l-p:0.11405666172504425
epoch£º85	 i:4 	 global-step:1704	 l-p:0.16791042685508728
epoch£º85	 i:5 	 global-step:1705	 l-p:-0.3653925359249115
epoch£º85	 i:6 	 global-step:1706	 l-p:0.10908480733633041
epoch£º85	 i:7 	 global-step:1707	 l-p:0.10579389333724976
epoch£º85	 i:8 	 global-step:1708	 l-p:0.12214692682027817
epoch£º85	 i:9 	 global-step:1709	 l-p:0.1056225374341011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8741, 4.5133, 4.7861],
        [3.8741, 3.8813, 3.8749],
        [3.8741, 3.8742, 3.8741],
        [3.8741, 4.1072, 4.0808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.1243625357747078 
model_pd.l_d.mean(): -21.000173568725586 
model_pd.lagr.mean(): -20.875810623168945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1935], device='cuda:0')), ('power', tensor([-21.1937], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.1243625357747078
epoch£º86	 i:1 	 global-step:1721	 l-p:0.10976619273424149
epoch£º86	 i:2 	 global-step:1722	 l-p:0.10808008164167404
epoch£º86	 i:3 	 global-step:1723	 l-p:0.14422453939914703
epoch£º86	 i:4 	 global-step:1724	 l-p:0.10733779519796371
epoch£º86	 i:5 	 global-step:1725	 l-p:0.08762077987194061
epoch£º86	 i:6 	 global-step:1726	 l-p:0.11520713567733765
epoch£º86	 i:7 	 global-step:1727	 l-p:0.08172088116407394
epoch£º86	 i:8 	 global-step:1728	 l-p:0.11708665639162064
epoch£º86	 i:9 	 global-step:1729	 l-p:0.03450896590948105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7202, 3.7862, 3.7493],
        [3.7202, 3.7232, 3.7204],
        [3.7202, 3.7205, 3.7202],
        [3.7202, 4.0833, 4.1362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.10401897132396698 
model_pd.l_d.mean(): -22.552194595336914 
model_pd.lagr.mean(): -22.44817543029785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1054], device='cuda:0')), ('power', tensor([-22.6576], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.10401897132396698
epoch£º87	 i:1 	 global-step:1741	 l-p:0.13883310556411743
epoch£º87	 i:2 	 global-step:1742	 l-p:0.08269695192575455
epoch£º87	 i:3 	 global-step:1743	 l-p:0.12311616539955139
epoch£º87	 i:4 	 global-step:1744	 l-p:0.12654148042201996
epoch£º87	 i:5 	 global-step:1745	 l-p:0.06949544697999954
epoch£º87	 i:6 	 global-step:1746	 l-p:0.0685066506266594
epoch£º87	 i:7 	 global-step:1747	 l-p:0.07955224812030792
epoch£º87	 i:8 	 global-step:1748	 l-p:0.12349027395248413
epoch£º87	 i:9 	 global-step:1749	 l-p:0.12172811478376389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7183, 3.7183, 3.7183],
        [3.7183, 3.9251, 3.8962],
        [3.7183, 3.7891, 3.7509],
        [3.7183, 3.7220, 3.7186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.23296904563903809 
model_pd.l_d.mean(): -23.268470764160156 
model_pd.lagr.mean(): -23.03550148010254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0102], device='cuda:0')), ('power', tensor([-23.2787], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.23296904563903809
epoch£º88	 i:1 	 global-step:1761	 l-p:0.08852127194404602
epoch£º88	 i:2 	 global-step:1762	 l-p:0.11659570783376694
epoch£º88	 i:3 	 global-step:1763	 l-p:0.14962007105350494
epoch£º88	 i:4 	 global-step:1764	 l-p:0.160677969455719
epoch£º88	 i:5 	 global-step:1765	 l-p:0.07949264347553253
epoch£º88	 i:6 	 global-step:1766	 l-p:0.11496180295944214
epoch£º88	 i:7 	 global-step:1767	 l-p:0.11261539161205292
epoch£º88	 i:8 	 global-step:1768	 l-p:0.1081513985991478
epoch£º88	 i:9 	 global-step:1769	 l-p:0.1252097487449646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9875, 4.7699, 5.1763],
        [3.9875, 4.0073, 3.9915],
        [3.9875, 4.0143, 3.9939],
        [3.9875, 4.1101, 4.0613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.1154017224907875 
model_pd.l_d.mean(): -22.320241928100586 
model_pd.lagr.mean(): -22.2048397064209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0419], device='cuda:0')), ('power', tensor([-22.3621], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.1154017224907875
epoch£º89	 i:1 	 global-step:1781	 l-p:0.8186987042427063
epoch£º89	 i:2 	 global-step:1782	 l-p:0.11680152267217636
epoch£º89	 i:3 	 global-step:1783	 l-p:0.13333438336849213
epoch£º89	 i:4 	 global-step:1784	 l-p:0.0931687206029892
epoch£º89	 i:5 	 global-step:1785	 l-p:0.11890356987714767
epoch£º89	 i:6 	 global-step:1786	 l-p:0.10253219306468964
epoch£º89	 i:7 	 global-step:1787	 l-p:0.12167472392320633
epoch£º89	 i:8 	 global-step:1788	 l-p:0.10166524350643158
epoch£º89	 i:9 	 global-step:1789	 l-p:0.10959421843290329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9563, 4.7986, 5.2768],
        [3.9563, 3.9564, 3.9563],
        [3.9563, 3.9564, 3.9563],
        [3.9563, 3.9851, 3.9636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.12418120354413986 
model_pd.l_d.mean(): -22.38272476196289 
model_pd.lagr.mean(): -22.258543014526367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0095], device='cuda:0')), ('power', tensor([-22.3733], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.12418120354413986
epoch£º90	 i:1 	 global-step:1801	 l-p:0.08432281017303467
epoch£º90	 i:2 	 global-step:1802	 l-p:0.16129136085510254
epoch£º90	 i:3 	 global-step:1803	 l-p:0.12308792024850845
epoch£º90	 i:4 	 global-step:1804	 l-p:0.08245710283517838
epoch£º90	 i:5 	 global-step:1805	 l-p:0.11247418820858002
epoch£º90	 i:6 	 global-step:1806	 l-p:0.13372930884361267
epoch£º90	 i:7 	 global-step:1807	 l-p:0.16857865452766418
epoch£º90	 i:8 	 global-step:1808	 l-p:0.10209638625383377
epoch£º90	 i:9 	 global-step:1809	 l-p:0.8537324070930481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7217, 3.7217, 3.7217],
        [3.7217, 3.7220, 3.7217],
        [3.7217, 3.7261, 3.7221],
        [3.7217, 4.2741, 4.4843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.11237873136997223 
model_pd.l_d.mean(): -22.38677978515625 
model_pd.lagr.mean(): -22.27440071105957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0503], device='cuda:0')), ('power', tensor([-22.4371], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.11237873136997223
epoch£º91	 i:1 	 global-step:1821	 l-p:0.1390535682439804
epoch£º91	 i:2 	 global-step:1822	 l-p:0.10313096642494202
epoch£º91	 i:3 	 global-step:1823	 l-p:0.013203294016420841
epoch£º91	 i:4 	 global-step:1824	 l-p:-0.03500037640333176
epoch£º91	 i:5 	 global-step:1825	 l-p:0.11626306176185608
epoch£º91	 i:6 	 global-step:1826	 l-p:0.012998039834201336
epoch£º91	 i:7 	 global-step:1827	 l-p:0.09387090802192688
epoch£º91	 i:8 	 global-step:1828	 l-p:0.09596297889947891
epoch£º91	 i:9 	 global-step:1829	 l-p:0.1030048355460167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8108, 3.8108, 3.8108],
        [3.8108, 4.5733, 4.9870],
        [3.8108, 3.9175, 3.8726],
        [3.8108, 3.8539, 3.8252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.4051617383956909 
model_pd.l_d.mean(): -22.516204833984375 
model_pd.lagr.mean(): -22.11104393005371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0872], device='cuda:0')), ('power', tensor([-22.6035], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.4051617383956909
epoch£º92	 i:1 	 global-step:1841	 l-p:0.1144491583108902
epoch£º92	 i:2 	 global-step:1842	 l-p:0.12173528224229813
epoch£º92	 i:3 	 global-step:1843	 l-p:0.11036358773708344
epoch£º92	 i:4 	 global-step:1844	 l-p:0.11295457929372787
epoch£º92	 i:5 	 global-step:1845	 l-p:0.11484719812870026
epoch£º92	 i:6 	 global-step:1846	 l-p:0.11634207516908646
epoch£º92	 i:7 	 global-step:1847	 l-p:0.1001049131155014
epoch£º92	 i:8 	 global-step:1848	 l-p:0.05940606817603111
epoch£º92	 i:9 	 global-step:1849	 l-p:0.12171344459056854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8249, 3.8249, 3.8249],
        [3.8249, 4.0934, 4.0857],
        [3.8249, 4.1183, 4.1231],
        [3.8249, 4.0260, 3.9923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.2371717244386673 
model_pd.l_d.mean(): -22.578628540039062 
model_pd.lagr.mean(): -22.34145736694336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0330], device='cuda:0')), ('power', tensor([-22.6116], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.2371717244386673
epoch£º93	 i:1 	 global-step:1861	 l-p:0.1600489616394043
epoch£º93	 i:2 	 global-step:1862	 l-p:0.11542344838380814
epoch£º93	 i:3 	 global-step:1863	 l-p:0.1355161964893341
epoch£º93	 i:4 	 global-step:1864	 l-p:0.050618089735507965
epoch£º93	 i:5 	 global-step:1865	 l-p:0.11773175746202469
epoch£º93	 i:6 	 global-step:1866	 l-p:0.13299913704395294
epoch£º93	 i:7 	 global-step:1867	 l-p:0.11782367527484894
epoch£º93	 i:8 	 global-step:1868	 l-p:0.10989231616258621
epoch£º93	 i:9 	 global-step:1869	 l-p:0.09637864679098129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8109, 4.2008, 4.2674],
        [3.8109, 3.8109, 3.8109],
        [3.8109, 3.8424, 3.8196],
        [3.8109, 3.8211, 3.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.11895818263292313 
model_pd.l_d.mean(): -23.289777755737305 
model_pd.lagr.mean(): -23.170820236206055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0308], device='cuda:0')), ('power', tensor([-23.2590], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.11895818263292313
epoch£º94	 i:1 	 global-step:1881	 l-p:0.26715004444122314
epoch£º94	 i:2 	 global-step:1882	 l-p:0.10843561589717865
epoch£º94	 i:3 	 global-step:1883	 l-p:0.14121130108833313
epoch£º94	 i:4 	 global-step:1884	 l-p:0.10833778232336044
epoch£º94	 i:5 	 global-step:1885	 l-p:0.10841172188520432
epoch£º94	 i:6 	 global-step:1886	 l-p:0.018436986953020096
epoch£º94	 i:7 	 global-step:1887	 l-p:0.1378931999206543
epoch£º94	 i:8 	 global-step:1888	 l-p:0.11702325195074081
epoch£º94	 i:9 	 global-step:1889	 l-p:0.12247847020626068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6462, 3.6462, 3.6462],
        [3.6462, 3.6462, 3.6462],
        [3.6462, 3.6471, 3.6462],
        [3.6462, 3.6565, 3.6477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.1322578638792038 
model_pd.l_d.mean(): -22.311220169067383 
model_pd.lagr.mean(): -22.17896270751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1315], device='cuda:0')), ('power', tensor([-22.4427], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.1322578638792038
epoch£º95	 i:1 	 global-step:1901	 l-p:0.11877809464931488
epoch£º95	 i:2 	 global-step:1902	 l-p:0.13720132410526276
epoch£º95	 i:3 	 global-step:1903	 l-p:0.12492132931947708
epoch£º95	 i:4 	 global-step:1904	 l-p:0.08613154292106628
epoch£º95	 i:5 	 global-step:1905	 l-p:0.12760014832019806
epoch£º95	 i:6 	 global-step:1906	 l-p:-0.48193639516830444
epoch£º95	 i:7 	 global-step:1907	 l-p:0.1089451014995575
epoch£º95	 i:8 	 global-step:1908	 l-p:0.2569533884525299
epoch£º95	 i:9 	 global-step:1909	 l-p:0.1222694143652916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7783, 4.6219, 5.1351],
        [3.7783, 3.7788, 3.7783],
        [3.7783, 4.3345, 4.5435],
        [3.7783, 3.7783, 3.7783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.15177148580551147 
model_pd.l_d.mean(): -21.6780948638916 
model_pd.lagr.mean(): -21.526323318481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1799], device='cuda:0')), ('power', tensor([-21.8580], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.15177148580551147
epoch£º96	 i:1 	 global-step:1921	 l-p:0.11432640999555588
epoch£º96	 i:2 	 global-step:1922	 l-p:0.10919082164764404
epoch£º96	 i:3 	 global-step:1923	 l-p:0.08338981121778488
epoch£º96	 i:4 	 global-step:1924	 l-p:0.14490608870983124
epoch£º96	 i:5 	 global-step:1925	 l-p:0.15778298676013947
epoch£º96	 i:6 	 global-step:1926	 l-p:0.2217729091644287
epoch£º96	 i:7 	 global-step:1927	 l-p:0.12130563706159592
epoch£º96	 i:8 	 global-step:1928	 l-p:0.1121235266327858
epoch£º96	 i:9 	 global-step:1929	 l-p:0.11190676689147949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8629, 4.2084, 4.2422],
        [3.8629, 4.4812, 4.7395],
        [3.8629, 3.8630, 3.8629],
        [3.8629, 4.8100, 5.4315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.11608345061540604 
model_pd.l_d.mean(): -23.013158798217773 
model_pd.lagr.mean(): -22.897075653076172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0139], device='cuda:0')), ('power', tensor([-22.9992], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.11608345061540604
epoch£º97	 i:1 	 global-step:1941	 l-p:0.12086091935634613
epoch£º97	 i:2 	 global-step:1942	 l-p:-0.1354520320892334
epoch£º97	 i:3 	 global-step:1943	 l-p:0.13434389233589172
epoch£º97	 i:4 	 global-step:1944	 l-p:0.11333538591861725
epoch£º97	 i:5 	 global-step:1945	 l-p:-0.059823207557201385
epoch£º97	 i:6 	 global-step:1946	 l-p:0.1123853251338005
epoch£º97	 i:7 	 global-step:1947	 l-p:0.11987220495939255
epoch£º97	 i:8 	 global-step:1948	 l-p:0.11936865746974945
epoch£º97	 i:9 	 global-step:1949	 l-p:-0.01259581558406353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6365, 3.8967, 3.8961],
        [3.6365, 3.6365, 3.6365],
        [3.6365, 3.6645, 3.6441],
        [3.6365, 4.0630, 4.1745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.11927861720323563 
model_pd.l_d.mean(): -23.0194149017334 
model_pd.lagr.mean(): -22.900136947631836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0788], device='cuda:0')), ('power', tensor([-23.0982], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.11927861720323563
epoch£º98	 i:1 	 global-step:1961	 l-p:0.10119274258613586
epoch£º98	 i:2 	 global-step:1962	 l-p:0.11862800270318985
epoch£º98	 i:3 	 global-step:1963	 l-p:0.11516375094652176
epoch£º98	 i:4 	 global-step:1964	 l-p:0.12099015712738037
epoch£º98	 i:5 	 global-step:1965	 l-p:0.3067264258861542
epoch£º98	 i:6 	 global-step:1966	 l-p:0.17896504700183868
epoch£º98	 i:7 	 global-step:1967	 l-p:0.12225467711687088
epoch£º98	 i:8 	 global-step:1968	 l-p:0.1101851612329483
epoch£º98	 i:9 	 global-step:1969	 l-p:0.07947169244289398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8741, 3.8741, 3.8741],
        [3.8741, 3.8742, 3.8741],
        [3.8741, 4.2192, 4.2525],
        [3.8741, 3.8742, 3.8741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.11378874629735947 
model_pd.l_d.mean(): -22.41547393798828 
model_pd.lagr.mean(): -22.301685333251953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0070], device='cuda:0')), ('power', tensor([-22.4085], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.11378874629735947
epoch£º99	 i:1 	 global-step:1981	 l-p:0.1471613198518753
epoch£º99	 i:2 	 global-step:1982	 l-p:0.1730128973722458
epoch£º99	 i:3 	 global-step:1983	 l-p:0.10542485862970352
epoch£º99	 i:4 	 global-step:1984	 l-p:0.0714106485247612
epoch£º99	 i:5 	 global-step:1985	 l-p:0.10803196579217911
epoch£º99	 i:6 	 global-step:1986	 l-p:0.10830733925104141
epoch£º99	 i:7 	 global-step:1987	 l-p:0.13059720396995544
epoch£º99	 i:8 	 global-step:1988	 l-p:-0.022466637194156647
epoch£º99	 i:9 	 global-step:1989	 l-p:0.11056386679410934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9142, 3.9147, 3.9143],
        [3.9142, 4.2985, 4.3549],
        [3.9142, 3.9822, 3.9439],
        [3.9142, 3.9142, 3.9142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.11540594696998596 
model_pd.l_d.mean(): -23.267841339111328 
model_pd.lagr.mean(): -23.152435302734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0670], device='cuda:0')), ('power', tensor([-23.2008], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.11540594696998596
epoch£º100	 i:1 	 global-step:2001	 l-p:0.11123430728912354
epoch£º100	 i:2 	 global-step:2002	 l-p:0.10790444910526276
epoch£º100	 i:3 	 global-step:2003	 l-p:-1.0638136863708496
epoch£º100	 i:4 	 global-step:2004	 l-p:0.17807944118976593
epoch£º100	 i:5 	 global-step:2005	 l-p:0.16468437016010284
epoch£º100	 i:6 	 global-step:2006	 l-p:0.12398412078619003
epoch£º100	 i:7 	 global-step:2007	 l-p:0.10911022126674652
epoch£º100	 i:8 	 global-step:2008	 l-p:0.09590958058834076
epoch£º100	 i:9 	 global-step:2009	 l-p:0.15824392437934875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8214, 3.8661, 3.8370],
        [3.8214, 3.8215, 3.8214],
        [3.8214, 3.8230, 3.8215],
        [3.8214, 3.8214, 3.8214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.10345017910003662 
model_pd.l_d.mean(): -22.862558364868164 
model_pd.lagr.mean(): -22.75910758972168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0085], device='cuda:0')), ('power', tensor([-22.8710], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.10345017910003662
epoch£º101	 i:1 	 global-step:2021	 l-p:0.1604047417640686
epoch£º101	 i:2 	 global-step:2022	 l-p:0.08320849388837814
epoch£º101	 i:3 	 global-step:2023	 l-p:0.12116269767284393
epoch£º101	 i:4 	 global-step:2024	 l-p:0.20445166528224945
epoch£º101	 i:5 	 global-step:2025	 l-p:0.11267177015542984
epoch£º101	 i:6 	 global-step:2026	 l-p:0.12208898365497589
epoch£º101	 i:7 	 global-step:2027	 l-p:0.09893222898244858
epoch£º101	 i:8 	 global-step:2028	 l-p:0.11123048514127731
epoch£º101	 i:9 	 global-step:2029	 l-p:0.115211620926857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[3.7939, 4.3037, 4.4711],
        [3.7939, 4.3072, 4.4776],
        [3.7939, 4.6704, 5.2222],
        [3.7939, 3.9149, 3.8707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.1066250428557396 
model_pd.l_d.mean(): -22.858339309692383 
model_pd.lagr.mean(): -22.7517147064209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0423], device='cuda:0')), ('power', tensor([-22.9007], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.1066250428557396
epoch£º102	 i:1 	 global-step:2041	 l-p:0.1097264438867569
epoch£º102	 i:2 	 global-step:2042	 l-p:-0.18954317271709442
epoch£º102	 i:3 	 global-step:2043	 l-p:0.132376566529274
epoch£º102	 i:4 	 global-step:2044	 l-p:0.13470515608787537
epoch£º102	 i:5 	 global-step:2045	 l-p:0.12382447719573975
epoch£º102	 i:6 	 global-step:2046	 l-p:0.12279064208269119
epoch£º102	 i:7 	 global-step:2047	 l-p:0.1189364418387413
epoch£º102	 i:8 	 global-step:2048	 l-p:0.1191486045718193
epoch£º102	 i:9 	 global-step:2049	 l-p:0.09044477343559265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5820, 3.6464, 3.6117],
        [3.5820, 3.8141, 3.8040],
        [3.5820, 3.6419, 3.6085],
        [3.5820, 3.5890, 3.5829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.048868559300899506 
model_pd.l_d.mean(): -22.736835479736328 
model_pd.lagr.mean(): -22.68796730041504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1367], device='cuda:0')), ('power', tensor([-22.8735], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.048868559300899506
epoch£º103	 i:1 	 global-step:2061	 l-p:0.11849762499332428
epoch£º103	 i:2 	 global-step:2062	 l-p:0.12215077877044678
epoch£º103	 i:3 	 global-step:2063	 l-p:0.13973607122898102
epoch£º103	 i:4 	 global-step:2064	 l-p:0.13930892944335938
epoch£º103	 i:5 	 global-step:2065	 l-p:0.06575077027082443
epoch£º103	 i:6 	 global-step:2066	 l-p:0.14893430471420288
epoch£º103	 i:7 	 global-step:2067	 l-p:0.10851588845252991
epoch£º103	 i:8 	 global-step:2068	 l-p:0.07809742540121078
epoch£º103	 i:9 	 global-step:2069	 l-p:0.10047977417707443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8580, 3.8593, 3.8581],
        [3.8580, 3.9656, 3.9210],
        [3.8580, 3.9005, 3.8723],
        [3.8580, 4.5399, 4.8656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.09009996056556702 
model_pd.l_d.mean(): -23.296056747436523 
model_pd.lagr.mean(): -23.205957412719727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0502], device='cuda:0')), ('power', tensor([-23.2459], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.09009996056556702
epoch£º104	 i:1 	 global-step:2081	 l-p:0.14705023169517517
epoch£º104	 i:2 	 global-step:2082	 l-p:0.11702724546194077
epoch£º104	 i:3 	 global-step:2083	 l-p:0.13735412061214447
epoch£º104	 i:4 	 global-step:2084	 l-p:0.08804140239953995
epoch£º104	 i:5 	 global-step:2085	 l-p:0.1372406780719757
epoch£º104	 i:6 	 global-step:2086	 l-p:0.12373725324869156
epoch£º104	 i:7 	 global-step:2087	 l-p:0.11372686177492142
epoch£º104	 i:8 	 global-step:2088	 l-p:0.11974280327558517
epoch£º104	 i:9 	 global-step:2089	 l-p:0.09481339156627655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9317, 3.9317, 3.9317],
        [3.9317, 3.9317, 3.9317],
        [3.9317, 4.0920, 4.0481],
        [3.9317, 4.3414, 4.4159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.1232047751545906 
model_pd.l_d.mean(): -22.996509552001953 
model_pd.lagr.mean(): -22.87330436706543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0370], device='cuda:0')), ('power', tensor([-22.9595], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.1232047751545906
epoch£º105	 i:1 	 global-step:2101	 l-p:0.12004836648702621
epoch£º105	 i:2 	 global-step:2102	 l-p:0.11846338212490082
epoch£º105	 i:3 	 global-step:2103	 l-p:0.1163400411605835
epoch£º105	 i:4 	 global-step:2104	 l-p:0.20120422542095184
epoch£º105	 i:5 	 global-step:2105	 l-p:0.0891275554895401
epoch£º105	 i:6 	 global-step:2106	 l-p:0.11992809921503067
epoch£º105	 i:7 	 global-step:2107	 l-p:0.1292690634727478
epoch£º105	 i:8 	 global-step:2108	 l-p:0.12540391087532043
epoch£º105	 i:9 	 global-step:2109	 l-p:0.0493168905377388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6998, 3.7017, 3.6999],
        [3.6998, 4.1926, 4.3570],
        [3.6998, 3.7124, 3.7019],
        [3.6998, 4.3385, 4.6432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): -0.06876260787248611 
model_pd.l_d.mean(): -21.79462432861328 
model_pd.lagr.mean(): -21.863386154174805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1586], device='cuda:0')), ('power', tensor([-21.9532], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:-0.06876260787248611
epoch£º106	 i:1 	 global-step:2121	 l-p:0.10235293954610825
epoch£º106	 i:2 	 global-step:2122	 l-p:0.13475510478019714
epoch£º106	 i:3 	 global-step:2123	 l-p:0.04878208041191101
epoch£º106	 i:4 	 global-step:2124	 l-p:0.21499866247177124
epoch£º106	 i:5 	 global-step:2125	 l-p:0.12681850790977478
epoch£º106	 i:6 	 global-step:2126	 l-p:0.11435358971357346
epoch£º106	 i:7 	 global-step:2127	 l-p:0.10475572943687439
epoch£º106	 i:8 	 global-step:2128	 l-p:0.13096551597118378
epoch£º106	 i:9 	 global-step:2129	 l-p:0.06422867625951767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8914, 3.8934, 3.8915],
        [3.8914, 3.8953, 3.8917],
        [3.8914, 4.6687, 5.0926],
        [3.8914, 4.5291, 4.8066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.10263064503669739 
model_pd.l_d.mean(): -22.378368377685547 
model_pd.lagr.mean(): -22.275737762451172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0297], device='cuda:0')), ('power', tensor([-22.4081], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.10263064503669739
epoch£º107	 i:1 	 global-step:2141	 l-p:0.08764467388391495
epoch£º107	 i:2 	 global-step:2142	 l-p:0.07684485614299774
epoch£º107	 i:3 	 global-step:2143	 l-p:0.14432869851589203
epoch£º107	 i:4 	 global-step:2144	 l-p:0.10830952227115631
epoch£º107	 i:5 	 global-step:2145	 l-p:0.1185847669839859
epoch£º107	 i:6 	 global-step:2146	 l-p:0.11390131711959839
epoch£º107	 i:7 	 global-step:2147	 l-p:0.08993817120790482
epoch£º107	 i:8 	 global-step:2148	 l-p:0.1364833563566208
epoch£º107	 i:9 	 global-step:2149	 l-p:0.10915306210517883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9129, 3.9210, 3.9139],
        [3.9129, 3.9130, 3.9129],
        [3.9129, 3.9882, 3.9483],
        [3.9129, 3.9129, 3.9129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.09298393130302429 
model_pd.l_d.mean(): -22.04151725769043 
model_pd.lagr.mean(): -21.94853401184082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0369], device='cuda:0')), ('power', tensor([-22.0784], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.09298393130302429
epoch£º108	 i:1 	 global-step:2161	 l-p:0.11616536974906921
epoch£º108	 i:2 	 global-step:2162	 l-p:0.11655746400356293
epoch£º108	 i:3 	 global-step:2163	 l-p:0.10050580650568008
epoch£º108	 i:4 	 global-step:2164	 l-p:0.12697246670722961
epoch£º108	 i:5 	 global-step:2165	 l-p:0.8659829497337341
epoch£º108	 i:6 	 global-step:2166	 l-p:0.11752988398075104
epoch£º108	 i:7 	 global-step:2167	 l-p:-0.23270805180072784
epoch£º108	 i:8 	 global-step:2168	 l-p:0.1349027007818222
epoch£º108	 i:9 	 global-step:2169	 l-p:-0.08250850439071655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7280, 3.7861, 3.7525],
        [3.7280, 3.7291, 3.7281],
        [3.7280, 4.6133, 5.1912],
        [3.7280, 3.7280, 3.7280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): -0.10226588696241379 
model_pd.l_d.mean(): -22.826656341552734 
model_pd.lagr.mean(): -22.928922653198242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0634], device='cuda:0')), ('power', tensor([-22.8900], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:-0.10226588696241379
epoch£º109	 i:1 	 global-step:2181	 l-p:0.09926696866750717
epoch£º109	 i:2 	 global-step:2182	 l-p:0.1397099792957306
epoch£º109	 i:3 	 global-step:2183	 l-p:0.11705856025218964
epoch£º109	 i:4 	 global-step:2184	 l-p:0.11852796375751495
epoch£º109	 i:5 	 global-step:2185	 l-p:0.10878883302211761
epoch£º109	 i:6 	 global-step:2186	 l-p:0.10362543165683746
epoch£º109	 i:7 	 global-step:2187	 l-p:0.12009744346141815
epoch£º109	 i:8 	 global-step:2188	 l-p:0.1427045613527298
epoch£º109	 i:9 	 global-step:2189	 l-p:0.10022871941328049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9386, 3.9390, 3.9386],
        [3.9386, 3.9386, 3.9386],
        [3.9386, 3.9394, 3.9386],
        [3.9386, 3.9545, 3.9415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.11420291662216187 
model_pd.l_d.mean(): -23.32093048095703 
model_pd.lagr.mean(): -23.206727981567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0865], device='cuda:0')), ('power', tensor([-23.2345], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.11420291662216187
epoch£º110	 i:1 	 global-step:2201	 l-p:0.12038429081439972
epoch£º110	 i:2 	 global-step:2202	 l-p:0.3664831221103668
epoch£º110	 i:3 	 global-step:2203	 l-p:0.11331561952829361
epoch£º110	 i:4 	 global-step:2204	 l-p:0.11732528358697891
epoch£º110	 i:5 	 global-step:2205	 l-p:0.11052197962999344
epoch£º110	 i:6 	 global-step:2206	 l-p:0.13122089207172394
epoch£º110	 i:7 	 global-step:2207	 l-p:0.08706396073102951
epoch£º110	 i:8 	 global-step:2208	 l-p:0.20550265908241272
epoch£º110	 i:9 	 global-step:2209	 l-p:0.12869730591773987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5490, 3.5518, 3.5492],
        [3.5490, 3.9993, 4.1445],
        [3.5490, 3.8890, 3.9466],
        [3.5490, 4.1243, 4.3873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.10483770072460175 
model_pd.l_d.mean(): -22.891603469848633 
model_pd.lagr.mean(): -22.786766052246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1292], device='cuda:0')), ('power', tensor([-23.0209], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.10483770072460175
epoch£º111	 i:1 	 global-step:2221	 l-p:0.1136557012796402
epoch£º111	 i:2 	 global-step:2222	 l-p:0.1339876651763916
epoch£º111	 i:3 	 global-step:2223	 l-p:0.24884289503097534
epoch£º111	 i:4 	 global-step:2224	 l-p:-1.056609869003296
epoch£º111	 i:5 	 global-step:2225	 l-p:0.11015984416007996
epoch£º111	 i:6 	 global-step:2226	 l-p:0.10903947800397873
epoch£º111	 i:7 	 global-step:2227	 l-p:0.07517417520284653
epoch£º111	 i:8 	 global-step:2228	 l-p:0.09829942137002945
epoch£º111	 i:9 	 global-step:2229	 l-p:0.145475372672081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7701, 3.8154, 3.7865],
        [3.7701, 3.8140, 3.7857],
        [3.7701, 3.7701, 3.7701],
        [3.7701, 3.7857, 3.7731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.10465942323207855 
model_pd.l_d.mean(): -23.19991111755371 
model_pd.lagr.mean(): -23.095251083374023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0059], device='cuda:0')), ('power', tensor([-23.2059], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.10465942323207855
epoch£º112	 i:1 	 global-step:2241	 l-p:0.11619529128074646
epoch£º112	 i:2 	 global-step:2242	 l-p:0.09521280229091644
epoch£º112	 i:3 	 global-step:2243	 l-p:0.11255922168493271
epoch£º112	 i:4 	 global-step:2244	 l-p:0.1241852343082428
epoch£º112	 i:5 	 global-step:2245	 l-p:0.10036561638116837
epoch£º112	 i:6 	 global-step:2246	 l-p:0.16347987949848175
epoch£º112	 i:7 	 global-step:2247	 l-p:0.04976162686944008
epoch£º112	 i:8 	 global-step:2248	 l-p:0.1210285946726799
epoch£º112	 i:9 	 global-step:2249	 l-p:0.10628693550825119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7979, 3.7979, 3.7979],
        [3.7979, 4.3021, 4.4684],
        [3.7979, 3.8436, 3.8144],
        [3.7979, 3.7979, 3.7979]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.10228860378265381 
model_pd.l_d.mean(): -22.468246459960938 
model_pd.lagr.mean(): -22.365957260131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0539], device='cuda:0')), ('power', tensor([-22.5221], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.10228860378265381
epoch£º113	 i:1 	 global-step:2261	 l-p:0.10609746724367142
epoch£º113	 i:2 	 global-step:2262	 l-p:0.11385905742645264
epoch£º113	 i:3 	 global-step:2263	 l-p:0.09837380796670914
epoch£º113	 i:4 	 global-step:2264	 l-p:0.1000964492559433
epoch£º113	 i:5 	 global-step:2265	 l-p:0.14548350870609283
epoch£º113	 i:6 	 global-step:2266	 l-p:0.205641508102417
epoch£º113	 i:7 	 global-step:2267	 l-p:0.0872134193778038
epoch£º113	 i:8 	 global-step:2268	 l-p:0.10563813149929047
epoch£º113	 i:9 	 global-step:2269	 l-p:0.12427806854248047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6845, 3.6845, 3.6845],
        [3.6845, 3.6856, 3.6846],
        [3.6845, 3.6851, 3.6846],
        [3.6845, 4.3068, 4.6003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): -0.022479766979813576 
model_pd.l_d.mean(): -22.525169372558594 
model_pd.lagr.mean(): -22.547649383544922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1230], device='cuda:0')), ('power', tensor([-22.6482], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:-0.022479766979813576
epoch£º114	 i:1 	 global-step:2281	 l-p:0.11365535110235214
epoch£º114	 i:2 	 global-step:2282	 l-p:-0.0446966178715229
epoch£º114	 i:3 	 global-step:2283	 l-p:0.12300866842269897
epoch£º114	 i:4 	 global-step:2284	 l-p:-0.03387892246246338
epoch£º114	 i:5 	 global-step:2285	 l-p:0.11968988180160522
epoch£º114	 i:6 	 global-step:2286	 l-p:0.10362228006124496
epoch£º114	 i:7 	 global-step:2287	 l-p:0.15077641606330872
epoch£º114	 i:8 	 global-step:2288	 l-p:0.1157793179154396
epoch£º114	 i:9 	 global-step:2289	 l-p:0.05972881242632866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9318, 3.9318, 3.9318],
        [3.9318, 3.9318, 3.9318],
        [3.9318, 3.9318, 3.9318],
        [3.9318, 4.0855, 4.0419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.09620637446641922 
model_pd.l_d.mean(): -22.939884185791016 
model_pd.lagr.mean(): -22.843677520751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0247], device='cuda:0')), ('power', tensor([-22.9152], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.09620637446641922
epoch£º115	 i:1 	 global-step:2301	 l-p:0.13203038275241852
epoch£º115	 i:2 	 global-step:2302	 l-p:0.11104072630405426
epoch£º115	 i:3 	 global-step:2303	 l-p:0.11126131564378738
epoch£º115	 i:4 	 global-step:2304	 l-p:0.11566898971796036
epoch£º115	 i:5 	 global-step:2305	 l-p:0.1585851013660431
epoch£º115	 i:6 	 global-step:2306	 l-p:0.1109001487493515
epoch£º115	 i:7 	 global-step:2307	 l-p:0.19891943037509918
epoch£º115	 i:8 	 global-step:2308	 l-p:0.08374936133623123
epoch£º115	 i:9 	 global-step:2309	 l-p:0.11827462166547775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8667, 3.9267, 3.8919],
        [3.8667, 3.8943, 3.8740],
        [3.8667, 3.8667, 3.8667],
        [3.8667, 4.0188, 3.9768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.13731707632541656 
model_pd.l_d.mean(): -22.614334106445312 
model_pd.lagr.mean(): -22.47701644897461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0363], device='cuda:0')), ('power', tensor([-22.6506], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.13731707632541656
epoch£º116	 i:1 	 global-step:2321	 l-p:0.10986018925905228
epoch£º116	 i:2 	 global-step:2322	 l-p:0.1164223849773407
epoch£º116	 i:3 	 global-step:2323	 l-p:0.11741622537374496
epoch£º116	 i:4 	 global-step:2324	 l-p:0.13160911202430725
epoch£º116	 i:5 	 global-step:2325	 l-p:0.12992271780967712
epoch£º116	 i:6 	 global-step:2326	 l-p:0.2868150770664215
epoch£º116	 i:7 	 global-step:2327	 l-p:0.11603758484125137
epoch£º116	 i:8 	 global-step:2328	 l-p:0.08054140955209732
epoch£º116	 i:9 	 global-step:2329	 l-p:0.09949685633182526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7112, 3.7118, 3.7112],
        [3.7112, 3.7523, 3.7255],
        [3.7112, 3.7112, 3.7112],
        [3.7112, 3.7112, 3.7112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.10666132718324661 
model_pd.l_d.mean(): -23.394264221191406 
model_pd.lagr.mean(): -23.2876033782959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0037], device='cuda:0')), ('power', tensor([-23.3905], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.10666132718324661
epoch£º117	 i:1 	 global-step:2341	 l-p:0.11879927664995193
epoch£º117	 i:2 	 global-step:2342	 l-p:0.1261952817440033
epoch£º117	 i:3 	 global-step:2343	 l-p:0.11628992855548859
epoch£º117	 i:4 	 global-step:2344	 l-p:0.12500232458114624
epoch£º117	 i:5 	 global-step:2345	 l-p:0.0879964604973793
epoch£º117	 i:6 	 global-step:2346	 l-p:-0.11478160321712494
epoch£º117	 i:7 	 global-step:2347	 l-p:0.10869578272104263
epoch£º117	 i:8 	 global-step:2348	 l-p:-0.8411127924919128
epoch£º117	 i:9 	 global-step:2349	 l-p:0.12389558553695679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5617, 4.3669, 4.8840],
        [3.5617, 3.7663, 3.7486],
        [3.5617, 3.5748, 3.5641],
        [3.5617, 3.5617, 3.5617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.25576457381248474 
model_pd.l_d.mean(): -22.64025115966797 
model_pd.lagr.mean(): -22.38448715209961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1510], device='cuda:0')), ('power', tensor([-22.7913], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.25576457381248474
epoch£º118	 i:1 	 global-step:2361	 l-p:0.11990564316511154
epoch£º118	 i:2 	 global-step:2362	 l-p:0.22228297591209412
epoch£º118	 i:3 	 global-step:2363	 l-p:0.08909589797258377
epoch£º118	 i:4 	 global-step:2364	 l-p:0.10169531404972076
epoch£º118	 i:5 	 global-step:2365	 l-p:0.13703836500644684
epoch£º118	 i:6 	 global-step:2366	 l-p:0.10084283351898193
epoch£º118	 i:7 	 global-step:2367	 l-p:0.006378669757395983
epoch£º118	 i:8 	 global-step:2368	 l-p:0.25820133090019226
epoch£º118	 i:9 	 global-step:2369	 l-p:0.017547402530908585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8419, 3.9868, 3.9448],
        [3.8419, 3.8479, 3.8426],
        [3.8419, 4.2415, 4.3207],
        [3.8419, 4.3423, 4.5028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.11830102652311325 
model_pd.l_d.mean(): -23.28133201599121 
model_pd.lagr.mean(): -23.16303062438965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0366], device='cuda:0')), ('power', tensor([-23.2448], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.11830102652311325
epoch£º119	 i:1 	 global-step:2381	 l-p:0.07916940748691559
epoch£º119	 i:2 	 global-step:2382	 l-p:0.10699062794446945
epoch£º119	 i:3 	 global-step:2383	 l-p:0.06222817301750183
epoch£º119	 i:4 	 global-step:2384	 l-p:0.13563506305217743
epoch£º119	 i:5 	 global-step:2385	 l-p:0.13675348460674286
epoch£º119	 i:6 	 global-step:2386	 l-p:0.09594321995973587
epoch£º119	 i:7 	 global-step:2387	 l-p:0.1114039421081543
epoch£º119	 i:8 	 global-step:2388	 l-p:0.10560570657253265
epoch£º119	 i:9 	 global-step:2389	 l-p:0.10319961607456207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9761, 3.9761, 3.9761],
        [3.9761, 3.9762, 3.9761],
        [3.9761, 3.9768, 3.9761],
        [3.9761, 4.2977, 4.3160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.10571889579296112 
model_pd.l_d.mean(): -22.956823348999023 
model_pd.lagr.mean(): -22.851104736328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0390], device='cuda:0')), ('power', tensor([-22.9178], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.10571889579296112
epoch£º120	 i:1 	 global-step:2401	 l-p:0.031582310795784
epoch£º120	 i:2 	 global-step:2402	 l-p:0.08987655490636826
epoch£º120	 i:3 	 global-step:2403	 l-p:0.11227542906999588
epoch£º120	 i:4 	 global-step:2404	 l-p:0.11272363364696503
epoch£º120	 i:5 	 global-step:2405	 l-p:0.11335655301809311
epoch£º120	 i:6 	 global-step:2406	 l-p:6.3337626457214355
epoch£º120	 i:7 	 global-step:2407	 l-p:1.0621614456176758
epoch£º120	 i:8 	 global-step:2408	 l-p:0.12059368193149567
epoch£º120	 i:9 	 global-step:2409	 l-p:-0.28619658946990967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7146, 3.8492, 3.8092],
        [3.7146, 3.7340, 3.7190],
        [3.7146, 3.7146, 3.7146],
        [3.7146, 3.7150, 3.7147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.11750249564647675 
model_pd.l_d.mean(): -22.507320404052734 
model_pd.lagr.mean(): -22.38981819152832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0386], device='cuda:0')), ('power', tensor([-22.5459], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.11750249564647675
epoch£º121	 i:1 	 global-step:2421	 l-p:-0.23293296992778778
epoch£º121	 i:2 	 global-step:2422	 l-p:0.08252768963575363
epoch£º121	 i:3 	 global-step:2423	 l-p:0.08464683592319489
epoch£º121	 i:4 	 global-step:2424	 l-p:0.19097529351711273
epoch£º121	 i:5 	 global-step:2425	 l-p:0.14462895691394806
epoch£º121	 i:6 	 global-step:2426	 l-p:0.12104188650846481
epoch£º121	 i:7 	 global-step:2427	 l-p:0.10753904283046722
epoch£º121	 i:8 	 global-step:2428	 l-p:0.11715737730264664
epoch£º121	 i:9 	 global-step:2429	 l-p:0.08425796031951904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8358, 3.8554, 3.8401],
        [3.8358, 3.8429, 3.8366],
        [3.8358, 3.8358, 3.8358],
        [3.8358, 3.8408, 3.8363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.10207951068878174 
model_pd.l_d.mean(): -21.880268096923828 
model_pd.lagr.mean(): -21.778188705444336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0748], device='cuda:0')), ('power', tensor([-21.9551], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.10207951068878174
epoch£º122	 i:1 	 global-step:2441	 l-p:0.13634738326072693
epoch£º122	 i:2 	 global-step:2442	 l-p:0.08995217829942703
epoch£º122	 i:3 	 global-step:2443	 l-p:0.11269481480121613
epoch£º122	 i:4 	 global-step:2444	 l-p:0.11822442710399628
epoch£º122	 i:5 	 global-step:2445	 l-p:0.09592782706022263
epoch£º122	 i:6 	 global-step:2446	 l-p:0.12012404948472977
epoch£º122	 i:7 	 global-step:2447	 l-p:0.11789918690919876
epoch£º122	 i:8 	 global-step:2448	 l-p:0.05720971152186394
epoch£º122	 i:9 	 global-step:2449	 l-p:0.16137142479419708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5269, 3.5271, 3.5269],
        [3.5269, 3.5607, 3.5381],
        [3.5269, 3.5306, 3.5272],
        [3.5269, 3.5269, 3.5269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): -0.17130057513713837 
model_pd.l_d.mean(): -23.200481414794922 
model_pd.lagr.mean(): -23.371782302856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1017], device='cuda:0')), ('power', tensor([-23.3022], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:-0.17130057513713837
epoch£º123	 i:1 	 global-step:2461	 l-p:0.1436089128255844
epoch£º123	 i:2 	 global-step:2462	 l-p:0.3547962009906769
epoch£º123	 i:3 	 global-step:2463	 l-p:0.11393037438392639
epoch£º123	 i:4 	 global-step:2464	 l-p:0.12844714522361755
epoch£º123	 i:5 	 global-step:2465	 l-p:0.05364718288183212
epoch£º123	 i:6 	 global-step:2466	 l-p:0.12646111845970154
epoch£º123	 i:7 	 global-step:2467	 l-p:0.0886065661907196
epoch£º123	 i:8 	 global-step:2468	 l-p:0.2507013976573944
epoch£º123	 i:9 	 global-step:2469	 l-p:0.10795535147190094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8699, 3.8702, 3.8699],
        [3.8699, 4.4332, 4.6487],
        [3.8699, 3.8969, 3.8771],
        [3.8699, 3.9099, 3.8833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.08948265016078949 
model_pd.l_d.mean(): -22.87002944946289 
model_pd.lagr.mean(): -22.780546188354492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0013], device='cuda:0')), ('power', tensor([-22.8688], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.08948265016078949
epoch£º124	 i:1 	 global-step:2481	 l-p:0.08536247164011002
epoch£º124	 i:2 	 global-step:2482	 l-p:0.1297403872013092
epoch£º124	 i:3 	 global-step:2483	 l-p:0.12194608151912689
epoch£º124	 i:4 	 global-step:2484	 l-p:0.10319820791482925
epoch£º124	 i:5 	 global-step:2485	 l-p:0.13671398162841797
epoch£º124	 i:6 	 global-step:2486	 l-p:0.11708153784275055
epoch£º124	 i:7 	 global-step:2487	 l-p:0.09569376707077026
epoch£º124	 i:8 	 global-step:2488	 l-p:0.10713008046150208
epoch£º124	 i:9 	 global-step:2489	 l-p:0.09420961141586304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8967, 3.9067, 3.8982],
        [3.8967, 4.8613, 5.5096],
        [3.8967, 3.9094, 3.8988],
        [3.8967, 4.2041, 4.2204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.11136975884437561 
model_pd.l_d.mean(): -22.16746711730957 
model_pd.lagr.mean(): -22.05609703063965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0217], device='cuda:0')), ('power', tensor([-22.1892], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.11136975884437561
epoch£º125	 i:1 	 global-step:2501	 l-p:0.14130878448486328
epoch£º125	 i:2 	 global-step:2502	 l-p:0.08830636739730835
epoch£º125	 i:3 	 global-step:2503	 l-p:0.18536148965358734
epoch£º125	 i:4 	 global-step:2504	 l-p:0.1256762593984604
epoch£º125	 i:5 	 global-step:2505	 l-p:0.1337384581565857
epoch£º125	 i:6 	 global-step:2506	 l-p:0.05466124042868614
epoch£º125	 i:7 	 global-step:2507	 l-p:-1.0437898635864258
epoch£º125	 i:8 	 global-step:2508	 l-p:0.1182267814874649
epoch£º125	 i:9 	 global-step:2509	 l-p:0.11997220665216446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7827, 4.2992, 4.4829],
        [3.7827, 4.1103, 4.1464],
        [3.7827, 3.7827, 3.7827],
        [3.7827, 3.7867, 3.7831]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.11241712421178818 
model_pd.l_d.mean(): -23.260892868041992 
model_pd.lagr.mean(): -23.148475646972656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0199], device='cuda:0')), ('power', tensor([-23.2410], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.11241712421178818
epoch£º126	 i:1 	 global-step:2521	 l-p:0.12950143218040466
epoch£º126	 i:2 	 global-step:2522	 l-p:0.10631310194730759
epoch£º126	 i:3 	 global-step:2523	 l-p:0.1147504523396492
epoch£º126	 i:4 	 global-step:2524	 l-p:0.10166198760271072
epoch£º126	 i:5 	 global-step:2525	 l-p:0.16882629692554474
epoch£º126	 i:6 	 global-step:2526	 l-p:0.12264614552259445
epoch£º126	 i:7 	 global-step:2527	 l-p:0.1607329547405243
epoch£º126	 i:8 	 global-step:2528	 l-p:0.098896823823452
epoch£º126	 i:9 	 global-step:2529	 l-p:0.07422108203172684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[3.6856, 3.7678, 3.7297],
        [3.6856, 3.7419, 3.7098],
        [3.6856, 3.8241, 3.7859],
        [3.6856, 4.2606, 4.5118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.1255645602941513 
model_pd.l_d.mean(): -22.63360023498535 
model_pd.lagr.mean(): -22.50803565979004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0748], device='cuda:0')), ('power', tensor([-22.7084], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.1255645602941513
epoch£º127	 i:1 	 global-step:2541	 l-p:0.09732697159051895
epoch£º127	 i:2 	 global-step:2542	 l-p:0.11459902673959732
epoch£º127	 i:3 	 global-step:2543	 l-p:0.13197258114814758
epoch£º127	 i:4 	 global-step:2544	 l-p:0.1855582743883133
epoch£º127	 i:5 	 global-step:2545	 l-p:0.18727168440818787
epoch£º127	 i:6 	 global-step:2546	 l-p:0.12232805788516998
epoch£º127	 i:7 	 global-step:2547	 l-p:0.10044977068901062
epoch£º127	 i:8 	 global-step:2548	 l-p:0.08086620271205902
epoch£º127	 i:9 	 global-step:2549	 l-p:0.11593223363161087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9091, 4.2406, 4.2710],
        [3.9091, 4.0391, 3.9954],
        [3.9091, 3.9111, 3.9092],
        [3.9091, 3.9096, 3.9091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.0843576192855835 
model_pd.l_d.mean(): -22.580577850341797 
model_pd.lagr.mean(): -22.496219635009766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0333], device='cuda:0')), ('power', tensor([-22.6138], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.0843576192855835
epoch£º128	 i:1 	 global-step:2561	 l-p:0.10139817744493484
epoch£º128	 i:2 	 global-step:2562	 l-p:0.11855602264404297
epoch£º128	 i:3 	 global-step:2563	 l-p:0.11153211444616318
epoch£º128	 i:4 	 global-step:2564	 l-p:0.11692575365304947
epoch£º128	 i:5 	 global-step:2565	 l-p:0.10788272321224213
epoch£º128	 i:6 	 global-step:2566	 l-p:0.1847178190946579
epoch£º128	 i:7 	 global-step:2567	 l-p:0.09899581968784332
epoch£º128	 i:8 	 global-step:2568	 l-p:0.030914029106497765
epoch£º128	 i:9 	 global-step:2569	 l-p:0.13931237161159515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7132, 3.7138, 3.7133],
        [3.7132, 3.7255, 3.7154],
        [3.7132, 3.7132, 3.7132],
        [3.7132, 3.8058, 3.7663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): -0.040835075080394745 
model_pd.l_d.mean(): -22.779644012451172 
model_pd.lagr.mean(): -22.820478439331055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0950], device='cuda:0')), ('power', tensor([-22.8746], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:-0.040835075080394745
epoch£º129	 i:1 	 global-step:2581	 l-p:0.08666226267814636
epoch£º129	 i:2 	 global-step:2582	 l-p:0.11724217981100082
epoch£º129	 i:3 	 global-step:2583	 l-p:1.0369089841842651
epoch£º129	 i:4 	 global-step:2584	 l-p:0.10915645211935043
epoch£º129	 i:5 	 global-step:2585	 l-p:0.11633258312940598
epoch£º129	 i:6 	 global-step:2586	 l-p:0.01744246296584606
epoch£º129	 i:7 	 global-step:2587	 l-p:0.11883976310491562
epoch£º129	 i:8 	 global-step:2588	 l-p:0.11244121193885803
epoch£º129	 i:9 	 global-step:2589	 l-p:0.2453410029411316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8005, 3.9015, 3.8601],
        [3.8005, 3.8005, 3.8005],
        [3.8005, 3.8005, 3.8005],
        [3.8005, 3.8012, 3.8005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.19658063352108002 
model_pd.l_d.mean(): -21.98043441772461 
model_pd.lagr.mean(): -21.78385353088379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0887], device='cuda:0')), ('power', tensor([-22.0692], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.19658063352108002
epoch£º130	 i:1 	 global-step:2601	 l-p:0.11594556272029877
epoch£º130	 i:2 	 global-step:2602	 l-p:0.11242282390594482
epoch£º130	 i:3 	 global-step:2603	 l-p:0.12123152613639832
epoch£º130	 i:4 	 global-step:2604	 l-p:-0.01912214793264866
epoch£º130	 i:5 	 global-step:2605	 l-p:0.20547603070735931
epoch£º130	 i:6 	 global-step:2606	 l-p:0.08400692045688629
epoch£º130	 i:7 	 global-step:2607	 l-p:0.12106103450059891
epoch£º130	 i:8 	 global-step:2608	 l-p:0.14949198067188263
epoch£º130	 i:9 	 global-step:2609	 l-p:0.12709487974643707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8239, 3.8239, 3.8239],
        [3.8239, 3.8791, 3.8466],
        [3.8239, 3.8239, 3.8239],
        [3.8239, 3.8239, 3.8239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.12351419031620026 
model_pd.l_d.mean(): -22.606332778930664 
model_pd.lagr.mean(): -22.482818603515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0499], device='cuda:0')), ('power', tensor([-22.6562], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.12351419031620026
epoch£º131	 i:1 	 global-step:2621	 l-p:0.11487293243408203
epoch£º131	 i:2 	 global-step:2622	 l-p:0.1209564208984375
epoch£º131	 i:3 	 global-step:2623	 l-p:0.1124885156750679
epoch£º131	 i:4 	 global-step:2624	 l-p:0.07660579681396484
epoch£º131	 i:5 	 global-step:2625	 l-p:0.117699533700943
epoch£º131	 i:6 	 global-step:2626	 l-p:0.047650448977947235
epoch£º131	 i:7 	 global-step:2627	 l-p:0.1272352784872055
epoch£º131	 i:8 	 global-step:2628	 l-p:0.1558159440755844
epoch£º131	 i:9 	 global-step:2629	 l-p:0.19664539396762848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6364, 3.6364, 3.6364],
        [3.6364, 4.3667, 4.7903],
        [3.6364, 3.6364, 3.6364],
        [3.6364, 3.9182, 3.9377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.10925102978944778 
model_pd.l_d.mean(): -22.219863891601562 
model_pd.lagr.mean(): -22.110612869262695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1521], device='cuda:0')), ('power', tensor([-22.3719], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.10925102978944778
epoch£º132	 i:1 	 global-step:2641	 l-p:0.10596978664398193
epoch£º132	 i:2 	 global-step:2642	 l-p:0.12193748354911804
epoch£º132	 i:3 	 global-step:2643	 l-p:0.11760003864765167
epoch£º132	 i:4 	 global-step:2644	 l-p:0.3025473654270172
epoch£º132	 i:5 	 global-step:2645	 l-p:0.1099405437707901
epoch£º132	 i:6 	 global-step:2646	 l-p:0.12712661921977997
epoch£º132	 i:7 	 global-step:2647	 l-p:0.11496899276971817
epoch£º132	 i:8 	 global-step:2648	 l-p:0.10249560326337814
epoch£º132	 i:9 	 global-step:2649	 l-p:0.11873344331979752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7901, 3.7901, 3.7901],
        [3.7901, 3.7901, 3.7901],
        [3.7901, 4.0074, 3.9875],
        [3.7901, 3.8689, 3.8306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.11517421156167984 
model_pd.l_d.mean(): -21.85698699951172 
model_pd.lagr.mean(): -21.74181365966797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0891], device='cuda:0')), ('power', tensor([-21.9461], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.11517421156167984
epoch£º133	 i:1 	 global-step:2661	 l-p:0.11889562010765076
epoch£º133	 i:2 	 global-step:2662	 l-p:0.13558663427829742
epoch£º133	 i:3 	 global-step:2663	 l-p:0.03726465627551079
epoch£º133	 i:4 	 global-step:2664	 l-p:0.10781626403331757
epoch£º133	 i:5 	 global-step:2665	 l-p:0.13200916349887848
epoch£º133	 i:6 	 global-step:2666	 l-p:0.0990380272269249
epoch£º133	 i:7 	 global-step:2667	 l-p:0.025383390486240387
epoch£º133	 i:8 	 global-step:2668	 l-p:0.12612541019916534
epoch£º133	 i:9 	 global-step:2669	 l-p:0.11930440366268158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7179, 3.9170, 3.8935],
        [3.7179, 3.7715, 3.7403],
        [3.7179, 3.7285, 3.7197],
        [3.7179, 3.7196, 3.7180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.12422708421945572 
model_pd.l_d.mean(): -23.06687355041504 
model_pd.lagr.mean(): -22.942646026611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0405], device='cuda:0')), ('power', tensor([-23.1073], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.12422708421945572
epoch£º134	 i:1 	 global-step:2681	 l-p:-0.0022844027262181044
epoch£º134	 i:2 	 global-step:2682	 l-p:0.10841153562068939
epoch£º134	 i:3 	 global-step:2683	 l-p:0.10324159264564514
epoch£º134	 i:4 	 global-step:2684	 l-p:0.14356698095798492
epoch£º134	 i:5 	 global-step:2685	 l-p:0.13669037818908691
epoch£º134	 i:6 	 global-step:2686	 l-p:0.13580574095249176
epoch£º134	 i:7 	 global-step:2687	 l-p:-0.05434488132596016
epoch£º134	 i:8 	 global-step:2688	 l-p:0.08558117598295212
epoch£º134	 i:9 	 global-step:2689	 l-p:0.11345285177230835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7779, 3.7779, 3.7779],
        [3.7779, 4.5688, 5.0384],
        [3.7779, 3.8660, 3.8266],
        [3.7779, 4.4328, 4.7552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.2239197939634323 
model_pd.l_d.mean(): -21.9544734954834 
model_pd.lagr.mean(): -21.730554580688477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1263], device='cuda:0')), ('power', tensor([-22.0808], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.2239197939634323
epoch£º135	 i:1 	 global-step:2701	 l-p:0.17966096103191376
epoch£º135	 i:2 	 global-step:2702	 l-p:0.12908722460269928
epoch£º135	 i:3 	 global-step:2703	 l-p:0.11725559085607529
epoch£º135	 i:4 	 global-step:2704	 l-p:0.09521865844726562
epoch£º135	 i:5 	 global-step:2705	 l-p:0.1503690630197525
epoch£º135	 i:6 	 global-step:2706	 l-p:0.26355740427970886
epoch£º135	 i:7 	 global-step:2707	 l-p:0.06692634522914886
epoch£º135	 i:8 	 global-step:2708	 l-p:0.07404748350381851
epoch£º135	 i:9 	 global-step:2709	 l-p:0.10964652895927429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0020, 4.0021, 4.0020],
        [4.0020, 4.1169, 4.0722],
        [4.0020, 4.0026, 4.0020],
        [4.0020, 4.1400, 4.0956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.11145596951246262 
model_pd.l_d.mean(): -23.392847061157227 
model_pd.lagr.mean(): -23.281391143798828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1180], device='cuda:0')), ('power', tensor([-23.2748], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.11145596951246262
epoch£º136	 i:1 	 global-step:2721	 l-p:0.11454235762357712
epoch£º136	 i:2 	 global-step:2722	 l-p:-2.9909749031066895
epoch£º136	 i:3 	 global-step:2723	 l-p:0.10186605900526047
epoch£º136	 i:4 	 global-step:2724	 l-p:0.12532685697078705
epoch£º136	 i:5 	 global-step:2725	 l-p:0.151931032538414
epoch£º136	 i:6 	 global-step:2726	 l-p:0.13263268768787384
epoch£º136	 i:7 	 global-step:2727	 l-p:0.09925799816846848
epoch£º136	 i:8 	 global-step:2728	 l-p:0.11528583616018295
epoch£º136	 i:9 	 global-step:2729	 l-p:0.10131010413169861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8178, 3.8178, 3.8178],
        [3.8178, 3.8298, 3.8199],
        [3.8178, 3.8178, 3.8178],
        [3.8178, 4.5443, 4.9369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.12423578649759293 
model_pd.l_d.mean(): -23.248811721801758 
model_pd.lagr.mean(): -23.124576568603516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0174], device='cuda:0')), ('power', tensor([-23.2315], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.12423578649759293
epoch£º137	 i:1 	 global-step:2741	 l-p:0.11522866040468216
epoch£º137	 i:2 	 global-step:2742	 l-p:-0.05739939212799072
epoch£º137	 i:3 	 global-step:2743	 l-p:0.07944230735301971
epoch£º137	 i:4 	 global-step:2744	 l-p:0.14506641030311584
epoch£º137	 i:5 	 global-step:2745	 l-p:0.11004799604415894
epoch£º137	 i:6 	 global-step:2746	 l-p:0.1199740469455719
epoch£º137	 i:7 	 global-step:2747	 l-p:0.14605316519737244
epoch£º137	 i:8 	 global-step:2748	 l-p:0.12054311484098434
epoch£º137	 i:9 	 global-step:2749	 l-p:0.16871029138565063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6769, 3.7239, 3.6954],
        [3.6769, 3.6769, 3.6769],
        [3.6769, 3.6769, 3.6769],
        [3.6769, 3.6769, 3.6769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.11404886096715927 
model_pd.l_d.mean(): -23.29007339477539 
model_pd.lagr.mean(): -23.176025390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0261], device='cuda:0')), ('power', tensor([-23.3162], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.11404886096715927
epoch£º138	 i:1 	 global-step:2761	 l-p:0.10489299893379211
epoch£º138	 i:2 	 global-step:2762	 l-p:0.0969027429819107
epoch£º138	 i:3 	 global-step:2763	 l-p:0.08012287318706512
epoch£º138	 i:4 	 global-step:2764	 l-p:0.07840482890605927
epoch£º138	 i:5 	 global-step:2765	 l-p:0.10887844115495682
epoch£º138	 i:6 	 global-step:2766	 l-p:0.12046851217746735
epoch£º138	 i:7 	 global-step:2767	 l-p:0.254982590675354
epoch£º138	 i:8 	 global-step:2768	 l-p:0.10419021546840668
epoch£º138	 i:9 	 global-step:2769	 l-p:0.13490280508995056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8301, 4.5571, 4.9492],
        [3.8301, 4.6964, 5.2450],
        [3.8301, 3.8301, 3.8301],
        [3.8301, 3.8317, 3.8302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.09234868735074997 
model_pd.l_d.mean(): -21.94554328918457 
model_pd.lagr.mean(): -21.853195190429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0853], device='cuda:0')), ('power', tensor([-22.0309], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.09234868735074997
epoch£º139	 i:1 	 global-step:2781	 l-p:0.11624746024608612
epoch£º139	 i:2 	 global-step:2782	 l-p:0.10699844360351562
epoch£º139	 i:3 	 global-step:2783	 l-p:0.19994069635868073
epoch£º139	 i:4 	 global-step:2784	 l-p:0.1265299767255783
epoch£º139	 i:5 	 global-step:2785	 l-p:0.13256031274795532
epoch£º139	 i:6 	 global-step:2786	 l-p:0.11590424925088882
epoch£º139	 i:7 	 global-step:2787	 l-p:6.713968276977539
epoch£º139	 i:8 	 global-step:2788	 l-p:0.11248767375946045
epoch£º139	 i:9 	 global-step:2789	 l-p:-0.11184781789779663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7336, 4.4703, 4.8885],
        [3.7336, 3.7576, 3.7399],
        [3.7336, 4.1603, 4.2771],
        [3.7336, 4.4841, 4.9176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.12948159873485565 
model_pd.l_d.mean(): -22.865020751953125 
model_pd.lagr.mean(): -22.735538482666016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0676], device='cuda:0')), ('power', tensor([-22.9326], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.12948159873485565
epoch£º140	 i:1 	 global-step:2801	 l-p:0.1047956645488739
epoch£º140	 i:2 	 global-step:2802	 l-p:0.06366785615682602
epoch£º140	 i:3 	 global-step:2803	 l-p:0.1610470414161682
epoch£º140	 i:4 	 global-step:2804	 l-p:0.1242867261171341
epoch£º140	 i:5 	 global-step:2805	 l-p:0.0767279863357544
epoch£º140	 i:6 	 global-step:2806	 l-p:0.10744205862283707
epoch£º140	 i:7 	 global-step:2807	 l-p:0.14767146110534668
epoch£º140	 i:8 	 global-step:2808	 l-p:-0.02373206615447998
epoch£º140	 i:9 	 global-step:2809	 l-p:0.11805057525634766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8121, 3.8121, 3.8121],
        [3.8121, 3.8121, 3.8121],
        [3.8121, 3.8121, 3.8121],
        [3.8121, 3.9029, 3.8630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.10584281384944916 
model_pd.l_d.mean(): -22.915489196777344 
model_pd.lagr.mean(): -22.809646606445312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0257], device='cuda:0')), ('power', tensor([-22.9412], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.10584281384944916
epoch£º141	 i:1 	 global-step:2821	 l-p:0.19525542855262756
epoch£º141	 i:2 	 global-step:2822	 l-p:0.11953767389059067
epoch£º141	 i:3 	 global-step:2823	 l-p:0.1498364955186844
epoch£º141	 i:4 	 global-step:2824	 l-p:0.12641432881355286
epoch£º141	 i:5 	 global-step:2825	 l-p:0.101075179874897
epoch£º141	 i:6 	 global-step:2826	 l-p:0.03750177100300789
epoch£º141	 i:7 	 global-step:2827	 l-p:0.12798720598220825
epoch£º141	 i:8 	 global-step:2828	 l-p:0.1818925440311432
epoch£º141	 i:9 	 global-step:2829	 l-p:0.10373454540967941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9342, 4.0149, 3.9753],
        [3.9342, 4.0709, 4.0284],
        [3.9342, 4.0569, 4.0137],
        [3.9342, 3.9346, 3.9342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.1200469359755516 
model_pd.l_d.mean(): -22.924911499023438 
model_pd.lagr.mean(): -22.80486488342285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0200], device='cuda:0')), ('power', tensor([-22.9049], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.1200469359755516
epoch£º142	 i:1 	 global-step:2841	 l-p:0.11398589611053467
epoch£º142	 i:2 	 global-step:2842	 l-p:0.1534031629562378
epoch£º142	 i:3 	 global-step:2843	 l-p:0.10891086608171463
epoch£º142	 i:4 	 global-step:2844	 l-p:0.10401046276092529
epoch£º142	 i:5 	 global-step:2845	 l-p:0.07578513026237488
epoch£º142	 i:6 	 global-step:2846	 l-p:0.17722190916538239
epoch£º142	 i:7 	 global-step:2847	 l-p:0.09370357543230057
epoch£º142	 i:8 	 global-step:2848	 l-p:0.09750021994113922
epoch£º142	 i:9 	 global-step:2849	 l-p:0.12318617850542068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9450, 3.9450, 3.9450],
        [3.9450, 3.9450, 3.9450],
        [3.9450, 3.9450, 3.9450],
        [3.9450, 4.4989, 4.7045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.11432358622550964 
model_pd.l_d.mean(): -22.102550506591797 
model_pd.lagr.mean(): -21.98822784423828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0365], device='cuda:0')), ('power', tensor([-22.1391], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.11432358622550964
epoch£º143	 i:1 	 global-step:2861	 l-p:0.09728133678436279
epoch£º143	 i:2 	 global-step:2862	 l-p:0.04846937954425812
epoch£º143	 i:3 	 global-step:2863	 l-p:0.11358451843261719
epoch£º143	 i:4 	 global-step:2864	 l-p:0.1272730827331543
epoch£º143	 i:5 	 global-step:2865	 l-p:0.09466162323951721
epoch£º143	 i:6 	 global-step:2866	 l-p:0.13903667032718658
epoch£º143	 i:7 	 global-step:2867	 l-p:0.11418884247541428
epoch£º143	 i:8 	 global-step:2868	 l-p:-0.30657485127449036
epoch£º143	 i:9 	 global-step:2869	 l-p:0.12088975310325623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7719, 4.6528, 5.2311],
        [3.7719, 4.1888, 4.2953],
        [3.7719, 3.8419, 3.8061],
        [3.7719, 3.7761, 3.7723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): -0.1417725682258606 
model_pd.l_d.mean(): -22.522384643554688 
model_pd.lagr.mean(): -22.66415786743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0444], device='cuda:0')), ('power', tensor([-22.5668], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:-0.1417725682258606
epoch£º144	 i:1 	 global-step:2881	 l-p:0.10889189690351486
epoch£º144	 i:2 	 global-step:2882	 l-p:0.13197118043899536
epoch£º144	 i:3 	 global-step:2883	 l-p:0.11894679814577103
epoch£º144	 i:4 	 global-step:2884	 l-p:0.11047857999801636
epoch£º144	 i:5 	 global-step:2885	 l-p:0.08718719333410263
epoch£º144	 i:6 	 global-step:2886	 l-p:0.1370420753955841
epoch£º144	 i:7 	 global-step:2887	 l-p:0.1029033288359642
epoch£º144	 i:8 	 global-step:2888	 l-p:0.07930939644575119
epoch£º144	 i:9 	 global-step:2889	 l-p:0.16274097561836243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6785, 4.1504, 4.3130],
        [3.6785, 4.2144, 4.4357],
        [3.6785, 3.6840, 3.6791],
        [3.6785, 4.4000, 4.8124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.14377020299434662 
model_pd.l_d.mean(): -23.176240921020508 
model_pd.lagr.mean(): -23.032470703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0511], device='cuda:0')), ('power', tensor([-23.2273], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.14377020299434662
epoch£º145	 i:1 	 global-step:2901	 l-p:0.15170997381210327
epoch£º145	 i:2 	 global-step:2902	 l-p:0.14188337326049805
epoch£º145	 i:3 	 global-step:2903	 l-p:0.35431399941444397
epoch£º145	 i:4 	 global-step:2904	 l-p:0.11554187536239624
epoch£º145	 i:5 	 global-step:2905	 l-p:0.1000218465924263
epoch£º145	 i:6 	 global-step:2906	 l-p:0.14043092727661133
epoch£º145	 i:7 	 global-step:2907	 l-p:0.225850448012352
epoch£º145	 i:8 	 global-step:2908	 l-p:0.13481301069259644
epoch£º145	 i:9 	 global-step:2909	 l-p:0.11350901424884796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9516, 3.9516, 3.9516],
        [3.9516, 3.9773, 3.9583],
        [3.9516, 4.1515, 4.1214],
        [3.9516, 3.9997, 3.9697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.11097680777311325 
model_pd.l_d.mean(): -22.541322708129883 
model_pd.lagr.mean(): -22.43034553527832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0028], device='cuda:0')), ('power', tensor([-22.5385], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.11097680777311325
epoch£º146	 i:1 	 global-step:2921	 l-p:0.09918155521154404
epoch£º146	 i:2 	 global-step:2922	 l-p:0.10645414888858795
epoch£º146	 i:3 	 global-step:2923	 l-p:0.14844539761543274
epoch£º146	 i:4 	 global-step:2924	 l-p:0.13820429146289825
epoch£º146	 i:5 	 global-step:2925	 l-p:0.19205039739608765
epoch£º146	 i:6 	 global-step:2926	 l-p:0.11925317347049713
epoch£º146	 i:7 	 global-step:2927	 l-p:0.24987639486789703
epoch£º146	 i:8 	 global-step:2928	 l-p:0.05150953680276871
epoch£º146	 i:9 	 global-step:2929	 l-p:0.1061927005648613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8042, 3.9327, 3.8927],
        [3.8042, 3.8042, 3.8042],
        [3.8042, 3.8380, 3.8149],
        [3.8042, 3.9929, 3.9647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.19930852949619293 
model_pd.l_d.mean(): -23.330360412597656 
model_pd.lagr.mean(): -23.131052017211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0275], device='cuda:0')), ('power', tensor([-23.3028], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.19930852949619293
epoch£º147	 i:1 	 global-step:2941	 l-p:0.09400776028633118
epoch£º147	 i:2 	 global-step:2942	 l-p:-0.2630413770675659
epoch£º147	 i:3 	 global-step:2943	 l-p:0.10950496792793274
epoch£º147	 i:4 	 global-step:2944	 l-p:0.11242581903934479
epoch£º147	 i:5 	 global-step:2945	 l-p:0.14549843966960907
epoch£º147	 i:6 	 global-step:2946	 l-p:0.0989147424697876
epoch£º147	 i:7 	 global-step:2947	 l-p:0.12579403817653656
epoch£º147	 i:8 	 global-step:2948	 l-p:0.10302875190973282
epoch£º147	 i:9 	 global-step:2949	 l-p:0.09814729541540146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9381, 3.9543, 3.9414],
        [3.9381, 3.9456, 3.9391],
        [3.9381, 4.7629, 5.2488],
        [3.9381, 3.9381, 3.9381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.11226794123649597 
model_pd.l_d.mean(): -23.13159942626953 
model_pd.lagr.mean(): -23.019330978393555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0496], device='cuda:0')), ('power', tensor([-23.0819], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.11226794123649597
epoch£º148	 i:1 	 global-step:2961	 l-p:0.13013339042663574
epoch£º148	 i:2 	 global-step:2962	 l-p:0.2244483083486557
epoch£º148	 i:3 	 global-step:2963	 l-p:0.07408350706100464
epoch£º148	 i:4 	 global-step:2964	 l-p:0.15087909996509552
epoch£º148	 i:5 	 global-step:2965	 l-p:0.10181766003370285
epoch£º148	 i:6 	 global-step:2966	 l-p:0.1135895699262619
epoch£º148	 i:7 	 global-step:2967	 l-p:0.11179954558610916
epoch£º148	 i:8 	 global-step:2968	 l-p:0.20445050299167633
epoch£º148	 i:9 	 global-step:2969	 l-p:0.0922972783446312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8325, 3.8800, 3.8509],
        [3.8325, 3.8553, 3.8383],
        [3.8325, 4.6089, 5.0582],
        [3.8325, 4.1736, 4.2211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.10544797778129578 
model_pd.l_d.mean(): -21.959224700927734 
model_pd.lagr.mean(): -21.853776931762695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1106], device='cuda:0')), ('power', tensor([-22.0698], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.10544797778129578
epoch£º149	 i:1 	 global-step:2981	 l-p:0.16609106957912445
epoch£º149	 i:2 	 global-step:2982	 l-p:0.11883844435214996
epoch£º149	 i:3 	 global-step:2983	 l-p:0.1113760769367218
epoch£º149	 i:4 	 global-step:2984	 l-p:0.12945424020290375
epoch£º149	 i:5 	 global-step:2985	 l-p:0.11487699300050735
epoch£º149	 i:6 	 global-step:2986	 l-p:0.09554990381002426
epoch£º149	 i:7 	 global-step:2987	 l-p:0.07593704760074615
epoch£º149	 i:8 	 global-step:2988	 l-p:-0.03469492495059967
epoch£º149	 i:9 	 global-step:2989	 l-p:0.10861089825630188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7496, 3.9601, 3.9421],
        [3.7496, 4.0503, 4.0781],
        [3.7496, 3.9556, 3.9359],
        [3.7496, 3.7510, 3.7497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.11322351545095444 
model_pd.l_d.mean(): -21.915742874145508 
model_pd.lagr.mean(): -21.802518844604492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1132], device='cuda:0')), ('power', tensor([-22.0289], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.11322351545095444
epoch£º150	 i:1 	 global-step:3001	 l-p:0.29856210947036743
epoch£º150	 i:2 	 global-step:3002	 l-p:0.1256881058216095
epoch£º150	 i:3 	 global-step:3003	 l-p:-0.05494600161910057
epoch£º150	 i:4 	 global-step:3004	 l-p:0.16218657791614532
epoch£º150	 i:5 	 global-step:3005	 l-p:0.13244369626045227
epoch£º150	 i:6 	 global-step:3006	 l-p:0.09261669963598251
epoch£º150	 i:7 	 global-step:3007	 l-p:0.12431132793426514
epoch£º150	 i:8 	 global-step:3008	 l-p:-0.08204931765794754
epoch£º150	 i:9 	 global-step:3009	 l-p:0.11651265621185303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9992, 4.2800, 4.2820],
        [3.9992, 3.9992, 3.9992],
        [3.9992, 4.6086, 4.8626],
        [3.9992, 4.3735, 4.4318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.11541736125946045 
model_pd.l_d.mean(): -23.246795654296875 
model_pd.lagr.mean(): -23.131378173828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0893], device='cuda:0')), ('power', tensor([-23.1574], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.11541736125946045
epoch£º151	 i:1 	 global-step:3021	 l-p:0.10213282704353333
epoch£º151	 i:2 	 global-step:3022	 l-p:0.13171705603599548
epoch£º151	 i:3 	 global-step:3023	 l-p:0.1242644265294075
epoch£º151	 i:4 	 global-step:3024	 l-p:0.11372742056846619
epoch£º151	 i:5 	 global-step:3025	 l-p:0.116209477186203
epoch£º151	 i:6 	 global-step:3026	 l-p:0.07613172382116318
epoch£º151	 i:7 	 global-step:3027	 l-p:0.1069529578089714
epoch£º151	 i:8 	 global-step:3028	 l-p:-0.15616652369499207
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12226864695549011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[3.5620, 3.6450, 3.6095],
        [3.5620, 4.1690, 4.4791],
        [3.5620, 3.6702, 3.6340],
        [3.5620, 3.6670, 3.6307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.07910741865634918 
model_pd.l_d.mean(): -22.728700637817383 
model_pd.lagr.mean(): -22.649593353271484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1678], device='cuda:0')), ('power', tensor([-22.8965], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.07910741865634918
epoch£º152	 i:1 	 global-step:3041	 l-p:0.14819443225860596
epoch£º152	 i:2 	 global-step:3042	 l-p:0.11854131519794464
epoch£º152	 i:3 	 global-step:3043	 l-p:0.10379210114479065
epoch£º152	 i:4 	 global-step:3044	 l-p:0.1606895923614502
epoch£º152	 i:5 	 global-step:3045	 l-p:-0.019305510446429253
epoch£º152	 i:6 	 global-step:3046	 l-p:0.17747586965560913
epoch£º152	 i:7 	 global-step:3047	 l-p:0.12106384336948395
epoch£º152	 i:8 	 global-step:3048	 l-p:0.133293017745018
epoch£º152	 i:9 	 global-step:3049	 l-p:0.10287683457136154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8053, 3.8080, 3.8055],
        [3.8053, 3.8053, 3.8053],
        [3.8053, 3.8126, 3.8063],
        [3.8053, 3.8053, 3.8053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.11762925982475281 
model_pd.l_d.mean(): -22.40690803527832 
model_pd.lagr.mean(): -22.289278030395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0318], device='cuda:0')), ('power', tensor([-22.4387], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.11762925982475281
epoch£º153	 i:1 	 global-step:3061	 l-p:0.09393705427646637
epoch£º153	 i:2 	 global-step:3062	 l-p:0.13408194482326508
epoch£º153	 i:3 	 global-step:3063	 l-p:0.0956428125500679
epoch£º153	 i:4 	 global-step:3064	 l-p:0.13807493448257446
epoch£º153	 i:5 	 global-step:3065	 l-p:-0.03352082148194313
epoch£º153	 i:6 	 global-step:3066	 l-p:-0.1748674511909485
epoch£º153	 i:7 	 global-step:3067	 l-p:0.11048807948827744
epoch£º153	 i:8 	 global-step:3068	 l-p:0.10520385205745697
epoch£º153	 i:9 	 global-step:3069	 l-p:-0.29448989033699036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7652, 4.2002, 4.3252],
        [3.7652, 3.8688, 3.8293],
        [3.7652, 3.7820, 3.7688],
        [3.7652, 3.7683, 3.7654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 1.0163592100143433 
model_pd.l_d.mean(): -21.970346450805664 
model_pd.lagr.mean(): -20.95398712158203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1127], device='cuda:0')), ('power', tensor([-22.0831], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:1.0163592100143433
epoch£º154	 i:1 	 global-step:3081	 l-p:0.10763182491064072
epoch£º154	 i:2 	 global-step:3082	 l-p:0.09859199076890945
epoch£º154	 i:3 	 global-step:3083	 l-p:0.12217225134372711
epoch£º154	 i:4 	 global-step:3084	 l-p:0.19677549600601196
epoch£º154	 i:5 	 global-step:3085	 l-p:0.14928986132144928
epoch£º154	 i:6 	 global-step:3086	 l-p:0.1215086430311203
epoch£º154	 i:7 	 global-step:3087	 l-p:0.10067849606275558
epoch£º154	 i:8 	 global-step:3088	 l-p:0.10851898044347763
epoch£º154	 i:9 	 global-step:3089	 l-p:0.131563201546669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8444, 4.2991, 4.4330],
        [3.8444, 4.2361, 4.3203],
        [3.8444, 3.8744, 3.8533],
        [3.8444, 4.0728, 4.0588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.11031059920787811 
model_pd.l_d.mean(): -22.784753799438477 
model_pd.lagr.mean(): -22.674442291259766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0270], device='cuda:0')), ('power', tensor([-22.8117], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.11031059920787811
epoch£º155	 i:1 	 global-step:3101	 l-p:0.12726256251335144
epoch£º155	 i:2 	 global-step:3102	 l-p:0.13986121118068695
epoch£º155	 i:3 	 global-step:3103	 l-p:0.11108168214559555
epoch£º155	 i:4 	 global-step:3104	 l-p:-0.07115425914525986
epoch£º155	 i:5 	 global-step:3105	 l-p:0.12989960610866547
epoch£º155	 i:6 	 global-step:3106	 l-p:0.10661159455776215
epoch£º155	 i:7 	 global-step:3107	 l-p:0.12603598833084106
epoch£º155	 i:8 	 global-step:3108	 l-p:0.13320954144001007
epoch£º155	 i:9 	 global-step:3109	 l-p:0.09768946468830109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[3.3543, 4.0037, 4.3915],
        [3.3543, 4.0179, 4.4215],
        [3.3543, 3.4344, 3.4016],
        [3.3543, 4.0016, 4.3872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.10982108861207962 
model_pd.l_d.mean(): -22.843185424804688 
model_pd.lagr.mean(): -22.73336410522461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2195], device='cuda:0')), ('power', tensor([-23.0627], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.10982108861207962
epoch£º156	 i:1 	 global-step:3121	 l-p:0.11296194046735764
epoch£º156	 i:2 	 global-step:3122	 l-p:0.10983655601739883
epoch£º156	 i:3 	 global-step:3123	 l-p:0.08588632196187973
epoch£º156	 i:4 	 global-step:3124	 l-p:0.1619066447019577
epoch£º156	 i:5 	 global-step:3125	 l-p:0.10698066651821136
epoch£º156	 i:6 	 global-step:3126	 l-p:0.12700845301151276
epoch£º156	 i:7 	 global-step:3127	 l-p:0.12341843545436859
epoch£º156	 i:8 	 global-step:3128	 l-p:0.30897873640060425
epoch£º156	 i:9 	 global-step:3129	 l-p:0.11420833319425583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[3.7041, 3.7458, 3.7196],
        [3.7041, 4.0384, 4.0932],
        [3.7041, 4.3089, 4.5986],
        [3.7041, 3.8590, 3.8264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.06034252420067787 
model_pd.l_d.mean(): -22.932552337646484 
model_pd.lagr.mean(): -22.872209548950195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0804], device='cuda:0')), ('power', tensor([-23.0129], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.06034252420067787
epoch£º157	 i:1 	 global-step:3141	 l-p:0.12461152672767639
epoch£º157	 i:2 	 global-step:3142	 l-p:0.13732197880744934
epoch£º157	 i:3 	 global-step:3143	 l-p:0.03970713168382645
epoch£º157	 i:4 	 global-step:3144	 l-p:0.11659704148769379
epoch£º157	 i:5 	 global-step:3145	 l-p:0.09752869606018066
epoch£º157	 i:6 	 global-step:3146	 l-p:0.09555179625749588
epoch£º157	 i:7 	 global-step:3147	 l-p:0.12402229011058807
epoch£º157	 i:8 	 global-step:3148	 l-p:0.1471063196659088
epoch£º157	 i:9 	 global-step:3149	 l-p:0.1614990532398224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8995, 3.8996, 3.8995],
        [3.8995, 3.9059, 3.9003],
        [3.8995, 3.8995, 3.8995],
        [3.8995, 4.2062, 4.2299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.1152825877070427 
model_pd.l_d.mean(): -22.524404525756836 
model_pd.lagr.mean(): -22.409122467041016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0020], device='cuda:0')), ('power', tensor([-22.5264], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.1152825877070427
epoch£º158	 i:1 	 global-step:3161	 l-p:0.13959716260433197
epoch£º158	 i:2 	 global-step:3162	 l-p:0.08495434373617172
epoch£º158	 i:3 	 global-step:3163	 l-p:0.3016449809074402
epoch£º158	 i:4 	 global-step:3164	 l-p:0.1348479688167572
epoch£º158	 i:5 	 global-step:3165	 l-p:0.1284644454717636
epoch£º158	 i:6 	 global-step:3166	 l-p:0.10592114925384521
epoch£º158	 i:7 	 global-step:3167	 l-p:0.11810082942247391
epoch£º158	 i:8 	 global-step:3168	 l-p:0.11334848403930664
epoch£º158	 i:9 	 global-step:3169	 l-p:0.11283433437347412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8661, 3.9728, 3.9320],
        [3.8661, 3.9131, 3.8842],
        [3.8661, 3.9463, 3.9081],
        [3.8661, 3.8921, 3.8732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.13345599174499512 
model_pd.l_d.mean(): -23.101104736328125 
model_pd.lagr.mean(): -22.967649459838867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0070], device='cuda:0')), ('power', tensor([-23.0941], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.13345599174499512
epoch£º159	 i:1 	 global-step:3181	 l-p:0.1466420739889145
epoch£º159	 i:2 	 global-step:3182	 l-p:0.07784871757030487
epoch£º159	 i:3 	 global-step:3183	 l-p:0.12368029356002808
epoch£º159	 i:4 	 global-step:3184	 l-p:0.0930967777967453
epoch£º159	 i:5 	 global-step:3185	 l-p:0.09825809299945831
epoch£º159	 i:6 	 global-step:3186	 l-p:0.11253680288791656
epoch£º159	 i:7 	 global-step:3187	 l-p:0.11750886589288712
epoch£º159	 i:8 	 global-step:3188	 l-p:0.12045518308877945
epoch£º159	 i:9 	 global-step:3189	 l-p:0.2241840362548828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5166, 3.5494, 3.5277],
        [3.5166, 3.9575, 4.1157],
        [3.5166, 3.5166, 3.5166],
        [3.5166, 3.5169, 3.5166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.1300085186958313 
model_pd.l_d.mean(): -23.493423461914062 
model_pd.lagr.mean(): -23.363414764404297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0667], device='cuda:0')), ('power', tensor([-23.5601], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.1300085186958313
epoch£º160	 i:1 	 global-step:3201	 l-p:0.08215144276618958
epoch£º160	 i:2 	 global-step:3202	 l-p:0.09345134347677231
epoch£º160	 i:3 	 global-step:3203	 l-p:0.11962946504354477
epoch£º160	 i:4 	 global-step:3204	 l-p:0.09661001712083817
epoch£º160	 i:5 	 global-step:3205	 l-p:0.12004314363002777
epoch£º160	 i:6 	 global-step:3206	 l-p:0.07169046998023987
epoch£º160	 i:7 	 global-step:3207	 l-p:0.12680906057357788
epoch£º160	 i:8 	 global-step:3208	 l-p:0.1179521232843399
epoch£º160	 i:9 	 global-step:3209	 l-p:0.16089516878128052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[3.7328, 3.8253, 3.7872],
        [3.7328, 3.7767, 3.7496],
        [3.7328, 4.1346, 4.2381],
        [3.7328, 4.1866, 4.3329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.12127941846847534 
model_pd.l_d.mean(): -23.4393367767334 
model_pd.lagr.mean(): -23.318058013916016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0109], device='cuda:0')), ('power', tensor([-23.4285], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.12127941846847534
epoch£º161	 i:1 	 global-step:3221	 l-p:0.0036702225916087627
epoch£º161	 i:2 	 global-step:3222	 l-p:0.11540468037128448
epoch£º161	 i:3 	 global-step:3223	 l-p:0.12325549870729446
epoch£º161	 i:4 	 global-step:3224	 l-p:0.11637592315673828
epoch£º161	 i:5 	 global-step:3225	 l-p:0.12189808487892151
epoch£º161	 i:6 	 global-step:3226	 l-p:-0.5482420325279236
epoch£º161	 i:7 	 global-step:3227	 l-p:0.2581741213798523
epoch£º161	 i:8 	 global-step:3228	 l-p:0.08205178380012512
epoch£º161	 i:9 	 global-step:3229	 l-p:0.1165573000907898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8879, 4.2789, 4.3612],
        [3.8879, 3.8884, 3.8879],
        [3.8879, 3.9560, 3.9203],
        [3.8879, 4.3730, 4.5308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.1047956645488739 
model_pd.l_d.mean(): -22.968971252441406 
model_pd.lagr.mean(): -22.86417579650879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0045], device='cuda:0')), ('power', tensor([-22.9735], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.1047956645488739
epoch£º162	 i:1 	 global-step:3241	 l-p:0.14575280249118805
epoch£º162	 i:2 	 global-step:3242	 l-p:0.08825942873954773
epoch£º162	 i:3 	 global-step:3243	 l-p:0.1134750172495842
epoch£º162	 i:4 	 global-step:3244	 l-p:0.12558740377426147
epoch£º162	 i:5 	 global-step:3245	 l-p:0.25659313797950745
epoch£º162	 i:6 	 global-step:3246	 l-p:0.10284198820590973
epoch£º162	 i:7 	 global-step:3247	 l-p:0.12034730613231659
epoch£º162	 i:8 	 global-step:3248	 l-p:0.1162741407752037
epoch£º162	 i:9 	 global-step:3249	 l-p:0.2654666602611542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7809, 4.3966, 4.6892],
        [3.7809, 3.7809, 3.7809],
        [3.7809, 4.0043, 3.9926],
        [3.7809, 3.7809, 3.7809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): -0.01982622593641281 
model_pd.l_d.mean(): -21.81693458557129 
model_pd.lagr.mean(): -21.836761474609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1403], device='cuda:0')), ('power', tensor([-21.9572], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:-0.01982622593641281
epoch£º163	 i:1 	 global-step:3261	 l-p:0.12031374871730804
epoch£º163	 i:2 	 global-step:3262	 l-p:0.34775954484939575
epoch£º163	 i:3 	 global-step:3263	 l-p:0.18849067389965057
epoch£º163	 i:4 	 global-step:3264	 l-p:0.11274877935647964
epoch£º163	 i:5 	 global-step:3265	 l-p:0.12239465117454529
epoch£º163	 i:6 	 global-step:3266	 l-p:0.11932820826768875
epoch£º163	 i:7 	 global-step:3267	 l-p:0.11124062538146973
epoch£º163	 i:8 	 global-step:3268	 l-p:0.10147048532962799
epoch£º163	 i:9 	 global-step:3269	 l-p:0.11584555357694626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7227, 3.8729, 3.8394],
        [3.7227, 3.7253, 3.7229],
        [3.7227, 3.7232, 3.7227],
        [3.7227, 3.7378, 3.7258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.11624181270599365 
model_pd.l_d.mean(): -22.943662643432617 
model_pd.lagr.mean(): -22.827421188354492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0596], device='cuda:0')), ('power', tensor([-23.0032], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.11624181270599365
epoch£º164	 i:1 	 global-step:3281	 l-p:0.1083117350935936
epoch£º164	 i:2 	 global-step:3282	 l-p:0.27213209867477417
epoch£º164	 i:3 	 global-step:3283	 l-p:0.1253088414669037
epoch£º164	 i:4 	 global-step:3284	 l-p:0.14772796630859375
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12033885717391968
epoch£º164	 i:6 	 global-step:3286	 l-p:0.12113136053085327
epoch£º164	 i:7 	 global-step:3287	 l-p:0.19013753533363342
epoch£º164	 i:8 	 global-step:3288	 l-p:0.12211357057094574
epoch£º164	 i:9 	 global-step:3289	 l-p:0.08999938517808914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5755, 3.5755, 3.5755],
        [3.5755, 4.0593, 4.2515],
        [3.5755, 3.9117, 3.9804],
        [3.5755, 3.6903, 3.6550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): -0.07005566358566284 
model_pd.l_d.mean(): -22.990459442138672 
model_pd.lagr.mean(): -23.060514450073242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1128], device='cuda:0')), ('power', tensor([-23.1032], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:-0.07005566358566284
epoch£º165	 i:1 	 global-step:3301	 l-p:0.10845191776752472
epoch£º165	 i:2 	 global-step:3302	 l-p:0.10641215741634369
epoch£º165	 i:3 	 global-step:3303	 l-p:0.1497931182384491
epoch£º165	 i:4 	 global-step:3304	 l-p:0.07963970303535461
epoch£º165	 i:5 	 global-step:3305	 l-p:0.12625652551651
epoch£º165	 i:6 	 global-step:3306	 l-p:0.12639310956001282
epoch£º165	 i:7 	 global-step:3307	 l-p:0.10203219950199127
epoch£º165	 i:8 	 global-step:3308	 l-p:0.09760212898254395
epoch£º165	 i:9 	 global-step:3309	 l-p:0.131223663687706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6371, 3.6381, 3.6371],
        [3.6371, 3.6810, 3.6542],
        [3.6371, 3.6872, 3.6582],
        [3.6371, 3.6371, 3.6371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.09958366304636002 
model_pd.l_d.mean(): -22.127321243286133 
model_pd.lagr.mean(): -22.02773666381836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1516], device='cuda:0')), ('power', tensor([-22.2789], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.09958366304636002
epoch£º166	 i:1 	 global-step:3321	 l-p:0.2886047959327698
epoch£º166	 i:2 	 global-step:3322	 l-p:0.12180932611227036
epoch£º166	 i:3 	 global-step:3323	 l-p:0.1089160293340683
epoch£º166	 i:4 	 global-step:3324	 l-p:0.11046222597360611
epoch£º166	 i:5 	 global-step:3325	 l-p:0.12497365474700928
epoch£º166	 i:6 	 global-step:3326	 l-p:0.11931100487709045
epoch£º166	 i:7 	 global-step:3327	 l-p:0.08128300309181213
epoch£º166	 i:8 	 global-step:3328	 l-p:0.1278277039527893
epoch£º166	 i:9 	 global-step:3329	 l-p:0.10029821842908859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7201, 3.7201, 3.7201],
        [3.7201, 3.8205, 3.7823],
        [3.7201, 4.5315, 5.0457],
        [3.7201, 3.7276, 3.7212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.13793015480041504 
model_pd.l_d.mean(): -22.464099884033203 
model_pd.lagr.mean(): -22.326169967651367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0939], device='cuda:0')), ('power', tensor([-22.5580], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.13793015480041504
epoch£º167	 i:1 	 global-step:3341	 l-p:0.104352205991745
epoch£º167	 i:2 	 global-step:3342	 l-p:0.09196040034294128
epoch£º167	 i:3 	 global-step:3343	 l-p:0.11200664192438126
epoch£º167	 i:4 	 global-step:3344	 l-p:0.0883900597691536
epoch£º167	 i:5 	 global-step:3345	 l-p:0.12281493842601776
epoch£º167	 i:6 	 global-step:3346	 l-p:0.138936385512352
epoch£º167	 i:7 	 global-step:3347	 l-p:0.11592891067266464
epoch£º167	 i:8 	 global-step:3348	 l-p:0.17442050576210022
epoch£º167	 i:9 	 global-step:3349	 l-p:0.1458396464586258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8611, 3.8616, 3.8612],
        [3.8611, 4.6418, 5.0988],
        [3.8611, 3.8611, 3.8611],
        [3.8611, 3.8612, 3.8611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): -0.05922176316380501 
model_pd.l_d.mean(): -21.908702850341797 
model_pd.lagr.mean(): -21.967924118041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0883], device='cuda:0')), ('power', tensor([-21.9970], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:-0.05922176316380501
epoch£º168	 i:1 	 global-step:3361	 l-p:0.11431749910116196
epoch£º168	 i:2 	 global-step:3362	 l-p:0.12001927942037582
epoch£º168	 i:3 	 global-step:3363	 l-p:0.12372002005577087
epoch£º168	 i:4 	 global-step:3364	 l-p:0.18043290078639984
epoch£º168	 i:5 	 global-step:3365	 l-p:0.195146843791008
epoch£º168	 i:6 	 global-step:3366	 l-p:0.09932921826839447
epoch£º168	 i:7 	 global-step:3367	 l-p:0.23116570711135864
epoch£º168	 i:8 	 global-step:3368	 l-p:0.11218839138746262
epoch£º168	 i:9 	 global-step:3369	 l-p:0.10829553008079529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8771, 3.8772, 3.8771],
        [3.8771, 3.8918, 3.8800],
        [3.8771, 3.9786, 3.9384],
        [3.8771, 3.9169, 3.8910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.10671006888151169 
model_pd.l_d.mean(): -23.278953552246094 
model_pd.lagr.mean(): -23.172243118286133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0456], device='cuda:0')), ('power', tensor([-23.2333], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.10671006888151169
epoch£º169	 i:1 	 global-step:3381	 l-p:0.11416361480951309
epoch£º169	 i:2 	 global-step:3382	 l-p:0.14955255389213562
epoch£º169	 i:3 	 global-step:3383	 l-p:-0.15908482670783997
epoch£º169	 i:4 	 global-step:3384	 l-p:0.10164300352334976
epoch£º169	 i:5 	 global-step:3385	 l-p:0.14211530983448029
epoch£º169	 i:6 	 global-step:3386	 l-p:-0.13289424777030945
epoch£º169	 i:7 	 global-step:3387	 l-p:-0.010558762587606907
epoch£º169	 i:8 	 global-step:3388	 l-p:0.11919426918029785
epoch£º169	 i:9 	 global-step:3389	 l-p:0.12689201533794403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8255, 3.9680, 3.9311],
        [3.8255, 3.8262, 3.8255],
        [3.8255, 3.8429, 3.8294],
        [3.8255, 3.8260, 3.8255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.12154242396354675 
model_pd.l_d.mean(): -21.988052368164062 
model_pd.lagr.mean(): -21.86651039123535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0943], device='cuda:0')), ('power', tensor([-22.0823], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.12154242396354675
epoch£º170	 i:1 	 global-step:3401	 l-p:0.21739596128463745
epoch£º170	 i:2 	 global-step:3402	 l-p:0.11722787469625473
epoch£º170	 i:3 	 global-step:3403	 l-p:0.11873593181371689
epoch£º170	 i:4 	 global-step:3404	 l-p:0.11110930889844894
epoch£º170	 i:5 	 global-step:3405	 l-p:0.12153936922550201
epoch£º170	 i:6 	 global-step:3406	 l-p:0.03260185196995735
epoch£º170	 i:7 	 global-step:3407	 l-p:0.15378889441490173
epoch£º170	 i:8 	 global-step:3408	 l-p:0.15859130024909973
epoch£º170	 i:9 	 global-step:3409	 l-p:0.08332250267267227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7453, 4.0847, 4.1439],
        [3.7453, 3.7462, 3.7454],
        [3.7453, 3.7623, 3.7491],
        [3.7453, 3.7453, 3.7453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.10587068647146225 
model_pd.l_d.mean(): -21.917455673217773 
model_pd.lagr.mean(): -21.81158447265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1398], device='cuda:0')), ('power', tensor([-22.0572], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.10587068647146225
epoch£º171	 i:1 	 global-step:3421	 l-p:0.10673120617866516
epoch£º171	 i:2 	 global-step:3422	 l-p:0.12654514610767365
epoch£º171	 i:3 	 global-step:3423	 l-p:2.2210822105407715
epoch£º171	 i:4 	 global-step:3424	 l-p:0.10516561567783356
epoch£º171	 i:5 	 global-step:3425	 l-p:0.3562697172164917
epoch£º171	 i:6 	 global-step:3426	 l-p:0.11100694537162781
epoch£º171	 i:7 	 global-step:3427	 l-p:0.1477590948343277
epoch£º171	 i:8 	 global-step:3428	 l-p:0.1319141685962677
epoch£º171	 i:9 	 global-step:3429	 l-p:0.20353834331035614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9617, 3.9622, 3.9618],
        [3.9617, 3.9669, 3.9623],
        [3.9617, 4.1705, 4.1469],
        [3.9617, 3.9617, 3.9617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.1215999647974968 
model_pd.l_d.mean(): -23.28438949584961 
model_pd.lagr.mean(): -23.162790298461914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0759], device='cuda:0')), ('power', tensor([-23.2085], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.1215999647974968
epoch£º172	 i:1 	 global-step:3441	 l-p:0.12909652292728424
epoch£º172	 i:2 	 global-step:3442	 l-p:0.09258607774972916
epoch£º172	 i:3 	 global-step:3443	 l-p:0.10593057423830032
epoch£º172	 i:4 	 global-step:3444	 l-p:0.1557483971118927
epoch£º172	 i:5 	 global-step:3445	 l-p:0.10472877323627472
epoch£º172	 i:6 	 global-step:3446	 l-p:0.12877675890922546
epoch£º172	 i:7 	 global-step:3447	 l-p:0.10947220772504807
epoch£º172	 i:8 	 global-step:3448	 l-p:0.07617351412773132
epoch£º172	 i:9 	 global-step:3449	 l-p:0.09635847061872482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[3.7125, 4.4453, 4.8743],
        [3.7125, 3.7736, 3.7408],
        [3.7125, 4.1598, 4.3070],
        [3.7125, 3.7812, 3.7467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.09467082470655441 
model_pd.l_d.mean(): -22.467241287231445 
model_pd.lagr.mean(): -22.372570037841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1241], device='cuda:0')), ('power', tensor([-22.5914], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.09467082470655441
epoch£º173	 i:1 	 global-step:3461	 l-p:0.10559453815221786
epoch£º173	 i:2 	 global-step:3462	 l-p:0.1469278484582901
epoch£º173	 i:3 	 global-step:3463	 l-p:0.12369412928819656
epoch£º173	 i:4 	 global-step:3464	 l-p:0.12758348882198334
epoch£º173	 i:5 	 global-step:3465	 l-p:0.08265288919210434
epoch£º173	 i:6 	 global-step:3466	 l-p:0.10674065351486206
epoch£º173	 i:7 	 global-step:3467	 l-p:0.1309358775615692
epoch£º173	 i:8 	 global-step:3468	 l-p:0.12090963125228882
epoch£º173	 i:9 	 global-step:3469	 l-p:0.04431179538369179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5983, 4.1394, 4.3864],
        [3.5983, 3.6067, 3.5996],
        [3.5983, 3.6458, 3.6176],
        [3.5983, 3.6841, 3.6482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.1293693482875824 
model_pd.l_d.mean(): -22.997026443481445 
model_pd.lagr.mean(): -22.867656707763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1044], device='cuda:0')), ('power', tensor([-23.1014], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.1293693482875824
epoch£º174	 i:1 	 global-step:3481	 l-p:0.05563114956021309
epoch£º174	 i:2 	 global-step:3482	 l-p:0.13790498673915863
epoch£º174	 i:3 	 global-step:3483	 l-p:0.113761305809021
epoch£º174	 i:4 	 global-step:3484	 l-p:0.13211235404014587
epoch£º174	 i:5 	 global-step:3485	 l-p:0.15191465616226196
epoch£º174	 i:6 	 global-step:3486	 l-p:0.12697798013687134
epoch£º174	 i:7 	 global-step:3487	 l-p:0.11864861100912094
epoch£º174	 i:8 	 global-step:3488	 l-p:0.013328341767191887
epoch£º174	 i:9 	 global-step:3489	 l-p:0.12072233110666275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7925, 3.7941, 3.7926],
        [3.7925, 3.7996, 3.7934],
        [3.7925, 3.8267, 3.8036],
        [3.7925, 3.8459, 3.8150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.11269808560609818 
model_pd.l_d.mean(): -23.29033660888672 
model_pd.lagr.mean(): -23.17763900756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0216], device='cuda:0')), ('power', tensor([-23.2688], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.11269808560609818
epoch£º175	 i:1 	 global-step:3501	 l-p:0.11252111196517944
epoch£º175	 i:2 	 global-step:3502	 l-p:0.11294165998697281
epoch£º175	 i:3 	 global-step:3503	 l-p:0.14179860055446625
epoch£º175	 i:4 	 global-step:3504	 l-p:0.12424758821725845
epoch£º175	 i:5 	 global-step:3505	 l-p:0.0762953907251358
epoch£º175	 i:6 	 global-step:3506	 l-p:0.09359981119632721
epoch£º175	 i:7 	 global-step:3507	 l-p:0.10127372294664383
epoch£º175	 i:8 	 global-step:3508	 l-p:0.07730454951524734
epoch£º175	 i:9 	 global-step:3509	 l-p:0.02196459285914898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8393, 3.8393, 3.8393],
        [3.8393, 3.8532, 3.8420],
        [3.8393, 4.2293, 4.3195],
        [3.8393, 3.9928, 3.9578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.12662269175052643 
model_pd.l_d.mean(): -22.3449764251709 
model_pd.lagr.mean(): -22.218353271484375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0562], device='cuda:0')), ('power', tensor([-22.4012], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.12662269175052643
epoch£º176	 i:1 	 global-step:3521	 l-p:0.11665792018175125
epoch£º176	 i:2 	 global-step:3522	 l-p:0.11677765846252441
epoch£º176	 i:3 	 global-step:3523	 l-p:0.11710655689239502
epoch£º176	 i:4 	 global-step:3524	 l-p:0.11429601162672043
epoch£º176	 i:5 	 global-step:3525	 l-p:0.12526653707027435
epoch£º176	 i:6 	 global-step:3526	 l-p:0.1516796052455902
epoch£º176	 i:7 	 global-step:3527	 l-p:0.20131339132785797
epoch£º176	 i:8 	 global-step:3528	 l-p:0.10707198083400726
epoch£º176	 i:9 	 global-step:3529	 l-p:0.12135537713766098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7990, 4.6009, 5.0943],
        [3.7990, 3.7994, 3.7990],
        [3.7990, 3.8736, 3.8372],
        [3.7990, 3.7992, 3.7990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.10378831624984741 
model_pd.l_d.mean(): -22.690759658813477 
model_pd.lagr.mean(): -22.586971282958984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0556], device='cuda:0')), ('power', tensor([-22.7464], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.10378831624984741
epoch£º177	 i:1 	 global-step:3541	 l-p:0.0667707547545433
epoch£º177	 i:2 	 global-step:3542	 l-p:0.11003991961479187
epoch£º177	 i:3 	 global-step:3543	 l-p:0.15668699145317078
epoch£º177	 i:4 	 global-step:3544	 l-p:0.19954995810985565
epoch£º177	 i:5 	 global-step:3545	 l-p:0.12485012412071228
epoch£º177	 i:6 	 global-step:3546	 l-p:0.1183769553899765
epoch£º177	 i:7 	 global-step:3547	 l-p:0.09920597076416016
epoch£º177	 i:8 	 global-step:3548	 l-p:0.12240326404571533
epoch£º177	 i:9 	 global-step:3549	 l-p:0.10762017220258713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6346, 3.6347, 3.6346],
        [3.6346, 3.6346, 3.6346],
        [3.6346, 3.6346, 3.6346],
        [3.6346, 3.6774, 3.6506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.28028345108032227 
model_pd.l_d.mean(): -21.674949645996094 
model_pd.lagr.mean(): -21.39466667175293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1660], device='cuda:0')), ('power', tensor([-21.8410], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.28028345108032227
epoch£º178	 i:1 	 global-step:3561	 l-p:0.13488554954528809
epoch£º178	 i:2 	 global-step:3562	 l-p:0.12451259046792984
epoch£º178	 i:3 	 global-step:3563	 l-p:0.06841695308685303
epoch£º178	 i:4 	 global-step:3564	 l-p:0.12911874055862427
epoch£º178	 i:5 	 global-step:3565	 l-p:0.1261221021413803
epoch£º178	 i:6 	 global-step:3566	 l-p:0.05561074614524841
epoch£º178	 i:7 	 global-step:3567	 l-p:0.15260863304138184
epoch£º178	 i:8 	 global-step:3568	 l-p:-0.6819627285003662
epoch£º178	 i:9 	 global-step:3569	 l-p:0.14386367797851562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5359, 3.5364, 3.5359],
        [3.5359, 3.5477, 3.5379],
        [3.5359, 3.5470, 3.5377],
        [3.5359, 3.5361, 3.5359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.1244896948337555 
model_pd.l_d.mean(): -23.247798919677734 
model_pd.lagr.mean(): -23.123310089111328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0969], device='cuda:0')), ('power', tensor([-23.3447], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.1244896948337555
epoch£º179	 i:1 	 global-step:3581	 l-p:0.11956050246953964
epoch£º179	 i:2 	 global-step:3582	 l-p:0.25285589694976807
epoch£º179	 i:3 	 global-step:3583	 l-p:0.09478147327899933
epoch£º179	 i:4 	 global-step:3584	 l-p:0.05364280566573143
epoch£º179	 i:5 	 global-step:3585	 l-p:0.14141331613063812
epoch£º179	 i:6 	 global-step:3586	 l-p:0.09607765078544617
epoch£º179	 i:7 	 global-step:3587	 l-p:-0.156869575381279
epoch£º179	 i:8 	 global-step:3588	 l-p:0.17165923118591309
epoch£º179	 i:9 	 global-step:3589	 l-p:0.11866382509469986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8007, 3.8012, 3.8007],
        [3.8007, 3.8011, 3.8007],
        [3.8007, 3.9397, 3.9033],
        [3.8007, 3.8007, 3.8007]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.12095765024423599 
model_pd.l_d.mean(): -23.421316146850586 
model_pd.lagr.mean(): -23.300357818603516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0347], device='cuda:0')), ('power', tensor([-23.3867], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.12095765024423599
epoch£º180	 i:1 	 global-step:3601	 l-p:0.3323054015636444
epoch£º180	 i:2 	 global-step:3602	 l-p:0.09561532735824585
epoch£º180	 i:3 	 global-step:3603	 l-p:0.9689847826957703
epoch£º180	 i:4 	 global-step:3604	 l-p:0.0792752280831337
epoch£º180	 i:5 	 global-step:3605	 l-p:0.10775583237409592
epoch£º180	 i:6 	 global-step:3606	 l-p:0.12195908278226852
epoch£º180	 i:7 	 global-step:3607	 l-p:0.11877908557653427
epoch£º180	 i:8 	 global-step:3608	 l-p:-0.2067771703004837
epoch£º180	 i:9 	 global-step:3609	 l-p:0.10958238691091537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.1161, 4.1227, 4.1169],
        [4.1161, 4.1829, 4.1462],
        [4.1161, 4.4525, 4.4863],
        [4.1161, 4.6670, 4.8656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.09754298627376556 
model_pd.l_d.mean(): -23.04081153869629 
model_pd.lagr.mean(): -22.943267822265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0923], device='cuda:0')), ('power', tensor([-22.9485], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.09754298627376556
epoch£º181	 i:1 	 global-step:3621	 l-p:0.1980404406785965
epoch£º181	 i:2 	 global-step:3622	 l-p:0.0771368220448494
epoch£º181	 i:3 	 global-step:3623	 l-p:0.12076415866613388
epoch£º181	 i:4 	 global-step:3624	 l-p:0.1028708890080452
epoch£º181	 i:5 	 global-step:3625	 l-p:0.09739332646131516
epoch£º181	 i:6 	 global-step:3626	 l-p:0.12648795545101166
epoch£º181	 i:7 	 global-step:3627	 l-p:0.1167500913143158
epoch£º181	 i:8 	 global-step:3628	 l-p:0.10886850953102112
epoch£º181	 i:9 	 global-step:3629	 l-p:0.17536765336990356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7999, 3.8057, 3.8006],
        [3.7999, 3.8356, 3.8116],
        [3.7999, 3.8877, 3.8493],
        [3.7999, 3.7999, 3.7999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.10797495394945145 
model_pd.l_d.mean(): -23.008304595947266 
model_pd.lagr.mean(): -22.90032958984375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0038], device='cuda:0')), ('power', tensor([-23.0121], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.10797495394945145
epoch£º182	 i:1 	 global-step:3641	 l-p:0.09224257618188858
epoch£º182	 i:2 	 global-step:3642	 l-p:0.1602998524904251
epoch£º182	 i:3 	 global-step:3643	 l-p:0.14447759091854095
epoch£º182	 i:4 	 global-step:3644	 l-p:0.23363278806209564
epoch£º182	 i:5 	 global-step:3645	 l-p:0.034089501947164536
epoch£º182	 i:6 	 global-step:3646	 l-p:0.11808569729328156
epoch£º182	 i:7 	 global-step:3647	 l-p:0.12621082365512848
epoch£º182	 i:8 	 global-step:3648	 l-p:0.10937793552875519
epoch£º182	 i:9 	 global-step:3649	 l-p:-0.14301006495952606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5863, 3.7736, 3.7576],
        [3.5863, 3.6654, 3.6298],
        [3.5863, 3.5901, 3.5866],
        [3.5863, 4.0328, 4.1960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.10658054053783417 
model_pd.l_d.mean(): -23.120630264282227 
model_pd.lagr.mean(): -23.014049530029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0919], device='cuda:0')), ('power', tensor([-23.2126], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.10658054053783417
epoch£º183	 i:1 	 global-step:3661	 l-p:0.10322362929582596
epoch£º183	 i:2 	 global-step:3662	 l-p:0.10406722873449326
epoch£º183	 i:3 	 global-step:3663	 l-p:0.16330306231975555
epoch£º183	 i:4 	 global-step:3664	 l-p:0.10873860120773315
epoch£º183	 i:5 	 global-step:3665	 l-p:0.12873484194278717
epoch£º183	 i:6 	 global-step:3666	 l-p:0.14291849732398987
epoch£º183	 i:7 	 global-step:3667	 l-p:0.16740655899047852
epoch£º183	 i:8 	 global-step:3668	 l-p:-0.11758428066968918
epoch£º183	 i:9 	 global-step:3669	 l-p:0.10084356367588043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5863, 3.6273, 3.6010],
        [3.5863, 4.0846, 4.2953],
        [3.5863, 3.6046, 3.5902],
        [3.5863, 3.5863, 3.5863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.13648396730422974 
model_pd.l_d.mean(): -22.162403106689453 
model_pd.lagr.mean(): -22.02591896057129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1650], device='cuda:0')), ('power', tensor([-22.3274], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.13648396730422974
epoch£º184	 i:1 	 global-step:3681	 l-p:0.09286561608314514
epoch£º184	 i:2 	 global-step:3682	 l-p:0.10989746451377869
epoch£º184	 i:3 	 global-step:3683	 l-p:0.10567329823970795
epoch£º184	 i:4 	 global-step:3684	 l-p:0.10835567116737366
epoch£º184	 i:5 	 global-step:3685	 l-p:0.013905815780162811
epoch£º184	 i:6 	 global-step:3686	 l-p:0.07616294920444489
epoch£º184	 i:7 	 global-step:3687	 l-p:0.17481854557991028
epoch£º184	 i:8 	 global-step:3688	 l-p:0.1191985085606575
epoch£º184	 i:9 	 global-step:3689	 l-p:0.11374916136264801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0380, 4.1389, 4.0969],
        [4.0380, 4.0380, 4.0380],
        [4.0380, 4.0515, 4.0404],
        [4.0380, 4.6281, 4.8723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.1264650672674179 
model_pd.l_d.mean(): -22.34592056274414 
model_pd.lagr.mean(): -22.21945571899414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0038], device='cuda:0')), ('power', tensor([-22.3497], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.1264650672674179
epoch£º185	 i:1 	 global-step:3701	 l-p:0.11492086201906204
epoch£º185	 i:2 	 global-step:3702	 l-p:0.054626401513814926
epoch£º185	 i:3 	 global-step:3703	 l-p:1.2742884159088135
epoch£º185	 i:4 	 global-step:3704	 l-p:0.10590644925832748
epoch£º185	 i:5 	 global-step:3705	 l-p:0.10736647248268127
epoch£º185	 i:6 	 global-step:3706	 l-p:0.11307079344987869
epoch£º185	 i:7 	 global-step:3707	 l-p:0.11794738471508026
epoch£º185	 i:8 	 global-step:3708	 l-p:0.09906058758497238
epoch£º185	 i:9 	 global-step:3709	 l-p:0.16438773274421692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9314, 3.9314, 3.9314],
        [3.9314, 4.2077, 4.2186],
        [3.9314, 4.1751, 4.1692],
        [3.9314, 4.0159, 3.9767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.09326044470071793 
model_pd.l_d.mean(): -22.91700553894043 
model_pd.lagr.mean(): -22.823745727539062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0061], device='cuda:0')), ('power', tensor([-22.9109], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.09326044470071793
epoch£º186	 i:1 	 global-step:3721	 l-p:0.16251277923583984
epoch£º186	 i:2 	 global-step:3722	 l-p:0.18068048357963562
epoch£º186	 i:3 	 global-step:3723	 l-p:0.12653625011444092
epoch£º186	 i:4 	 global-step:3724	 l-p:0.7257059812545776
epoch£º186	 i:5 	 global-step:3725	 l-p:0.0994410365819931
epoch£º186	 i:6 	 global-step:3726	 l-p:0.11474982649087906
epoch£º186	 i:7 	 global-step:3727	 l-p:0.10974667966365814
epoch£º186	 i:8 	 global-step:3728	 l-p:0.1201615110039711
epoch£º186	 i:9 	 global-step:3729	 l-p:0.13556109368801117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7901, 3.7901, 3.7901],
        [3.7901, 3.9078, 3.8691],
        [3.7901, 4.0003, 3.9858],
        [3.7901, 3.7901, 3.7901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.1294000744819641 
model_pd.l_d.mean(): -22.41997718811035 
model_pd.lagr.mean(): -22.290576934814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0697], device='cuda:0')), ('power', tensor([-22.4897], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.1294000744819641
epoch£º187	 i:1 	 global-step:3741	 l-p:0.09780967235565186
epoch£º187	 i:2 	 global-step:3742	 l-p:0.10814763605594635
epoch£º187	 i:3 	 global-step:3743	 l-p:0.12294892966747284
epoch£º187	 i:4 	 global-step:3744	 l-p:0.12166647613048553
epoch£º187	 i:5 	 global-step:3745	 l-p:0.10064392536878586
epoch£º187	 i:6 	 global-step:3746	 l-p:0.08293857425451279
epoch£º187	 i:7 	 global-step:3747	 l-p:0.14025667309761047
epoch£º187	 i:8 	 global-step:3748	 l-p:4.642197132110596
epoch£º187	 i:9 	 global-step:3749	 l-p:0.12413913756608963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2272, 3.6223, 3.7803],
        [3.2272, 3.2533, 3.2322],
        [3.2272, 3.2272, 3.2272],
        [3.2272, 3.6156, 3.7676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.1256730556488037 
model_pd.l_d.mean(): -23.361066818237305 
model_pd.lagr.mean(): -23.235393524169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2149], device='cuda:0')), ('power', tensor([-23.5759], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.1256730556488037
epoch£º188	 i:1 	 global-step:3761	 l-p:0.11980711668729782
epoch£º188	 i:2 	 global-step:3762	 l-p:0.1354328840970993
epoch£º188	 i:3 	 global-step:3763	 l-p:0.11513349413871765
epoch£º188	 i:4 	 global-step:3764	 l-p:0.14504146575927734
epoch£º188	 i:5 	 global-step:3765	 l-p:0.12915408611297607
epoch£º188	 i:6 	 global-step:3766	 l-p:0.13843446969985962
epoch£º188	 i:7 	 global-step:3767	 l-p:0.10324174910783768
epoch£º188	 i:8 	 global-step:3768	 l-p:0.11621537804603577
epoch£º188	 i:9 	 global-step:3769	 l-p:0.1592872440814972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5855, 4.2052, 4.5369],
        [3.5855, 4.2156, 4.5580],
        [3.5855, 3.5855, 3.5855],
        [3.5855, 3.5855, 3.5855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): -0.0034098767209798098 
model_pd.l_d.mean(): -22.63610076904297 
model_pd.lagr.mean(): -22.639511108398438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1371], device='cuda:0')), ('power', tensor([-22.7732], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:-0.0034098767209798098
epoch£º189	 i:1 	 global-step:3781	 l-p:0.11503200232982635
epoch£º189	 i:2 	 global-step:3782	 l-p:0.10955984145402908
epoch£º189	 i:3 	 global-step:3783	 l-p:0.11297889798879623
epoch£º189	 i:4 	 global-step:3784	 l-p:0.1167260929942131
epoch£º189	 i:5 	 global-step:3785	 l-p:0.11255044490098953
epoch£º189	 i:6 	 global-step:3786	 l-p:0.36889001727104187
epoch£º189	 i:7 	 global-step:3787	 l-p:0.09854518622159958
epoch£º189	 i:8 	 global-step:3788	 l-p:0.1561315357685089
epoch£º189	 i:9 	 global-step:3789	 l-p:0.13641056418418884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8916, 3.8916, 3.8916],
        [3.8916, 3.8916, 3.8916],
        [3.8916, 3.8916, 3.8916],
        [3.8916, 3.9022, 3.8933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.11817308515310287 
model_pd.l_d.mean(): -22.96181297302246 
model_pd.lagr.mean(): -22.843639373779297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0091], device='cuda:0')), ('power', tensor([-22.9527], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.11817308515310287
epoch£º190	 i:1 	 global-step:3801	 l-p:0.10823159664869308
epoch£º190	 i:2 	 global-step:3802	 l-p:0.11687660217285156
epoch£º190	 i:3 	 global-step:3803	 l-p:-0.03339316323399544
epoch£º190	 i:4 	 global-step:3804	 l-p:-0.12202145904302597
epoch£º190	 i:5 	 global-step:3805	 l-p:0.11008208990097046
epoch£º190	 i:6 	 global-step:3806	 l-p:0.058660659939050674
epoch£º190	 i:7 	 global-step:3807	 l-p:0.164616659283638
epoch£º190	 i:8 	 global-step:3808	 l-p:0.11457359045743942
epoch£º190	 i:9 	 global-step:3809	 l-p:0.06155572831630707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0930, 4.8469, 5.2489],
        [4.0930, 4.1378, 4.1089],
        [4.0930, 4.0930, 4.0930],
        [4.0930, 4.0930, 4.0930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.12998118996620178 
model_pd.l_d.mean(): -21.985971450805664 
model_pd.lagr.mean(): -21.855989456176758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0449], device='cuda:0')), ('power', tensor([-22.0309], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.12998118996620178
epoch£º191	 i:1 	 global-step:3821	 l-p:0.11420081555843353
epoch£º191	 i:2 	 global-step:3822	 l-p:0.1059216633439064
epoch£º191	 i:3 	 global-step:3823	 l-p:0.2003130316734314
epoch£º191	 i:4 	 global-step:3824	 l-p:0.10408662259578705
epoch£º191	 i:5 	 global-step:3825	 l-p:0.10438486188650131
epoch£º191	 i:6 	 global-step:3826	 l-p:0.13407039642333984
epoch£º191	 i:7 	 global-step:3827	 l-p:0.09137695282697678
epoch£º191	 i:8 	 global-step:3828	 l-p:0.10767076164484024
epoch£º191	 i:9 	 global-step:3829	 l-p:0.0730699747800827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9216, 3.9347, 3.9239],
        [3.9216, 3.9295, 3.9226],
        [3.9216, 4.5564, 4.8598],
        [3.9216, 3.9401, 3.9255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.09320461004972458 
model_pd.l_d.mean(): -22.918214797973633 
model_pd.lagr.mean(): -22.825010299682617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0035], device='cuda:0')), ('power', tensor([-22.9147], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.09320461004972458
epoch£º192	 i:1 	 global-step:3841	 l-p:0.12095616012811661
epoch£º192	 i:2 	 global-step:3842	 l-p:0.06044973060488701
epoch£º192	 i:3 	 global-step:3843	 l-p:0.09655356407165527
epoch£º192	 i:4 	 global-step:3844	 l-p:0.1386863887310028
epoch£º192	 i:5 	 global-step:3845	 l-p:0.13531890511512756
epoch£º192	 i:6 	 global-step:3846	 l-p:0.4027940630912781
epoch£º192	 i:7 	 global-step:3847	 l-p:0.45338284969329834
epoch£º192	 i:8 	 global-step:3848	 l-p:0.09312307089567184
epoch£º192	 i:9 	 global-step:3849	 l-p:0.08538291603326797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7221, 4.2505, 4.4754],
        [3.7221, 3.7310, 3.7232],
        [3.7221, 3.7231, 3.7221],
        [3.7221, 3.7221, 3.7221]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.05916430801153183 
model_pd.l_d.mean(): -23.211326599121094 
model_pd.lagr.mean(): -23.152162551879883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0364], device='cuda:0')), ('power', tensor([-23.2477], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.05916430801153183
epoch£º193	 i:1 	 global-step:3861	 l-p:0.11782603710889816
epoch£º193	 i:2 	 global-step:3862	 l-p:0.13010625541210175
epoch£º193	 i:3 	 global-step:3863	 l-p:0.1258157640695572
epoch£º193	 i:4 	 global-step:3864	 l-p:0.05051548406481743
epoch£º193	 i:5 	 global-step:3865	 l-p:0.11414677649736404
epoch£º193	 i:6 	 global-step:3866	 l-p:0.1059778556227684
epoch£º193	 i:7 	 global-step:3867	 l-p:0.1388026773929596
epoch£º193	 i:8 	 global-step:3868	 l-p:0.11245182156562805
epoch£º193	 i:9 	 global-step:3869	 l-p:0.1173710823059082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9743, 3.9743, 3.9743],
        [3.9743, 4.3839, 4.4826],
        [3.9743, 4.1208, 4.0821],
        [3.9743, 3.9996, 3.9807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.11365047842264175 
model_pd.l_d.mean(): -23.349145889282227 
model_pd.lagr.mean(): -23.23549461364746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0868], device='cuda:0')), ('power', tensor([-23.2624], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.11365047842264175
epoch£º194	 i:1 	 global-step:3881	 l-p:0.13220523297786713
epoch£º194	 i:2 	 global-step:3882	 l-p:0.12594914436340332
epoch£º194	 i:3 	 global-step:3883	 l-p:0.1737726926803589
epoch£º194	 i:4 	 global-step:3884	 l-p:0.11482638120651245
epoch£º194	 i:5 	 global-step:3885	 l-p:0.16966071724891663
epoch£º194	 i:6 	 global-step:3886	 l-p:0.09906115382909775
epoch£º194	 i:7 	 global-step:3887	 l-p:0.11379148811101913
epoch£º194	 i:8 	 global-step:3888	 l-p:0.10616435110569
epoch£º194	 i:9 	 global-step:3889	 l-p:0.06664988398551941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8367, 3.8816, 3.8528],
        [3.8367, 4.1811, 4.2429],
        [3.8367, 3.8381, 3.8368],
        [3.8367, 4.1137, 4.1320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.6439244747161865 
model_pd.l_d.mean(): -23.047243118286133 
model_pd.lagr.mean(): -22.403318405151367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0090], device='cuda:0')), ('power', tensor([-23.0562], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.6439244747161865
epoch£º195	 i:1 	 global-step:3901	 l-p:0.1989956647157669
epoch£º195	 i:2 	 global-step:3902	 l-p:0.12102052569389343
epoch£º195	 i:3 	 global-step:3903	 l-p:0.10082384943962097
epoch£º195	 i:4 	 global-step:3904	 l-p:0.11053347587585449
epoch£º195	 i:5 	 global-step:3905	 l-p:0.12655821442604065
epoch£º195	 i:6 	 global-step:3906	 l-p:0.11555676907300949
epoch£º195	 i:7 	 global-step:3907	 l-p:0.10209401696920395
epoch£º195	 i:8 	 global-step:3908	 l-p:0.11883380264043808
epoch£º195	 i:9 	 global-step:3909	 l-p:0.14169174432754517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6488, 3.6488, 3.6488],
        [3.6488, 3.6514, 3.6489],
        [3.6488, 3.6489, 3.6488],
        [3.6488, 3.6506, 3.6489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.2946392893791199 
model_pd.l_d.mean(): -22.557594299316406 
model_pd.lagr.mean(): -22.262954711914062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1548], device='cuda:0')), ('power', tensor([-22.7124], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.2946392893791199
epoch£º196	 i:1 	 global-step:3921	 l-p:-0.09867040067911148
epoch£º196	 i:2 	 global-step:3922	 l-p:0.024115391075611115
epoch£º196	 i:3 	 global-step:3923	 l-p:0.12475625425577164
epoch£º196	 i:4 	 global-step:3924	 l-p:0.15015892684459686
epoch£º196	 i:5 	 global-step:3925	 l-p:0.12380246073007584
epoch£º196	 i:6 	 global-step:3926	 l-p:0.1062101498246193
epoch£º196	 i:7 	 global-step:3927	 l-p:0.0890047699213028
epoch£º196	 i:8 	 global-step:3928	 l-p:0.12039613723754883
epoch£º196	 i:9 	 global-step:3929	 l-p:0.10820155590772629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6057, 3.6058, 3.6057],
        [3.6057, 3.8596, 3.8780],
        [3.6057, 3.6057, 3.6057],
        [3.6057, 3.6120, 3.6061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.16034817695617676 
model_pd.l_d.mean(): -22.19234848022461 
model_pd.lagr.mean(): -22.031999588012695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1731], device='cuda:0')), ('power', tensor([-22.3655], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.16034817695617676
epoch£º197	 i:1 	 global-step:3941	 l-p:0.1225355863571167
epoch£º197	 i:2 	 global-step:3942	 l-p:0.09526174515485764
epoch£º197	 i:3 	 global-step:3943	 l-p:0.044818341732025146
epoch£º197	 i:4 	 global-step:3944	 l-p:0.03709142282605171
epoch£º197	 i:5 	 global-step:3945	 l-p:0.12366177886724472
epoch£º197	 i:6 	 global-step:3946	 l-p:0.1128198504447937
epoch£º197	 i:7 	 global-step:3947	 l-p:0.11212669312953949
epoch£º197	 i:8 	 global-step:3948	 l-p:0.14092673361301422
epoch£º197	 i:9 	 global-step:3949	 l-p:0.06538894772529602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6034, 3.6126, 3.6041],
        [3.6034, 3.6034, 3.6034],
        [3.6034, 3.6034, 3.6034],
        [3.6034, 3.6036, 3.6034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.11162039637565613 
model_pd.l_d.mean(): -23.23875617980957 
model_pd.lagr.mean(): -23.12713623046875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0729], device='cuda:0')), ('power', tensor([-23.3117], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.11162039637565613
epoch£º198	 i:1 	 global-step:3961	 l-p:0.09563031047582626
epoch£º198	 i:2 	 global-step:3962	 l-p:0.10587009787559509
epoch£º198	 i:3 	 global-step:3963	 l-p:0.1255929172039032
epoch£º198	 i:4 	 global-step:3964	 l-p:0.11074939370155334
epoch£º198	 i:5 	 global-step:3965	 l-p:-0.041286878287792206
epoch£º198	 i:6 	 global-step:3966	 l-p:0.11653146147727966
epoch£º198	 i:7 	 global-step:3967	 l-p:0.11039404571056366
epoch£º198	 i:8 	 global-step:3968	 l-p:0.05761691555380821
epoch£º198	 i:9 	 global-step:3969	 l-p:0.09486649185419083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5952, 3.5954, 3.5952],
        [3.5952, 3.5993, 3.5953],
        [3.5952, 3.5952, 3.5952],
        [3.5952, 3.6569, 3.6222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.12235013395547867 
model_pd.l_d.mean(): -22.190319061279297 
model_pd.lagr.mean(): -22.067968368530273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1491], device='cuda:0')), ('power', tensor([-22.3394], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.12235013395547867
epoch£º199	 i:1 	 global-step:3981	 l-p:-0.02354947105050087
epoch£º199	 i:2 	 global-step:3982	 l-p:0.06526506692171097
epoch£º199	 i:3 	 global-step:3983	 l-p:0.1202821359038353
epoch£º199	 i:4 	 global-step:3984	 l-p:0.10930981487035751
epoch£º199	 i:5 	 global-step:3985	 l-p:0.15277458727359772
epoch£º199	 i:6 	 global-step:3986	 l-p:0.12815521657466888
epoch£º199	 i:7 	 global-step:3987	 l-p:0.11213681846857071
epoch£º199	 i:8 	 global-step:3988	 l-p:0.10960010439157486
epoch£º199	 i:9 	 global-step:3989	 l-p:0.11167474836111069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7704, 3.7704, 3.7704],
        [3.7704, 3.7706, 3.7704],
        [3.7704, 3.7771, 3.7710],
        [3.7704, 3.9930, 3.9864]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.11553622782230377 
model_pd.l_d.mean(): -23.204620361328125 
model_pd.lagr.mean(): -23.08908462524414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0148], device='cuda:0')), ('power', tensor([-23.2195], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.11553622782230377
epoch£º200	 i:1 	 global-step:4001	 l-p:0.12023864686489105
epoch£º200	 i:2 	 global-step:4002	 l-p:0.11190237104892731
epoch£º200	 i:3 	 global-step:4003	 l-p:0.12021270394325256
epoch£º200	 i:4 	 global-step:4004	 l-p:0.18159300088882446
epoch£º200	 i:5 	 global-step:4005	 l-p:0.1247079074382782
epoch£º200	 i:6 	 global-step:4006	 l-p:0.3582587242126465
epoch£º200	 i:7 	 global-step:4007	 l-p:0.09926068782806396
epoch£º200	 i:8 	 global-step:4008	 l-p:0.10451776534318924
epoch£º200	 i:9 	 global-step:4009	 l-p:0.11876142024993896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6058, 3.6058, 3.6058],
        [3.6058, 3.9000, 3.9449],
        [3.6058, 3.6058, 3.6058],
        [3.6058, 3.6560, 3.6241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.11808174848556519 
model_pd.l_d.mean(): -21.834314346313477 
model_pd.lagr.mean(): -21.716232299804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1935], device='cuda:0')), ('power', tensor([-22.0278], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.11808174848556519
epoch£º201	 i:1 	 global-step:4021	 l-p:0.11809299886226654
epoch£º201	 i:2 	 global-step:4022	 l-p:-0.43955421447753906
epoch£º201	 i:3 	 global-step:4023	 l-p:0.12737134099006653
epoch£º201	 i:4 	 global-step:4024	 l-p:0.11811980605125427
epoch£º201	 i:5 	 global-step:4025	 l-p:0.10953760892152786
epoch£º201	 i:6 	 global-step:4026	 l-p:0.12128497660160065
epoch£º201	 i:7 	 global-step:4027	 l-p:2.1236116886138916
epoch£º201	 i:8 	 global-step:4028	 l-p:0.1093982383608818
epoch£º201	 i:9 	 global-step:4029	 l-p:0.12078391015529633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6465, 3.6466, 3.6465],
        [3.6465, 3.6465, 3.6465],
        [3.6465, 3.6951, 3.6637],
        [3.6465, 3.6520, 3.6466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.10335532575845718 
model_pd.l_d.mean(): -22.907018661499023 
model_pd.lagr.mean(): -22.80366325378418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0910], device='cuda:0')), ('power', tensor([-22.9980], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.10335532575845718
epoch£º202	 i:1 	 global-step:4041	 l-p:0.037844665348529816
epoch£º202	 i:2 	 global-step:4042	 l-p:0.10801251977682114
epoch£º202	 i:3 	 global-step:4043	 l-p:0.11960818618535995
epoch£º202	 i:4 	 global-step:4044	 l-p:0.10047595947980881
epoch£º202	 i:5 	 global-step:4045	 l-p:0.11791133880615234
epoch£º202	 i:6 	 global-step:4046	 l-p:0.17670638859272003
epoch£º202	 i:7 	 global-step:4047	 l-p:0.1367592066526413
epoch£º202	 i:8 	 global-step:4048	 l-p:-0.21847441792488098
epoch£º202	 i:9 	 global-step:4049	 l-p:0.11393800377845764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6239, 3.9725, 4.0557],
        [3.6239, 3.6753, 3.6428],
        [3.6239, 3.6543, 3.6308],
        [3.6239, 3.6239, 3.6239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.1140732541680336 
model_pd.l_d.mean(): -22.115234375 
model_pd.lagr.mean(): -22.001161575317383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1425], device='cuda:0')), ('power', tensor([-22.2578], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.1140732541680336
epoch£º203	 i:1 	 global-step:4061	 l-p:-0.2513957917690277
epoch£º203	 i:2 	 global-step:4062	 l-p:0.1085657924413681
epoch£º203	 i:3 	 global-step:4063	 l-p:0.10139647871255875
epoch£º203	 i:4 	 global-step:4064	 l-p:0.12755699455738068
epoch£º203	 i:5 	 global-step:4065	 l-p:0.14367793500423431
epoch£º203	 i:6 	 global-step:4066	 l-p:0.13865023851394653
epoch£º203	 i:7 	 global-step:4067	 l-p:0.053584251552820206
epoch£º203	 i:8 	 global-step:4068	 l-p:0.10713811963796616
epoch£º203	 i:9 	 global-step:4069	 l-p:0.11641795933246613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5995, 3.9134, 3.9731],
        [3.5995, 3.5995, 3.5995],
        [3.5995, 3.9073, 3.9625],
        [3.5995, 3.6578, 3.6232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.11298473924398422 
model_pd.l_d.mean(): -23.11544418334961 
model_pd.lagr.mean(): -23.002458572387695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1063], device='cuda:0')), ('power', tensor([-23.2217], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.11298473924398422
epoch£º204	 i:1 	 global-step:4081	 l-p:0.12488826364278793
epoch£º204	 i:2 	 global-step:4082	 l-p:0.10965997725725174
epoch£º204	 i:3 	 global-step:4083	 l-p:0.1381666511297226
epoch£º204	 i:4 	 global-step:4084	 l-p:0.11784304678440094
epoch£º204	 i:5 	 global-step:4085	 l-p:0.08930648863315582
epoch£º204	 i:6 	 global-step:4086	 l-p:0.08610817044973373
epoch£º204	 i:7 	 global-step:4087	 l-p:0.0408068522810936
epoch£º204	 i:8 	 global-step:4088	 l-p:0.12426535040140152
epoch£º204	 i:9 	 global-step:4089	 l-p:0.11034517735242844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5296, 3.5604, 3.5356],
        [3.5296, 3.5296, 3.5296],
        [3.5296, 3.5296, 3.5296],
        [3.5296, 3.5296, 3.5296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.10177767276763916 
model_pd.l_d.mean(): -22.642427444458008 
model_pd.lagr.mean(): -22.5406494140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1737], device='cuda:0')), ('power', tensor([-22.8161], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.10177767276763916
epoch£º205	 i:1 	 global-step:4101	 l-p:0.12371807545423508
epoch£º205	 i:2 	 global-step:4102	 l-p:0.07598045468330383
epoch£º205	 i:3 	 global-step:4103	 l-p:0.09661858528852463
epoch£º205	 i:4 	 global-step:4104	 l-p:0.11271964758634567
epoch£º205	 i:5 	 global-step:4105	 l-p:0.18395791947841644
epoch£º205	 i:6 	 global-step:4106	 l-p:0.11528331786394119
epoch£º205	 i:7 	 global-step:4107	 l-p:0.12251652777194977
epoch£º205	 i:8 	 global-step:4108	 l-p:0.11174461245536804
epoch£º205	 i:9 	 global-step:4109	 l-p:0.12329589575529099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6115, 3.6290, 3.6132],
        [3.6115, 4.1985, 4.4972],
        [3.6115, 3.9531, 4.0324],
        [3.6115, 3.6115, 3.6115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.175755113363266 
model_pd.l_d.mean(): -23.014528274536133 
model_pd.lagr.mean(): -22.838773727416992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1229], device='cuda:0')), ('power', tensor([-23.1375], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.175755113363266
epoch£º206	 i:1 	 global-step:4121	 l-p:0.09437663108110428
epoch£º206	 i:2 	 global-step:4122	 l-p:-1.3825416564941406
epoch£º206	 i:3 	 global-step:4123	 l-p:0.6054394245147705
epoch£º206	 i:4 	 global-step:4124	 l-p:0.09634473919868469
epoch£º206	 i:5 	 global-step:4125	 l-p:0.2322445660829544
epoch£º206	 i:6 	 global-step:4126	 l-p:0.1193036213517189
epoch£º206	 i:7 	 global-step:4127	 l-p:0.12193190306425095
epoch£º206	 i:8 	 global-step:4128	 l-p:0.11720732599496841
epoch£º206	 i:9 	 global-step:4129	 l-p:0.10183917731046677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7923, 3.8054, 3.7937],
        [3.7923, 3.7923, 3.7923],
        [3.7923, 3.9860, 3.9661],
        [3.7923, 3.9066, 3.8667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.11113643646240234 
model_pd.l_d.mean(): -22.73304557800293 
model_pd.lagr.mean(): -22.621910095214844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0361], device='cuda:0')), ('power', tensor([-22.7692], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.11113643646240234
epoch£º207	 i:1 	 global-step:4141	 l-p:0.12245216965675354
epoch£º207	 i:2 	 global-step:4142	 l-p:0.10912685096263885
epoch£º207	 i:3 	 global-step:4143	 l-p:0.10671783983707428
epoch£º207	 i:4 	 global-step:4144	 l-p:0.068757064640522
epoch£º207	 i:5 	 global-step:4145	 l-p:0.057675089687108994
epoch£º207	 i:6 	 global-step:4146	 l-p:0.15610253810882568
epoch£º207	 i:7 	 global-step:4147	 l-p:0.13128173351287842
epoch£º207	 i:8 	 global-step:4148	 l-p:0.11514447629451752
epoch£º207	 i:9 	 global-step:4149	 l-p:0.10176295787096024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8427, 3.8428, 3.8427],
        [3.8427, 4.2005, 4.2736],
        [3.8427, 3.9376, 3.8968],
        [3.8427, 3.8427, 3.8427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): -0.6262130737304688 
model_pd.l_d.mean(): -23.4005126953125 
model_pd.lagr.mean(): -24.02672576904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0437], device='cuda:0')), ('power', tensor([-23.3568], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:-0.6262130737304688
epoch£º208	 i:1 	 global-step:4161	 l-p:0.12250953167676926
epoch£º208	 i:2 	 global-step:4162	 l-p:0.12479571253061295
epoch£º208	 i:3 	 global-step:4163	 l-p:0.08230401575565338
epoch£º208	 i:4 	 global-step:4164	 l-p:0.23887409269809723
epoch£º208	 i:5 	 global-step:4165	 l-p:-0.31412193179130554
epoch£º208	 i:6 	 global-step:4166	 l-p:0.10838392376899719
epoch£º208	 i:7 	 global-step:4167	 l-p:0.11459627002477646
epoch£º208	 i:8 	 global-step:4168	 l-p:0.10098884254693985
epoch£º208	 i:9 	 global-step:4169	 l-p:0.11490058898925781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[3.8374, 4.0876, 4.0923],
        [3.8374, 4.5215, 4.8890],
        [3.8374, 3.8745, 3.8478],
        [3.8374, 4.0229, 3.9983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): -0.010068130679428577 
model_pd.l_d.mean(): -22.944517135620117 
model_pd.lagr.mean(): -22.954586029052734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0439], device='cuda:0')), ('power', tensor([-22.9884], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:-0.010068130679428577
epoch£º209	 i:1 	 global-step:4181	 l-p:0.11405196785926819
epoch£º209	 i:2 	 global-step:4182	 l-p:0.10851968824863434
epoch£º209	 i:3 	 global-step:4183	 l-p:0.07588820904493332
epoch£º209	 i:4 	 global-step:4184	 l-p:0.1195317804813385
epoch£º209	 i:5 	 global-step:4185	 l-p:0.12271598726511002
epoch£º209	 i:6 	 global-step:4186	 l-p:0.1007976159453392
epoch£º209	 i:7 	 global-step:4187	 l-p:0.11449320614337921
epoch£º209	 i:8 	 global-step:4188	 l-p:-1.5684700012207031
epoch£º209	 i:9 	 global-step:4189	 l-p:0.26038745045661926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6806, 3.6806, 3.6806],
        [3.6806, 3.6806, 3.6806],
        [3.6806, 3.7713, 3.7316],
        [3.6806, 3.6806, 3.6806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.10291782021522522 
model_pd.l_d.mean(): -23.294055938720703 
model_pd.lagr.mean(): -23.191137313842773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0445], device='cuda:0')), ('power', tensor([-23.3386], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.10291782021522522
epoch£º210	 i:1 	 global-step:4201	 l-p:0.0902053490281105
epoch£º210	 i:2 	 global-step:4202	 l-p:0.10360978543758392
epoch£º210	 i:3 	 global-step:4203	 l-p:-0.17559130489826202
epoch£º210	 i:4 	 global-step:4204	 l-p:0.10405470430850983
epoch£º210	 i:5 	 global-step:4205	 l-p:0.13190452754497528
epoch£º210	 i:6 	 global-step:4206	 l-p:0.12691374123096466
epoch£º210	 i:7 	 global-step:4207	 l-p:0.11040685325860977
epoch£º210	 i:8 	 global-step:4208	 l-p:0.3666743338108063
epoch£º210	 i:9 	 global-step:4209	 l-p:0.33184871077537537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6657, 3.9085, 3.9182],
        [3.6657, 3.6657, 3.6657],
        [3.6657, 3.7436, 3.7047],
        [3.6657, 3.6657, 3.6657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.10060833394527435 
model_pd.l_d.mean(): -22.564680099487305 
model_pd.lagr.mean(): -22.46407127380371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0953], device='cuda:0')), ('power', tensor([-22.6600], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.10060833394527435
epoch£º211	 i:1 	 global-step:4221	 l-p:0.130025714635849
epoch£º211	 i:2 	 global-step:4222	 l-p:0.25202465057373047
epoch£º211	 i:3 	 global-step:4223	 l-p:0.13792239129543304
epoch£º211	 i:4 	 global-step:4224	 l-p:0.11072137951850891
epoch£º211	 i:5 	 global-step:4225	 l-p:1.311414122581482
epoch£º211	 i:6 	 global-step:4226	 l-p:0.12206429243087769
epoch£º211	 i:7 	 global-step:4227	 l-p:0.1667718142271042
epoch£º211	 i:8 	 global-step:4228	 l-p:0.08507946878671646
epoch£º211	 i:9 	 global-step:4229	 l-p:0.10522057116031647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7517, 4.0677, 4.1182],
        [3.7517, 3.7517, 3.7517],
        [3.7517, 3.7517, 3.7517],
        [3.7517, 4.0467, 4.0832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.11932002753019333 
model_pd.l_d.mean(): -23.499696731567383 
model_pd.lagr.mean(): -23.3803768157959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0215], device='cuda:0')), ('power', tensor([-23.4782], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.11932002753019333
epoch£º212	 i:1 	 global-step:4241	 l-p:0.07990270107984543
epoch£º212	 i:2 	 global-step:4242	 l-p:0.10637876391410828
epoch£º212	 i:3 	 global-step:4243	 l-p:0.12382744252681732
epoch£º212	 i:4 	 global-step:4244	 l-p:0.124368816614151
epoch£º212	 i:5 	 global-step:4245	 l-p:0.11589368432760239
epoch£º212	 i:6 	 global-step:4246	 l-p:0.04718721657991409
epoch£º212	 i:7 	 global-step:4247	 l-p:0.044549401849508286
epoch£º212	 i:8 	 global-step:4248	 l-p:0.12670637667179108
epoch£º212	 i:9 	 global-step:4249	 l-p:0.13681362569332123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8302, 3.8306, 3.8302],
        [3.8302, 3.8302, 3.8302],
        [3.8302, 3.9445, 3.9038],
        [3.8302, 3.8360, 3.8304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.09633442759513855 
model_pd.l_d.mean(): -22.473907470703125 
model_pd.lagr.mean(): -22.377573013305664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0898], device='cuda:0')), ('power', tensor([-22.5637], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.09633442759513855
epoch£º213	 i:1 	 global-step:4261	 l-p:-0.0005301237106323242
epoch£º213	 i:2 	 global-step:4262	 l-p:0.10308746248483658
epoch£º213	 i:3 	 global-step:4263	 l-p:0.1203233003616333
epoch£º213	 i:4 	 global-step:4264	 l-p:0.0449751652777195
epoch£º213	 i:5 	 global-step:4265	 l-p:0.1043584793806076
epoch£º213	 i:6 	 global-step:4266	 l-p:0.1162932813167572
epoch£º213	 i:7 	 global-step:4267	 l-p:-0.10171274095773697
epoch£º213	 i:8 	 global-step:4268	 l-p:0.12217811495065689
epoch£º213	 i:9 	 global-step:4269	 l-p:0.11473623663187027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8022, 3.8023, 3.8022],
        [3.8022, 3.8211, 3.8048],
        [3.8022, 3.9072, 3.8663],
        [3.8022, 3.8022, 3.8022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.03892037272453308 
model_pd.l_d.mean(): -23.2210750579834 
model_pd.lagr.mean(): -23.18215560913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0018], device='cuda:0')), ('power', tensor([-23.2229], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.03892037272453308
epoch£º214	 i:1 	 global-step:4281	 l-p:0.10112395137548447
epoch£º214	 i:2 	 global-step:4282	 l-p:0.05825380980968475
epoch£º214	 i:3 	 global-step:4283	 l-p:0.11186465620994568
epoch£º214	 i:4 	 global-step:4284	 l-p:0.1666853278875351
epoch£º214	 i:5 	 global-step:4285	 l-p:0.11419780552387238
epoch£º214	 i:6 	 global-step:4286	 l-p:0.15467599034309387
epoch£º214	 i:7 	 global-step:4287	 l-p:0.0901830643415451
epoch£º214	 i:8 	 global-step:4288	 l-p:0.08089543879032135
epoch£º214	 i:9 	 global-step:4289	 l-p:0.14321394264698029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8197, 3.9258, 3.8847],
        [3.8197, 3.8198, 3.8197],
        [3.8197, 3.8267, 3.8199],
        [3.8197, 3.8197, 3.8197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.13708171248435974 
model_pd.l_d.mean(): -23.17966079711914 
model_pd.lagr.mean(): -23.042579650878906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0105], device='cuda:0')), ('power', tensor([-23.1902], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.13708171248435974
epoch£º215	 i:1 	 global-step:4301	 l-p:-0.08425186574459076
epoch£º215	 i:2 	 global-step:4302	 l-p:0.08754117786884308
epoch£º215	 i:3 	 global-step:4303	 l-p:0.11780013889074326
epoch£º215	 i:4 	 global-step:4304	 l-p:0.10605858266353607
epoch£º215	 i:5 	 global-step:4305	 l-p:0.11347795277833939
epoch£º215	 i:6 	 global-step:4306	 l-p:0.14717721939086914
epoch£º215	 i:7 	 global-step:4307	 l-p:0.021078215911984444
epoch£º215	 i:8 	 global-step:4308	 l-p:0.1014498695731163
epoch£º215	 i:9 	 global-step:4309	 l-p:0.13619548082351685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8107, 3.8109, 3.8107],
        [3.8107, 4.0410, 4.0370],
        [3.8107, 3.8194, 3.8111],
        [3.8107, 3.8107, 3.8107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.08290792256593704 
model_pd.l_d.mean(): -22.245834350585938 
model_pd.lagr.mean(): -22.162925720214844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0607], device='cuda:0')), ('power', tensor([-22.3065], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.08290792256593704
epoch£º216	 i:1 	 global-step:4321	 l-p:0.02656109817326069
epoch£º216	 i:2 	 global-step:4322	 l-p:0.09883209317922592
epoch£º216	 i:3 	 global-step:4323	 l-p:0.1172867864370346
epoch£º216	 i:4 	 global-step:4324	 l-p:0.21849748492240906
epoch£º216	 i:5 	 global-step:4325	 l-p:0.12805971503257751
epoch£º216	 i:6 	 global-step:4326	 l-p:0.10996509343385696
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12207794934511185
epoch£º216	 i:8 	 global-step:4328	 l-p:0.16582120954990387
epoch£º216	 i:9 	 global-step:4329	 l-p:0.10992051661014557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8928, 3.8989, 3.8931],
        [3.8928, 4.2177, 4.2649],
        [3.8928, 4.0300, 3.9909],
        [3.8928, 3.8928, 3.8928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.11044355481863022 
model_pd.l_d.mean(): -23.232574462890625 
model_pd.lagr.mean(): -23.12213134765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0413], device='cuda:0')), ('power', tensor([-23.1913], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.11044355481863022
epoch£º217	 i:1 	 global-step:4341	 l-p:0.11951275914907455
epoch£º217	 i:2 	 global-step:4342	 l-p:0.08296112716197968
epoch£º217	 i:3 	 global-step:4343	 l-p:0.13497920334339142
epoch£º217	 i:4 	 global-step:4344	 l-p:0.11671435087919235
epoch£º217	 i:5 	 global-step:4345	 l-p:0.09642185270786285
epoch£º217	 i:6 	 global-step:4346	 l-p:0.07584350556135178
epoch£º217	 i:7 	 global-step:4347	 l-p:0.10771465301513672
epoch£º217	 i:8 	 global-step:4348	 l-p:0.060082726180553436
epoch£º217	 i:9 	 global-step:4349	 l-p:0.09059038013219833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8059, 3.8071, 3.8058],
        [3.8059, 3.8059, 3.8059],
        [3.8059, 3.8059, 3.8059],
        [3.8059, 3.8059, 3.8059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.11666158586740494 
model_pd.l_d.mean(): -23.281522750854492 
model_pd.lagr.mean(): -23.16486167907715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0095], device='cuda:0')), ('power', tensor([-23.2720], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.11666158586740494
epoch£º218	 i:1 	 global-step:4361	 l-p:0.13293491303920746
epoch£º218	 i:2 	 global-step:4362	 l-p:0.12887714803218842
epoch£º218	 i:3 	 global-step:4363	 l-p:0.11972752958536148
epoch£º218	 i:4 	 global-step:4364	 l-p:0.0915493294596672
epoch£º218	 i:5 	 global-step:4365	 l-p:0.08951184898614883
epoch£º218	 i:6 	 global-step:4366	 l-p:0.1232946440577507
epoch£º218	 i:7 	 global-step:4367	 l-p:0.005682541988790035
epoch£º218	 i:8 	 global-step:4368	 l-p:0.07255568355321884
epoch£º218	 i:9 	 global-step:4369	 l-p:0.12800298631191254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8285, 3.8308, 3.8285],
        [3.8285, 3.8343, 3.8286],
        [3.8285, 3.9337, 3.8922],
        [3.8285, 4.5431, 4.9443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.11267482489347458 
model_pd.l_d.mean(): -22.05986785888672 
model_pd.lagr.mean(): -21.947193145751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0807], device='cuda:0')), ('power', tensor([-22.1405], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.11267482489347458
epoch£º219	 i:1 	 global-step:4381	 l-p:0.0987560898065567
epoch£º219	 i:2 	 global-step:4382	 l-p:0.12298497557640076
epoch£º219	 i:3 	 global-step:4383	 l-p:0.16093622148036957
epoch£º219	 i:4 	 global-step:4384	 l-p:0.009703769348561764
epoch£º219	 i:5 	 global-step:4385	 l-p:-0.01786438375711441
epoch£º219	 i:6 	 global-step:4386	 l-p:0.03897223249077797
epoch£º219	 i:7 	 global-step:4387	 l-p:0.11362777650356293
epoch£º219	 i:8 	 global-step:4388	 l-p:0.10390293598175049
epoch£º219	 i:9 	 global-step:4389	 l-p:0.12499698251485825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8790, 3.8791, 3.8790],
        [3.8790, 3.9206, 3.8912],
        [3.8790, 3.8790, 3.8790],
        [3.8790, 3.8863, 3.8793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.09774340689182281 
model_pd.l_d.mean(): -23.061349868774414 
model_pd.lagr.mean(): -22.963605880737305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0089], device='cuda:0')), ('power', tensor([-23.0524], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.09774340689182281
epoch£º220	 i:1 	 global-step:4401	 l-p:0.09512616693973541
epoch£º220	 i:2 	 global-step:4402	 l-p:0.255813866853714
epoch£º220	 i:3 	 global-step:4403	 l-p:0.11228729039430618
epoch£º220	 i:4 	 global-step:4404	 l-p:0.11213607341051102
epoch£º220	 i:5 	 global-step:4405	 l-p:0.12307414412498474
epoch£º220	 i:6 	 global-step:4406	 l-p:0.1436440795660019
epoch£º220	 i:7 	 global-step:4407	 l-p:0.04262756183743477
epoch£º220	 i:8 	 global-step:4408	 l-p:0.03762291744351387
epoch£º220	 i:9 	 global-step:4409	 l-p:0.11666274070739746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7954, 4.1127, 4.1617],
        [3.7954, 3.9910, 3.9718],
        [3.7954, 3.8680, 3.8288],
        [3.7954, 3.7954, 3.7954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.1239418014883995 
model_pd.l_d.mean(): -22.92609977722168 
model_pd.lagr.mean(): -22.80215835571289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0423], device='cuda:0')), ('power', tensor([-22.9684], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.1239418014883995
epoch£º221	 i:1 	 global-step:4421	 l-p:0.10018058866262436
epoch£º221	 i:2 	 global-step:4422	 l-p:0.11332389712333679
epoch£º221	 i:3 	 global-step:4423	 l-p:0.1388157457113266
epoch£º221	 i:4 	 global-step:4424	 l-p:0.08399313688278198
epoch£º221	 i:5 	 global-step:4425	 l-p:0.12579958140850067
epoch£º221	 i:6 	 global-step:4426	 l-p:2.9762868881225586
epoch£º221	 i:7 	 global-step:4427	 l-p:0.08065216988325119
epoch£º221	 i:8 	 global-step:4428	 l-p:0.14148084819316864
epoch£º221	 i:9 	 global-step:4429	 l-p:0.20956824719905853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[3.6941, 4.3663, 4.7423],
        [3.6941, 4.0411, 4.1192],
        [3.6941, 4.3359, 4.6804],
        [3.6941, 4.0118, 4.0685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): -0.7085631489753723 
model_pd.l_d.mean(): -22.54058074951172 
model_pd.lagr.mean(): -23.249143600463867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1514], device='cuda:0')), ('power', tensor([-22.6919], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:-0.7085631489753723
epoch£º222	 i:1 	 global-step:4441	 l-p:0.11936279386281967
epoch£º222	 i:2 	 global-step:4442	 l-p:0.14418038725852966
epoch£º222	 i:3 	 global-step:4443	 l-p:0.07431422173976898
epoch£º222	 i:4 	 global-step:4444	 l-p:0.12014083564281464
epoch£º222	 i:5 	 global-step:4445	 l-p:0.11664241552352905
epoch£º222	 i:6 	 global-step:4446	 l-p:0.11200623959302902
epoch£º222	 i:7 	 global-step:4447	 l-p:0.09225631505250931
epoch£º222	 i:8 	 global-step:4448	 l-p:0.11982671171426773
epoch£º222	 i:9 	 global-step:4449	 l-p:-0.01455590222030878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6319, 3.6318, 3.6319],
        [3.6319, 3.6949, 3.6570],
        [3.6319, 3.7491, 3.7107],
        [3.6319, 3.6352, 3.6310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.12877105176448822 
model_pd.l_d.mean(): -23.351091384887695 
model_pd.lagr.mean(): -23.222320556640625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0604], device='cuda:0')), ('power', tensor([-23.4114], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.12877105176448822
epoch£º223	 i:1 	 global-step:4461	 l-p:0.11438676714897156
epoch£º223	 i:2 	 global-step:4462	 l-p:0.06989736109972
epoch£º223	 i:3 	 global-step:4463	 l-p:0.07018982619047165
epoch£º223	 i:4 	 global-step:4464	 l-p:0.05367026850581169
epoch£º223	 i:5 	 global-step:4465	 l-p:0.12501242756843567
epoch£º223	 i:6 	 global-step:4466	 l-p:0.15585775673389435
epoch£º223	 i:7 	 global-step:4467	 l-p:0.2132025808095932
epoch£º223	 i:8 	 global-step:4468	 l-p:0.1247679814696312
epoch£º223	 i:9 	 global-step:4469	 l-p:0.12270542979240417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6005, 3.6004, 3.6005],
        [3.6005, 3.6004, 3.6005],
        [3.6005, 3.6114, 3.5990],
        [3.6005, 3.6003, 3.6004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.11212942004203796 
model_pd.l_d.mean(): -23.296804428100586 
model_pd.lagr.mean(): -23.184675216674805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0611], device='cuda:0')), ('power', tensor([-23.3579], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.11212942004203796
epoch£º224	 i:1 	 global-step:4481	 l-p:0.10944531112909317
epoch£º224	 i:2 	 global-step:4482	 l-p:0.12347644567489624
epoch£º224	 i:3 	 global-step:4483	 l-p:0.16550011932849884
epoch£º224	 i:4 	 global-step:4484	 l-p:0.0848885104060173
epoch£º224	 i:5 	 global-step:4485	 l-p:0.1232922151684761
epoch£º224	 i:6 	 global-step:4486	 l-p:0.1258874535560608
epoch£º224	 i:7 	 global-step:4487	 l-p:-0.09562135487794876
epoch£º224	 i:8 	 global-step:4488	 l-p:0.10981252789497375
epoch£º224	 i:9 	 global-step:4489	 l-p:0.11055738478899002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5831, 3.5831, 3.5831],
        [3.5831, 3.5831, 3.5831],
        [3.5831, 3.5831, 3.5831],
        [3.5831, 3.6478, 3.6091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.10903441905975342 
model_pd.l_d.mean(): -22.700830459594727 
model_pd.lagr.mean(): -22.591796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1233], device='cuda:0')), ('power', tensor([-22.8241], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.10903441905975342
epoch£º225	 i:1 	 global-step:4501	 l-p:0.03424014896154404
epoch£º225	 i:2 	 global-step:4502	 l-p:0.0987967774271965
epoch£º225	 i:3 	 global-step:4503	 l-p:0.12546497583389282
epoch£º225	 i:4 	 global-step:4504	 l-p:0.12634660303592682
epoch£º225	 i:5 	 global-step:4505	 l-p:0.1252232939004898
epoch£º225	 i:6 	 global-step:4506	 l-p:0.14405612647533417
epoch£º225	 i:7 	 global-step:4507	 l-p:0.23792022466659546
epoch£º225	 i:8 	 global-step:4508	 l-p:0.07238123565912247
epoch£º225	 i:9 	 global-step:4509	 l-p:0.13228410482406616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5897, 3.6738, 3.6331],
        [3.5897, 3.9072, 3.9721],
        [3.5897, 3.6191, 3.5927],
        [3.5897, 3.5896, 3.5897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.1004406213760376 
model_pd.l_d.mean(): -23.291210174560547 
model_pd.lagr.mean(): -23.19076919555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0810], device='cuda:0')), ('power', tensor([-23.3722], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.1004406213760376
epoch£º226	 i:1 	 global-step:4521	 l-p:0.1306353360414505
epoch£º226	 i:2 	 global-step:4522	 l-p:0.12764279544353485
epoch£º226	 i:3 	 global-step:4523	 l-p:0.2823050916194916
epoch£º226	 i:4 	 global-step:4524	 l-p:0.12575659155845642
epoch£º226	 i:5 	 global-step:4525	 l-p:0.10479535907506943
epoch£º226	 i:6 	 global-step:4526	 l-p:0.07820598036050797
epoch£º226	 i:7 	 global-step:4527	 l-p:0.11800459772348404
epoch£º226	 i:8 	 global-step:4528	 l-p:0.12425393611192703
epoch£º226	 i:9 	 global-step:4529	 l-p:0.08056817948818207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5927, 3.6503, 3.6128],
        [3.5927, 3.5924, 3.5926],
        [3.5927, 3.5998, 3.5907],
        [3.5927, 3.5927, 3.5927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.12590043246746063 
model_pd.l_d.mean(): -22.868337631225586 
model_pd.lagr.mean(): -22.7424373626709 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1109], device='cuda:0')), ('power', tensor([-22.9793], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.12590043246746063
epoch£º227	 i:1 	 global-step:4541	 l-p:0.09105828404426575
epoch£º227	 i:2 	 global-step:4542	 l-p:0.11841503530740738
epoch£º227	 i:3 	 global-step:4543	 l-p:0.12129436433315277
epoch£º227	 i:4 	 global-step:4544	 l-p:0.04076501354575157
epoch£º227	 i:5 	 global-step:4545	 l-p:0.12368571013212204
epoch£º227	 i:6 	 global-step:4546	 l-p:0.04008874669671059
epoch£º227	 i:7 	 global-step:4547	 l-p:0.15651288628578186
epoch£º227	 i:8 	 global-step:4548	 l-p:0.09860176593065262
epoch£º227	 i:9 	 global-step:4549	 l-p:0.13546347618103027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6867, 3.6867, 3.6867],
        [3.6867, 3.6867, 3.6867],
        [3.6867, 3.6866, 3.6867],
        [3.6867, 3.7000, 3.6859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.1201266199350357 
model_pd.l_d.mean(): -23.2224178314209 
model_pd.lagr.mean(): -23.102291107177734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0563], device='cuda:0')), ('power', tensor([-23.2787], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.1201266199350357
epoch£º228	 i:1 	 global-step:4561	 l-p:0.10790591686964035
epoch£º228	 i:2 	 global-step:4562	 l-p:0.11650151014328003
epoch£º228	 i:3 	 global-step:4563	 l-p:0.1277996450662613
epoch£º228	 i:4 	 global-step:4564	 l-p:0.4697965979576111
epoch£º228	 i:5 	 global-step:4565	 l-p:0.5968994498252869
epoch£º228	 i:6 	 global-step:4566	 l-p:0.12125759571790695
epoch£º228	 i:7 	 global-step:4567	 l-p:0.13426265120506287
epoch£º228	 i:8 	 global-step:4568	 l-p:0.10483270138502121
epoch£º228	 i:9 	 global-step:4569	 l-p:0.08598380535840988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6925, 3.6925, 3.6925],
        [3.6925, 4.0500, 4.1367],
        [3.6925, 3.6975, 3.6914],
        [3.6925, 3.6952, 3.6917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.10484927147626877 
model_pd.l_d.mean(): -21.905344009399414 
model_pd.lagr.mean(): -21.800495147705078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1601], device='cuda:0')), ('power', tensor([-22.0654], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.10484927147626877
epoch£º229	 i:1 	 global-step:4581	 l-p:0.09972024708986282
epoch£º229	 i:2 	 global-step:4582	 l-p:0.1103241890668869
epoch£º229	 i:3 	 global-step:4583	 l-p:0.12474194914102554
epoch£º229	 i:4 	 global-step:4584	 l-p:0.15342316031455994
epoch£º229	 i:5 	 global-step:4585	 l-p:0.13109475374221802
epoch£º229	 i:6 	 global-step:4586	 l-p:0.11975479871034622
epoch£º229	 i:7 	 global-step:4587	 l-p:0.20764903724193573
epoch£º229	 i:8 	 global-step:4588	 l-p:1.4673922061920166
epoch£º229	 i:9 	 global-step:4589	 l-p:0.15862533450126648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[3.7231, 3.7316, 3.7220],
        [3.7231, 4.3522, 4.6806],
        [3.7231, 3.7360, 3.7223],
        [3.7231, 3.8416, 3.8015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.14309702813625336 
model_pd.l_d.mean(): -22.187416076660156 
model_pd.lagr.mean(): -22.04431915283203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1254], device='cuda:0')), ('power', tensor([-22.3128], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.14309702813625336
epoch£º230	 i:1 	 global-step:4601	 l-p:0.12801112234592438
epoch£º230	 i:2 	 global-step:4602	 l-p:0.1204208955168724
epoch£º230	 i:3 	 global-step:4603	 l-p:0.12328173965215683
epoch£º230	 i:4 	 global-step:4604	 l-p:0.0951230376958847
epoch£º230	 i:5 	 global-step:4605	 l-p:0.2011602222919464
epoch£º230	 i:6 	 global-step:4606	 l-p:0.11194011569023132
epoch£º230	 i:7 	 global-step:4607	 l-p:0.0857122391462326
epoch£º230	 i:8 	 global-step:4608	 l-p:0.11591939628124237
epoch£º230	 i:9 	 global-step:4609	 l-p:0.10551333427429199
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7998, 3.7998, 3.7998],
        [3.7998, 3.7998, 3.7998],
        [3.7998, 4.4325, 4.7546],
        [3.7998, 3.8318, 3.8055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.19527441263198853 
model_pd.l_d.mean(): -22.89887809753418 
model_pd.lagr.mean(): -22.703603744506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0602], device='cuda:0')), ('power', tensor([-22.9591], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.19527441263198853
epoch£º231	 i:1 	 global-step:4621	 l-p:0.11408919095993042
epoch£º231	 i:2 	 global-step:4622	 l-p:0.029705194756388664
epoch£º231	 i:3 	 global-step:4623	 l-p:0.13282501697540283
epoch£º231	 i:4 	 global-step:4624	 l-p:0.11553820222616196
epoch£º231	 i:5 	 global-step:4625	 l-p:0.1180810034275055
epoch£º231	 i:6 	 global-step:4626	 l-p:0.09323924779891968
epoch£º231	 i:7 	 global-step:4627	 l-p:0.08850131928920746
epoch£º231	 i:8 	 global-step:4628	 l-p:0.08640680462121964
epoch£º231	 i:9 	 global-step:4629	 l-p:0.0967036709189415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7634, 3.7634, 3.7634],
        [3.7634, 3.7789, 3.7633],
        [3.7634, 4.0315, 4.0514],
        [3.7634, 3.7637, 3.7632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.13171884417533875 
model_pd.l_d.mean(): -23.263872146606445 
model_pd.lagr.mean(): -23.132152557373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0220], device='cuda:0')), ('power', tensor([-23.2859], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.13171884417533875
epoch£º232	 i:1 	 global-step:4641	 l-p:0.10393873602151871
epoch£º232	 i:2 	 global-step:4642	 l-p:0.09706549346446991
epoch£º232	 i:3 	 global-step:4643	 l-p:0.10947627574205399
epoch£º232	 i:4 	 global-step:4644	 l-p:0.10949071496725082
epoch£º232	 i:5 	 global-step:4645	 l-p:0.13954973220825195
epoch£º232	 i:6 	 global-step:4646	 l-p:0.11504123359918594
epoch£º232	 i:7 	 global-step:4647	 l-p:0.06046260520815849
epoch£º232	 i:8 	 global-step:4648	 l-p:0.015389026142656803
epoch£º232	 i:9 	 global-step:4649	 l-p:0.10487540811300278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6191, 3.6347, 3.6169],
        [3.6191, 3.6236, 3.6167],
        [3.6191, 3.7866, 3.7619],
        [3.6191, 4.1218, 4.3408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.15553927421569824 
model_pd.l_d.mean(): -22.8583984375 
model_pd.lagr.mean(): -22.70285987854004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1423], device='cuda:0')), ('power', tensor([-23.0007], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.15553927421569824
epoch£º233	 i:1 	 global-step:4661	 l-p:0.11925582587718964
epoch£º233	 i:2 	 global-step:4662	 l-p:0.02308018133044243
epoch£º233	 i:3 	 global-step:4663	 l-p:0.11873257160186768
epoch£º233	 i:4 	 global-step:4664	 l-p:0.051542796194553375
epoch£º233	 i:5 	 global-step:4665	 l-p:0.11972649395465851
epoch£º233	 i:6 	 global-step:4666	 l-p:0.06370727717876434
epoch£º233	 i:7 	 global-step:4667	 l-p:0.12225036323070526
epoch£º233	 i:8 	 global-step:4668	 l-p:0.4224531352519989
epoch£º233	 i:9 	 global-step:4669	 l-p:0.13182920217514038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7173, 3.7174, 3.7170],
        [3.7173, 3.7199, 3.7162],
        [3.7173, 3.7225, 3.7158],
        [3.7173, 3.7173, 3.7173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.1265147179365158 
model_pd.l_d.mean(): -22.849964141845703 
model_pd.lagr.mean(): -22.72344970703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0982], device='cuda:0')), ('power', tensor([-22.9482], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.1265147179365158
epoch£º234	 i:1 	 global-step:4681	 l-p:0.11955916881561279
epoch£º234	 i:2 	 global-step:4682	 l-p:0.12309470772743225
epoch£º234	 i:3 	 global-step:4683	 l-p:0.3673924207687378
epoch£º234	 i:4 	 global-step:4684	 l-p:0.10004443675279617
epoch£º234	 i:5 	 global-step:4685	 l-p:0.11636865139007568
epoch£º234	 i:6 	 global-step:4686	 l-p:0.19148509204387665
epoch£º234	 i:7 	 global-step:4687	 l-p:0.1251668930053711
epoch£º234	 i:8 	 global-step:4688	 l-p:0.08086113631725311
epoch£º234	 i:9 	 global-step:4689	 l-p:0.1056436151266098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7293, 4.1847, 4.3491],
        [3.7293, 3.7293, 3.7293],
        [3.7293, 3.7296, 3.7289],
        [3.7293, 3.7293, 3.7293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.10771320015192032 
model_pd.l_d.mean(): -23.19949722290039 
model_pd.lagr.mean(): -23.09178352355957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0300], device='cuda:0')), ('power', tensor([-23.2295], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.10771320015192032
epoch£º235	 i:1 	 global-step:4701	 l-p:0.10754384845495224
epoch£º235	 i:2 	 global-step:4702	 l-p:0.1251407265663147
epoch£º235	 i:3 	 global-step:4703	 l-p:0.42248672246932983
epoch£º235	 i:4 	 global-step:4704	 l-p:0.11992951482534409
epoch£º235	 i:5 	 global-step:4705	 l-p:0.3774568438529968
epoch£º235	 i:6 	 global-step:4706	 l-p:0.11582297086715698
epoch£º235	 i:7 	 global-step:4707	 l-p:-0.1169125884771347
epoch£º235	 i:8 	 global-step:4708	 l-p:0.10838087648153305
epoch£º235	 i:9 	 global-step:4709	 l-p:0.09789146482944489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7308, 3.8656, 3.8279],
        [3.7308, 3.7307, 3.7308],
        [3.7308, 3.8155, 3.7729],
        [3.7308, 3.7420, 3.7291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.12527117133140564 
model_pd.l_d.mean(): -23.16976547241211 
model_pd.lagr.mean(): -23.04449462890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0422], device='cuda:0')), ('power', tensor([-23.2119], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.12527117133140564
epoch£º236	 i:1 	 global-step:4721	 l-p:0.18528203666210175
epoch£º236	 i:2 	 global-step:4722	 l-p:0.1971946656703949
epoch£º236	 i:3 	 global-step:4723	 l-p:0.12175912410020828
epoch£º236	 i:4 	 global-step:4724	 l-p:0.12426231056451797
epoch£º236	 i:5 	 global-step:4725	 l-p:0.06208711117506027
epoch£º236	 i:6 	 global-step:4726	 l-p:0.09943876415491104
epoch£º236	 i:7 	 global-step:4727	 l-p:0.12024679780006409
epoch£º236	 i:8 	 global-step:4728	 l-p:0.12407726794481277
epoch£º236	 i:9 	 global-step:4729	 l-p:0.16019542515277863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7489, 4.2694, 4.4902],
        [3.7489, 3.8003, 3.7639],
        [3.7489, 3.7489, 3.7489],
        [3.7489, 3.8347, 3.7919]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.1009085550904274 
model_pd.l_d.mean(): -22.536907196044922 
model_pd.lagr.mean(): -22.435998916625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0958], device='cuda:0')), ('power', tensor([-22.6327], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.1009085550904274
epoch£º237	 i:1 	 global-step:4741	 l-p:0.12823402881622314
epoch£º237	 i:2 	 global-step:4742	 l-p:0.1294381469488144
epoch£º237	 i:3 	 global-step:4743	 l-p:0.1162782609462738
epoch£º237	 i:4 	 global-step:4744	 l-p:0.10125984996557236
epoch£º237	 i:5 	 global-step:4745	 l-p:0.6400161385536194
epoch£º237	 i:6 	 global-step:4746	 l-p:0.21861030161380768
epoch£º237	 i:7 	 global-step:4747	 l-p:0.09304311871528625
epoch£º237	 i:8 	 global-step:4748	 l-p:0.17852729558944702
epoch£º237	 i:9 	 global-step:4749	 l-p:0.09078788757324219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7507, 4.3474, 4.6407],
        [3.7507, 3.7770, 3.7523],
        [3.7507, 4.0855, 4.1513],
        [3.7507, 3.7507, 3.7507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.15651674568653107 
model_pd.l_d.mean(): -20.969873428344727 
model_pd.lagr.mean(): -20.813356399536133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2309], device='cuda:0')), ('power', tensor([-21.2008], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.15651674568653107
epoch£º238	 i:1 	 global-step:4761	 l-p:0.11396446079015732
epoch£º238	 i:2 	 global-step:4762	 l-p:0.15642046928405762
epoch£º238	 i:3 	 global-step:4763	 l-p:0.09051382541656494
epoch£º238	 i:4 	 global-step:4764	 l-p:0.11407220363616943
epoch£º238	 i:5 	 global-step:4765	 l-p:0.0595681369304657
epoch£º238	 i:6 	 global-step:4766	 l-p:-0.2749389111995697
epoch£º238	 i:7 	 global-step:4767	 l-p:0.11989888548851013
epoch£º238	 i:8 	 global-step:4768	 l-p:0.3663971722126007
epoch£º238	 i:9 	 global-step:4769	 l-p:0.09583576023578644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[3.9173, 4.2816, 4.3564],
        [3.9173, 4.5051, 4.7701],
        [3.9173, 3.9796, 3.9406],
        [3.9173, 4.1793, 4.1876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.1121087446808815 
model_pd.l_d.mean(): -23.0177059173584 
model_pd.lagr.mean(): -22.905597686767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0032], device='cuda:0')), ('power', tensor([-23.0209], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.1121087446808815
epoch£º239	 i:1 	 global-step:4781	 l-p:0.11284180730581284
epoch£º239	 i:2 	 global-step:4782	 l-p:0.10977718234062195
epoch£º239	 i:3 	 global-step:4783	 l-p:0.1397150307893753
epoch£º239	 i:4 	 global-step:4784	 l-p:-5.485781192779541
epoch£º239	 i:5 	 global-step:4785	 l-p:-0.006896691396832466
epoch£º239	 i:6 	 global-step:4786	 l-p:-0.6159513592720032
epoch£º239	 i:7 	 global-step:4787	 l-p:0.1177309975028038
epoch£º239	 i:8 	 global-step:4788	 l-p:0.11102981120347977
epoch£º239	 i:9 	 global-step:4789	 l-p:0.10555451363325119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8350, 4.0803, 4.0834],
        [3.8350, 3.8604, 3.8371],
        [3.8350, 3.8830, 3.8481],
        [3.8350, 3.8349, 3.8350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.11246951669454575 
model_pd.l_d.mean(): -23.279874801635742 
model_pd.lagr.mean(): -23.16740608215332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0228], device='cuda:0')), ('power', tensor([-23.2570], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.11246951669454575
epoch£º240	 i:1 	 global-step:4801	 l-p:0.09244269132614136
epoch£º240	 i:2 	 global-step:4802	 l-p:0.13838228583335876
epoch£º240	 i:3 	 global-step:4803	 l-p:0.10409767925739288
epoch£º240	 i:4 	 global-step:4804	 l-p:0.10723718255758286
epoch£º240	 i:5 	 global-step:4805	 l-p:0.10459598153829575
epoch£º240	 i:6 	 global-step:4806	 l-p:0.161356583237648
epoch£º240	 i:7 	 global-step:4807	 l-p:0.1101708859205246
epoch£º240	 i:8 	 global-step:4808	 l-p:-1.8217525482177734
epoch£º240	 i:9 	 global-step:4809	 l-p:0.12436271458864212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7103, 3.7367, 3.7110],
        [3.7103, 3.7103, 3.7103],
        [3.7103, 4.0402, 4.1056],
        [3.7103, 4.2139, 4.4238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.12052955478429794 
model_pd.l_d.mean(): -23.148338317871094 
model_pd.lagr.mean(): -23.027809143066406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0545], device='cuda:0')), ('power', tensor([-23.2029], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.12052955478429794
epoch£º241	 i:1 	 global-step:4821	 l-p:0.14540977776050568
epoch£º241	 i:2 	 global-step:4822	 l-p:0.11075936257839203
epoch£º241	 i:3 	 global-step:4823	 l-p:0.12386683374643326
epoch£º241	 i:4 	 global-step:4824	 l-p:0.11809568107128143
epoch£º241	 i:5 	 global-step:4825	 l-p:0.10122749209403992
epoch£º241	 i:6 	 global-step:4826	 l-p:0.9330366849899292
epoch£º241	 i:7 	 global-step:4827	 l-p:0.18944188952445984
epoch£º241	 i:8 	 global-step:4828	 l-p:0.10181529819965363
epoch£º241	 i:9 	 global-step:4829	 l-p:0.09340652078390121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7304, 3.9089, 3.8845],
        [3.7304, 3.7363, 3.7280],
        [3.7304, 3.7304, 3.7304],
        [3.7304, 4.1786, 4.3372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.13738156855106354 
model_pd.l_d.mean(): -23.045516967773438 
model_pd.lagr.mean(): -22.90813446044922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0620], device='cuda:0')), ('power', tensor([-23.1075], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.13738156855106354
epoch£º242	 i:1 	 global-step:4841	 l-p:0.09007280319929123
epoch£º242	 i:2 	 global-step:4842	 l-p:0.2997420132160187
epoch£º242	 i:3 	 global-step:4843	 l-p:0.1610621213912964
epoch£º242	 i:4 	 global-step:4844	 l-p:0.15968480706214905
epoch£º242	 i:5 	 global-step:4845	 l-p:0.07946102321147919
epoch£º242	 i:6 	 global-step:4846	 l-p:0.12639987468719482
epoch£º242	 i:7 	 global-step:4847	 l-p:0.10394904017448425
epoch£º242	 i:8 	 global-step:4848	 l-p:0.005783858243376017
epoch£º242	 i:9 	 global-step:4849	 l-p:-0.006807913538068533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8708, 3.8869, 3.8704],
        [3.8708, 3.8707, 3.8708],
        [3.8708, 3.8708, 3.8708],
        [3.8708, 4.3133, 4.4537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.1150222048163414 
model_pd.l_d.mean(): -22.86296844482422 
model_pd.lagr.mean(): -22.74794578552246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0037], device='cuda:0')), ('power', tensor([-22.8666], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.1150222048163414
epoch£º243	 i:1 	 global-step:4861	 l-p:0.1195850521326065
epoch£º243	 i:2 	 global-step:4862	 l-p:-0.08130690455436707
epoch£º243	 i:3 	 global-step:4863	 l-p:0.12206044793128967
epoch£º243	 i:4 	 global-step:4864	 l-p:0.14210163056850433
epoch£º243	 i:5 	 global-step:4865	 l-p:0.1142548993229866
epoch£º243	 i:6 	 global-step:4866	 l-p:0.5749232172966003
epoch£º243	 i:7 	 global-step:4867	 l-p:0.1258915662765503
epoch£º243	 i:8 	 global-step:4868	 l-p:-0.0737491026520729
epoch£º243	 i:9 	 global-step:4869	 l-p:0.07303912192583084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[3.8559, 3.8645, 3.8545],
        [3.8559, 4.0624, 4.0454],
        [3.8559, 3.8784, 3.8568],
        [3.8559, 4.2561, 4.3635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.15194416046142578 
model_pd.l_d.mean(): -23.128738403320312 
model_pd.lagr.mean(): -22.976795196533203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0070], device='cuda:0')), ('power', tensor([-23.1358], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.15194416046142578
epoch£º244	 i:1 	 global-step:4881	 l-p:0.10478988289833069
epoch£º244	 i:2 	 global-step:4882	 l-p:0.11932831257581711
epoch£º244	 i:3 	 global-step:4883	 l-p:0.12995783984661102
epoch£º244	 i:4 	 global-step:4884	 l-p:0.0717768445611
epoch£º244	 i:5 	 global-step:4885	 l-p:0.0556907057762146
epoch£º244	 i:6 	 global-step:4886	 l-p:0.09462367743253708
epoch£º244	 i:7 	 global-step:4887	 l-p:0.11257228255271912
epoch£º244	 i:8 	 global-step:4888	 l-p:0.1588268280029297
epoch£º244	 i:9 	 global-step:4889	 l-p:0.11696376651525497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7726, 3.7726, 3.7726],
        [3.7726, 3.8697, 3.8258],
        [3.7726, 3.9702, 3.9523],
        [3.7726, 3.7726, 3.7726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.12622304260730743 
model_pd.l_d.mean(): -22.809690475463867 
model_pd.lagr.mean(): -22.683467864990234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0845], device='cuda:0')), ('power', tensor([-22.8942], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.12622304260730743
epoch£º245	 i:1 	 global-step:4901	 l-p:0.11589299887418747
epoch£º245	 i:2 	 global-step:4902	 l-p:0.09247729182243347
epoch£º245	 i:3 	 global-step:4903	 l-p:0.08568085730075836
epoch£º245	 i:4 	 global-step:4904	 l-p:0.2200590819120407
epoch£º245	 i:5 	 global-step:4905	 l-p:0.11870105564594269
epoch£º245	 i:6 	 global-step:4906	 l-p:0.12691782414913177
epoch£º245	 i:7 	 global-step:4907	 l-p:0.12171262502670288
epoch£º245	 i:8 	 global-step:4908	 l-p:0.04723135754466057
epoch£º245	 i:9 	 global-step:4909	 l-p:0.006031861063092947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6573, 3.9348, 3.9678],
        [3.6573, 3.6571, 3.6573],
        [3.6573, 3.6759, 3.6539],
        [3.6573, 3.6573, 3.6573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.10019289702177048 
model_pd.l_d.mean(): -22.487346649169922 
model_pd.lagr.mean(): -22.38715362548828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1415], device='cuda:0')), ('power', tensor([-22.6289], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.10019289702177048
epoch£º246	 i:1 	 global-step:4921	 l-p:0.11337394267320633
epoch£º246	 i:2 	 global-step:4922	 l-p:0.11955925077199936
epoch£º246	 i:3 	 global-step:4923	 l-p:0.09838731586933136
epoch£º246	 i:4 	 global-step:4924	 l-p:0.11009481549263
epoch£º246	 i:5 	 global-step:4925	 l-p:0.0758449137210846
epoch£º246	 i:6 	 global-step:4926	 l-p:0.7322513461112976
epoch£º246	 i:7 	 global-step:4927	 l-p:0.13869690895080566
epoch£º246	 i:8 	 global-step:4928	 l-p:0.0924966037273407
epoch£º246	 i:9 	 global-step:4929	 l-p:0.08626605570316315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6020, 3.6020, 3.6020],
        [3.6020, 3.6012, 3.5995],
        [3.6020, 3.6392, 3.6045],
        [3.6020, 3.6020, 3.6020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.11029980331659317 
model_pd.l_d.mean(): -22.84987449645996 
model_pd.lagr.mean(): -22.739574432373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1088], device='cuda:0')), ('power', tensor([-22.9587], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.11029980331659317
epoch£º247	 i:1 	 global-step:4941	 l-p:0.10303129255771637
epoch£º247	 i:2 	 global-step:4942	 l-p:0.12185343354940414
epoch£º247	 i:3 	 global-step:4943	 l-p:0.13036587834358215
epoch£º247	 i:4 	 global-step:4944	 l-p:0.049436986446380615
epoch£º247	 i:5 	 global-step:4945	 l-p:0.18229812383651733
epoch£º247	 i:6 	 global-step:4946	 l-p:0.1247260645031929
epoch£º247	 i:7 	 global-step:4947	 l-p:0.0461869090795517
epoch£º247	 i:8 	 global-step:4948	 l-p:0.26146286725997925
epoch£º247	 i:9 	 global-step:4949	 l-p:0.11076546460390091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7832, 3.7832, 3.7832],
        [3.7832, 3.7832, 3.7832],
        [3.7832, 3.7850, 3.7816],
        [3.7832, 3.7832, 3.7832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.1318741887807846 
model_pd.l_d.mean(): -23.27008056640625 
model_pd.lagr.mean(): -23.138206481933594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0137], device='cuda:0')), ('power', tensor([-23.2838], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.1318741887807846
epoch£º248	 i:1 	 global-step:4961	 l-p:0.13670672476291656
epoch£º248	 i:2 	 global-step:4962	 l-p:0.05610212683677673
epoch£º248	 i:3 	 global-step:4963	 l-p:0.04364301636815071
epoch£º248	 i:4 	 global-step:4964	 l-p:0.11278048902750015
epoch£º248	 i:5 	 global-step:4965	 l-p:0.20611943304538727
epoch£º248	 i:6 	 global-step:4966	 l-p:0.1263512223958969
epoch£º248	 i:7 	 global-step:4967	 l-p:-0.0790787935256958
epoch£º248	 i:8 	 global-step:4968	 l-p:0.1017051637172699
epoch£º248	 i:9 	 global-step:4969	 l-p:0.09834611415863037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[4.0103, 4.3297, 4.3677],
        [4.0103, 4.3677, 4.4315],
        [4.0103, 4.0678, 4.0294],
        [4.0103, 4.5980, 4.8532]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.11973992735147476 
model_pd.l_d.mean(): -23.20440673828125 
model_pd.lagr.mean(): -23.084667205810547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0660], device='cuda:0')), ('power', tensor([-23.1384], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.11973992735147476
epoch£º249	 i:1 	 global-step:4981	 l-p:0.10927905887365341
epoch£º249	 i:2 	 global-step:4982	 l-p:0.19910377264022827
epoch£º249	 i:3 	 global-step:4983	 l-p:0.11256565153598785
epoch£º249	 i:4 	 global-step:4984	 l-p:0.1359201967716217
epoch£º249	 i:5 	 global-step:4985	 l-p:0.1481981873512268
epoch£º249	 i:6 	 global-step:4986	 l-p:0.12450283020734787
epoch£º249	 i:7 	 global-step:4987	 l-p:0.11795855313539505
epoch£º249	 i:8 	 global-step:4988	 l-p:0.09906128793954849
epoch£º249	 i:9 	 global-step:4989	 l-p:0.11483652144670486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0004, 4.1423, 4.1003],
        [4.0004, 4.0004, 4.0004],
        [4.0004, 4.0099, 3.9998],
        [4.0004, 4.0004, 4.0004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.13222618401050568 
model_pd.l_d.mean(): -22.461530685424805 
model_pd.lagr.mean(): -22.329303741455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0143], device='cuda:0')), ('power', tensor([-22.4758], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.13222618401050568
epoch£º250	 i:1 	 global-step:5001	 l-p:0.10840417444705963
epoch£º250	 i:2 	 global-step:5002	 l-p:0.11028368026018143
epoch£º250	 i:3 	 global-step:5003	 l-p:0.10938872396945953
epoch£º250	 i:4 	 global-step:5004	 l-p:0.10524094104766846
epoch£º250	 i:5 	 global-step:5005	 l-p:0.19720694422721863
epoch£º250	 i:6 	 global-step:5006	 l-p:0.09236175566911697
epoch£º250	 i:7 	 global-step:5007	 l-p:0.11918289959430695
epoch£º250	 i:8 	 global-step:5008	 l-p:0.04470442980527878
epoch£º250	 i:9 	 global-step:5009	 l-p:0.08942058682441711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8343, 3.9337, 3.8887],
        [3.8343, 3.8343, 3.8343],
        [3.8343, 3.8806, 3.8446],
        [3.8343, 3.8343, 3.8343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.043238867074251175 
model_pd.l_d.mean(): -22.9311580657959 
model_pd.lagr.mean(): -22.88791847229004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0423], device='cuda:0')), ('power', tensor([-22.9734], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.043238867074251175
epoch£º251	 i:1 	 global-step:5021	 l-p:0.13468898832798004
epoch£º251	 i:2 	 global-step:5022	 l-p:0.1544416844844818
epoch£º251	 i:3 	 global-step:5023	 l-p:0.1441270262002945
epoch£º251	 i:4 	 global-step:5024	 l-p:0.09045194089412689
epoch£º251	 i:5 	 global-step:5025	 l-p:-0.01829836331307888
epoch£º251	 i:6 	 global-step:5026	 l-p:0.0953255444765091
epoch£º251	 i:7 	 global-step:5027	 l-p:0.12795782089233398
epoch£º251	 i:8 	 global-step:5028	 l-p:0.10901594161987305
epoch£º251	 i:9 	 global-step:5029	 l-p:0.12160550802946091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7803, 3.7801, 3.7803],
        [3.7803, 4.3931, 4.6994],
        [3.7803, 3.8217, 3.7869],
        [3.7803, 3.8795, 3.8347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.09975631535053253 
model_pd.l_d.mean(): -22.88724136352539 
model_pd.lagr.mean(): -22.787485122680664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0835], device='cuda:0')), ('power', tensor([-22.9707], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.09975631535053253
epoch£º252	 i:1 	 global-step:5041	 l-p:0.12357061356306076
epoch£º252	 i:2 	 global-step:5042	 l-p:0.09833429008722305
epoch£º252	 i:3 	 global-step:5043	 l-p:0.10761789977550507
epoch£º252	 i:4 	 global-step:5044	 l-p:-0.20122100412845612
epoch£º252	 i:5 	 global-step:5045	 l-p:0.11367475241422653
epoch£º252	 i:6 	 global-step:5046	 l-p:0.12235641479492188
epoch£º252	 i:7 	 global-step:5047	 l-p:0.1502845287322998
epoch£º252	 i:8 	 global-step:5048	 l-p:0.06461068242788315
epoch£º252	 i:9 	 global-step:5049	 l-p:0.12295734137296677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[3.6034, 4.1452, 4.4042],
        [3.6034, 3.6374, 3.6028],
        [3.6034, 3.6325, 3.6005],
        [3.6034, 3.8476, 3.8628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.08928239345550537 
model_pd.l_d.mean(): -23.267841339111328 
model_pd.lagr.mean(): -23.178558349609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0827], device='cuda:0')), ('power', tensor([-23.3505], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.08928239345550537
epoch£º253	 i:1 	 global-step:5061	 l-p:0.10437288135290146
epoch£º253	 i:2 	 global-step:5062	 l-p:0.13213112950325012
epoch£º253	 i:3 	 global-step:5063	 l-p:0.03688092529773712
epoch£º253	 i:4 	 global-step:5064	 l-p:0.167433500289917
epoch£º253	 i:5 	 global-step:5065	 l-p:0.10797163844108582
epoch£º253	 i:6 	 global-step:5066	 l-p:0.1321445107460022
epoch£º253	 i:7 	 global-step:5067	 l-p:0.12563474476337433
epoch£º253	 i:8 	 global-step:5068	 l-p:0.1184692531824112
epoch£º253	 i:9 	 global-step:5069	 l-p:0.11218799650669098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7887, 3.7915, 3.7860],
        [3.7887, 3.8095, 3.7865],
        [3.7887, 3.7887, 3.7887],
        [3.7887, 3.7887, 3.7887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.07828395813703537 
model_pd.l_d.mean(): -22.974260330200195 
model_pd.lagr.mean(): -22.895977020263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0482], device='cuda:0')), ('power', tensor([-23.0224], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.07828395813703537
epoch£º254	 i:1 	 global-step:5081	 l-p:0.16407661139965057
epoch£º254	 i:2 	 global-step:5082	 l-p:0.04913567379117012
epoch£º254	 i:3 	 global-step:5083	 l-p:-0.021889114752411842
epoch£º254	 i:4 	 global-step:5084	 l-p:0.1388615518808365
epoch£º254	 i:5 	 global-step:5085	 l-p:0.11497150361537933
epoch£º254	 i:6 	 global-step:5086	 l-p:0.10697706043720245
epoch£º254	 i:7 	 global-step:5087	 l-p:0.10523348301649094
epoch£º254	 i:8 	 global-step:5088	 l-p:0.11500894278287888
epoch£º254	 i:9 	 global-step:5089	 l-p:0.09820584952831268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[3.9839, 4.6517, 4.9872],
        [3.9839, 4.0812, 4.0352],
        [3.9839, 4.3105, 4.3552],
        [3.9839, 4.7668, 5.2204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.09238725155591965 
model_pd.l_d.mean(): -22.022796630859375 
model_pd.lagr.mean(): -21.930408477783203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0492], device='cuda:0')), ('power', tensor([-22.0720], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.09238725155591965
epoch£º255	 i:1 	 global-step:5101	 l-p:0.48148903250694275
epoch£º255	 i:2 	 global-step:5102	 l-p:0.15837956964969635
epoch£º255	 i:3 	 global-step:5103	 l-p:0.1311303824186325
epoch£º255	 i:4 	 global-step:5104	 l-p:0.1335688680410385
epoch£º255	 i:5 	 global-step:5105	 l-p:0.09958859533071518
epoch£º255	 i:6 	 global-step:5106	 l-p:0.0879959836602211
epoch£º255	 i:7 	 global-step:5107	 l-p:0.11258847266435623
epoch£º255	 i:8 	 global-step:5108	 l-p:0.11290540546178818
epoch£º255	 i:9 	 global-step:5109	 l-p:0.10369404405355453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9611, 3.9829, 3.9614],
        [3.9611, 3.9617, 3.9606],
        [3.9611, 3.9611, 3.9611],
        [3.9611, 4.2355, 4.2489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.10542318224906921 
model_pd.l_d.mean(): -23.20653533935547 
model_pd.lagr.mean(): -23.101112365722656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0503], device='cuda:0')), ('power', tensor([-23.1563], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.10542318224906921
epoch£º256	 i:1 	 global-step:5121	 l-p:0.13559961318969727
epoch£º256	 i:2 	 global-step:5122	 l-p:0.11414835602045059
epoch£º256	 i:3 	 global-step:5123	 l-p:0.0711071640253067
epoch£º256	 i:4 	 global-step:5124	 l-p:0.12773792445659637
epoch£º256	 i:5 	 global-step:5125	 l-p:0.11144920438528061
epoch£º256	 i:6 	 global-step:5126	 l-p:0.09905526787042618
epoch£º256	 i:7 	 global-step:5127	 l-p:0.12989549338817596
epoch£º256	 i:8 	 global-step:5128	 l-p:-0.0973573699593544
epoch£º256	 i:9 	 global-step:5129	 l-p:0.103672556579113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6299, 3.6299, 3.6299],
        [3.6299, 3.6299, 3.6299],
        [3.6299, 3.7265, 3.6814],
        [3.6299, 3.6280, 3.6273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.09109234809875488 
model_pd.l_d.mean(): -22.593990325927734 
model_pd.lagr.mean(): -22.502897262573242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1070], device='cuda:0')), ('power', tensor([-22.7009], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.09109234809875488
epoch£º257	 i:1 	 global-step:5141	 l-p:0.08429627865552902
epoch£º257	 i:2 	 global-step:5142	 l-p:0.1449306309223175
epoch£º257	 i:3 	 global-step:5143	 l-p:0.14742466807365417
epoch£º257	 i:4 	 global-step:5144	 l-p:0.10566746443510056
epoch£º257	 i:5 	 global-step:5145	 l-p:0.20509906113147736
epoch£º257	 i:6 	 global-step:5146	 l-p:0.10840219259262085
epoch£º257	 i:7 	 global-step:5147	 l-p:0.09341482073068619
epoch£º257	 i:8 	 global-step:5148	 l-p:0.05151708051562309
epoch£º257	 i:9 	 global-step:5149	 l-p:0.10690448433160782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6567, 3.6564, 3.6567],
        [3.6567, 3.6652, 3.6495],
        [3.6567, 3.6567, 3.6567],
        [3.6567, 3.8752, 3.8724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.10841313004493713 
model_pd.l_d.mean(): -22.926612854003906 
model_pd.lagr.mean(): -22.818199157714844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1038], device='cuda:0')), ('power', tensor([-23.0304], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.10841313004493713
epoch£º258	 i:1 	 global-step:5161	 l-p:0.10372939705848694
epoch£º258	 i:2 	 global-step:5162	 l-p:0.09342316538095474
epoch£º258	 i:3 	 global-step:5163	 l-p:0.1266596019268036
epoch£º258	 i:4 	 global-step:5164	 l-p:0.12789684534072876
epoch£º258	 i:5 	 global-step:5165	 l-p:0.12750031054019928
epoch£º258	 i:6 	 global-step:5166	 l-p:0.08944274485111237
epoch£º258	 i:7 	 global-step:5167	 l-p:-0.023553133010864258
epoch£º258	 i:8 	 global-step:5168	 l-p:0.10364803671836853
epoch£º258	 i:9 	 global-step:5169	 l-p:0.115260548889637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5598, 3.7585, 3.7498],
        [3.5598, 3.5598, 3.5598],
        [3.5598, 3.5598, 3.5598],
        [3.5598, 3.5817, 3.5513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): -0.07263244688510895 
model_pd.l_d.mean(): -22.151992797851562 
model_pd.lagr.mean(): -22.224624633789062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1720], device='cuda:0')), ('power', tensor([-22.3240], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:-0.07263244688510895
epoch£º259	 i:1 	 global-step:5181	 l-p:0.1136014312505722
epoch£º259	 i:2 	 global-step:5182	 l-p:0.10681790858507156
epoch£º259	 i:3 	 global-step:5183	 l-p:0.1142430454492569
epoch£º259	 i:4 	 global-step:5184	 l-p:0.1273876577615738
epoch£º259	 i:5 	 global-step:5185	 l-p:0.11663005501031876
epoch£º259	 i:6 	 global-step:5186	 l-p:0.1079535260796547
epoch£º259	 i:7 	 global-step:5187	 l-p:0.09298240393400192
epoch£º259	 i:8 	 global-step:5188	 l-p:0.3064214289188385
epoch£º259	 i:9 	 global-step:5189	 l-p:0.13062003254890442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5125, 3.8087, 3.8660],
        [3.5125, 3.6441, 3.6076],
        [3.5125, 3.5125, 3.5125],
        [3.5125, 3.5366, 3.5030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.12402404844760895 
model_pd.l_d.mean(): -22.961923599243164 
model_pd.lagr.mean(): -22.837900161743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1753], device='cuda:0')), ('power', tensor([-23.1372], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.12402404844760895
epoch£º260	 i:1 	 global-step:5201	 l-p:0.11579891294240952
epoch£º260	 i:2 	 global-step:5202	 l-p:0.13319389522075653
epoch£º260	 i:3 	 global-step:5203	 l-p:0.16155920922756195
epoch£º260	 i:4 	 global-step:5204	 l-p:0.12085969746112823
epoch£º260	 i:5 	 global-step:5205	 l-p:0.10403905808925629
epoch£º260	 i:6 	 global-step:5206	 l-p:0.12546610832214355
epoch£º260	 i:7 	 global-step:5207	 l-p:0.13136178255081177
epoch£º260	 i:8 	 global-step:5208	 l-p:0.09000478684902191
epoch£º260	 i:9 	 global-step:5209	 l-p:0.1082736924290657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5172, 3.5172, 3.5172],
        [3.5172, 3.6801, 3.6555],
        [3.5172, 3.7053, 3.6929],
        [3.5172, 3.5128, 3.5131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.12877008318901062 
model_pd.l_d.mean(): -22.93612289428711 
model_pd.lagr.mean(): -22.80735206604004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1347], device='cuda:0')), ('power', tensor([-23.0708], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.12877008318901062
epoch£º261	 i:1 	 global-step:5221	 l-p:0.1266750693321228
epoch£º261	 i:2 	 global-step:5222	 l-p:0.0967455729842186
epoch£º261	 i:3 	 global-step:5223	 l-p:-0.031760718673467636
epoch£º261	 i:4 	 global-step:5224	 l-p:0.10738948732614517
epoch£º261	 i:5 	 global-step:5225	 l-p:0.1592048555612564
epoch£º261	 i:6 	 global-step:5226	 l-p:0.1148814931511879
epoch£º261	 i:7 	 global-step:5227	 l-p:0.49484798312187195
epoch£º261	 i:8 	 global-step:5228	 l-p:0.12674392759799957
epoch£º261	 i:9 	 global-step:5229	 l-p:0.13187570869922638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4926, 3.4896, 3.4917],
        [3.4926, 3.4877, 3.4860],
        [3.4926, 3.4914, 3.4924],
        [3.4926, 3.4916, 3.4925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.10606595128774643 
model_pd.l_d.mean(): -22.55879020690918 
model_pd.lagr.mean(): -22.45272445678711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1868], device='cuda:0')), ('power', tensor([-22.7456], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.10606595128774643
epoch£º262	 i:1 	 global-step:5241	 l-p:0.1239936426281929
epoch£º262	 i:2 	 global-step:5242	 l-p:0.13001039624214172
epoch£º262	 i:3 	 global-step:5243	 l-p:0.09217569977045059
epoch£º262	 i:4 	 global-step:5244	 l-p:0.12396888434886932
epoch£º262	 i:5 	 global-step:5245	 l-p:0.12329699844121933
epoch£º262	 i:6 	 global-step:5246	 l-p:0.1556096076965332
epoch£º262	 i:7 	 global-step:5247	 l-p:0.17983321845531464
epoch£º262	 i:8 	 global-step:5248	 l-p:-0.029870623722672462
epoch£º262	 i:9 	 global-step:5249	 l-p:0.12922663986682892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6194, 3.8907, 3.9224],
        [3.6194, 3.7734, 3.7414],
        [3.6194, 3.6161, 3.6170],
        [3.6194, 3.8545, 3.8630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.10960081219673157 
model_pd.l_d.mean(): -22.409801483154297 
model_pd.lagr.mean(): -22.300201416015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1394], device='cuda:0')), ('power', tensor([-22.5492], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.10960081219673157
epoch£º263	 i:1 	 global-step:5261	 l-p:0.12556831538677216
epoch£º263	 i:2 	 global-step:5262	 l-p:0.11712699383497238
epoch£º263	 i:3 	 global-step:5263	 l-p:0.10542874038219452
epoch£º263	 i:4 	 global-step:5264	 l-p:0.07611072063446045
epoch£º263	 i:5 	 global-step:5265	 l-p:0.14698895812034607
epoch£º263	 i:6 	 global-step:5266	 l-p:0.028276292607188225
epoch£º263	 i:7 	 global-step:5267	 l-p:0.1128147691488266
epoch£º263	 i:8 	 global-step:5268	 l-p:0.11161670088768005
epoch£º263	 i:9 	 global-step:5269	 l-p:0.11391695588827133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6861, 3.6845, 3.6857],
        [3.6861, 3.9821, 4.0261],
        [3.6861, 4.1538, 4.3352],
        [3.6861, 3.8805, 3.8636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.12478616088628769 
model_pd.l_d.mean(): -23.219629287719727 
model_pd.lagr.mean(): -23.0948429107666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0636], device='cuda:0')), ('power', tensor([-23.2832], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.12478616088628769
epoch£º264	 i:1 	 global-step:5281	 l-p:0.1104101613163948
epoch£º264	 i:2 	 global-step:5282	 l-p:0.1266658753156662
epoch£º264	 i:3 	 global-step:5283	 l-p:0.13956628739833832
epoch£º264	 i:4 	 global-step:5284	 l-p:0.3464164137840271
epoch£º264	 i:5 	 global-step:5285	 l-p:0.11830227822065353
epoch£º264	 i:6 	 global-step:5286	 l-p:0.0846342071890831
epoch£º264	 i:7 	 global-step:5287	 l-p:0.1212494745850563
epoch£º264	 i:8 	 global-step:5288	 l-p:0.0966159775853157
epoch£º264	 i:9 	 global-step:5289	 l-p:0.09471888095140457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5919, 4.0838, 4.2974],
        [3.5919, 3.6581, 3.6106],
        [3.5919, 3.5919, 3.5919],
        [3.5919, 3.5918, 3.5919]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.15409517288208008 
model_pd.l_d.mean(): -22.72381019592285 
model_pd.lagr.mean(): -22.56971549987793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1676], device='cuda:0')), ('power', tensor([-22.8914], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.15409517288208008
epoch£º265	 i:1 	 global-step:5301	 l-p:0.12284912914037704
epoch£º265	 i:2 	 global-step:5302	 l-p:0.10755283385515213
epoch£º265	 i:3 	 global-step:5303	 l-p:0.05752556771039963
epoch£º265	 i:4 	 global-step:5304	 l-p:0.014102761633694172
epoch£º265	 i:5 	 global-step:5305	 l-p:0.16064275801181793
epoch£º265	 i:6 	 global-step:5306	 l-p:-0.1974194347858429
epoch£º265	 i:7 	 global-step:5307	 l-p:0.08668062835931778
epoch£º265	 i:8 	 global-step:5308	 l-p:0.11422768980264664
epoch£º265	 i:9 	 global-step:5309	 l-p:0.10641269385814667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8150, 3.8142, 3.8149],
        [3.8150, 3.8148, 3.8150],
        [3.8150, 3.8134, 3.8134],
        [3.8150, 3.8149, 3.8150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.08763593435287476 
model_pd.l_d.mean(): -23.078754425048828 
model_pd.lagr.mean(): -22.991119384765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0309], device='cuda:0')), ('power', tensor([-23.1096], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.08763593435287476
epoch£º266	 i:1 	 global-step:5321	 l-p:0.13942141830921173
epoch£º266	 i:2 	 global-step:5322	 l-p:0.122370146214962
epoch£º266	 i:3 	 global-step:5323	 l-p:0.11520034819841385
epoch£º266	 i:4 	 global-step:5324	 l-p:0.09415052086114883
epoch£º266	 i:5 	 global-step:5325	 l-p:0.03899577632546425
epoch£º266	 i:6 	 global-step:5326	 l-p:0.10804051905870438
epoch£º266	 i:7 	 global-step:5327	 l-p:0.12620729207992554
epoch£º266	 i:8 	 global-step:5328	 l-p:0.14058320224285126
epoch£º266	 i:9 	 global-step:5329	 l-p:0.03911938890814781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8541, 3.8541, 3.8541],
        [3.8541, 3.9066, 3.8647],
        [3.8541, 3.8541, 3.8541],
        [3.8541, 3.8527, 3.8533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.013973236083984375 
model_pd.l_d.mean(): -22.7884464263916 
model_pd.lagr.mean(): -22.774473190307617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0608], device='cuda:0')), ('power', tensor([-22.8492], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.013973236083984375
epoch£º267	 i:1 	 global-step:5341	 l-p:0.13771700859069824
epoch£º267	 i:2 	 global-step:5342	 l-p:0.1200706735253334
epoch£º267	 i:3 	 global-step:5343	 l-p:0.10880181938409805
epoch£º267	 i:4 	 global-step:5344	 l-p:0.0735536590218544
epoch£º267	 i:5 	 global-step:5345	 l-p:-0.5978372097015381
epoch£º267	 i:6 	 global-step:5346	 l-p:0.10375574231147766
epoch£º267	 i:7 	 global-step:5347	 l-p:0.11880719661712646
epoch£º267	 i:8 	 global-step:5348	 l-p:0.11144685745239258
epoch£º267	 i:9 	 global-step:5349	 l-p:0.11820501834154129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8798, 3.8797, 3.8798],
        [3.8798, 4.0214, 3.9794],
        [3.8798, 3.8785, 3.8789],
        [3.8798, 3.8798, 3.8798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.06203896552324295 
model_pd.l_d.mean(): -22.9661808013916 
model_pd.lagr.mean(): -22.904142379760742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0256], device='cuda:0')), ('power', tensor([-22.9918], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.06203896552324295
epoch£º268	 i:1 	 global-step:5361	 l-p:0.11174976080656052
epoch£º268	 i:2 	 global-step:5362	 l-p:0.12257276475429535
epoch£º268	 i:3 	 global-step:5363	 l-p:0.14512872695922852
epoch£º268	 i:4 	 global-step:5364	 l-p:0.014214987866580486
epoch£º268	 i:5 	 global-step:5365	 l-p:0.10422144830226898
epoch£º268	 i:6 	 global-step:5366	 l-p:0.1231754869222641
epoch£º268	 i:7 	 global-step:5367	 l-p:0.11189853399991989
epoch£º268	 i:8 	 global-step:5368	 l-p:0.12948237359523773
epoch£º268	 i:9 	 global-step:5369	 l-p:0.11206178367137909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8121, 3.8119, 3.8121],
        [3.8121, 3.8109, 3.8118],
        [3.8121, 3.8141, 3.8066],
        [3.8121, 3.8109, 3.8089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.15357418358325958 
model_pd.l_d.mean(): -23.113496780395508 
model_pd.lagr.mean(): -22.959922790527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0397], device='cuda:0')), ('power', tensor([-23.1532], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.15357418358325958
epoch£º269	 i:1 	 global-step:5381	 l-p:0.12057080119848251
epoch£º269	 i:2 	 global-step:5382	 l-p:0.11625341325998306
epoch£º269	 i:3 	 global-step:5383	 l-p:0.09527922421693802
epoch£º269	 i:4 	 global-step:5384	 l-p:0.1717134416103363
epoch£º269	 i:5 	 global-step:5385	 l-p:0.10976415127515793
epoch£º269	 i:6 	 global-step:5386	 l-p:-0.2471112459897995
epoch£º269	 i:7 	 global-step:5387	 l-p:0.10637025535106659
epoch£º269	 i:8 	 global-step:5388	 l-p:0.11586657166481018
epoch£º269	 i:9 	 global-step:5389	 l-p:0.08931725472211838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6954, 3.6940, 3.6951],
        [3.6954, 3.8435, 3.8067],
        [3.6954, 3.6951, 3.6954],
        [3.6954, 3.7507, 3.7048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.11840248107910156 
model_pd.l_d.mean(): -23.297473907470703 
model_pd.lagr.mean(): -23.1790714263916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0443], device='cuda:0')), ('power', tensor([-23.3418], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.11840248107910156
epoch£º270	 i:1 	 global-step:5401	 l-p:0.12881383299827576
epoch£º270	 i:2 	 global-step:5402	 l-p:0.13707707822322845
epoch£º270	 i:3 	 global-step:5403	 l-p:0.12826977670192719
epoch£º270	 i:4 	 global-step:5404	 l-p:0.09490035474300385
epoch£º270	 i:5 	 global-step:5405	 l-p:0.10613922029733658
epoch£º270	 i:6 	 global-step:5406	 l-p:-0.08122154325246811
epoch£º270	 i:7 	 global-step:5407	 l-p:0.08066815137863159
epoch£º270	 i:8 	 global-step:5408	 l-p:0.11108440160751343
epoch£º270	 i:9 	 global-step:5409	 l-p:0.07774024456739426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5933, 3.5933, 3.5933],
        [3.5933, 3.6286, 3.5865],
        [3.5933, 3.8117, 3.8117],
        [3.5933, 4.0251, 4.1849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 1.664833664894104 
model_pd.l_d.mean(): -23.19382667541504 
model_pd.lagr.mean(): -21.528993606567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1107], device='cuda:0')), ('power', tensor([-23.3046], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:1.664833664894104
epoch£º271	 i:1 	 global-step:5421	 l-p:0.12011346220970154
epoch£º271	 i:2 	 global-step:5422	 l-p:0.14326278865337372
epoch£º271	 i:3 	 global-step:5423	 l-p:0.09988483041524887
epoch£º271	 i:4 	 global-step:5424	 l-p:0.09727611392736435
epoch£º271	 i:5 	 global-step:5425	 l-p:0.0550154373049736
epoch£º271	 i:6 	 global-step:5426	 l-p:0.11764074116945267
epoch£º271	 i:7 	 global-step:5427	 l-p:0.10358968377113342
epoch£º271	 i:8 	 global-step:5428	 l-p:0.11682713031768799
epoch£º271	 i:9 	 global-step:5429	 l-p:0.13071802258491516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6066, 3.6031, 3.6054],
        [3.6066, 3.8797, 3.9136],
        [3.6066, 3.6015, 3.6032],
        [3.6066, 3.6041, 3.6061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.1153782457113266 
model_pd.l_d.mean(): -22.31870460510254 
model_pd.lagr.mean(): -22.203327178955078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1516], device='cuda:0')), ('power', tensor([-22.4703], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.1153782457113266
epoch£º272	 i:1 	 global-step:5441	 l-p:0.07578576356172562
epoch£º272	 i:2 	 global-step:5442	 l-p:0.1143813505768776
epoch£º272	 i:3 	 global-step:5443	 l-p:0.12395478785037994
epoch£º272	 i:4 	 global-step:5444	 l-p:0.12316480278968811
epoch£º272	 i:5 	 global-step:5445	 l-p:0.10216624289751053
epoch£º272	 i:6 	 global-step:5446	 l-p:0.10180804878473282
epoch£º272	 i:7 	 global-step:5447	 l-p:0.02840769849717617
epoch£º272	 i:8 	 global-step:5448	 l-p:0.1277913898229599
epoch£º272	 i:9 	 global-step:5449	 l-p:0.20669101178646088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5417, 3.5558, 3.5234],
        [3.5417, 3.5408, 3.5416],
        [3.5417, 3.5349, 3.5366],
        [3.5417, 3.5378, 3.5405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.11418453603982925 
model_pd.l_d.mean(): -21.820755004882812 
model_pd.lagr.mean(): -21.70656967163086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2258], device='cuda:0')), ('power', tensor([-22.0465], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.11418453603982925
epoch£º273	 i:1 	 global-step:5461	 l-p:0.10659845918416977
epoch£º273	 i:2 	 global-step:5462	 l-p:0.13189348578453064
epoch£º273	 i:3 	 global-step:5463	 l-p:0.15674488246440887
epoch£º273	 i:4 	 global-step:5464	 l-p:0.10215907543897629
epoch£º273	 i:5 	 global-step:5465	 l-p:0.1822272092103958
epoch£º273	 i:6 	 global-step:5466	 l-p:0.10979367792606354
epoch£º273	 i:7 	 global-step:5467	 l-p:0.02141655422747135
epoch£º273	 i:8 	 global-step:5468	 l-p:0.13121134042739868
epoch£º273	 i:9 	 global-step:5469	 l-p:0.0703735202550888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5979, 3.7023, 3.6544],
        [3.5979, 4.0254, 4.1809],
        [3.5979, 3.5968, 3.5978],
        [3.5979, 4.0241, 4.1786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.1196216493844986 
model_pd.l_d.mean(): -23.337434768676758 
model_pd.lagr.mean(): -23.21781349182129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0797], device='cuda:0')), ('power', tensor([-23.4171], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.1196216493844986
epoch£º274	 i:1 	 global-step:5481	 l-p:0.08466537296772003
epoch£º274	 i:2 	 global-step:5482	 l-p:0.2966477572917938
epoch£º274	 i:3 	 global-step:5483	 l-p:0.11524956673383713
epoch£º274	 i:4 	 global-step:5484	 l-p:0.1481725424528122
epoch£º274	 i:5 	 global-step:5485	 l-p:0.11160942167043686
epoch£º274	 i:6 	 global-step:5486	 l-p:0.0899217426776886
epoch£º274	 i:7 	 global-step:5487	 l-p:0.05494428053498268
epoch£º274	 i:8 	 global-step:5488	 l-p:0.058358822017908096
epoch£º274	 i:9 	 global-step:5489	 l-p:0.12283319234848022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7002, 3.8615, 3.8287],
        [3.7002, 3.8769, 3.8505],
        [3.7002, 3.7002, 3.7002],
        [3.7002, 3.6971, 3.6991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.10941394418478012 
model_pd.l_d.mean(): -23.38969612121582 
model_pd.lagr.mean(): -23.280282974243164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0263], device='cuda:0')), ('power', tensor([-23.4159], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.10941394418478012
epoch£º275	 i:1 	 global-step:5501	 l-p:0.14010995626449585
epoch£º275	 i:2 	 global-step:5502	 l-p:49.226932525634766
epoch£º275	 i:3 	 global-step:5503	 l-p:0.08300874382257462
epoch£º275	 i:4 	 global-step:5504	 l-p:-0.2563541829586029
epoch£º275	 i:5 	 global-step:5505	 l-p:0.12108313292264938
epoch£º275	 i:6 	 global-step:5506	 l-p:0.118527352809906
epoch£º275	 i:7 	 global-step:5507	 l-p:0.06432962417602539
epoch£º275	 i:8 	 global-step:5508	 l-p:0.11878406256437302
epoch£º275	 i:9 	 global-step:5509	 l-p:0.1088685616850853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7976, 4.2281, 4.3668],
        [3.7976, 3.7949, 3.7964],
        [3.7976, 3.7975, 3.7976],
        [3.7976, 3.7971, 3.7975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.12666457891464233 
model_pd.l_d.mean(): -23.406522750854492 
model_pd.lagr.mean(): -23.279857635498047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0148], device='cuda:0')), ('power', tensor([-23.3917], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.12666457891464233
epoch£º276	 i:1 	 global-step:5521	 l-p:0.1179438978433609
epoch£º276	 i:2 	 global-step:5522	 l-p:0.4778022766113281
epoch£º276	 i:3 	 global-step:5523	 l-p:0.11998481303453445
epoch£º276	 i:4 	 global-step:5524	 l-p:0.12582850456237793
epoch£º276	 i:5 	 global-step:5525	 l-p:0.1165883019566536
epoch£º276	 i:6 	 global-step:5526	 l-p:0.02414960414171219
epoch£º276	 i:7 	 global-step:5527	 l-p:0.0693308562040329
epoch£º276	 i:8 	 global-step:5528	 l-p:0.1997925192117691
epoch£º276	 i:9 	 global-step:5529	 l-p:0.15345703065395355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6282, 3.9000, 3.9314],
        [3.6282, 3.9558, 4.0274],
        [3.6282, 3.6281, 3.6282],
        [3.6282, 3.8302, 3.8189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.11205969750881195 
model_pd.l_d.mean(): -23.003185272216797 
model_pd.lagr.mean(): -22.891124725341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1249], device='cuda:0')), ('power', tensor([-23.1281], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.11205969750881195
epoch£º277	 i:1 	 global-step:5541	 l-p:0.16354545950889587
epoch£º277	 i:2 	 global-step:5542	 l-p:0.09614160656929016
epoch£º277	 i:3 	 global-step:5543	 l-p:0.09445010870695114
epoch£º277	 i:4 	 global-step:5544	 l-p:0.10588350147008896
epoch£º277	 i:5 	 global-step:5545	 l-p:0.18062864243984222
epoch£º277	 i:6 	 global-step:5546	 l-p:0.04482470825314522
epoch£º277	 i:7 	 global-step:5547	 l-p:0.1265333741903305
epoch£º277	 i:8 	 global-step:5548	 l-p:-0.1033652126789093
epoch£º277	 i:9 	 global-step:5549	 l-p:0.11568907648324966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7164, 3.7144, 3.7160],
        [3.7164, 3.7119, 3.7135],
        [3.7164, 3.7165, 3.7051],
        [3.7164, 4.0393, 4.1004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.12818343937397003 
model_pd.l_d.mean(): -22.290693283081055 
model_pd.lagr.mean(): -22.16250991821289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1072], device='cuda:0')), ('power', tensor([-22.3979], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.12818343937397003
epoch£º278	 i:1 	 global-step:5561	 l-p:-0.07068529725074768
epoch£º278	 i:2 	 global-step:5562	 l-p:0.0970781147480011
epoch£º278	 i:3 	 global-step:5563	 l-p:0.09158674627542496
epoch£º278	 i:4 	 global-step:5564	 l-p:0.11120252311229706
epoch£º278	 i:5 	 global-step:5565	 l-p:0.15444116294384003
epoch£º278	 i:6 	 global-step:5566	 l-p:0.18674367666244507
epoch£º278	 i:7 	 global-step:5567	 l-p:0.07877770066261292
epoch£º278	 i:8 	 global-step:5568	 l-p:0.10420481115579605
epoch£º278	 i:9 	 global-step:5569	 l-p:0.1123248040676117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8374, 3.8562, 3.8280],
        [3.8374, 3.8345, 3.8340],
        [3.8374, 4.1555, 4.2046],
        [3.8374, 3.8507, 3.8273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.0620921328663826 
model_pd.l_d.mean(): -23.166149139404297 
model_pd.lagr.mean(): -23.10405731201172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0089], device='cuda:0')), ('power', tensor([-23.1750], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.0620921328663826
epoch£º279	 i:1 	 global-step:5581	 l-p:0.12830349802970886
epoch£º279	 i:2 	 global-step:5582	 l-p:0.0756092444062233
epoch£º279	 i:3 	 global-step:5583	 l-p:0.11182542890310287
epoch£º279	 i:4 	 global-step:5584	 l-p:0.027062956243753433
epoch£º279	 i:5 	 global-step:5585	 l-p:0.1089489758014679
epoch£º279	 i:6 	 global-step:5586	 l-p:0.13289201259613037
epoch£º279	 i:7 	 global-step:5587	 l-p:0.28945019841194153
epoch£º279	 i:8 	 global-step:5588	 l-p:0.12541347742080688
epoch£º279	 i:9 	 global-step:5589	 l-p:0.11510896682739258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9131, 3.9129, 3.9131],
        [3.9131, 3.9112, 3.9101],
        [3.9131, 3.9162, 3.9064],
        [3.9131, 3.9314, 3.9054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.12733373045921326 
model_pd.l_d.mean(): -22.85915184020996 
model_pd.lagr.mean(): -22.7318172454834 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0087], device='cuda:0')), ('power', tensor([-22.8505], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.12733373045921326
epoch£º280	 i:1 	 global-step:5601	 l-p:0.11127457022666931
epoch£º280	 i:2 	 global-step:5602	 l-p:0.1034945622086525
epoch£º280	 i:3 	 global-step:5603	 l-p:0.11287454515695572
epoch£º280	 i:4 	 global-step:5604	 l-p:0.1188310757279396
epoch£º280	 i:5 	 global-step:5605	 l-p:0.12240632623434067
epoch£º280	 i:6 	 global-step:5606	 l-p:0.10983365774154663
epoch£º280	 i:7 	 global-step:5607	 l-p:0.12105141580104828
epoch£º280	 i:8 	 global-step:5608	 l-p:0.07105028629302979
epoch£º280	 i:9 	 global-step:5609	 l-p:0.11776211112737656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6788, 3.7234, 3.6763],
        [3.6788, 3.6786, 3.6788],
        [3.6788, 3.6755, 3.6779],
        [3.6788, 3.6738, 3.6691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.11577153205871582 
model_pd.l_d.mean(): -23.26646614074707 
model_pd.lagr.mean(): -23.150693893432617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0628], device='cuda:0')), ('power', tensor([-23.3293], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.11577153205871582
epoch£º281	 i:1 	 global-step:5621	 l-p:0.08170575648546219
epoch£º281	 i:2 	 global-step:5622	 l-p:0.12012449651956558
epoch£º281	 i:3 	 global-step:5623	 l-p:-0.210615336894989
epoch£º281	 i:4 	 global-step:5624	 l-p:0.12542319297790527
epoch£º281	 i:5 	 global-step:5625	 l-p:0.16331584751605988
epoch£º281	 i:6 	 global-step:5626	 l-p:0.11811511963605881
epoch£º281	 i:7 	 global-step:5627	 l-p:0.10605878382921219
epoch£º281	 i:8 	 global-step:5628	 l-p:0.10337231308221817
epoch£º281	 i:9 	 global-step:5629	 l-p:0.19817359745502472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[3.5431, 3.5340, 3.5306],
        [3.5431, 3.5884, 3.5374],
        [3.5431, 3.5346, 3.5283],
        [3.5431, 3.9139, 4.0272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.037845734506845474 
model_pd.l_d.mean(): -21.84935760498047 
model_pd.lagr.mean(): -21.811511993408203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2707], device='cuda:0')), ('power', tensor([-22.1200], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.037845734506845474
epoch£º282	 i:1 	 global-step:5641	 l-p:0.12397580593824387
epoch£º282	 i:2 	 global-step:5642	 l-p:0.1328515261411667
epoch£º282	 i:3 	 global-step:5643	 l-p:0.11307210475206375
epoch£º282	 i:4 	 global-step:5644	 l-p:0.12196438759565353
epoch£º282	 i:5 	 global-step:5645	 l-p:0.11867891252040863
epoch£º282	 i:6 	 global-step:5646	 l-p:0.17076539993286133
epoch£º282	 i:7 	 global-step:5647	 l-p:0.14857697486877441
epoch£º282	 i:8 	 global-step:5648	 l-p:0.09387347847223282
epoch£º282	 i:9 	 global-step:5649	 l-p:0.11094485968351364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5946, 3.7089, 3.6615],
        [3.5946, 3.9782, 4.0971],
        [3.5946, 3.7088, 3.6614],
        [3.5946, 3.5931, 3.5944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.08757565915584564 
model_pd.l_d.mean(): -22.93142318725586 
model_pd.lagr.mean(): -22.843847274780273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1449], device='cuda:0')), ('power', tensor([-23.0763], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.08757565915584564
epoch£º283	 i:1 	 global-step:5661	 l-p:0.11229077726602554
epoch£º283	 i:2 	 global-step:5662	 l-p:0.10740804672241211
epoch£º283	 i:3 	 global-step:5663	 l-p:9.89755916595459
epoch£º283	 i:4 	 global-step:5664	 l-p:0.136994868516922
epoch£º283	 i:5 	 global-step:5665	 l-p:0.12156066298484802
epoch£º283	 i:6 	 global-step:5666	 l-p:0.10041603446006775
epoch£º283	 i:7 	 global-step:5667	 l-p:0.13833825290203094
epoch£º283	 i:8 	 global-step:5668	 l-p:0.12556999921798706
epoch£º283	 i:9 	 global-step:5669	 l-p:0.1195007935166359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6438, 3.6399, 3.6427],
        [3.6438, 3.6414, 3.6434],
        [3.6438, 3.6436, 3.6438],
        [3.6438, 3.7044, 3.6520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.12817806005477905 
model_pd.l_d.mean(): -23.407840728759766 
model_pd.lagr.mean(): -23.2796630859375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0548], device='cuda:0')), ('power', tensor([-23.4627], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.12817806005477905
epoch£º284	 i:1 	 global-step:5681	 l-p:0.09517016261816025
epoch£º284	 i:2 	 global-step:5682	 l-p:0.14482225477695465
epoch£º284	 i:3 	 global-step:5683	 l-p:0.08286375552415848
epoch£º284	 i:4 	 global-step:5684	 l-p:0.18594954907894135
epoch£º284	 i:5 	 global-step:5685	 l-p:0.10672542452812195
epoch£º284	 i:6 	 global-step:5686	 l-p:0.11626087874174118
epoch£º284	 i:7 	 global-step:5687	 l-p:0.09900019317865372
epoch£º284	 i:8 	 global-step:5688	 l-p:-0.3885689079761505
epoch£º284	 i:9 	 global-step:5689	 l-p:0.11423232406377792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7341, 3.7341, 3.7341],
        [3.7341, 3.7312, 3.7334],
        [3.7341, 3.7296, 3.7235],
        [3.7341, 3.8567, 3.8093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): -0.026567058637738228 
model_pd.l_d.mean(): -23.194843292236328 
model_pd.lagr.mean(): -23.221410751342773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0476], device='cuda:0')), ('power', tensor([-23.2425], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:-0.026567058637738228
epoch£º285	 i:1 	 global-step:5701	 l-p:0.26071053743362427
epoch£º285	 i:2 	 global-step:5702	 l-p:0.1252872198820114
epoch£º285	 i:3 	 global-step:5703	 l-p:0.12668202817440033
epoch£º285	 i:4 	 global-step:5704	 l-p:0.09267476946115494
epoch£º285	 i:5 	 global-step:5705	 l-p:0.06886294484138489
epoch£º285	 i:6 	 global-step:5706	 l-p:0.012151617556810379
epoch£º285	 i:7 	 global-step:5707	 l-p:0.12009666115045547
epoch£º285	 i:8 	 global-step:5708	 l-p:0.10942132771015167
epoch£º285	 i:9 	 global-step:5709	 l-p:-0.2339010238647461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9178, 3.9149, 3.9164],
        [3.9178, 3.9178, 3.9178],
        [3.9178, 3.9178, 3.9178],
        [3.9178, 3.9172, 3.9178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 4.495301246643066 
model_pd.l_d.mean(): -23.222471237182617 
model_pd.lagr.mean(): -18.727169036865234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0320], device='cuda:0')), ('power', tensor([-23.1905], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:4.495301246643066
epoch£º286	 i:1 	 global-step:5721	 l-p:0.1257035881280899
epoch£º286	 i:2 	 global-step:5722	 l-p:0.18301136791706085
epoch£º286	 i:3 	 global-step:5723	 l-p:0.12080208212137222
epoch£º286	 i:4 	 global-step:5724	 l-p:0.10918468236923218
epoch£º286	 i:5 	 global-step:5725	 l-p:0.2552337646484375
epoch£º286	 i:6 	 global-step:5726	 l-p:0.12155485153198242
epoch£º286	 i:7 	 global-step:5727	 l-p:0.11485689878463745
epoch£º286	 i:8 	 global-step:5728	 l-p:0.10635968297719955
epoch£º286	 i:9 	 global-step:5729	 l-p:0.12011920660734177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9180, 4.0471, 3.9988],
        [3.9180, 4.1030, 4.0722],
        [3.9180, 3.9297, 3.9072],
        [3.9180, 3.9148, 3.9150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 11.826102256774902 
model_pd.l_d.mean(): -23.09636878967285 
model_pd.lagr.mean(): -11.27026653289795 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0080], device='cuda:0')), ('power', tensor([-23.0883], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:11.826102256774902
epoch£º287	 i:1 	 global-step:5741	 l-p:0.11614175885915756
epoch£º287	 i:2 	 global-step:5742	 l-p:0.08881811052560806
epoch£º287	 i:3 	 global-step:5743	 l-p:0.11422479152679443
epoch£º287	 i:4 	 global-step:5744	 l-p:0.14140073955059052
epoch£º287	 i:5 	 global-step:5745	 l-p:0.16933102905750275
epoch£º287	 i:6 	 global-step:5746	 l-p:0.28702229261398315
epoch£º287	 i:7 	 global-step:5747	 l-p:0.10305646806955338
epoch£º287	 i:8 	 global-step:5748	 l-p:0.11357197165489197
epoch£º287	 i:9 	 global-step:5749	 l-p:0.12932835519313812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7240, 3.7196, 3.7224],
        [3.7240, 3.7240, 3.7240],
        [3.7240, 3.7235, 3.7240],
        [3.7240, 3.7954, 3.7419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): -1.0002164840698242 
model_pd.l_d.mean(): -23.23615264892578 
model_pd.lagr.mean(): -24.236370086669922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0511], device='cuda:0')), ('power', tensor([-23.2873], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:-1.0002164840698242
epoch£º288	 i:1 	 global-step:5761	 l-p:0.12239896506071091
epoch£º288	 i:2 	 global-step:5762	 l-p:0.009243679232895374
epoch£º288	 i:3 	 global-step:5763	 l-p:0.10721038281917572
epoch£º288	 i:4 	 global-step:5764	 l-p:0.11593730002641678
epoch£º288	 i:5 	 global-step:5765	 l-p:0.13788294792175293
epoch£º288	 i:6 	 global-step:5766	 l-p:0.06232293322682381
epoch£º288	 i:7 	 global-step:5767	 l-p:0.11307305842638016
epoch£º288	 i:8 	 global-step:5768	 l-p:0.12415570020675659
epoch£º288	 i:9 	 global-step:5769	 l-p:0.1160706877708435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5672, 3.7323, 3.7038],
        [3.5672, 3.5666, 3.5672],
        [3.5672, 3.5668, 3.5672],
        [3.5672, 3.5616, 3.5655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.14923498034477234 
model_pd.l_d.mean(): -22.182558059692383 
model_pd.lagr.mean(): -22.033323287963867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2061], device='cuda:0')), ('power', tensor([-22.3886], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.14923498034477234
epoch£º289	 i:1 	 global-step:5781	 l-p:0.12119490653276443
epoch£º289	 i:2 	 global-step:5782	 l-p:0.10828885436058044
epoch£º289	 i:3 	 global-step:5783	 l-p:0.126657634973526
epoch£º289	 i:4 	 global-step:5784	 l-p:0.17295892536640167
epoch£º289	 i:5 	 global-step:5785	 l-p:0.12750349938869476
epoch£º289	 i:6 	 global-step:5786	 l-p:0.1074703112244606
epoch£º289	 i:7 	 global-step:5787	 l-p:0.11088208854198456
epoch£º289	 i:8 	 global-step:5788	 l-p:0.09442443400621414
epoch£º289	 i:9 	 global-step:5789	 l-p:0.12737663090229034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5357, 3.5357, 3.5357],
        [3.5357, 3.5339, 3.5355],
        [3.5357, 3.5357, 3.5357],
        [3.5357, 3.5260, 3.5122]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.11535686999559402 
model_pd.l_d.mean(): -23.332962036132812 
model_pd.lagr.mean(): -23.217605590820312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0978], device='cuda:0')), ('power', tensor([-23.4308], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.11535686999559402
epoch£º290	 i:1 	 global-step:5801	 l-p:0.10867170989513397
epoch£º290	 i:2 	 global-step:5802	 l-p:0.11689216643571854
epoch£º290	 i:3 	 global-step:5803	 l-p:-0.14296115934848785
epoch£º290	 i:4 	 global-step:5804	 l-p:0.11951753497123718
epoch£º290	 i:5 	 global-step:5805	 l-p:0.11661386489868164
epoch£º290	 i:6 	 global-step:5806	 l-p:0.068951815366745
epoch£º290	 i:7 	 global-step:5807	 l-p:0.13126277923583984
epoch£º290	 i:8 	 global-step:5808	 l-p:0.11010405421257019
epoch£º290	 i:9 	 global-step:5809	 l-p:0.1599864363670349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4749, 3.4656, 3.4435],
        [3.4749, 3.5717, 3.5192],
        [3.4749, 3.4680, 3.4728],
        [3.4749, 3.4749, 3.4749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.11588960140943527 
model_pd.l_d.mean(): -23.53655242919922 
model_pd.lagr.mean(): -23.42066192626953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0956], device='cuda:0')), ('power', tensor([-23.6321], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.11588960140943527
epoch£º291	 i:1 	 global-step:5821	 l-p:0.02605213038623333
epoch£º291	 i:2 	 global-step:5822	 l-p:0.13203488290309906
epoch£º291	 i:3 	 global-step:5823	 l-p:0.12141504138708115
epoch£º291	 i:4 	 global-step:5824	 l-p:0.3375188410282135
epoch£º291	 i:5 	 global-step:5825	 l-p:0.0905688926577568
epoch£º291	 i:6 	 global-step:5826	 l-p:0.12130296230316162
epoch£º291	 i:7 	 global-step:5827	 l-p:0.1220649853348732
epoch£º291	 i:8 	 global-step:5828	 l-p:0.02649453841149807
epoch£º291	 i:9 	 global-step:5829	 l-p:0.0937788337469101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6322, 3.6322, 3.6322],
        [3.6322, 3.6296, 3.6318],
        [3.6322, 3.6320, 3.6322],
        [3.6322, 3.6322, 3.6322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.11820030212402344 
model_pd.l_d.mean(): -22.92536735534668 
model_pd.lagr.mean(): -22.807167053222656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1086], device='cuda:0')), ('power', tensor([-23.0339], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.11820030212402344
epoch£º292	 i:1 	 global-step:5841	 l-p:0.08107410371303558
epoch£º292	 i:2 	 global-step:5842	 l-p:0.10774136334657669
epoch£º292	 i:3 	 global-step:5843	 l-p:0.10646922886371613
epoch£º292	 i:4 	 global-step:5844	 l-p:0.1532578468322754
epoch£º292	 i:5 	 global-step:5845	 l-p:0.07867471873760223
epoch£º292	 i:6 	 global-step:5846	 l-p:0.07648465782403946
epoch£º292	 i:7 	 global-step:5847	 l-p:0.1200360655784607
epoch£º292	 i:8 	 global-step:5848	 l-p:0.1294916570186615
epoch£º292	 i:9 	 global-step:5849	 l-p:0.11559034138917923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[3.7467, 3.7623, 3.7261],
        [3.7467, 3.7609, 3.7258],
        [3.7467, 3.7394, 3.7344],
        [3.7467, 3.7387, 3.7375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.11573193967342377 
model_pd.l_d.mean(): -23.29432487487793 
model_pd.lagr.mean(): -23.178592681884766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0201], device='cuda:0')), ('power', tensor([-23.3144], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.11573193967342377
epoch£º293	 i:1 	 global-step:5861	 l-p:0.26098281145095825
epoch£º293	 i:2 	 global-step:5862	 l-p:0.08620904386043549
epoch£º293	 i:3 	 global-step:5863	 l-p:0.5717213153839111
epoch£º293	 i:4 	 global-step:5864	 l-p:0.10224243253469467
epoch£º293	 i:5 	 global-step:5865	 l-p:0.11199361830949783
epoch£º293	 i:6 	 global-step:5866	 l-p:0.11369732767343521
epoch£º293	 i:7 	 global-step:5867	 l-p:0.1096307560801506
epoch£º293	 i:8 	 global-step:5868	 l-p:0.12028859555721283
epoch£º293	 i:9 	 global-step:5869	 l-p:0.19800250232219696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[3.7561, 3.7504, 3.7408],
        [3.7561, 3.9187, 3.8821],
        [3.7561, 3.9670, 3.9532],
        [3.7561, 3.7830, 3.7392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.11477909982204437 
model_pd.l_d.mean(): -22.502784729003906 
model_pd.lagr.mean(): -22.38800621032715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0816], device='cuda:0')), ('power', tensor([-22.5844], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.11477909982204437
epoch£º294	 i:1 	 global-step:5881	 l-p:0.10163038223981857
epoch£º294	 i:2 	 global-step:5882	 l-p:-0.18675033748149872
epoch£º294	 i:3 	 global-step:5883	 l-p:0.06916236132383347
epoch£º294	 i:4 	 global-step:5884	 l-p:0.09362305700778961
epoch£º294	 i:5 	 global-step:5885	 l-p:0.13803188502788544
epoch£º294	 i:6 	 global-step:5886	 l-p:0.12158364057540894
epoch£º294	 i:7 	 global-step:5887	 l-p:0.1471651941537857
epoch£º294	 i:8 	 global-step:5888	 l-p:0.2789202928543091
epoch£º294	 i:9 	 global-step:5889	 l-p:0.09741423279047012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6245, 3.6188, 3.5983],
        [3.6245, 3.6244, 3.6245],
        [3.6245, 3.6236, 3.6244],
        [3.6245, 3.6245, 3.6245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.12583336234092712 
model_pd.l_d.mean(): -23.486356735229492 
model_pd.lagr.mean(): -23.360523223876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0508], device='cuda:0')), ('power', tensor([-23.5371], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.12583336234092712
epoch£º295	 i:1 	 global-step:5901	 l-p:0.11101183295249939
epoch£º295	 i:2 	 global-step:5902	 l-p:0.13942468166351318
epoch£º295	 i:3 	 global-step:5903	 l-p:0.10872085392475128
epoch£º295	 i:4 	 global-step:5904	 l-p:0.0942501649260521
epoch£º295	 i:5 	 global-step:5905	 l-p:0.1250939965248108
epoch£º295	 i:6 	 global-step:5906	 l-p:-0.009435215033590794
epoch£º295	 i:7 	 global-step:5907	 l-p:0.11825546622276306
epoch£º295	 i:8 	 global-step:5908	 l-p:0.11811529099941254
epoch£º295	 i:9 	 global-step:5909	 l-p:0.12277962267398834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6234, 3.6223, 3.5939],
        [3.6234, 3.6201, 3.6229],
        [3.6234, 3.6230, 3.6234],
        [3.6234, 3.6234, 3.6234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.10818615555763245 
model_pd.l_d.mean(): -23.220951080322266 
model_pd.lagr.mean(): -23.112764358520508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0894], device='cuda:0')), ('power', tensor([-23.3103], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.10818615555763245
epoch£º296	 i:1 	 global-step:5921	 l-p:0.7864298820495605
epoch£º296	 i:2 	 global-step:5922	 l-p:0.0995696410536766
epoch£º296	 i:3 	 global-step:5923	 l-p:0.1458250731229782
epoch£º296	 i:4 	 global-step:5924	 l-p:0.1212734580039978
epoch£º296	 i:5 	 global-step:5925	 l-p:0.048724912106990814
epoch£º296	 i:6 	 global-step:5926	 l-p:0.12888935208320618
epoch£º296	 i:7 	 global-step:5927	 l-p:0.12284591048955917
epoch£º296	 i:8 	 global-step:5928	 l-p:0.11204511672258377
epoch£º296	 i:9 	 global-step:5929	 l-p:-0.030635660514235497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7119, 4.1374, 4.2797],
        [3.7119, 3.7067, 3.6899],
        [3.7119, 3.7063, 3.7101],
        [3.7119, 3.7115, 3.7119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.11530498415231705 
model_pd.l_d.mean(): -22.51726722717285 
model_pd.lagr.mean(): -22.401962280273438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1310], device='cuda:0')), ('power', tensor([-22.6482], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.11530498415231705
epoch£º297	 i:1 	 global-step:5941	 l-p:0.1004519835114479
epoch£º297	 i:2 	 global-step:5942	 l-p:0.08829940855503082
epoch£º297	 i:3 	 global-step:5943	 l-p:0.10787275433540344
epoch£º297	 i:4 	 global-step:5944	 l-p:-0.11877500265836716
epoch£º297	 i:5 	 global-step:5945	 l-p:0.1772294044494629
epoch£º297	 i:6 	 global-step:5946	 l-p:0.11442965269088745
epoch£º297	 i:7 	 global-step:5947	 l-p:0.11910434812307358
epoch£º297	 i:8 	 global-step:5948	 l-p:0.1475261151790619
epoch£º297	 i:9 	 global-step:5949	 l-p:0.12542562186717987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8382, 3.8382, 3.8382],
        [3.8382, 3.8307, 3.8298],
        [3.8382, 3.8381, 3.8382],
        [3.8382, 4.4282, 4.7050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.0660778060555458 
model_pd.l_d.mean(): -22.71211814880371 
model_pd.lagr.mean(): -22.646039962768555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0459], device='cuda:0')), ('power', tensor([-22.7580], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.0660778060555458
epoch£º298	 i:1 	 global-step:5961	 l-p:0.12003777176141739
epoch£º298	 i:2 	 global-step:5962	 l-p:0.0738782063126564
epoch£º298	 i:3 	 global-step:5963	 l-p:0.1277715265750885
epoch£º298	 i:4 	 global-step:5964	 l-p:0.14494486153125763
epoch£º298	 i:5 	 global-step:5965	 l-p:0.13683193922042847
epoch£º298	 i:6 	 global-step:5966	 l-p:0.12963904440402985
epoch£º298	 i:7 	 global-step:5967	 l-p:0.10415821522474289
epoch£º298	 i:8 	 global-step:5968	 l-p:0.16053393483161926
epoch£º298	 i:9 	 global-step:5969	 l-p:0.0656905323266983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8579, 3.8541, 3.8569],
        [3.8579, 3.8536, 3.8430],
        [3.8579, 3.8768, 3.8390],
        [3.8579, 3.9297, 3.8732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.11245252192020416 
model_pd.l_d.mean(): -23.14775276184082 
model_pd.lagr.mean(): -23.03529930114746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0126], device='cuda:0')), ('power', tensor([-23.1604], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.11245252192020416
epoch£º299	 i:1 	 global-step:5981	 l-p:0.07230435311794281
epoch£º299	 i:2 	 global-step:5982	 l-p:0.0565621443092823
epoch£º299	 i:3 	 global-step:5983	 l-p:0.13268102705478668
epoch£º299	 i:4 	 global-step:5984	 l-p:0.11445840448141098
epoch£º299	 i:5 	 global-step:5985	 l-p:-4.264871597290039
epoch£º299	 i:6 	 global-step:5986	 l-p:0.1203448474407196
epoch£º299	 i:7 	 global-step:5987	 l-p:0.12753282487392426
epoch£º299	 i:8 	 global-step:5988	 l-p:0.10267804563045502
epoch£º299	 i:9 	 global-step:5989	 l-p:0.10468677431344986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9372, 4.4200, 4.5901],
        [3.9372, 3.9312, 3.9323],
        [3.9372, 3.9372, 3.9372],
        [3.9372, 3.9313, 3.9326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 1.1133501529693604 
model_pd.l_d.mean(): -23.253524780273438 
model_pd.lagr.mean(): -22.140174865722656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0386], device='cuda:0')), ('power', tensor([-23.2149], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:1.1133501529693604
epoch£º300	 i:1 	 global-step:6001	 l-p:-0.10424744337797165
epoch£º300	 i:2 	 global-step:6002	 l-p:0.11722084134817123
epoch£º300	 i:3 	 global-step:6003	 l-p:0.08204560726881027
epoch£º300	 i:4 	 global-step:6004	 l-p:0.11386474221944809
epoch£º300	 i:5 	 global-step:6005	 l-p:0.07194630801677704
epoch£º300	 i:6 	 global-step:6006	 l-p:0.14187563955783844
epoch£º300	 i:7 	 global-step:6007	 l-p:0.12276405841112137
epoch£º300	 i:8 	 global-step:6008	 l-p:0.013572046533226967
epoch£º300	 i:9 	 global-step:6009	 l-p:0.1163361519575119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8657, 3.8604, 3.8515],
        [3.8657, 3.8657, 3.8657],
        [3.8657, 3.9543, 3.8972],
        [3.8657, 3.8657, 3.8657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.12632299959659576 
model_pd.l_d.mean(): -22.312196731567383 
model_pd.lagr.mean(): -22.18587303161621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0678], device='cuda:0')), ('power', tensor([-22.3800], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.12632299959659576
epoch£º301	 i:1 	 global-step:6021	 l-p:0.10240712761878967
epoch£º301	 i:2 	 global-step:6022	 l-p:0.07432053983211517
epoch£º301	 i:3 	 global-step:6023	 l-p:0.11370448768138885
epoch£º301	 i:4 	 global-step:6024	 l-p:0.18928231298923492
epoch£º301	 i:5 	 global-step:6025	 l-p:0.10724899917840958
epoch£º301	 i:6 	 global-step:6026	 l-p:0.11792120337486267
epoch£º301	 i:7 	 global-step:6027	 l-p:0.1484585404396057
epoch£º301	 i:8 	 global-step:6028	 l-p:0.11002826690673828
epoch£º301	 i:9 	 global-step:6029	 l-p:0.119380883872509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6908, 3.6807, 3.6682],
        [3.6908, 3.6902, 3.6908],
        [3.6908, 4.0530, 4.1456],
        [3.6908, 3.6902, 3.6908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.11652983725070953 
model_pd.l_d.mean(): -23.35630989074707 
model_pd.lagr.mean(): -23.23978042602539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0410], device='cuda:0')), ('power', tensor([-23.3973], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.11652983725070953
epoch£º302	 i:1 	 global-step:6041	 l-p:0.1159997433423996
epoch£º302	 i:2 	 global-step:6042	 l-p:0.15257307887077332
epoch£º302	 i:3 	 global-step:6043	 l-p:0.050256356596946716
epoch£º302	 i:4 	 global-step:6044	 l-p:0.12710119783878326
epoch£º302	 i:5 	 global-step:6045	 l-p:0.11604849249124527
epoch£º302	 i:6 	 global-step:6046	 l-p:0.12102609127759933
epoch£º302	 i:7 	 global-step:6047	 l-p:0.12536431849002838
epoch£º302	 i:8 	 global-step:6048	 l-p:0.17131972312927246
epoch£º302	 i:9 	 global-step:6049	 l-p:0.08843132108449936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4039, 3.4038, 3.4039],
        [3.4039, 3.3804, 3.3769],
        [3.4039, 3.8188, 3.9807],
        [3.4039, 3.3848, 3.3578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.12214113026857376 
model_pd.l_d.mean(): -22.288043975830078 
model_pd.lagr.mean(): -22.165903091430664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2318], device='cuda:0')), ('power', tensor([-22.5199], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.12214113026857376
epoch£º303	 i:1 	 global-step:6061	 l-p:0.125733882188797
epoch£º303	 i:2 	 global-step:6062	 l-p:0.13131427764892578
epoch£º303	 i:3 	 global-step:6063	 l-p:0.1360025405883789
epoch£º303	 i:4 	 global-step:6064	 l-p:0.13659434020519257
epoch£º303	 i:5 	 global-step:6065	 l-p:0.128560870885849
epoch£º303	 i:6 	 global-step:6066	 l-p:0.0790887176990509
epoch£º303	 i:7 	 global-step:6067	 l-p:0.12822699546813965
epoch£º303	 i:8 	 global-step:6068	 l-p:0.13859674334526062
epoch£º303	 i:9 	 global-step:6069	 l-p:0.0885559618473053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2793, 3.2793, 3.2793],
        [3.2793, 3.2793, 3.2793],
        [3.2793, 3.2785, 3.2793],
        [3.2793, 3.2790, 3.2793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.13722147047519684 
model_pd.l_d.mean(): -22.21812629699707 
model_pd.lagr.mean(): -22.080904006958008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2768], device='cuda:0')), ('power', tensor([-22.4949], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.13722147047519684
epoch£º304	 i:1 	 global-step:6081	 l-p:0.06817720085382462
epoch£º304	 i:2 	 global-step:6082	 l-p:0.13634677231311798
epoch£º304	 i:3 	 global-step:6083	 l-p:0.13089655339717865
epoch£º304	 i:4 	 global-step:6084	 l-p:0.10846347361803055
epoch£º304	 i:5 	 global-step:6085	 l-p:0.12585695087909698
epoch£º304	 i:6 	 global-step:6086	 l-p:0.10138779133558273
epoch£º304	 i:7 	 global-step:6087	 l-p:0.11691826581954956
epoch£º304	 i:8 	 global-step:6088	 l-p:0.11124533414840698
epoch£º304	 i:9 	 global-step:6089	 l-p:0.11950773000717163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5448, 3.5351, 3.5032],
        [3.5448, 3.5447, 3.5448],
        [3.5448, 3.8936, 3.9875],
        [3.5448, 3.5264, 3.5270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.10274902731180191 
model_pd.l_d.mean(): -23.015180587768555 
model_pd.lagr.mean(): -22.912431716918945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1585], device='cuda:0')), ('power', tensor([-23.1737], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.10274902731180191
epoch£º305	 i:1 	 global-step:6101	 l-p:0.12579603493213654
epoch£º305	 i:2 	 global-step:6102	 l-p:0.12161245197057724
epoch£º305	 i:3 	 global-step:6103	 l-p:0.019371775910258293
epoch£º305	 i:4 	 global-step:6104	 l-p:0.11622677743434906
epoch£º305	 i:5 	 global-step:6105	 l-p:0.11942175775766373
epoch£º305	 i:6 	 global-step:6106	 l-p:0.15425163507461548
epoch£º305	 i:7 	 global-step:6107	 l-p:0.12414051592350006
epoch£º305	 i:8 	 global-step:6108	 l-p:0.10938029736280441
epoch£º305	 i:9 	 global-step:6109	 l-p:-48.07157516479492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5350, 3.5350, 3.5350],
        [3.5350, 3.8630, 3.9411],
        [3.5350, 3.5350, 3.5350],
        [3.5350, 3.5460, 3.4924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.11023938655853271 
model_pd.l_d.mean(): -22.201852798461914 
model_pd.lagr.mean(): -22.09161376953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2153], device='cuda:0')), ('power', tensor([-22.4171], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.11023938655853271
epoch£º306	 i:1 	 global-step:6121	 l-p:0.09023534506559372
epoch£º306	 i:2 	 global-step:6122	 l-p:0.08764415234327316
epoch£º306	 i:3 	 global-step:6123	 l-p:0.09745582193136215
epoch£º306	 i:4 	 global-step:6124	 l-p:0.12169650942087173
epoch£º306	 i:5 	 global-step:6125	 l-p:0.10388419032096863
epoch£º306	 i:6 	 global-step:6126	 l-p:0.12117351591587067
epoch£º306	 i:7 	 global-step:6127	 l-p:0.14569279551506042
epoch£º306	 i:8 	 global-step:6128	 l-p:0.12242937088012695
epoch£º306	 i:9 	 global-step:6129	 l-p:0.3459513485431671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6521, 3.6521, 3.6521],
        [3.6521, 3.6379, 3.6424],
        [3.6521, 3.8763, 3.8731],
        [3.6521, 3.6388, 3.6440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.1200600117444992 
model_pd.l_d.mean(): -23.284900665283203 
model_pd.lagr.mean(): -23.164840698242188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0775], device='cuda:0')), ('power', tensor([-23.3624], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.1200600117444992
epoch£º307	 i:1 	 global-step:6141	 l-p:0.1031622365117073
epoch£º307	 i:2 	 global-step:6142	 l-p:0.10172932595014572
epoch£º307	 i:3 	 global-step:6143	 l-p:0.15784074366092682
epoch£º307	 i:4 	 global-step:6144	 l-p:-2.058990240097046
epoch£º307	 i:5 	 global-step:6145	 l-p:0.10097197443246841
epoch£º307	 i:6 	 global-step:6146	 l-p:0.12464848160743713
epoch£º307	 i:7 	 global-step:6147	 l-p:0.0531599223613739
epoch£º307	 i:8 	 global-step:6148	 l-p:0.11499933153390884
epoch£º307	 i:9 	 global-step:6149	 l-p:0.11641069501638412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6899, 3.6848, 3.6888],
        [3.6899, 3.6830, 3.6879],
        [3.6899, 3.6751, 3.6775],
        [3.6899, 3.7110, 3.6578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.13667802512645721 
model_pd.l_d.mean(): -23.538278579711914 
model_pd.lagr.mean(): -23.401599884033203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0098], device='cuda:0')), ('power', tensor([-23.5481], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.13667802512645721
epoch£º308	 i:1 	 global-step:6161	 l-p:0.0975445881485939
epoch£º308	 i:2 	 global-step:6162	 l-p:0.07958473265171051
epoch£º308	 i:3 	 global-step:6163	 l-p:0.20615413784980774
epoch£º308	 i:4 	 global-step:6164	 l-p:0.11709916591644287
epoch£º308	 i:5 	 global-step:6165	 l-p:0.1262151598930359
epoch£º308	 i:6 	 global-step:6166	 l-p:0.09845080226659775
epoch£º308	 i:7 	 global-step:6167	 l-p:0.1289924532175064
epoch£º308	 i:8 	 global-step:6168	 l-p:0.11422500014305115
epoch£º308	 i:9 	 global-step:6169	 l-p:0.07061665505170822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6674, 3.6521, 3.6377],
        [3.6674, 3.7771, 3.7200],
        [3.6674, 3.6672, 3.6674],
        [3.6674, 3.8627, 3.8417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.10500179976224899 
model_pd.l_d.mean(): -22.219512939453125 
model_pd.lagr.mean(): -22.114511489868164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1506], device='cuda:0')), ('power', tensor([-22.3701], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.10500179976224899
epoch£º309	 i:1 	 global-step:6181	 l-p:0.10612247884273529
epoch£º309	 i:2 	 global-step:6182	 l-p:0.12026926875114441
epoch£º309	 i:3 	 global-step:6183	 l-p:-3.538747549057007
epoch£º309	 i:4 	 global-step:6184	 l-p:0.11446460336446762
epoch£º309	 i:5 	 global-step:6185	 l-p:0.14784063398838043
epoch£º309	 i:6 	 global-step:6186	 l-p:0.1007322296500206
epoch£º309	 i:7 	 global-step:6187	 l-p:0.10751722007989883
epoch£º309	 i:8 	 global-step:6188	 l-p:0.06709861010313034
epoch£º309	 i:9 	 global-step:6189	 l-p:0.11820583045482635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6895, 3.6850, 3.6887],
        [3.6895, 3.6894, 3.6895],
        [3.6895, 3.7937, 3.7347],
        [3.6895, 3.9678, 3.9972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.12616491317749023 
model_pd.l_d.mean(): -23.403905868530273 
model_pd.lagr.mean(): -23.277740478515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0442], device='cuda:0')), ('power', tensor([-23.4481], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.12616491317749023
epoch£º310	 i:1 	 global-step:6201	 l-p:0.12364596128463745
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12566642463207245
epoch£º310	 i:3 	 global-step:6203	 l-p:0.09564492106437683
epoch£º310	 i:4 	 global-step:6204	 l-p:0.10913194715976715
epoch£º310	 i:5 	 global-step:6205	 l-p:0.10905348509550095
epoch£º310	 i:6 	 global-step:6206	 l-p:0.08599673956632614
epoch£º310	 i:7 	 global-step:6207	 l-p:0.10116854310035706
epoch£º310	 i:8 	 global-step:6208	 l-p:0.029253611341118813
epoch£º310	 i:9 	 global-step:6209	 l-p:0.007528939284384251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[3.4970, 3.4785, 3.4455],
        [3.4970, 3.4773, 3.4466],
        [3.4970, 3.4716, 3.4645],
        [3.4970, 3.4869, 3.4418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): -0.03575510531663895 
model_pd.l_d.mean(): -22.2369327545166 
model_pd.lagr.mean(): -22.272687911987305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2063], device='cuda:0')), ('power', tensor([-22.4433], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:-0.03575510531663895
epoch£º311	 i:1 	 global-step:6221	 l-p:0.12565778195858002
epoch£º311	 i:2 	 global-step:6222	 l-p:0.11530531942844391
epoch£º311	 i:3 	 global-step:6223	 l-p:0.12624505162239075
epoch£º311	 i:4 	 global-step:6224	 l-p:0.07525400817394257
epoch£º311	 i:5 	 global-step:6225	 l-p:0.08631564676761627
epoch£º311	 i:6 	 global-step:6226	 l-p:0.16123077273368835
epoch£º311	 i:7 	 global-step:6227	 l-p:0.11269672214984894
epoch£º311	 i:8 	 global-step:6228	 l-p:0.13133133947849274
epoch£º311	 i:9 	 global-step:6229	 l-p:0.09890736639499664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5512, 3.5370, 3.5014],
        [3.5512, 3.5468, 3.5506],
        [3.5512, 3.5512, 3.5512],
        [3.5512, 3.7777, 3.7811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.13420502841472626 
model_pd.l_d.mean(): -23.437110900878906 
model_pd.lagr.mean(): -23.302906036376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0952], device='cuda:0')), ('power', tensor([-23.5323], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.13420502841472626
epoch£º312	 i:1 	 global-step:6241	 l-p:0.143205463886261
epoch£º312	 i:2 	 global-step:6242	 l-p:0.05113314092159271
epoch£º312	 i:3 	 global-step:6243	 l-p:0.11130979657173157
epoch£º312	 i:4 	 global-step:6244	 l-p:0.12400972098112106
epoch£º312	 i:5 	 global-step:6245	 l-p:0.11088252812623978
epoch£º312	 i:6 	 global-step:6246	 l-p:0.30719852447509766
epoch£º312	 i:7 	 global-step:6247	 l-p:0.1134113222360611
epoch£º312	 i:8 	 global-step:6248	 l-p:0.10361708700656891
epoch£º312	 i:9 	 global-step:6249	 l-p:0.19413207471370697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5575, 3.5401, 3.5474],
        [3.5575, 3.8588, 3.9139],
        [3.5575, 3.5426, 3.5069],
        [3.5575, 3.5503, 3.5558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.11253038048744202 
model_pd.l_d.mean(): -23.3697452545166 
model_pd.lagr.mean(): -23.25721549987793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1023], device='cuda:0')), ('power', tensor([-23.4721], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.11253038048744202
epoch£º313	 i:1 	 global-step:6261	 l-p:0.11603135615587234
epoch£º313	 i:2 	 global-step:6262	 l-p:1.848041296005249
epoch£º313	 i:3 	 global-step:6263	 l-p:0.1064211055636406
epoch£º313	 i:4 	 global-step:6264	 l-p:0.11381880193948746
epoch£º313	 i:5 	 global-step:6265	 l-p:0.13320474326610565
epoch£º313	 i:6 	 global-step:6266	 l-p:0.12182720005512238
epoch£º313	 i:7 	 global-step:6267	 l-p:0.10709043592214584
epoch£º313	 i:8 	 global-step:6268	 l-p:0.2015225887298584
epoch£º313	 i:9 	 global-step:6269	 l-p:0.1210770383477211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4311, 3.4311, 3.4311],
        [3.4311, 3.4210, 3.4284],
        [3.4311, 3.4045, 3.4095],
        [3.4311, 3.4010, 3.3978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.0973905697464943 
model_pd.l_d.mean(): -22.882389068603516 
model_pd.lagr.mean(): -22.784997940063477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2358], device='cuda:0')), ('power', tensor([-23.1182], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.0973905697464943
epoch£º314	 i:1 	 global-step:6281	 l-p:0.13351570069789886
epoch£º314	 i:2 	 global-step:6282	 l-p:0.11555793136358261
epoch£º314	 i:3 	 global-step:6283	 l-p:0.19035808742046356
epoch£º314	 i:4 	 global-step:6284	 l-p:0.08937803655862808
epoch£º314	 i:5 	 global-step:6285	 l-p:0.1083899438381195
epoch£º314	 i:6 	 global-step:6286	 l-p:0.12395944446325302
epoch£º314	 i:7 	 global-step:6287	 l-p:0.11794217675924301
epoch£º314	 i:8 	 global-step:6288	 l-p:0.11542017012834549
epoch£º314	 i:9 	 global-step:6289	 l-p:0.16868865489959717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6262, 3.6991, 3.6317],
        [3.6262, 3.6099, 3.5804],
        [3.6262, 3.6262, 3.6262],
        [3.6262, 3.8227, 3.8030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.09812017530202866 
model_pd.l_d.mean(): -23.04817008972168 
model_pd.lagr.mean(): -22.950050354003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1079], device='cuda:0')), ('power', tensor([-23.1560], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.09812017530202866
epoch£º315	 i:1 	 global-step:6301	 l-p:0.11372904479503632
epoch£º315	 i:2 	 global-step:6302	 l-p:0.08711781352758408
epoch£º315	 i:3 	 global-step:6303	 l-p:0.12588804960250854
epoch£º315	 i:4 	 global-step:6304	 l-p:0.09071335941553116
epoch£º315	 i:5 	 global-step:6305	 l-p:0.10228784382343292
epoch£º315	 i:6 	 global-step:6306	 l-p:0.11385004967451096
epoch£º315	 i:7 	 global-step:6307	 l-p:0.1165684163570404
epoch£º315	 i:8 	 global-step:6308	 l-p:0.03891453519463539
epoch£º315	 i:9 	 global-step:6309	 l-p:0.11652448028326035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7579, 3.7523, 3.7567],
        [3.7579, 3.7569, 3.7578],
        [3.7579, 3.7628, 3.7162],
        [3.7579, 3.7425, 3.7475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.11991500854492188 
model_pd.l_d.mean(): -23.530227661132812 
model_pd.lagr.mean(): -23.41031265258789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0141], device='cuda:0')), ('power', tensor([-23.5161], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.11991500854492188
epoch£º316	 i:1 	 global-step:6321	 l-p:0.11789409071207047
epoch£º316	 i:2 	 global-step:6322	 l-p:0.09281284362077713
epoch£º316	 i:3 	 global-step:6323	 l-p:-0.004248995799571276
epoch£º316	 i:4 	 global-step:6324	 l-p:0.11681635677814484
epoch£º316	 i:5 	 global-step:6325	 l-p:0.2057311236858368
epoch£º316	 i:6 	 global-step:6326	 l-p:0.12648923695087433
epoch£º316	 i:7 	 global-step:6327	 l-p:0.15055468678474426
epoch£º316	 i:8 	 global-step:6328	 l-p:0.10280385613441467
epoch£º316	 i:9 	 global-step:6329	 l-p:0.0884552001953125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6834, 3.6651, 3.6702],
        [3.6834, 3.6827, 3.6834],
        [3.6834, 3.6626, 3.6496],
        [3.6834, 3.6619, 3.6551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.13432690501213074 
model_pd.l_d.mean(): -23.100563049316406 
model_pd.lagr.mean(): -22.966236114501953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0950], device='cuda:0')), ('power', tensor([-23.1956], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.13432690501213074
epoch£º317	 i:1 	 global-step:6341	 l-p:0.07518533617258072
epoch£º317	 i:2 	 global-step:6342	 l-p:0.10320913046598434
epoch£º317	 i:3 	 global-step:6343	 l-p:0.10696014016866684
epoch£º317	 i:4 	 global-step:6344	 l-p:0.1236107125878334
epoch£º317	 i:5 	 global-step:6345	 l-p:0.1187213659286499
epoch£º317	 i:6 	 global-step:6346	 l-p:0.09752622991800308
epoch£º317	 i:7 	 global-step:6347	 l-p:0.06118964031338692
epoch£º317	 i:8 	 global-step:6348	 l-p:0.1200079545378685
epoch£º317	 i:9 	 global-step:6349	 l-p:-0.037512969225645065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7493, 3.7317, 3.7355],
        [3.7493, 3.7491, 3.7493],
        [3.7493, 3.7432, 3.7479],
        [3.7493, 3.7488, 3.7493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.1134280189871788 
model_pd.l_d.mean(): -23.00920867919922 
model_pd.lagr.mean(): -22.895780563354492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0756], device='cuda:0')), ('power', tensor([-23.0848], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.1134280189871788
epoch£º318	 i:1 	 global-step:6361	 l-p:0.1033945083618164
epoch£º318	 i:2 	 global-step:6362	 l-p:0.09229520708322525
epoch£º318	 i:3 	 global-step:6363	 l-p:0.10437449812889099
epoch£º318	 i:4 	 global-step:6364	 l-p:0.11103058606386185
epoch£º318	 i:5 	 global-step:6365	 l-p:-0.01949215866625309
epoch£º318	 i:6 	 global-step:6366	 l-p:0.13751383125782013
epoch£º318	 i:7 	 global-step:6367	 l-p:-3.628626823425293
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12147361785173416
epoch£º318	 i:9 	 global-step:6369	 l-p:0.09854936599731445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7549, 3.7353, 3.7271],
        [3.7549, 3.7423, 3.7134],
        [3.7549, 3.7549, 3.7549],
        [3.7549, 3.7549, 3.7549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.1178009957075119 
model_pd.l_d.mean(): -23.226600646972656 
model_pd.lagr.mean(): -23.10879898071289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0491], device='cuda:0')), ('power', tensor([-23.2757], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.1178009957075119
epoch£º319	 i:1 	 global-step:6381	 l-p:-0.02110658399760723
epoch£º319	 i:2 	 global-step:6382	 l-p:0.10057882219552994
epoch£º319	 i:3 	 global-step:6383	 l-p:0.173272505402565
epoch£º319	 i:4 	 global-step:6384	 l-p:0.11176399141550064
epoch£º319	 i:5 	 global-step:6385	 l-p:0.08348549902439117
epoch£º319	 i:6 	 global-step:6386	 l-p:0.1123376339673996
epoch£º319	 i:7 	 global-step:6387	 l-p:0.10095590353012085
epoch£º319	 i:8 	 global-step:6388	 l-p:0.1495785415172577
epoch£º319	 i:9 	 global-step:6389	 l-p:0.1285237818956375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6710, 3.6514, 3.6258],
        [3.6710, 3.6720, 3.6189],
        [3.6710, 3.6656, 3.6183],
        [3.6710, 3.6681, 3.6707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.12188765406608582 
model_pd.l_d.mean(): -23.001493453979492 
model_pd.lagr.mean(): -22.879606246948242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0734], device='cuda:0')), ('power', tensor([-23.0749], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.12188765406608582
epoch£º320	 i:1 	 global-step:6401	 l-p:0.11351516097784042
epoch£º320	 i:2 	 global-step:6402	 l-p:-0.14386312663555145
epoch£º320	 i:3 	 global-step:6403	 l-p:0.11839650571346283
epoch£º320	 i:4 	 global-step:6404	 l-p:0.12751266360282898
epoch£º320	 i:5 	 global-step:6405	 l-p:0.10115623474121094
epoch£º320	 i:6 	 global-step:6406	 l-p:0.261679083108902
epoch£º320	 i:7 	 global-step:6407	 l-p:0.1299070119857788
epoch£º320	 i:8 	 global-step:6408	 l-p:0.11549599468708038
epoch£º320	 i:9 	 global-step:6409	 l-p:0.08267218619585037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4949, 3.4798, 3.4892],
        [3.4949, 3.6071, 3.5490],
        [3.4949, 3.4903, 3.4942],
        [3.4949, 3.4946, 3.4949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.09261084347963333 
model_pd.l_d.mean(): -22.60504913330078 
model_pd.lagr.mean(): -22.51243782043457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1960], device='cuda:0')), ('power', tensor([-22.8010], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.09261084347963333
epoch£º321	 i:1 	 global-step:6421	 l-p:0.13245511054992676
epoch£º321	 i:2 	 global-step:6422	 l-p:0.09895097464323044
epoch£º321	 i:3 	 global-step:6423	 l-p:0.12368196994066238
epoch£º321	 i:4 	 global-step:6424	 l-p:0.11326748877763748
epoch£º321	 i:5 	 global-step:6425	 l-p:0.10545625537633896
epoch£º321	 i:6 	 global-step:6426	 l-p:0.039055414497852325
epoch£º321	 i:7 	 global-step:6427	 l-p:0.13453420996665955
epoch£º321	 i:8 	 global-step:6428	 l-p:0.11497059464454651
epoch£º321	 i:9 	 global-step:6429	 l-p:0.1283557415008545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5167, 3.9461, 4.1061],
        [3.5167, 3.5166, 3.5167],
        [3.5167, 3.5163, 3.5167],
        [3.5167, 3.5167, 3.5167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.1283155232667923 
model_pd.l_d.mean(): -23.363126754760742 
model_pd.lagr.mean(): -23.234811782836914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1193], device='cuda:0')), ('power', tensor([-23.4824], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.1283155232667923
epoch£º322	 i:1 	 global-step:6441	 l-p:0.13189272582530975
epoch£º322	 i:2 	 global-step:6442	 l-p:0.0915115550160408
epoch£º322	 i:3 	 global-step:6443	 l-p:0.12734168767929077
epoch£º322	 i:4 	 global-step:6444	 l-p:0.15691065788269043
epoch£º322	 i:5 	 global-step:6445	 l-p:0.07428698986768723
epoch£º322	 i:6 	 global-step:6446	 l-p:0.09685993939638138
epoch£º322	 i:7 	 global-step:6447	 l-p:0.11349038779735565
epoch£º322	 i:8 	 global-step:6448	 l-p:0.2373657524585724
epoch£º322	 i:9 	 global-step:6449	 l-p:0.12438587099313736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6251, 3.6251, 3.6251],
        [3.6251, 3.6208, 3.6245],
        [3.6251, 3.6627, 3.5898],
        [3.6251, 3.6092, 3.6181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.08797479420900345 
model_pd.l_d.mean(): -23.150508880615234 
model_pd.lagr.mean(): -23.06253433227539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1145], device='cuda:0')), ('power', tensor([-23.2650], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.08797479420900345
epoch£º323	 i:1 	 global-step:6461	 l-p:0.11725633591413498
epoch£º323	 i:2 	 global-step:6462	 l-p:0.8237388134002686
epoch£º323	 i:3 	 global-step:6463	 l-p:0.1437370777130127
epoch£º323	 i:4 	 global-step:6464	 l-p:0.10566205531358719
epoch£º323	 i:5 	 global-step:6465	 l-p:-0.47298622131347656
epoch£º323	 i:6 	 global-step:6466	 l-p:0.12462638318538666
epoch£º323	 i:7 	 global-step:6467	 l-p:0.10476092994213104
epoch£º323	 i:8 	 global-step:6468	 l-p:0.09815173596143723
epoch£º323	 i:9 	 global-step:6469	 l-p:0.1048615500330925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7958, 3.7958, 3.7958],
        [3.7958, 3.7958, 3.7958],
        [3.7958, 3.7881, 3.7939],
        [3.7958, 3.7747, 3.7655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.2325393557548523 
model_pd.l_d.mean(): -22.638595581054688 
model_pd.lagr.mean(): -22.406055450439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1310], device='cuda:0')), ('power', tensor([-22.7696], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.2325393557548523
epoch£º324	 i:1 	 global-step:6481	 l-p:0.08513832837343216
epoch£º324	 i:2 	 global-step:6482	 l-p:0.11999734491109848
epoch£º324	 i:3 	 global-step:6483	 l-p:0.10828185081481934
epoch£º324	 i:4 	 global-step:6484	 l-p:0.10604865849018097
epoch£º324	 i:5 	 global-step:6485	 l-p:0.13843660056591034
epoch£º324	 i:6 	 global-step:6486	 l-p:0.07487116754055023
epoch£º324	 i:7 	 global-step:6487	 l-p:0.09300317615270615
epoch£º324	 i:8 	 global-step:6488	 l-p:0.11311929672956467
epoch£º324	 i:9 	 global-step:6489	 l-p:-0.015137872658669949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6232, 3.5982, 3.5660],
        [3.6232, 3.6142, 3.6210],
        [3.6232, 3.6230, 3.6232],
        [3.6232, 3.6308, 3.5642]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.09965454041957855 
model_pd.l_d.mean(): -22.392620086669922 
model_pd.lagr.mean(): -22.292964935302734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1435], device='cuda:0')), ('power', tensor([-22.5361], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.09965454041957855
epoch£º325	 i:1 	 global-step:6501	 l-p:0.17734801769256592
epoch£º325	 i:2 	 global-step:6502	 l-p:0.12150847911834717
epoch£º325	 i:3 	 global-step:6503	 l-p:0.11334818601608276
epoch£º325	 i:4 	 global-step:6504	 l-p:-0.14960061013698578
epoch£º325	 i:5 	 global-step:6505	 l-p:0.11380993574857712
epoch£º325	 i:6 	 global-step:6506	 l-p:0.08937866985797882
epoch£º325	 i:7 	 global-step:6507	 l-p:0.11703085899353027
epoch£º325	 i:8 	 global-step:6508	 l-p:0.11174371838569641
epoch£º325	 i:9 	 global-step:6509	 l-p:0.12906493246555328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6817, 3.6885, 3.6249],
        [3.6817, 3.6808, 3.6816],
        [3.6817, 4.1343, 4.2985],
        [3.6817, 3.6815, 3.6224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.09361038357019424 
model_pd.l_d.mean(): -23.080297470092773 
model_pd.lagr.mean(): -22.98668670654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0928], device='cuda:0')), ('power', tensor([-23.1731], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.09361038357019424
epoch£º326	 i:1 	 global-step:6521	 l-p:-0.2677405774593353
epoch£º326	 i:2 	 global-step:6522	 l-p:0.1283188760280609
epoch£º326	 i:3 	 global-step:6523	 l-p:0.11021140962839127
epoch£º326	 i:4 	 global-step:6524	 l-p:0.10301410406827927
epoch£º326	 i:5 	 global-step:6525	 l-p:0.1327526718378067
epoch£º326	 i:6 	 global-step:6526	 l-p:0.26723048090934753
epoch£º326	 i:7 	 global-step:6527	 l-p:0.10900178551673889
epoch£º326	 i:8 	 global-step:6528	 l-p:-0.0185699462890625
epoch£º326	 i:9 	 global-step:6529	 l-p:0.11935123056173325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5613, 3.9821, 4.1294],
        [3.5613, 3.5611, 3.5613],
        [3.5613, 3.5317, 3.5372],
        [3.5613, 3.5602, 3.5612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.137539342045784 
model_pd.l_d.mean(): -22.99420928955078 
model_pd.lagr.mean(): -22.856670379638672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1440], device='cuda:0')), ('power', tensor([-23.1382], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.137539342045784
epoch£º327	 i:1 	 global-step:6541	 l-p:-0.22229862213134766
epoch£º327	 i:2 	 global-step:6542	 l-p:0.13205599784851074
epoch£º327	 i:3 	 global-step:6543	 l-p:0.11586956679821014
epoch£º327	 i:4 	 global-step:6544	 l-p:0.2303675413131714
epoch£º327	 i:5 	 global-step:6545	 l-p:0.13067826628684998
epoch£º327	 i:6 	 global-step:6546	 l-p:0.09586191177368164
epoch£º327	 i:7 	 global-step:6547	 l-p:0.11029713600873947
epoch£º327	 i:8 	 global-step:6548	 l-p:0.07887870818376541
epoch£º327	 i:9 	 global-step:6549	 l-p:0.11598887294530869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5723, 3.5656, 3.5711],
        [3.5723, 3.5696, 3.5721],
        [3.5723, 3.5600, 3.4988],
        [3.5723, 3.7949, 3.7913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.11465813219547272 
model_pd.l_d.mean(): -22.964155197143555 
model_pd.lagr.mean(): -22.849496841430664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1611], device='cuda:0')), ('power', tensor([-23.1253], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.11465813219547272
epoch£º328	 i:1 	 global-step:6561	 l-p:0.14222487807273865
epoch£º328	 i:2 	 global-step:6562	 l-p:0.11642692983150482
epoch£º328	 i:3 	 global-step:6563	 l-p:0.0777384489774704
epoch£º328	 i:4 	 global-step:6564	 l-p:0.1340869665145874
epoch£º328	 i:5 	 global-step:6565	 l-p:0.09994480013847351
epoch£º328	 i:6 	 global-step:6566	 l-p:0.09884549677371979
epoch£º328	 i:7 	 global-step:6567	 l-p:0.12765254080295563
epoch£º328	 i:8 	 global-step:6568	 l-p:0.11662241816520691
epoch£º328	 i:9 	 global-step:6569	 l-p:0.1009281724691391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4379, 3.4379, 3.4379],
        [3.4379, 3.4850, 3.4034],
        [3.4379, 3.4369, 3.4378],
        [3.4379, 3.4077, 3.4187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.11535131186246872 
model_pd.l_d.mean(): -22.2155818939209 
model_pd.lagr.mean(): -22.100231170654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2713], device='cuda:0')), ('power', tensor([-22.4869], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.11535131186246872
epoch£º329	 i:1 	 global-step:6581	 l-p:0.1954619139432907
epoch£º329	 i:2 	 global-step:6582	 l-p:0.1067400872707367
epoch£º329	 i:3 	 global-step:6583	 l-p:0.1308998316526413
epoch£º329	 i:4 	 global-step:6584	 l-p:0.09565068781375885
epoch£º329	 i:5 	 global-step:6585	 l-p:0.1331741362810135
epoch£º329	 i:6 	 global-step:6586	 l-p:0.12282679975032806
epoch£º329	 i:7 	 global-step:6587	 l-p:0.107370525598526
epoch£º329	 i:8 	 global-step:6588	 l-p:0.13712213933467865
epoch£º329	 i:9 	 global-step:6589	 l-p:0.13211777806282043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3995, 3.3987, 3.3995],
        [3.3995, 3.3982, 3.3995],
        [3.3995, 3.3988, 3.3995],
        [3.3995, 3.3995, 3.3995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.12472617626190186 
model_pd.l_d.mean(): -23.306354522705078 
model_pd.lagr.mean(): -23.181629180908203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1755], device='cuda:0')), ('power', tensor([-23.4819], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.12472617626190186
epoch£º330	 i:1 	 global-step:6601	 l-p:0.13979867100715637
epoch£º330	 i:2 	 global-step:6602	 l-p:0.09502473473548889
epoch£º330	 i:3 	 global-step:6603	 l-p:0.10723714530467987
epoch£º330	 i:4 	 global-step:6604	 l-p:0.12493495643138885
epoch£º330	 i:5 	 global-step:6605	 l-p:0.13797126710414886
epoch£º330	 i:6 	 global-step:6606	 l-p:0.1159827932715416
epoch£º330	 i:7 	 global-step:6607	 l-p:0.13595719635486603
epoch£º330	 i:8 	 global-step:6608	 l-p:0.12550859153270721
epoch£º330	 i:9 	 global-step:6609	 l-p:0.12122274935245514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3656, 3.3209, 3.2675],
        [3.3656, 3.6762, 3.7477],
        [3.3656, 3.3617, 3.3652],
        [3.3656, 3.6131, 3.6380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.10212653130292892 
model_pd.l_d.mean(): -23.099090576171875 
model_pd.lagr.mean(): -22.996963500976562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2303], device='cuda:0')), ('power', tensor([-23.3294], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.10212653130292892
epoch£º331	 i:1 	 global-step:6621	 l-p:0.14100538194179535
epoch£º331	 i:2 	 global-step:6622	 l-p:-0.023774636909365654
epoch£º331	 i:3 	 global-step:6623	 l-p:0.13696394860744476
epoch£º331	 i:4 	 global-step:6624	 l-p:0.10532502830028534
epoch£º331	 i:5 	 global-step:6625	 l-p:0.12314596027135849
epoch£º331	 i:6 	 global-step:6626	 l-p:0.12758848071098328
epoch£º331	 i:7 	 global-step:6627	 l-p:0.12378650158643723
epoch£º331	 i:8 	 global-step:6628	 l-p:0.10767161846160889
epoch£º331	 i:9 	 global-step:6629	 l-p:0.10861220210790634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5108, 3.4677, 3.4452],
        [3.5108, 3.5242, 3.4405],
        [3.5108, 3.4786, 3.4236],
        [3.5108, 3.5076, 3.5105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.045273151248693466 
model_pd.l_d.mean(): -23.128131866455078 
model_pd.lagr.mean(): -23.08285903930664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1757], device='cuda:0')), ('power', tensor([-23.3038], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.045273151248693466
epoch£º332	 i:1 	 global-step:6641	 l-p:0.11822625994682312
epoch£º332	 i:2 	 global-step:6642	 l-p:0.1279783546924591
epoch£º332	 i:3 	 global-step:6643	 l-p:0.12907367944717407
epoch£º332	 i:4 	 global-step:6644	 l-p:0.1329599767923355
epoch£º332	 i:5 	 global-step:6645	 l-p:0.12180839478969574
epoch£º332	 i:6 	 global-step:6646	 l-p:0.12349210679531097
epoch£º332	 i:7 	 global-step:6647	 l-p:0.05671083182096481
epoch£º332	 i:8 	 global-step:6648	 l-p:0.11052518337965012
epoch£º332	 i:9 	 global-step:6649	 l-p:0.09116175770759583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5482, 3.5482, 3.5482],
        [3.5482, 3.6023, 3.5191],
        [3.5482, 3.5482, 3.5482],
        [3.5482, 3.5482, 3.5482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.11315090209245682 
model_pd.l_d.mean(): -23.1854248046875 
model_pd.lagr.mean(): -23.07227325439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1351], device='cuda:0')), ('power', tensor([-23.3205], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.11315090209245682
epoch£º333	 i:1 	 global-step:6661	 l-p:-0.15851697325706482
epoch£º333	 i:2 	 global-step:6662	 l-p:0.07037793844938278
epoch£º333	 i:3 	 global-step:6663	 l-p:0.12290570139884949
epoch£º333	 i:4 	 global-step:6664	 l-p:0.12035083770751953
epoch£º333	 i:5 	 global-step:6665	 l-p:0.12470419704914093
epoch£º333	 i:6 	 global-step:6666	 l-p:0.11895038932561874
epoch£º333	 i:7 	 global-step:6667	 l-p:0.1360938996076584
epoch£º333	 i:8 	 global-step:6668	 l-p:0.11344794183969498
epoch£º333	 i:9 	 global-step:6669	 l-p:0.0815141573548317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4637, 3.4632, 3.4636],
        [3.4637, 3.4605, 3.4634],
        [3.4637, 3.4175, 3.3771],
        [3.4637, 3.4171, 3.4113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.13523022830486298 
model_pd.l_d.mean(): -23.56609535217285 
model_pd.lagr.mean(): -23.430864334106445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1143], device='cuda:0')), ('power', tensor([-23.6804], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.13523022830486298
epoch£º334	 i:1 	 global-step:6681	 l-p:0.12194923311471939
epoch£º334	 i:2 	 global-step:6682	 l-p:0.1280766874551773
epoch£º334	 i:3 	 global-step:6683	 l-p:0.12494361400604248
epoch£º334	 i:4 	 global-step:6684	 l-p:0.10347828269004822
epoch£º334	 i:5 	 global-step:6685	 l-p:0.11890929937362671
epoch£º334	 i:6 	 global-step:6686	 l-p:0.13115166127681732
epoch£º334	 i:7 	 global-step:6687	 l-p:0.12458530068397522
epoch£º334	 i:8 	 global-step:6688	 l-p:0.13280919194221497
epoch£º334	 i:9 	 global-step:6689	 l-p:0.08574022352695465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3630, 3.3218, 3.3334],
        [3.3630, 3.6509, 3.7031],
        [3.3630, 3.3630, 3.3630],
        [3.3630, 3.6967, 3.7841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.06778902560472488 
model_pd.l_d.mean(): -22.372724533081055 
model_pd.lagr.mean(): -22.304935455322266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2906], device='cuda:0')), ('power', tensor([-22.6633], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.06778902560472488
epoch£º335	 i:1 	 global-step:6701	 l-p:0.13382920622825623
epoch£º335	 i:2 	 global-step:6702	 l-p:0.11857207864522934
epoch£º335	 i:3 	 global-step:6703	 l-p:0.1403975635766983
epoch£º335	 i:4 	 global-step:6704	 l-p:0.11957812309265137
epoch£º335	 i:5 	 global-step:6705	 l-p:0.10723377019166946
epoch£º335	 i:6 	 global-step:6706	 l-p:0.13586275279521942
epoch£º335	 i:7 	 global-step:6707	 l-p:0.09584750980138779
epoch£º335	 i:8 	 global-step:6708	 l-p:0.1349513828754425
epoch£º335	 i:9 	 global-step:6709	 l-p:0.1218072846531868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[3.4337, 3.3952, 3.4069],
        [3.4337, 3.3819, 3.3739],
        [3.4337, 3.4625, 3.3705],
        [3.4337, 3.3915, 3.4003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11678890138864517 
model_pd.l_d.mean(): -22.48685646057129 
model_pd.lagr.mean(): -22.370067596435547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1945], device='cuda:0')), ('power', tensor([-22.6814], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11678890138864517
epoch£º336	 i:1 	 global-step:6721	 l-p:0.1268325001001358
epoch£º336	 i:2 	 global-step:6722	 l-p:0.07985437661409378
epoch£º336	 i:3 	 global-step:6723	 l-p:0.13109537959098816
epoch£º336	 i:4 	 global-step:6724	 l-p:0.11893842369318008
epoch£º336	 i:5 	 global-step:6725	 l-p:0.1260267049074173
epoch£º336	 i:6 	 global-step:6726	 l-p:0.12865544855594635
epoch£º336	 i:7 	 global-step:6727	 l-p:0.11568246781826019
epoch£º336	 i:8 	 global-step:6728	 l-p:0.12299920618534088
epoch£º336	 i:9 	 global-step:6729	 l-p:0.1206672415137291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4700, 3.9042, 4.0645],
        [3.4700, 3.4376, 3.4515],
        [3.4700, 3.4372, 3.3619],
        [3.4700, 3.6287, 3.5868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.11127548664808273 
model_pd.l_d.mean(): -21.830947875976562 
model_pd.lagr.mean(): -21.71967315673828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2480], device='cuda:0')), ('power', tensor([-22.0789], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.11127548664808273
epoch£º337	 i:1 	 global-step:6741	 l-p:0.13412508368492126
epoch£º337	 i:2 	 global-step:6742	 l-p:0.11601006984710693
epoch£º337	 i:3 	 global-step:6743	 l-p:0.10914499312639236
epoch£º337	 i:4 	 global-step:6744	 l-p:0.13503873348236084
epoch£º337	 i:5 	 global-step:6745	 l-p:0.052266925573349
epoch£º337	 i:6 	 global-step:6746	 l-p:0.10625329613685608
epoch£º337	 i:7 	 global-step:6747	 l-p:0.13009119033813477
epoch£º337	 i:8 	 global-step:6748	 l-p:0.1361463963985443
epoch£º337	 i:9 	 global-step:6749	 l-p:0.12814000248908997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3779, 3.3281, 3.3358],
        [3.3779, 3.3509, 3.3664],
        [3.3779, 3.3719, 3.3771],
        [3.3779, 3.3779, 3.3779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.1206967905163765 
model_pd.l_d.mean(): -23.118982315063477 
model_pd.lagr.mean(): -22.9982852935791 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2226], device='cuda:0')), ('power', tensor([-23.3416], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.1206967905163765
epoch£º338	 i:1 	 global-step:6761	 l-p:0.11555789411067963
epoch£º338	 i:2 	 global-step:6762	 l-p:0.14209146797657013
epoch£º338	 i:3 	 global-step:6763	 l-p:0.12783364951610565
epoch£º338	 i:4 	 global-step:6764	 l-p:0.13615432381629944
epoch£º338	 i:5 	 global-step:6765	 l-p:0.14776431024074554
epoch£º338	 i:6 	 global-step:6766	 l-p:0.14729376137256622
epoch£º338	 i:7 	 global-step:6767	 l-p:0.11641212552785873
epoch£º338	 i:8 	 global-step:6768	 l-p:0.13184785842895508
epoch£º338	 i:9 	 global-step:6769	 l-p:0.14343984425067902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3383, 3.5116, 3.4828],
        [3.3383, 3.2974, 3.3132],
        [3.3383, 3.3092, 3.3257],
        [3.3383, 3.3383, 3.3383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.11909265071153641 
model_pd.l_d.mean(): -23.24789810180664 
model_pd.lagr.mean(): -23.12880516052246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2282], device='cuda:0')), ('power', tensor([-23.4761], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.11909265071153641
epoch£º339	 i:1 	 global-step:6781	 l-p:0.13615669310092926
epoch£º339	 i:2 	 global-step:6782	 l-p:0.11149995774030685
epoch£º339	 i:3 	 global-step:6783	 l-p:0.13040481507778168
epoch£º339	 i:4 	 global-step:6784	 l-p:0.12295370548963547
epoch£º339	 i:5 	 global-step:6785	 l-p:-0.02690841630101204
epoch£º339	 i:6 	 global-step:6786	 l-p:0.1360054612159729
epoch£º339	 i:7 	 global-step:6787	 l-p:0.09613190591335297
epoch£º339	 i:8 	 global-step:6788	 l-p:0.12900757789611816
epoch£º339	 i:9 	 global-step:6789	 l-p:0.10320436209440231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5253, 3.6366, 3.5651],
        [3.5253, 3.5226, 3.5251],
        [3.5253, 3.5251, 3.5253],
        [3.5253, 3.4800, 3.4876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.045503225177526474 
model_pd.l_d.mean(): -22.875171661376953 
model_pd.lagr.mean(): -22.829668045043945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1908], device='cuda:0')), ('power', tensor([-23.0659], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.045503225177526474
epoch£º340	 i:1 	 global-step:6801	 l-p:0.11667174845933914
epoch£º340	 i:2 	 global-step:6802	 l-p:0.11701730638742447
epoch£º340	 i:3 	 global-step:6803	 l-p:0.11791552603244781
epoch£º340	 i:4 	 global-step:6804	 l-p:0.10835984349250793
epoch£º340	 i:5 	 global-step:6805	 l-p:0.12856924533843994
epoch£º340	 i:6 	 global-step:6806	 l-p:0.11935349553823471
epoch£º340	 i:7 	 global-step:6807	 l-p:0.11275475472211838
epoch£º340	 i:8 	 global-step:6808	 l-p:0.12829986214637756
epoch£º340	 i:9 	 global-step:6809	 l-p:0.7989715337753296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6122, 3.6113, 3.6122],
        [3.6122, 3.5622, 3.5538],
        [3.6122, 3.7910, 3.7511],
        [3.6122, 3.8016, 3.7679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.10631337761878967 
model_pd.l_d.mean(): -22.98842430114746 
model_pd.lagr.mean(): -22.882110595703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1559], device='cuda:0')), ('power', tensor([-23.1444], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.10631337761878967
epoch£º341	 i:1 	 global-step:6821	 l-p:0.11661268770694733
epoch£º341	 i:2 	 global-step:6822	 l-p:0.12008307874202728
epoch£º341	 i:3 	 global-step:6823	 l-p:0.13622312247753143
epoch£º341	 i:4 	 global-step:6824	 l-p:0.11332492530345917
epoch£º341	 i:5 	 global-step:6825	 l-p:0.08438224345445633
epoch£º341	 i:6 	 global-step:6826	 l-p:0.12131480872631073
epoch£º341	 i:7 	 global-step:6827	 l-p:0.11904304474592209
epoch£º341	 i:8 	 global-step:6828	 l-p:0.13010700047016144
epoch£º341	 i:9 	 global-step:6829	 l-p:0.10275858640670776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4595, 3.4595, 3.4595],
        [3.4595, 3.6537, 3.6300],
        [3.4595, 3.7730, 3.8327],
        [3.4595, 3.6543, 3.6309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.1252526193857193 
model_pd.l_d.mean(): -23.434284210205078 
model_pd.lagr.mean(): -23.309032440185547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1475], device='cuda:0')), ('power', tensor([-23.5817], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.1252526193857193
epoch£º342	 i:1 	 global-step:6841	 l-p:0.12409646809101105
epoch£º342	 i:2 	 global-step:6842	 l-p:0.12782351672649384
epoch£º342	 i:3 	 global-step:6843	 l-p:0.11456966400146484
epoch£º342	 i:4 	 global-step:6844	 l-p:0.10947170853614807
epoch£º342	 i:5 	 global-step:6845	 l-p:0.13233323395252228
epoch£º342	 i:6 	 global-step:6846	 l-p:0.12592998147010803
epoch£º342	 i:7 	 global-step:6847	 l-p:0.14067474007606506
epoch£º342	 i:8 	 global-step:6848	 l-p:0.08064233511686325
epoch£º342	 i:9 	 global-step:6849	 l-p:0.1368430256843567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3868, 3.5577, 3.5220],
        [3.3868, 3.3868, 3.3868],
        [3.3868, 3.6665, 3.7045],
        [3.3868, 3.3619, 3.3779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.12767814099788666 
model_pd.l_d.mean(): -23.498815536499023 
model_pd.lagr.mean(): -23.371137619018555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1629], device='cuda:0')), ('power', tensor([-23.6617], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.12767814099788666
epoch£º343	 i:1 	 global-step:6861	 l-p:0.13093365728855133
epoch£º343	 i:2 	 global-step:6862	 l-p:0.13493238389492035
epoch£º343	 i:3 	 global-step:6863	 l-p:0.134615957736969
epoch£º343	 i:4 	 global-step:6864	 l-p:0.13736288249492645
epoch£º343	 i:5 	 global-step:6865	 l-p:0.15419289469718933
epoch£º343	 i:6 	 global-step:6866	 l-p:0.13123105466365814
epoch£º343	 i:7 	 global-step:6867	 l-p:0.13226903975009918
epoch£º343	 i:8 	 global-step:6868	 l-p:0.1273718923330307
epoch£º343	 i:9 	 global-step:6869	 l-p:0.1316445767879486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[3.2651, 3.5026, 3.5154],
        [3.2651, 3.4099, 3.3624],
        [3.2651, 3.1950, 3.1977],
        [3.2651, 3.1973, 3.2029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.13967183232307434 
model_pd.l_d.mean(): -23.527624130249023 
model_pd.lagr.mean(): -23.38795280456543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2112], device='cuda:0')), ('power', tensor([-23.7389], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.13967183232307434
epoch£º344	 i:1 	 global-step:6881	 l-p:0.14654883742332458
epoch£º344	 i:2 	 global-step:6882	 l-p:0.12390319257974625
epoch£º344	 i:3 	 global-step:6883	 l-p:0.11698693782091141
epoch£º344	 i:4 	 global-step:6884	 l-p:0.12185194343328476
epoch£º344	 i:5 	 global-step:6885	 l-p:0.13833509385585785
epoch£º344	 i:6 	 global-step:6886	 l-p:0.11386944353580475
epoch£º344	 i:7 	 global-step:6887	 l-p:0.02382301352918148
epoch£º344	 i:8 	 global-step:6888	 l-p:0.13472984731197357
epoch£º344	 i:9 	 global-step:6889	 l-p:0.13168928027153015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5004, 3.4833, 3.4958],
        [3.5004, 3.8158, 3.8719],
        [3.5004, 3.4790, 3.4935],
        [3.5004, 3.4475, 3.3680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.12137505412101746 
model_pd.l_d.mean(): -22.57728385925293 
model_pd.lagr.mean(): -22.455909729003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1698], device='cuda:0')), ('power', tensor([-22.7471], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.12137505412101746
epoch£º345	 i:1 	 global-step:6901	 l-p:0.11937647312879562
epoch£º345	 i:2 	 global-step:6902	 l-p:0.13524292409420013
epoch£º345	 i:3 	 global-step:6903	 l-p:0.1296035796403885
epoch£º345	 i:4 	 global-step:6904	 l-p:0.11926504969596863
epoch£º345	 i:5 	 global-step:6905	 l-p:0.12485712021589279
epoch£º345	 i:6 	 global-step:6906	 l-p:0.09695115685462952
epoch£º345	 i:7 	 global-step:6907	 l-p:0.13076896965503693
epoch£º345	 i:8 	 global-step:6908	 l-p:0.12262675166130066
epoch£º345	 i:9 	 global-step:6909	 l-p:0.07326418906450272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4801, 3.4695, 3.4782],
        [3.4801, 3.4800, 3.4801],
        [3.4801, 3.4511, 3.4682],
        [3.4801, 3.4371, 3.3408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.07894239574670792 
model_pd.l_d.mean(): -22.850799560546875 
model_pd.lagr.mean(): -22.7718563079834 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2345], device='cuda:0')), ('power', tensor([-23.0853], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.07894239574670792
epoch£º346	 i:1 	 global-step:6921	 l-p:0.1252387911081314
epoch£º346	 i:2 	 global-step:6922	 l-p:0.1132311001420021
epoch£º346	 i:3 	 global-step:6923	 l-p:0.11765020340681076
epoch£º346	 i:4 	 global-step:6924	 l-p:0.10914048552513123
epoch£º346	 i:5 	 global-step:6925	 l-p:0.11551956087350845
epoch£º346	 i:6 	 global-step:6926	 l-p:0.11342372745275497
epoch£º346	 i:7 	 global-step:6927	 l-p:0.12537060678005219
epoch£º346	 i:8 	 global-step:6928	 l-p:0.29529571533203125
epoch£º346	 i:9 	 global-step:6929	 l-p:0.1409839242696762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4675, 3.4675, 3.4675],
        [3.4675, 3.4060, 3.3226],
        [3.4675, 3.7950, 3.8600],
        [3.4675, 3.4627, 3.4670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): -0.5856220722198486 
model_pd.l_d.mean(): -22.902002334594727 
model_pd.lagr.mean(): -23.487625122070312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2262], device='cuda:0')), ('power', tensor([-23.1282], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:-0.5856220722198486
epoch£º347	 i:1 	 global-step:6941	 l-p:0.12606725096702576
epoch£º347	 i:2 	 global-step:6942	 l-p:0.13658320903778076
epoch£º347	 i:3 	 global-step:6943	 l-p:0.13116797804832458
epoch£º347	 i:4 	 global-step:6944	 l-p:0.11234906315803528
epoch£º347	 i:5 	 global-step:6945	 l-p:0.12477278709411621
epoch£º347	 i:6 	 global-step:6946	 l-p:0.14176352322101593
epoch£º347	 i:7 	 global-step:6947	 l-p:0.12340697646141052
epoch£º347	 i:8 	 global-step:6948	 l-p:0.12347325682640076
epoch£º347	 i:9 	 global-step:6949	 l-p:0.13564467430114746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2721, 3.2189, 3.2392],
        [3.2721, 3.2404, 3.2600],
        [3.2721, 3.2228, 3.2439],
        [3.2721, 3.2721, 3.2721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.1370990127325058 
model_pd.l_d.mean(): -23.27313995361328 
model_pd.lagr.mean(): -23.13604164123535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2392], device='cuda:0')), ('power', tensor([-23.5124], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.1370990127325058
epoch£º348	 i:1 	 global-step:6961	 l-p:0.141441211104393
epoch£º348	 i:2 	 global-step:6962	 l-p:0.1449679285287857
epoch£º348	 i:3 	 global-step:6963	 l-p:0.14741995930671692
epoch£º348	 i:4 	 global-step:6964	 l-p:0.14009590446949005
epoch£º348	 i:5 	 global-step:6965	 l-p:0.12874102592468262
epoch£º348	 i:6 	 global-step:6966	 l-p:0.1346368044614792
epoch£º348	 i:7 	 global-step:6967	 l-p:0.12646669149398804
epoch£º348	 i:8 	 global-step:6968	 l-p:0.13414931297302246
epoch£º348	 i:9 	 global-step:6969	 l-p:0.13382886350154877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3736, 3.3736, 3.3736],
        [3.3736, 3.3720, 3.3735],
        [3.3736, 3.3716, 3.3735],
        [3.3736, 3.3431, 3.3620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.12006082385778427 
model_pd.l_d.mean(): -22.147451400756836 
model_pd.lagr.mean(): -22.02739143371582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2968], device='cuda:0')), ('power', tensor([-22.4442], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.12006082385778427
epoch£º349	 i:1 	 global-step:6981	 l-p:0.1191951334476471
epoch£º349	 i:2 	 global-step:6982	 l-p:0.1421922892332077
epoch£º349	 i:3 	 global-step:6983	 l-p:0.11929591000080109
epoch£º349	 i:4 	 global-step:6984	 l-p:0.2525322139263153
epoch£º349	 i:5 	 global-step:6985	 l-p:0.12628290057182312
epoch£º349	 i:6 	 global-step:6986	 l-p:0.0619002990424633
epoch£º349	 i:7 	 global-step:6987	 l-p:0.10284318774938583
epoch£º349	 i:8 	 global-step:6988	 l-p:0.10349181294441223
epoch£º349	 i:9 	 global-step:6989	 l-p:-0.014275221154093742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6296, 3.6277, 3.6295],
        [3.6296, 3.8123, 3.7663],
        [3.6296, 3.7224, 3.6307],
        [3.6296, 3.8180, 3.7755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.127383291721344 
model_pd.l_d.mean(): -23.218984603881836 
model_pd.lagr.mean(): -23.09160041809082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1190], device='cuda:0')), ('power', tensor([-23.3380], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.127383291721344
epoch£º350	 i:1 	 global-step:7001	 l-p:0.10715985298156738
epoch£º350	 i:2 	 global-step:7002	 l-p:-0.009390506893396378
epoch£º350	 i:3 	 global-step:7003	 l-p:0.11853145062923431
epoch£º350	 i:4 	 global-step:7004	 l-p:0.13321323692798615
epoch£º350	 i:5 	 global-step:7005	 l-p:0.12132260948419571
epoch£º350	 i:6 	 global-step:7006	 l-p:0.08700648695230484
epoch£º350	 i:7 	 global-step:7007	 l-p:0.09706737101078033
epoch£º350	 i:8 	 global-step:7008	 l-p:-0.05813605338335037
epoch£º350	 i:9 	 global-step:7009	 l-p:0.10050669312477112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6233, 3.5820, 3.5995],
        [3.6233, 3.5709, 3.5833],
        [3.6233, 3.6047, 3.4958],
        [3.6233, 3.6141, 3.5032]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.12714511156082153 
model_pd.l_d.mean(): -23.031646728515625 
model_pd.lagr.mean(): -22.90450096130371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1530], device='cuda:0')), ('power', tensor([-23.1847], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.12714511156082153
epoch£º351	 i:1 	 global-step:7021	 l-p:0.12821462750434875
epoch£º351	 i:2 	 global-step:7022	 l-p:0.2095731943845749
epoch£º351	 i:3 	 global-step:7023	 l-p:0.11143802106380463
epoch£º351	 i:4 	 global-step:7024	 l-p:0.11694922298192978
epoch£º351	 i:5 	 global-step:7025	 l-p:0.10220840573310852
epoch£º351	 i:6 	 global-step:7026	 l-p:0.11754617094993591
epoch£º351	 i:7 	 global-step:7027	 l-p:0.08458186686038971
epoch£º351	 i:8 	 global-step:7028	 l-p:0.12601619958877563
epoch£º351	 i:9 	 global-step:7029	 l-p:0.07434116303920746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5304, 3.4901, 3.5096],
        [3.5304, 3.4666, 3.3736],
        [3.5304, 3.4663, 3.4741],
        [3.5304, 3.5231, 3.4047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.13163943588733673 
model_pd.l_d.mean(): -23.625423431396484 
model_pd.lagr.mean(): -23.493783950805664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0825], device='cuda:0')), ('power', tensor([-23.7079], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.13163943588733673
epoch£º352	 i:1 	 global-step:7041	 l-p:0.12749570608139038
epoch£º352	 i:2 	 global-step:7042	 l-p:0.11600059270858765
epoch£º352	 i:3 	 global-step:7043	 l-p:0.12508781254291534
epoch£º352	 i:4 	 global-step:7044	 l-p:0.1268758922815323
epoch£º352	 i:5 	 global-step:7045	 l-p:0.1214841976761818
epoch£º352	 i:6 	 global-step:7046	 l-p:0.12697754800319672
epoch£º352	 i:7 	 global-step:7047	 l-p:0.09620673954486847
epoch£º352	 i:8 	 global-step:7048	 l-p:0.12975355982780457
epoch£º352	 i:9 	 global-step:7049	 l-p:0.11435403674840927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3965, 3.3871, 3.3951],
        [3.3965, 3.3931, 3.3963],
        [3.3965, 3.3935, 3.3963],
        [3.3965, 3.3965, 3.3965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.12082178890705109 
model_pd.l_d.mean(): -23.30293846130371 
model_pd.lagr.mean(): -23.182117462158203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1936], device='cuda:0')), ('power', tensor([-23.4966], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.12082178890705109
epoch£º353	 i:1 	 global-step:7061	 l-p:0.12679719924926758
epoch£º353	 i:2 	 global-step:7062	 l-p:0.10692233592271805
epoch£º353	 i:3 	 global-step:7063	 l-p:0.13756902515888214
epoch£º353	 i:4 	 global-step:7064	 l-p:0.13725322484970093
epoch£º353	 i:5 	 global-step:7065	 l-p:0.11618665605783463
epoch£º353	 i:6 	 global-step:7066	 l-p:0.11613980680704117
epoch£º353	 i:7 	 global-step:7067	 l-p:0.13345472514629364
epoch£º353	 i:8 	 global-step:7068	 l-p:0.1490732580423355
epoch£º353	 i:9 	 global-step:7069	 l-p:0.08467824757099152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3919, 3.3531, 3.3749],
        [3.3919, 3.3779, 3.2488],
        [3.3919, 3.6865, 3.7232],
        [3.3919, 3.3825, 3.3906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.12972097098827362 
model_pd.l_d.mean(): -23.501968383789062 
model_pd.lagr.mean(): -23.37224769592285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1507], device='cuda:0')), ('power', tensor([-23.6527], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.12972097098827362
epoch£º354	 i:1 	 global-step:7081	 l-p:0.1272168904542923
epoch£º354	 i:2 	 global-step:7082	 l-p:0.12582382559776306
epoch£º354	 i:3 	 global-step:7083	 l-p:0.13675791025161743
epoch£º354	 i:4 	 global-step:7084	 l-p:0.11823056638240814
epoch£º354	 i:5 	 global-step:7085	 l-p:0.12931106984615326
epoch£º354	 i:6 	 global-step:7086	 l-p:0.12149941176176071
epoch£º354	 i:7 	 global-step:7087	 l-p:0.12986870110034943
epoch£º354	 i:8 	 global-step:7088	 l-p:0.1290326714515686
epoch£º354	 i:9 	 global-step:7089	 l-p:0.1587875634431839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2902, 3.5496, 3.5645],
        [3.2902, 3.2637, 3.2824],
        [3.2902, 3.2847, 3.2897],
        [3.2902, 3.2883, 3.2901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.13056887686252594 
model_pd.l_d.mean(): -23.057109832763672 
model_pd.lagr.mean(): -22.92654037475586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2576], device='cuda:0')), ('power', tensor([-23.3147], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.13056887686252594
epoch£º355	 i:1 	 global-step:7101	 l-p:0.11390751600265503
epoch£º355	 i:2 	 global-step:7102	 l-p:0.15872468054294586
epoch£º355	 i:3 	 global-step:7103	 l-p:0.12376390397548676
epoch£º355	 i:4 	 global-step:7104	 l-p:0.1333906650543213
epoch£º355	 i:5 	 global-step:7105	 l-p:0.13789987564086914
epoch£º355	 i:6 	 global-step:7106	 l-p:0.11996199935674667
epoch£º355	 i:7 	 global-step:7107	 l-p:0.12885785102844238
epoch£º355	 i:8 	 global-step:7108	 l-p:0.11596467345952988
epoch£º355	 i:9 	 global-step:7109	 l-p:0.13260459899902344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4313, 3.3658, 3.3838],
        [3.4313, 3.5191, 3.4212],
        [3.4313, 3.4313, 3.4313],
        [3.4313, 3.3632, 3.3797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.12862135469913483 
model_pd.l_d.mean(): -22.858163833618164 
model_pd.lagr.mean(): -22.729541778564453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2020], device='cuda:0')), ('power', tensor([-23.0602], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.12862135469913483
epoch£º356	 i:1 	 global-step:7121	 l-p:0.10269571095705032
epoch£º356	 i:2 	 global-step:7122	 l-p:0.11302760988473892
epoch£º356	 i:3 	 global-step:7123	 l-p:0.13184788823127747
epoch£º356	 i:4 	 global-step:7124	 l-p:0.13904890418052673
epoch£º356	 i:5 	 global-step:7125	 l-p:0.13578759133815765
epoch£º356	 i:6 	 global-step:7126	 l-p:0.13373161852359772
epoch£º356	 i:7 	 global-step:7127	 l-p:0.1254737228155136
epoch£º356	 i:8 	 global-step:7128	 l-p:0.11598064005374908
epoch£º356	 i:9 	 global-step:7129	 l-p:0.13267208635807037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3376, 3.5761, 3.5713],
        [3.3376, 3.3346, 3.3374],
        [3.3376, 3.3232, 3.3350],
        [3.3376, 3.2448, 3.2450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.11098982393741608 
model_pd.l_d.mean(): -22.946613311767578 
model_pd.lagr.mean(): -22.835622787475586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2956], device='cuda:0')), ('power', tensor([-23.2422], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.11098982393741608
epoch£º357	 i:1 	 global-step:7141	 l-p:0.14693310856819153
epoch£º357	 i:2 	 global-step:7142	 l-p:0.12587928771972656
epoch£º357	 i:3 	 global-step:7143	 l-p:0.1418762356042862
epoch£º357	 i:4 	 global-step:7144	 l-p:0.13865207135677338
epoch£º357	 i:5 	 global-step:7145	 l-p:0.13307063281536102
epoch£º357	 i:6 	 global-step:7146	 l-p:0.13257578015327454
epoch£º357	 i:7 	 global-step:7147	 l-p:0.13146953284740448
epoch£º357	 i:8 	 global-step:7148	 l-p:0.13816522061824799
epoch£º357	 i:9 	 global-step:7149	 l-p:0.126825213432312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3876, 3.3863, 3.3875],
        [3.3876, 3.3876, 3.3876],
        [3.3876, 3.2789, 3.2365],
        [3.3876, 3.5362, 3.4697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.14601019024848938 
model_pd.l_d.mean(): -23.39259910583496 
model_pd.lagr.mean(): -23.24658966064453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1898], device='cuda:0')), ('power', tensor([-23.5824], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.14601019024848938
epoch£º358	 i:1 	 global-step:7161	 l-p:0.10866573452949524
epoch£º358	 i:2 	 global-step:7162	 l-p:0.13834379613399506
epoch£º358	 i:3 	 global-step:7163	 l-p:0.11140719801187515
epoch£º358	 i:4 	 global-step:7164	 l-p:0.14277586340904236
epoch£º358	 i:5 	 global-step:7165	 l-p:0.10859351605176926
epoch£º358	 i:6 	 global-step:7166	 l-p:0.12957966327667236
epoch£º358	 i:7 	 global-step:7167	 l-p:0.11576618254184723
epoch£º358	 i:8 	 global-step:7168	 l-p:0.09364091604948044
epoch£º358	 i:9 	 global-step:7169	 l-p:0.12242086976766586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4161, 3.3802, 3.4027],
        [3.4161, 3.4161, 3.4161],
        [3.4161, 3.3127, 3.2922],
        [3.4161, 3.4161, 3.4161]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.09756893664598465 
model_pd.l_d.mean(): -23.15699005126953 
model_pd.lagr.mean(): -23.05942153930664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2098], device='cuda:0')), ('power', tensor([-23.3668], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.09756893664598465
epoch£º359	 i:1 	 global-step:7181	 l-p:0.12400967627763748
epoch£º359	 i:2 	 global-step:7182	 l-p:0.11263518780469894
epoch£º359	 i:3 	 global-step:7183	 l-p:0.12128239125013351
epoch£º359	 i:4 	 global-step:7184	 l-p:0.14219807088375092
epoch£º359	 i:5 	 global-step:7185	 l-p:0.13286589086055756
epoch£º359	 i:6 	 global-step:7186	 l-p:0.13397283852100372
epoch£º359	 i:7 	 global-step:7187	 l-p:0.11045145988464355
epoch£º359	 i:8 	 global-step:7188	 l-p:0.1297736018896103
epoch£º359	 i:9 	 global-step:7189	 l-p:0.11405250430107117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3828, 3.2873, 3.2901],
        [3.3828, 3.2856, 3.2860],
        [3.3828, 3.3057, 3.3250],
        [3.3828, 3.3828, 3.3828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.10810144245624542 
model_pd.l_d.mean(): -23.041479110717773 
model_pd.lagr.mean(): -22.933378219604492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2567], device='cuda:0')), ('power', tensor([-23.2982], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.10810144245624542
epoch£º360	 i:1 	 global-step:7201	 l-p:0.14122024178504944
epoch£º360	 i:2 	 global-step:7202	 l-p:0.1501009464263916
epoch£º360	 i:3 	 global-step:7203	 l-p:0.15179748833179474
epoch£º360	 i:4 	 global-step:7204	 l-p:0.1700431853532791
epoch£º360	 i:5 	 global-step:7205	 l-p:0.13426916301250458
epoch£º360	 i:6 	 global-step:7206	 l-p:0.14856621623039246
epoch£º360	 i:7 	 global-step:7207	 l-p:0.16077208518981934
epoch£º360	 i:8 	 global-step:7208	 l-p:0.136499285697937
epoch£º360	 i:9 	 global-step:7209	 l-p:0.16823938488960266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2191, 3.2191, 3.2191],
        [3.2191, 3.2191, 3.2191],
        [3.2191, 3.2189, 3.2191],
        [3.2191, 3.2165, 3.2190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.16743451356887817 
model_pd.l_d.mean(): -23.421289443969727 
model_pd.lagr.mean(): -23.253854751586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2758], device='cuda:0')), ('power', tensor([-23.6971], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.16743451356887817
epoch£º361	 i:1 	 global-step:7221	 l-p:0.14958524703979492
epoch£º361	 i:2 	 global-step:7222	 l-p:0.13716308772563934
epoch£º361	 i:3 	 global-step:7223	 l-p:0.14470724761486053
epoch£º361	 i:4 	 global-step:7224	 l-p:0.10711193829774857
epoch£º361	 i:5 	 global-step:7225	 l-p:0.0960836261510849
epoch£º361	 i:6 	 global-step:7226	 l-p:0.12587304413318634
epoch£º361	 i:7 	 global-step:7227	 l-p:0.12984365224838257
epoch£º361	 i:8 	 global-step:7228	 l-p:0.12645617127418518
epoch£º361	 i:9 	 global-step:7229	 l-p:0.12092690914869308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5055, 3.4065, 3.3989],
        [3.5055, 3.5055, 3.5055],
        [3.5055, 3.5055, 3.5055],
        [3.5055, 3.5667, 3.4451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.12328178435564041 
model_pd.l_d.mean(): -23.223766326904297 
model_pd.lagr.mean(): -23.10048484802246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1772], device='cuda:0')), ('power', tensor([-23.4010], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.12328178435564041
epoch£º362	 i:1 	 global-step:7241	 l-p:0.12527009844779968
epoch£º362	 i:2 	 global-step:7242	 l-p:0.11588635295629501
epoch£º362	 i:3 	 global-step:7243	 l-p:0.1278473287820816
epoch£º362	 i:4 	 global-step:7244	 l-p:0.11830129474401474
epoch£º362	 i:5 	 global-step:7245	 l-p:0.09340527653694153
epoch£º362	 i:6 	 global-step:7246	 l-p:0.13721247017383575
epoch£º362	 i:7 	 global-step:7247	 l-p:0.1370428502559662
epoch£º362	 i:8 	 global-step:7248	 l-p:0.12727102637290955
epoch£º362	 i:9 	 global-step:7249	 l-p:0.13346044719219208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3898, 3.3167, 3.3416],
        [3.3898, 3.3888, 3.3898],
        [3.3898, 3.3874, 3.3897],
        [3.3898, 3.2710, 3.2441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.13802877068519592 
model_pd.l_d.mean(): -23.495758056640625 
model_pd.lagr.mean(): -23.357728958129883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1753], device='cuda:0')), ('power', tensor([-23.6711], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.13802877068519592
epoch£º363	 i:1 	 global-step:7261	 l-p:0.13159571588039398
epoch£º363	 i:2 	 global-step:7262	 l-p:0.13822099566459656
epoch£º363	 i:3 	 global-step:7263	 l-p:0.11906560510396957
epoch£º363	 i:4 	 global-step:7264	 l-p:0.13728757202625275
epoch£º363	 i:5 	 global-step:7265	 l-p:0.13584545254707336
epoch£º363	 i:6 	 global-step:7266	 l-p:0.12041644752025604
epoch£º363	 i:7 	 global-step:7267	 l-p:0.13501295447349548
epoch£º363	 i:8 	 global-step:7268	 l-p:0.130121648311615
epoch£º363	 i:9 	 global-step:7269	 l-p:0.13453131914138794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2750, 3.5129, 3.4977],
        [3.2750, 3.1965, 3.2240],
        [3.2750, 3.2748, 3.2750],
        [3.2750, 3.1384, 3.0972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.14814765751361847 
model_pd.l_d.mean(): -22.51105308532715 
model_pd.lagr.mean(): -22.362905502319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3053], device='cuda:0')), ('power', tensor([-22.8164], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.14814765751361847
epoch£º364	 i:1 	 global-step:7281	 l-p:0.132993683218956
epoch£º364	 i:2 	 global-step:7282	 l-p:0.13712650537490845
epoch£º364	 i:3 	 global-step:7283	 l-p:0.11261766403913498
epoch£º364	 i:4 	 global-step:7284	 l-p:0.13273471593856812
epoch£º364	 i:5 	 global-step:7285	 l-p:0.15856637060642242
epoch£º364	 i:6 	 global-step:7286	 l-p:0.1226675733923912
epoch£º364	 i:7 	 global-step:7287	 l-p:0.1196669191122055
epoch£º364	 i:8 	 global-step:7288	 l-p:0.11053037643432617
epoch£º364	 i:9 	 global-step:7289	 l-p:0.12501327693462372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4711, 3.4711, 3.4711],
        [3.4711, 3.3520, 3.3242],
        [3.4711, 3.3671, 3.3683],
        [3.4711, 3.4604, 3.4697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.12924739718437195 
model_pd.l_d.mean(): -23.476970672607422 
model_pd.lagr.mean(): -23.34772300720215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1455], device='cuda:0')), ('power', tensor([-23.6224], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.12924739718437195
epoch£º365	 i:1 	 global-step:7301	 l-p:0.11389079689979553
epoch£º365	 i:2 	 global-step:7302	 l-p:0.12656886875629425
epoch£º365	 i:3 	 global-step:7303	 l-p:0.14784018695354462
epoch£º365	 i:4 	 global-step:7304	 l-p:0.12712839245796204
epoch£º365	 i:5 	 global-step:7305	 l-p:0.13532531261444092
epoch£º365	 i:6 	 global-step:7306	 l-p:0.14345033466815948
epoch£º365	 i:7 	 global-step:7307	 l-p:0.1384705901145935
epoch£º365	 i:8 	 global-step:7308	 l-p:0.1580866277217865
epoch£º365	 i:9 	 global-step:7309	 l-p:0.14003151655197144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2555, 3.2484, 3.2548],
        [3.2555, 3.3865, 3.3003],
        [3.2555, 3.4367, 3.3810],
        [3.2555, 3.2499, 3.2550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.11124812811613083 
model_pd.l_d.mean(): -22.030155181884766 
model_pd.lagr.mean(): -21.918907165527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4181], device='cuda:0')), ('power', tensor([-22.4483], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.11124812811613083
epoch£º366	 i:1 	 global-step:7321	 l-p:0.131103515625
epoch£º366	 i:2 	 global-step:7322	 l-p:0.18843582272529602
epoch£º366	 i:3 	 global-step:7323	 l-p:0.13064493238925934
epoch£º366	 i:4 	 global-step:7324	 l-p:0.14064286649227142
epoch£º366	 i:5 	 global-step:7325	 l-p:0.13755063712596893
epoch£º366	 i:6 	 global-step:7326	 l-p:0.15200543403625488
epoch£º366	 i:7 	 global-step:7327	 l-p:0.12152788043022156
epoch£º366	 i:8 	 global-step:7328	 l-p:0.15416540205478668
epoch£º366	 i:9 	 global-step:7329	 l-p:0.12075021117925644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[3.3776, 3.5533, 3.4886],
        [3.3776, 3.2516, 3.2359],
        [3.3776, 3.2753, 3.2899],
        [3.3776, 3.2388, 3.1157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.12323392927646637 
model_pd.l_d.mean(): -22.52366065979004 
model_pd.lagr.mean(): -22.400426864624023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2920], device='cuda:0')), ('power', tensor([-22.8156], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.12323392927646637
epoch£º367	 i:1 	 global-step:7341	 l-p:0.13771389424800873
epoch£º367	 i:2 	 global-step:7342	 l-p:0.14384792745113373
epoch£º367	 i:3 	 global-step:7343	 l-p:0.12148972600698471
epoch£º367	 i:4 	 global-step:7344	 l-p:0.13101790845394135
epoch£º367	 i:5 	 global-step:7345	 l-p:0.1262722909450531
epoch£º367	 i:6 	 global-step:7346	 l-p:0.14826276898384094
epoch£º367	 i:7 	 global-step:7347	 l-p:0.1325085461139679
epoch£º367	 i:8 	 global-step:7348	 l-p:0.10309012979269028
epoch£º367	 i:9 	 global-step:7349	 l-p:0.14140360057353973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3018, 3.3018, 3.3018],
        [3.3018, 3.3018, 3.3018],
        [3.3018, 3.2972, 3.3015],
        [3.3018, 3.1461, 3.0600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.15467455983161926 
model_pd.l_d.mean(): -23.24173927307129 
model_pd.lagr.mean(): -23.087064743041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2708], device='cuda:0')), ('power', tensor([-23.5125], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.15467455983161926
epoch£º368	 i:1 	 global-step:7361	 l-p:0.16614070534706116
epoch£º368	 i:2 	 global-step:7362	 l-p:0.13698743283748627
epoch£º368	 i:3 	 global-step:7363	 l-p:0.14778533577919006
epoch£º368	 i:4 	 global-step:7364	 l-p:0.12776191532611847
epoch£º368	 i:5 	 global-step:7365	 l-p:0.14255309104919434
epoch£º368	 i:6 	 global-step:7366	 l-p:0.15090808272361755
epoch£º368	 i:7 	 global-step:7367	 l-p:0.11584851890802383
epoch£º368	 i:8 	 global-step:7368	 l-p:0.11226148158311844
epoch£º368	 i:9 	 global-step:7369	 l-p:0.1372549831867218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3061, 3.1568, 3.0095],
        [3.3061, 3.3028, 3.3060],
        [3.3061, 3.3061, 3.3061],
        [3.3061, 3.4520, 3.3679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.15106917917728424 
model_pd.l_d.mean(): -22.792736053466797 
model_pd.lagr.mean(): -22.641666412353516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3439], device='cuda:0')), ('power', tensor([-23.1366], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.15106917917728424
epoch£º369	 i:1 	 global-step:7381	 l-p:0.12365519255399704
epoch£º369	 i:2 	 global-step:7382	 l-p:0.14536817371845245
epoch£º369	 i:3 	 global-step:7383	 l-p:0.13343700766563416
epoch£º369	 i:4 	 global-step:7384	 l-p:0.1222868487238884
epoch£º369	 i:5 	 global-step:7385	 l-p:0.12090682983398438
epoch£º369	 i:6 	 global-step:7386	 l-p:0.13510003685951233
epoch£º369	 i:7 	 global-step:7387	 l-p:0.1425623595714569
epoch£º369	 i:8 	 global-step:7388	 l-p:0.16033825278282166
epoch£º369	 i:9 	 global-step:7389	 l-p:0.1289391666650772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3527, 3.1948, 3.0926],
        [3.3527, 3.3481, 3.3523],
        [3.3527, 3.3901, 3.2447],
        [3.3527, 3.2633, 3.2915]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.12818823754787445 
model_pd.l_d.mean(): -21.542160034179688 
model_pd.lagr.mean(): -21.413970947265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3451], device='cuda:0')), ('power', tensor([-21.8873], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.12818823754787445
epoch£º370	 i:1 	 global-step:7401	 l-p:0.13078910112380981
epoch£º370	 i:2 	 global-step:7402	 l-p:0.14082904160022736
epoch£º370	 i:3 	 global-step:7403	 l-p:0.1103656068444252
epoch£º370	 i:4 	 global-step:7404	 l-p:0.13676877319812775
epoch£º370	 i:5 	 global-step:7405	 l-p:0.1322525143623352
epoch£º370	 i:6 	 global-step:7406	 l-p:0.1428736299276352
epoch£º370	 i:7 	 global-step:7407	 l-p:0.12159529328346252
epoch£º370	 i:8 	 global-step:7408	 l-p:0.18112921714782715
epoch£º370	 i:9 	 global-step:7409	 l-p:0.11992261558771133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2941, 3.2940, 3.2941],
        [3.2941, 3.2917, 3.2940],
        [3.2941, 3.4205, 3.3216],
        [3.2941, 3.2705, 3.2890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.16684989631175995 
model_pd.l_d.mean(): -23.196016311645508 
model_pd.lagr.mean(): -23.02916717529297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2741], device='cuda:0')), ('power', tensor([-23.4701], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.16684989631175995
epoch£º371	 i:1 	 global-step:7421	 l-p:0.13600365817546844
epoch£º371	 i:2 	 global-step:7422	 l-p:0.12361057102680206
epoch£º371	 i:3 	 global-step:7423	 l-p:0.14459383487701416
epoch£º371	 i:4 	 global-step:7424	 l-p:0.10681182891130447
epoch£º371	 i:5 	 global-step:7425	 l-p:0.1593504548072815
epoch£º371	 i:6 	 global-step:7426	 l-p:0.1438911110162735
epoch£º371	 i:7 	 global-step:7427	 l-p:0.14116430282592773
epoch£º371	 i:8 	 global-step:7428	 l-p:0.16248644888401031
epoch£º371	 i:9 	 global-step:7429	 l-p:0.14088523387908936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3593, 3.5817, 3.5381],
        [3.3593, 3.3593, 3.3593],
        [3.3593, 3.3569, 3.3592],
        [3.3593, 3.1948, 3.1051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.14523179829120636 
model_pd.l_d.mean(): -22.57299041748047 
model_pd.lagr.mean(): -22.427759170532227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2843], device='cuda:0')), ('power', tensor([-22.8573], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.14523179829120636
epoch£º372	 i:1 	 global-step:7441	 l-p:0.1334993988275528
epoch£º372	 i:2 	 global-step:7442	 l-p:0.11325820535421371
epoch£º372	 i:3 	 global-step:7443	 l-p:0.11945793032646179
epoch£º372	 i:4 	 global-step:7444	 l-p:0.11768671125173569
epoch£º372	 i:5 	 global-step:7445	 l-p:0.11618953198194504
epoch£º372	 i:6 	 global-step:7446	 l-p:0.15085966885089874
epoch£º372	 i:7 	 global-step:7447	 l-p:0.1380431354045868
epoch£º372	 i:8 	 global-step:7448	 l-p:0.13801658153533936
epoch£º372	 i:9 	 global-step:7449	 l-p:0.15336230397224426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3019, 3.4195, 3.3112],
        [3.3019, 3.2639, 3.2905],
        [3.3019, 3.3019, 3.3019],
        [3.3019, 3.1262, 2.9985]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.12992790341377258 
model_pd.l_d.mean(): -22.290761947631836 
model_pd.lagr.mean(): -22.16083335876465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3288], device='cuda:0')), ('power', tensor([-22.6196], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.12992790341377258
epoch£º373	 i:1 	 global-step:7461	 l-p:0.15045084059238434
epoch£º373	 i:2 	 global-step:7462	 l-p:0.14906202256679535
epoch£º373	 i:3 	 global-step:7463	 l-p:0.13585086166858673
epoch£º373	 i:4 	 global-step:7464	 l-p:0.11263898015022278
epoch£º373	 i:5 	 global-step:7465	 l-p:0.12687177956104279
epoch£º373	 i:6 	 global-step:7466	 l-p:0.1427062749862671
epoch£º373	 i:7 	 global-step:7467	 l-p:0.14031441509723663
epoch£º373	 i:8 	 global-step:7468	 l-p:0.1425851285457611
epoch£º373	 i:9 	 global-step:7469	 l-p:0.123109832406044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3759, 3.2091, 3.1422],
        [3.3759, 3.3195, 3.3522],
        [3.3759, 3.3344, 3.3624],
        [3.3759, 3.5521, 3.4748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.1431180089712143 
model_pd.l_d.mean(): -23.42577362060547 
model_pd.lagr.mean(): -23.282655715942383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2078], device='cuda:0')), ('power', tensor([-23.6336], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.1431180089712143
epoch£º374	 i:1 	 global-step:7481	 l-p:0.1325601041316986
epoch£º374	 i:2 	 global-step:7482	 l-p:0.13265475630760193
epoch£º374	 i:3 	 global-step:7483	 l-p:0.1215137243270874
epoch£º374	 i:4 	 global-step:7484	 l-p:0.12499811500310898
epoch£º374	 i:5 	 global-step:7485	 l-p:0.13014329969882965
epoch£º374	 i:6 	 global-step:7486	 l-p:0.15661059319972992
epoch£º374	 i:7 	 global-step:7487	 l-p:0.1450803577899933
epoch£º374	 i:8 	 global-step:7488	 l-p:0.13775216042995453
epoch£º374	 i:9 	 global-step:7489	 l-p:0.14943091571331024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2545, 3.2469, 3.2538],
        [3.2545, 3.2545, 3.2545],
        [3.2545, 3.2455, 3.2536],
        [3.2545, 3.2543, 3.2545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.1688978672027588 
model_pd.l_d.mean(): -23.32457733154297 
model_pd.lagr.mean(): -23.15567970275879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2471], device='cuda:0')), ('power', tensor([-23.5717], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.1688978672027588
epoch£º375	 i:1 	 global-step:7501	 l-p:0.15478460490703583
epoch£º375	 i:2 	 global-step:7502	 l-p:0.1427934616804123
epoch£º375	 i:3 	 global-step:7503	 l-p:0.1584460735321045
epoch£º375	 i:4 	 global-step:7504	 l-p:0.12686561048030853
epoch£º375	 i:5 	 global-step:7505	 l-p:0.16680428385734558
epoch£º375	 i:6 	 global-step:7506	 l-p:0.1310107707977295
epoch£º375	 i:7 	 global-step:7507	 l-p:0.12223044782876968
epoch£º375	 i:8 	 global-step:7508	 l-p:0.14736995100975037
epoch£º375	 i:9 	 global-step:7509	 l-p:0.11347825825214386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3043, 3.2246, 3.2612],
        [3.3043, 3.1163, 3.0011],
        [3.3043, 3.3040, 3.3043],
        [3.3043, 3.1460, 3.1285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.11449896544218063 
model_pd.l_d.mean(): -22.769840240478516 
model_pd.lagr.mean(): -22.65534210205078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3080], device='cuda:0')), ('power', tensor([-23.0778], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.11449896544218063
epoch£º376	 i:1 	 global-step:7521	 l-p:0.1503998041152954
epoch£º376	 i:2 	 global-step:7522	 l-p:0.13624542951583862
epoch£º376	 i:3 	 global-step:7523	 l-p:0.12569944560527802
epoch£º376	 i:4 	 global-step:7524	 l-p:0.1473851501941681
epoch£º376	 i:5 	 global-step:7525	 l-p:0.14035175740718842
epoch£º376	 i:6 	 global-step:7526	 l-p:0.12071208655834198
epoch£º376	 i:7 	 global-step:7527	 l-p:0.10323375463485718
epoch£º376	 i:8 	 global-step:7528	 l-p:0.13299153745174408
epoch£º376	 i:9 	 global-step:7529	 l-p:0.1581188589334488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3922, 3.2297, 3.1971],
        [3.3922, 3.3154, 3.3514],
        [3.3922, 3.3280, 3.3631],
        [3.3922, 3.5006, 3.3778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.1338808536529541 
model_pd.l_d.mean(): -23.20355224609375 
model_pd.lagr.mean(): -23.069671630859375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2190], device='cuda:0')), ('power', tensor([-23.4225], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.1338808536529541
epoch£º377	 i:1 	 global-step:7541	 l-p:0.14001494646072388
epoch£º377	 i:2 	 global-step:7542	 l-p:0.14706189930438995
epoch£º377	 i:3 	 global-step:7543	 l-p:0.14782005548477173
epoch£º377	 i:4 	 global-step:7544	 l-p:0.12506911158561707
epoch£º377	 i:5 	 global-step:7545	 l-p:0.13260547816753387
epoch£º377	 i:6 	 global-step:7546	 l-p:0.15994155406951904
epoch£º377	 i:7 	 global-step:7547	 l-p:0.14864148199558258
epoch£º377	 i:8 	 global-step:7548	 l-p:0.5845414400100708
epoch£º377	 i:9 	 global-step:7549	 l-p:0.14010773599147797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2667, 3.0698, 2.9819],
        [3.2667, 3.2667, 3.2667],
        [3.2667, 3.1509, 3.1819],
        [3.2667, 3.2017, 3.2386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.1545485258102417 
model_pd.l_d.mean(): -22.97270393371582 
model_pd.lagr.mean(): -22.81815528869629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3364], device='cuda:0')), ('power', tensor([-23.3091], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.1545485258102417
epoch£º378	 i:1 	 global-step:7561	 l-p:0.19025905430316925
epoch£º378	 i:2 	 global-step:7562	 l-p:0.14234884083271027
epoch£º378	 i:3 	 global-step:7563	 l-p:0.12108113616704941
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10600009560585022
epoch£º378	 i:5 	 global-step:7565	 l-p:0.1298435628414154
epoch£º378	 i:6 	 global-step:7566	 l-p:0.13315953314304352
epoch£º378	 i:7 	 global-step:7567	 l-p:0.12627865374088287
epoch£º378	 i:8 	 global-step:7568	 l-p:0.11792346835136414
epoch£º378	 i:9 	 global-step:7569	 l-p:0.13713817298412323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3541, 3.2004, 3.1973],
        [3.3541, 3.3541, 3.3541],
        [3.3541, 3.3347, 3.3508],
        [3.3541, 3.1821, 3.1499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.13832025229930878 
model_pd.l_d.mean(): -22.392179489135742 
model_pd.lagr.mean(): -22.25385856628418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3213], device='cuda:0')), ('power', tensor([-22.7134], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.13832025229930878
epoch£º379	 i:1 	 global-step:7581	 l-p:0.1321035772562027
epoch£º379	 i:2 	 global-step:7582	 l-p:0.15119193494319916
epoch£º379	 i:3 	 global-step:7583	 l-p:0.1485355794429779
epoch£º379	 i:4 	 global-step:7584	 l-p:0.17081622779369354
epoch£º379	 i:5 	 global-step:7585	 l-p:0.13021662831306458
epoch£º379	 i:6 	 global-step:7586	 l-p:0.16184568405151367
epoch£º379	 i:7 	 global-step:7587	 l-p:0.13008394837379456
epoch£º379	 i:8 	 global-step:7588	 l-p:0.1296566277742386
epoch£º379	 i:9 	 global-step:7589	 l-p:0.14057739078998566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3004, 3.1253, 2.9144],
        [3.3004, 3.2732, 3.2946],
        [3.3004, 3.2998, 3.3004],
        [3.3004, 3.2950, 3.3000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.1383257806301117 
model_pd.l_d.mean(): -23.270971298217773 
model_pd.lagr.mean(): -23.132644653320312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2548], device='cuda:0')), ('power', tensor([-23.5257], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.1383257806301117
epoch£º380	 i:1 	 global-step:7601	 l-p:0.16324597597122192
epoch£º380	 i:2 	 global-step:7602	 l-p:0.14629404246807098
epoch£º380	 i:3 	 global-step:7603	 l-p:0.14429983496665955
epoch£º380	 i:4 	 global-step:7604	 l-p:0.12824678421020508
epoch£º380	 i:5 	 global-step:7605	 l-p:0.1252279430627823
epoch£º380	 i:6 	 global-step:7606	 l-p:0.12943650782108307
epoch£º380	 i:7 	 global-step:7607	 l-p:0.1575126349925995
epoch£º380	 i:8 	 global-step:7608	 l-p:0.19886241853237152
epoch£º380	 i:9 	 global-step:7609	 l-p:0.14603066444396973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3313, 3.3311, 3.3313],
        [3.3313, 3.2906, 3.0894],
        [3.3313, 3.2524, 3.2920],
        [3.3313, 3.1819, 3.1927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.17423364520072937 
model_pd.l_d.mean(): -23.052701950073242 
model_pd.lagr.mean(): -22.878467559814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3178], device='cuda:0')), ('power', tensor([-23.3705], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.17423364520072937
epoch£º381	 i:1 	 global-step:7621	 l-p:0.1400594711303711
epoch£º381	 i:2 	 global-step:7622	 l-p:0.11751445382833481
epoch£º381	 i:3 	 global-step:7623	 l-p:0.1198754832148552
epoch£º381	 i:4 	 global-step:7624	 l-p:0.12862446904182434
epoch£º381	 i:5 	 global-step:7625	 l-p:0.12548108398914337
epoch£º381	 i:6 	 global-step:7626	 l-p:0.1285596489906311
epoch£º381	 i:7 	 global-step:7627	 l-p:0.11956167221069336
epoch£º381	 i:8 	 global-step:7628	 l-p:0.14922362565994263
epoch£º381	 i:9 	 global-step:7629	 l-p:0.1316811740398407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4466, 3.3332, 3.3655],
        [3.4466, 3.4462, 3.4466],
        [3.4466, 3.4453, 3.4466],
        [3.4466, 3.3034, 3.3154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.13016274571418762 
model_pd.l_d.mean(): -23.56886100769043 
model_pd.lagr.mean(): -23.438697814941406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1349], device='cuda:0')), ('power', tensor([-23.7038], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.13016274571418762
epoch£º382	 i:1 	 global-step:7641	 l-p:0.11957599967718124
epoch£º382	 i:2 	 global-step:7642	 l-p:0.14747275412082672
epoch£º382	 i:3 	 global-step:7643	 l-p:0.1382295936346054
epoch£º382	 i:4 	 global-step:7644	 l-p:0.14820751547813416
epoch£º382	 i:5 	 global-step:7645	 l-p:0.12669497728347778
epoch£º382	 i:6 	 global-step:7646	 l-p:0.20050792396068573
epoch£º382	 i:7 	 global-step:7647	 l-p:0.170231431722641
epoch£º382	 i:8 	 global-step:7648	 l-p:0.17544181644916534
epoch£º382	 i:9 	 global-step:7649	 l-p:0.04901746287941933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2748, 3.2689, 3.2744],
        [3.2748, 3.0637, 2.9928],
        [3.2748, 3.2748, 3.2748],
        [3.2748, 3.1918, 3.2337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.13634252548217773 
model_pd.l_d.mean(): -23.173683166503906 
model_pd.lagr.mean(): -23.03734016418457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2889], device='cuda:0')), ('power', tensor([-23.4626], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.13634252548217773
epoch£º383	 i:1 	 global-step:7661	 l-p:0.16539554297924042
epoch£º383	 i:2 	 global-step:7662	 l-p:0.13218289613723755
epoch£º383	 i:3 	 global-step:7663	 l-p:0.12884923815727234
epoch£º383	 i:4 	 global-step:7664	 l-p:0.137846902012825
epoch£º383	 i:5 	 global-step:7665	 l-p:0.132302924990654
epoch£º383	 i:6 	 global-step:7666	 l-p:0.13034768402576447
epoch£º383	 i:7 	 global-step:7667	 l-p:0.1360977292060852
epoch£º383	 i:8 	 global-step:7668	 l-p:0.12010852247476578
epoch£º383	 i:9 	 global-step:7669	 l-p:0.13839808106422424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4142, 3.4115, 3.2182],
        [3.4142, 3.2416, 3.2288],
        [3.4142, 3.2664, 3.0383],
        [3.4142, 3.3592, 3.3944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.1520577073097229 
model_pd.l_d.mean(): -23.549291610717773 
model_pd.lagr.mean(): -23.397233963012695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1933], device='cuda:0')), ('power', tensor([-23.7426], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.1520577073097229
epoch£º384	 i:1 	 global-step:7681	 l-p:0.13553109765052795
epoch£º384	 i:2 	 global-step:7682	 l-p:0.12746398150920868
epoch£º384	 i:3 	 global-step:7683	 l-p:0.1447187066078186
epoch£º384	 i:4 	 global-step:7684	 l-p:0.13738949596881866
epoch£º384	 i:5 	 global-step:7685	 l-p:0.14780907332897186
epoch£º384	 i:6 	 global-step:7686	 l-p:0.15434174239635468
epoch£º384	 i:7 	 global-step:7687	 l-p:0.17741847038269043
epoch£º384	 i:8 	 global-step:7688	 l-p:0.0942140594124794
epoch£º384	 i:9 	 global-step:7689	 l-p:0.13257482647895813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4578, 3.2544, 3.1720],
        [3.4578, 3.4539, 3.4576],
        [3.4578, 3.4578, 3.4578],
        [3.4578, 3.4396, 3.2376]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.12071149796247482 
model_pd.l_d.mean(): -23.167512893676758 
model_pd.lagr.mean(): -23.04680061340332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2267], device='cuda:0')), ('power', tensor([-23.3943], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.12071149796247482
epoch£º385	 i:1 	 global-step:7701	 l-p:0.11265622079372406
epoch£º385	 i:2 	 global-step:7702	 l-p:0.13335824012756348
epoch£º385	 i:3 	 global-step:7703	 l-p:0.13604940474033356
epoch£º385	 i:4 	 global-step:7704	 l-p:0.14635254442691803
epoch£º385	 i:5 	 global-step:7705	 l-p:0.1326458752155304
epoch£º385	 i:6 	 global-step:7706	 l-p:0.16085584461688995
epoch£º385	 i:7 	 global-step:7707	 l-p:0.14331911504268646
epoch£º385	 i:8 	 global-step:7708	 l-p:-0.011220169253647327
epoch£º385	 i:9 	 global-step:7709	 l-p:0.14178398251533508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3083, 3.2146, 3.2584],
        [3.3083, 3.1157, 2.8770],
        [3.3083, 3.3083, 3.3083],
        [3.3083, 3.2384, 3.2793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.13263024389743805 
model_pd.l_d.mean(): -23.12388801574707 
model_pd.lagr.mean(): -22.99125862121582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3155], device='cuda:0')), ('power', tensor([-23.4394], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.13263024389743805
epoch£º386	 i:1 	 global-step:7721	 l-p:0.1549532115459442
epoch£º386	 i:2 	 global-step:7722	 l-p:0.14100714027881622
epoch£º386	 i:3 	 global-step:7723	 l-p:0.26080837845802307
epoch£º386	 i:4 	 global-step:7724	 l-p:0.15044759213924408
epoch£º386	 i:5 	 global-step:7725	 l-p:0.1574670523405075
epoch£º386	 i:6 	 global-step:7726	 l-p:0.11920200288295746
epoch£º386	 i:7 	 global-step:7727	 l-p:0.13204210996627808
epoch£º386	 i:8 	 global-step:7728	 l-p:0.16509976983070374
epoch£º386	 i:9 	 global-step:7729	 l-p:0.12731654942035675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3594, 3.3054, 3.3413],
        [3.3594, 3.2710, 3.0385],
        [3.3594, 3.3584, 3.3594],
        [3.3594, 3.2629, 3.3065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.14132241904735565 
model_pd.l_d.mean(): -23.516281127929688 
model_pd.lagr.mean(): -23.374958038330078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2190], device='cuda:0')), ('power', tensor([-23.7353], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.14132241904735565
epoch£º387	 i:1 	 global-step:7741	 l-p:0.13465379178524017
epoch£º387	 i:2 	 global-step:7742	 l-p:0.1309642642736435
epoch£º387	 i:3 	 global-step:7743	 l-p:0.1623203009366989
epoch£º387	 i:4 	 global-step:7744	 l-p:0.12518678605556488
epoch£º387	 i:5 	 global-step:7745	 l-p:0.13917963206768036
epoch£º387	 i:6 	 global-step:7746	 l-p:0.11684868484735489
epoch£º387	 i:7 	 global-step:7747	 l-p:0.13386216759681702
epoch£º387	 i:8 	 global-step:7748	 l-p:0.14112281799316406
epoch£º387	 i:9 	 global-step:7749	 l-p:0.11828584223985672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3775, 3.3713, 3.3770],
        [3.3775, 3.3217, 3.3584],
        [3.3775, 3.3700, 3.3769],
        [3.3775, 3.1616, 3.0983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.10580404847860336 
model_pd.l_d.mean(): -22.64061164855957 
model_pd.lagr.mean(): -22.534807205200195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3655], device='cuda:0')), ('power', tensor([-23.0061], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.10580404847860336
epoch£º388	 i:1 	 global-step:7761	 l-p:0.1409170925617218
epoch£º388	 i:2 	 global-step:7762	 l-p:0.13925504684448242
epoch£º388	 i:3 	 global-step:7763	 l-p:0.13234953582286835
epoch£º388	 i:4 	 global-step:7764	 l-p:0.11787664145231247
epoch£º388	 i:5 	 global-step:7765	 l-p:0.1302434355020523
epoch£º388	 i:6 	 global-step:7766	 l-p:0.13549716770648956
epoch£º388	 i:7 	 global-step:7767	 l-p:0.13383255898952484
epoch£º388	 i:8 	 global-step:7768	 l-p:0.17094771564006805
epoch£º388	 i:9 	 global-step:7769	 l-p:0.13201572000980377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3178, 3.0696, 2.8868],
        [3.3178, 3.1002, 3.0537],
        [3.3178, 3.3174, 3.3178],
        [3.3178, 3.2226, 3.2683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.1532820612192154 
model_pd.l_d.mean(): -22.612031936645508 
model_pd.lagr.mean(): -22.458749771118164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3310], device='cuda:0')), ('power', tensor([-22.9430], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.1532820612192154
epoch£º389	 i:1 	 global-step:7781	 l-p:0.113126240670681
epoch£º389	 i:2 	 global-step:7782	 l-p:0.1478012651205063
epoch£º389	 i:3 	 global-step:7783	 l-p:0.13156338036060333
epoch£º389	 i:4 	 global-step:7784	 l-p:0.13508857786655426
epoch£º389	 i:5 	 global-step:7785	 l-p:0.13259318470954895
epoch£º389	 i:6 	 global-step:7786	 l-p:0.6201269626617432
epoch£º389	 i:7 	 global-step:7787	 l-p:0.16668067872524261
epoch£º389	 i:8 	 global-step:7788	 l-p:0.1497146040201187
epoch£º389	 i:9 	 global-step:7789	 l-p:0.16266313195228577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3656, 3.3652, 3.3656],
        [3.3656, 3.1976, 3.2150],
        [3.3656, 3.3229, 3.0989],
        [3.3656, 3.3652, 3.3656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.16330379247665405 
model_pd.l_d.mean(): -23.497949600219727 
model_pd.lagr.mean(): -23.334646224975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2170], device='cuda:0')), ('power', tensor([-23.7149], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.16330379247665405
epoch£º390	 i:1 	 global-step:7801	 l-p:0.15867263078689575
epoch£º390	 i:2 	 global-step:7802	 l-p:0.12019410729408264
epoch£º390	 i:3 	 global-step:7803	 l-p:0.12176815420389175
epoch£º390	 i:4 	 global-step:7804	 l-p:0.10728926211595535
epoch£º390	 i:5 	 global-step:7805	 l-p:0.10064774006605148
epoch£º390	 i:6 	 global-step:7806	 l-p:0.11635202169418335
epoch£º390	 i:7 	 global-step:7807	 l-p:0.13120627403259277
epoch£º390	 i:8 	 global-step:7808	 l-p:0.13879813253879547
epoch£º390	 i:9 	 global-step:7809	 l-p:0.12330684065818787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4497, 3.4325, 3.4473],
        [3.4497, 3.2211, 3.1346],
        [3.4497, 3.2361, 3.1876],
        [3.4497, 3.4419, 3.4490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.13986532390117645 
model_pd.l_d.mean(): -23.50916290283203 
model_pd.lagr.mean(): -23.36929702758789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1727], device='cuda:0')), ('power', tensor([-23.6819], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.13986532390117645
epoch£º391	 i:1 	 global-step:7821	 l-p:0.15581713616847992
epoch£º391	 i:2 	 global-step:7822	 l-p:0.13535942137241364
epoch£º391	 i:3 	 global-step:7823	 l-p:0.12860120832920074
epoch£º391	 i:4 	 global-step:7824	 l-p:0.09086091071367264
epoch£º391	 i:5 	 global-step:7825	 l-p:0.401488333940506
epoch£º391	 i:6 	 global-step:7826	 l-p:0.19271816313266754
epoch£º391	 i:7 	 global-step:7827	 l-p:0.12393704056739807
epoch£º391	 i:8 	 global-step:7828	 l-p:0.13106244802474976
epoch£º391	 i:9 	 global-step:7829	 l-p:0.3981660008430481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2301, 3.1379, 3.1863],
        [3.2301, 3.2296, 3.2301],
        [3.2301, 3.2880, 3.1045],
        [3.2301, 3.2301, 3.2301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.19659247994422913 
model_pd.l_d.mean(): -22.965229034423828 
model_pd.lagr.mean(): -22.76863670349121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2988], device='cuda:0')), ('power', tensor([-23.2640], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.19659247994422913
epoch£º392	 i:1 	 global-step:7841	 l-p:0.15352806448936462
epoch£º392	 i:2 	 global-step:7842	 l-p:0.1446996033191681
epoch£º392	 i:3 	 global-step:7843	 l-p:0.16009357571601868
epoch£º392	 i:4 	 global-step:7844	 l-p:0.14735111594200134
epoch£º392	 i:5 	 global-step:7845	 l-p:0.11242103576660156
epoch£º392	 i:6 	 global-step:7846	 l-p:0.1332649290561676
epoch£º392	 i:7 	 global-step:7847	 l-p:0.1336951106786728
epoch£º392	 i:8 	 global-step:7848	 l-p:0.16315630078315735
epoch£º392	 i:9 	 global-step:7849	 l-p:0.11537053436040878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4026, 3.4113, 3.2028],
        [3.4026, 3.4026, 3.4026],
        [3.4026, 3.1915, 2.9342],
        [3.4026, 3.4026, 3.4026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.11343694478273392 
model_pd.l_d.mean(): -23.26140022277832 
model_pd.lagr.mean(): -23.14796257019043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2620], device='cuda:0')), ('power', tensor([-23.5234], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.11343694478273392
epoch£º393	 i:1 	 global-step:7861	 l-p:0.1529105305671692
epoch£º393	 i:2 	 global-step:7862	 l-p:0.15267136693000793
epoch£º393	 i:3 	 global-step:7863	 l-p:0.12244260311126709
epoch£º393	 i:4 	 global-step:7864	 l-p:0.12151685357093811
epoch£º393	 i:5 	 global-step:7865	 l-p:0.1339007169008255
epoch£º393	 i:6 	 global-step:7866	 l-p:0.1597493290901184
epoch£º393	 i:7 	 global-step:7867	 l-p:0.14886528253555298
epoch£º393	 i:8 	 global-step:7868	 l-p:0.1407635509967804
epoch£º393	 i:9 	 global-step:7869	 l-p:0.2135709673166275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3806, 3.2828, 3.3307],
        [3.3806, 3.3798, 3.3806],
        [3.3806, 3.1587, 3.1226],
        [3.3806, 3.4245, 3.2305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.13733676075935364 
model_pd.l_d.mean(): -23.43922233581543 
model_pd.lagr.mean(): -23.3018856048584 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2191], device='cuda:0')), ('power', tensor([-23.6583], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.13733676075935364
epoch£º394	 i:1 	 global-step:7881	 l-p:0.11637475341558456
epoch£º394	 i:2 	 global-step:7882	 l-p:0.17816458642482758
epoch£º394	 i:3 	 global-step:7883	 l-p:0.15456512570381165
epoch£º394	 i:4 	 global-step:7884	 l-p:0.13996058702468872
epoch£º394	 i:5 	 global-step:7885	 l-p:0.14170925319194794
epoch£º394	 i:6 	 global-step:7886	 l-p:0.13677869737148285
epoch£º394	 i:7 	 global-step:7887	 l-p:0.13175712525844574
epoch£º394	 i:8 	 global-step:7888	 l-p:0.11147569119930267
epoch£º394	 i:9 	 global-step:7889	 l-p:0.13811829686164856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4371, 3.4339, 3.4369],
        [3.4371, 3.3063, 3.3501],
        [3.4371, 3.4371, 3.4371],
        [3.4371, 3.3409, 3.3886]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.1059565618634224 
model_pd.l_d.mean(): -23.10060691833496 
model_pd.lagr.mean(): -22.99464988708496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2563], device='cuda:0')), ('power', tensor([-23.3569], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.1059565618634224
epoch£º395	 i:1 	 global-step:7901	 l-p:0.12098774313926697
epoch£º395	 i:2 	 global-step:7902	 l-p:0.138649120926857
epoch£º395	 i:3 	 global-step:7903	 l-p:0.1482834368944168
epoch£º395	 i:4 	 global-step:7904	 l-p:0.14233218133449554
epoch£º395	 i:5 	 global-step:7905	 l-p:0.18189655244350433
epoch£º395	 i:6 	 global-step:7906	 l-p:0.23173308372497559
epoch£º395	 i:7 	 global-step:7907	 l-p:0.11291816830635071
epoch£º395	 i:8 	 global-step:7908	 l-p:0.1117037758231163
epoch£º395	 i:9 	 global-step:7909	 l-p:0.13528499007225037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3109, 3.0635, 2.7919],
        [3.3109, 3.0884, 2.8087],
        [3.3109, 3.2694, 3.3007],
        [3.3109, 3.2862, 3.3068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.14305076003074646 
model_pd.l_d.mean(): -23.771318435668945 
model_pd.lagr.mean(): -23.628267288208008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1883], device='cuda:0')), ('power', tensor([-23.9596], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.14305076003074646
epoch£º396	 i:1 	 global-step:7921	 l-p:0.15601901710033417
epoch£º396	 i:2 	 global-step:7922	 l-p:0.12247388809919357
epoch£º396	 i:3 	 global-step:7923	 l-p:0.24661152064800262
epoch£º396	 i:4 	 global-step:7924	 l-p:0.12685003876686096
epoch£º396	 i:5 	 global-step:7925	 l-p:0.13749271631240845
epoch£º396	 i:6 	 global-step:7926	 l-p:0.1420162320137024
epoch£º396	 i:7 	 global-step:7927	 l-p:0.13753123581409454
epoch£º396	 i:8 	 global-step:7928	 l-p:0.1271514743566513
epoch£º396	 i:9 	 global-step:7929	 l-p:0.13250812888145447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4193, 3.4572, 3.2521],
        [3.4193, 3.4079, 3.4182],
        [3.4193, 3.2925, 3.3402],
        [3.4193, 3.3042, 3.3535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.10151154547929764 
model_pd.l_d.mean(): -22.551048278808594 
model_pd.lagr.mean(): -22.44953727722168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3518], device='cuda:0')), ('power', tensor([-22.9028], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.10151154547929764
epoch£º397	 i:1 	 global-step:7941	 l-p:0.13179533183574677
epoch£º397	 i:2 	 global-step:7942	 l-p:0.14367219805717468
epoch£º397	 i:3 	 global-step:7943	 l-p:0.1491689383983612
epoch£º397	 i:4 	 global-step:7944	 l-p:0.16940517723560333
epoch£º397	 i:5 	 global-step:7945	 l-p:0.14717276394367218
epoch£º397	 i:6 	 global-step:7946	 l-p:0.11019483953714371
epoch£º397	 i:7 	 global-step:7947	 l-p:0.14027591049671173
epoch£º397	 i:8 	 global-step:7948	 l-p:0.13569562137126923
epoch£º397	 i:9 	 global-step:7949	 l-p:0.15341392159461975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3406, 3.0993, 2.8167],
        [3.3406, 3.3406, 3.3407],
        [3.3406, 3.3373, 3.3405],
        [3.3406, 3.0743, 2.8071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.1562562882900238 
model_pd.l_d.mean(): -22.5996036529541 
model_pd.lagr.mean(): -22.443347930908203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3417], device='cuda:0')), ('power', tensor([-22.9413], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.1562562882900238
epoch£º398	 i:1 	 global-step:7961	 l-p:0.11644411832094193
epoch£º398	 i:2 	 global-step:7962	 l-p:0.16270539164543152
epoch£º398	 i:3 	 global-step:7963	 l-p:0.14771170914173126
epoch£º398	 i:4 	 global-step:7964	 l-p:0.15365946292877197
epoch£º398	 i:5 	 global-step:7965	 l-p:0.12649330496788025
epoch£º398	 i:6 	 global-step:7966	 l-p:0.13638809323310852
epoch£º398	 i:7 	 global-step:7967	 l-p:0.13863082230091095
epoch£º398	 i:8 	 global-step:7968	 l-p:0.13649164140224457
epoch£º398	 i:9 	 global-step:7969	 l-p:0.0628354474902153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2977, 3.2967, 3.2976],
        [3.2977, 3.1729, 3.2252],
        [3.2977, 2.9998, 2.7698],
        [3.2977, 3.0324, 2.9632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.1260972023010254 
model_pd.l_d.mean(): -23.101892471313477 
model_pd.lagr.mean(): -22.97579574584961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3108], device='cuda:0')), ('power', tensor([-23.4127], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.1260972023010254
epoch£º399	 i:1 	 global-step:7981	 l-p:0.19835591316223145
epoch£º399	 i:2 	 global-step:7982	 l-p:0.103797048330307
epoch£º399	 i:3 	 global-step:7983	 l-p:0.1992710828781128
epoch£º399	 i:4 	 global-step:7984	 l-p:0.13182204961776733
epoch£º399	 i:5 	 global-step:7985	 l-p:0.13782986998558044
epoch£º399	 i:6 	 global-step:7986	 l-p:0.11934869736433029
epoch£º399	 i:7 	 global-step:7987	 l-p:0.12017663568258286
epoch£º399	 i:8 	 global-step:7988	 l-p:0.15893785655498505
epoch£º399	 i:9 	 global-step:7989	 l-p:0.14731158316135406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3804, 3.0955, 2.9573],
        [3.3804, 3.1010, 2.8430],
        [3.3804, 3.3441, 3.3725],
        [3.3804, 3.1002, 2.9797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.13889610767364502 
model_pd.l_d.mean(): -22.81806755065918 
model_pd.lagr.mean(): -22.679170608520508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2698], device='cuda:0')), ('power', tensor([-23.0879], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.13889610767364502
epoch£º400	 i:1 	 global-step:8001	 l-p:0.14803197979927063
epoch£º400	 i:2 	 global-step:8002	 l-p:0.15493004024028778
epoch£º400	 i:3 	 global-step:8003	 l-p:0.13193471729755402
epoch£º400	 i:4 	 global-step:8004	 l-p:0.13201814889907837
epoch£º400	 i:5 	 global-step:8005	 l-p:0.4161635637283325
epoch£º400	 i:6 	 global-step:8006	 l-p:0.12943951785564423
epoch£º400	 i:7 	 global-step:8007	 l-p:0.14036692678928375
epoch£º400	 i:8 	 global-step:8008	 l-p:0.1523667424917221
epoch£º400	 i:9 	 global-step:8009	 l-p:0.13123750686645508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3895, 3.1236, 2.8468],
        [3.3895, 3.3891, 3.3895],
        [3.3895, 3.1070, 2.8513],
        [3.3895, 3.1852, 2.8914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.13325953483581543 
model_pd.l_d.mean(): -22.712018966674805 
model_pd.lagr.mean(): -22.578760147094727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3724], device='cuda:0')), ('power', tensor([-23.0844], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.13325953483581543
epoch£º401	 i:1 	 global-step:8021	 l-p:0.14434663951396942
epoch£º401	 i:2 	 global-step:8022	 l-p:0.1356400102376938
epoch£º401	 i:3 	 global-step:8023	 l-p:0.13189539313316345
epoch£º401	 i:4 	 global-step:8024	 l-p:0.16359791159629822
epoch£º401	 i:5 	 global-step:8025	 l-p:-0.0962366834282875
epoch£º401	 i:6 	 global-step:8026	 l-p:0.14884158968925476
epoch£º401	 i:7 	 global-step:8027	 l-p:0.11661031097173691
epoch£º401	 i:8 	 global-step:8028	 l-p:0.1890777200460434
epoch£º401	 i:9 	 global-step:8029	 l-p:0.1483079046010971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3400, 3.3400, 3.3400],
        [3.3400, 3.3390, 3.3399],
        [3.3400, 3.2119, 3.2648],
        [3.3400, 3.3400, 3.3400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.12910319864749908 
model_pd.l_d.mean(): -23.559314727783203 
model_pd.lagr.mean(): -23.430212020874023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2026], device='cuda:0')), ('power', tensor([-23.7619], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.12910319864749908
epoch£º402	 i:1 	 global-step:8041	 l-p:0.13929685950279236
epoch£º402	 i:2 	 global-step:8042	 l-p:0.1337129920721054
epoch£º402	 i:3 	 global-step:8043	 l-p:0.15151047706604004
epoch£º402	 i:4 	 global-step:8044	 l-p:0.3222793638706207
epoch£º402	 i:5 	 global-step:8045	 l-p:0.1489698588848114
epoch£º402	 i:6 	 global-step:8046	 l-p:0.13583484292030334
epoch£º402	 i:7 	 global-step:8047	 l-p:0.12736782431602478
epoch£º402	 i:8 	 global-step:8048	 l-p:0.16546262800693512
epoch£º402	 i:9 	 global-step:8049	 l-p:0.14067505300045013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3975, 3.2922, 3.3449],
        [3.3975, 3.3975, 3.3975],
        [3.3975, 3.3017, 3.3533],
        [3.3975, 3.3622, 3.3901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.13575108349323273 
model_pd.l_d.mean(): -23.396883010864258 
model_pd.lagr.mean(): -23.261131286621094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2047], device='cuda:0')), ('power', tensor([-23.6016], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.13575108349323273
epoch£º403	 i:1 	 global-step:8061	 l-p:0.21003404259681702
epoch£º403	 i:2 	 global-step:8062	 l-p:0.12698499858379364
epoch£º403	 i:3 	 global-step:8063	 l-p:0.1256042867898941
epoch£º403	 i:4 	 global-step:8064	 l-p:0.14892145991325378
epoch£º403	 i:5 	 global-step:8065	 l-p:0.13760386407375336
epoch£º403	 i:6 	 global-step:8066	 l-p:0.11978865414857864
epoch£º403	 i:7 	 global-step:8067	 l-p:0.1794518232345581
epoch£º403	 i:8 	 global-step:8068	 l-p:0.11895820498466492
epoch£º403	 i:9 	 global-step:8069	 l-p:0.1566213220357895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3816, 3.0913, 2.8319],
        [3.3816, 3.2466, 2.9609],
        [3.3816, 3.3115, 3.3568],
        [3.3816, 3.3710, 3.3806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.14933590590953827 
model_pd.l_d.mean(): -22.506074905395508 
model_pd.lagr.mean(): -22.356739044189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3463], device='cuda:0')), ('power', tensor([-22.8524], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.14933590590953827
epoch£º404	 i:1 	 global-step:8081	 l-p:0.11695069819688797
epoch£º404	 i:2 	 global-step:8082	 l-p:0.15040922164916992
epoch£º404	 i:3 	 global-step:8083	 l-p:0.1295837014913559
epoch£º404	 i:4 	 global-step:8084	 l-p:0.12276769429445267
epoch£º404	 i:5 	 global-step:8085	 l-p:0.2224436104297638
epoch£º404	 i:6 	 global-step:8086	 l-p:0.1283787190914154
epoch£º404	 i:7 	 global-step:8087	 l-p:0.12530197203159332
epoch£º404	 i:8 	 global-step:8088	 l-p:0.15027198195457458
epoch£º404	 i:9 	 global-step:8089	 l-p:0.14094914495944977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3842, 3.0966, 2.9795],
        [3.3842, 3.3842, 3.3842],
        [3.3842, 3.3842, 3.3842],
        [3.3842, 3.1573, 3.1520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.20413058996200562 
model_pd.l_d.mean(): -23.320880889892578 
model_pd.lagr.mean(): -23.116750717163086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2572], device='cuda:0')), ('power', tensor([-23.5781], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.20413058996200562
epoch£º405	 i:1 	 global-step:8101	 l-p:0.15055686235427856
epoch£º405	 i:2 	 global-step:8102	 l-p:0.10490196943283081
epoch£º405	 i:3 	 global-step:8103	 l-p:0.14876610040664673
epoch£º405	 i:4 	 global-step:8104	 l-p:0.146727055311203
epoch£º405	 i:5 	 global-step:8105	 l-p:0.15925920009613037
epoch£º405	 i:6 	 global-step:8106	 l-p:0.16165320575237274
epoch£º405	 i:7 	 global-step:8107	 l-p:0.15750135481357574
epoch£º405	 i:8 	 global-step:8108	 l-p:0.13675649464130402
epoch£º405	 i:9 	 global-step:8109	 l-p:0.14036692678928375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3351, 3.2031, 3.2571],
        [3.3351, 3.2907, 3.3242],
        [3.3351, 3.1054, 3.1030],
        [3.3351, 3.0428, 2.7614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.14908553659915924 
model_pd.l_d.mean(): -22.86121940612793 
model_pd.lagr.mean(): -22.712133407592773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3189], device='cuda:0')), ('power', tensor([-23.1801], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.14908553659915924
epoch£º406	 i:1 	 global-step:8121	 l-p:-0.04022232070565224
epoch£º406	 i:2 	 global-step:8122	 l-p:0.1377311497926712
epoch£º406	 i:3 	 global-step:8123	 l-p:0.13671629130840302
epoch£º406	 i:4 	 global-step:8124	 l-p:0.14693978428840637
epoch£º406	 i:5 	 global-step:8125	 l-p:0.17712955176830292
epoch£º406	 i:6 	 global-step:8126	 l-p:0.16368061304092407
epoch£º406	 i:7 	 global-step:8127	 l-p:0.1493285745382309
epoch£º406	 i:8 	 global-step:8128	 l-p:0.13087217509746552
epoch£º406	 i:9 	 global-step:8129	 l-p:0.13076341152191162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4032, 3.3899, 3.4018],
        [3.4032, 3.3913, 3.4020],
        [3.4032, 3.1025, 2.8588],
        [3.4032, 3.1104, 2.9806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.1977226585149765 
model_pd.l_d.mean(): -23.499788284301758 
model_pd.lagr.mean(): -23.302064895629883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2045], device='cuda:0')), ('power', tensor([-23.7043], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.1977226585149765
epoch£º407	 i:1 	 global-step:8141	 l-p:0.11717378348112106
epoch£º407	 i:2 	 global-step:8142	 l-p:0.13135109841823578
epoch£º407	 i:3 	 global-step:8143	 l-p:0.13004399836063385
epoch£º407	 i:4 	 global-step:8144	 l-p:0.13779118657112122
epoch£º407	 i:5 	 global-step:8145	 l-p:0.13920216262340546
epoch£º407	 i:6 	 global-step:8146	 l-p:0.13528072834014893
epoch£º407	 i:7 	 global-step:8147	 l-p:0.1287381798028946
epoch£º407	 i:8 	 global-step:8148	 l-p:0.11718951910734177
epoch£º407	 i:9 	 global-step:8149	 l-p:0.11984193325042725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4661, 3.3909, 3.4380],
        [3.4661, 3.4010, 3.4445],
        [3.4661, 3.2404, 3.2341],
        [3.4661, 3.1696, 2.9411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.14826218783855438 
model_pd.l_d.mean(): -23.48160743713379 
model_pd.lagr.mean(): -23.333345413208008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2101], device='cuda:0')), ('power', tensor([-23.6917], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.14826218783855438
epoch£º408	 i:1 	 global-step:8161	 l-p:0.13870269060134888
epoch£º408	 i:2 	 global-step:8162	 l-p:0.13011960685253143
epoch£º408	 i:3 	 global-step:8163	 l-p:0.13761460781097412
epoch£º408	 i:4 	 global-step:8164	 l-p:0.16161420941352844
epoch£º408	 i:5 	 global-step:8165	 l-p:0.167628213763237
epoch£º408	 i:6 	 global-step:8166	 l-p:0.13301552832126617
epoch£º408	 i:7 	 global-step:8167	 l-p:0.13395002484321594
epoch£º408	 i:8 	 global-step:8168	 l-p:0.14492855966091156
epoch£º408	 i:9 	 global-step:8169	 l-p:0.08892301470041275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2967, 3.2474, 3.2840],
        [3.2967, 3.2418, 3.2813],
        [3.2967, 3.2930, 3.2966],
        [3.2967, 3.2941, 3.2966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.14562098681926727 
model_pd.l_d.mean(): -23.517780303955078 
model_pd.lagr.mean(): -23.372159957885742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2367], device='cuda:0')), ('power', tensor([-23.7544], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.14562098681926727
epoch£º409	 i:1 	 global-step:8181	 l-p:0.18060782551765442
epoch£º409	 i:2 	 global-step:8182	 l-p:0.2044118195772171
epoch£º409	 i:3 	 global-step:8183	 l-p:0.11544790118932724
epoch£º409	 i:4 	 global-step:8184	 l-p:0.15388435125350952
epoch£º409	 i:5 	 global-step:8185	 l-p:0.1340826153755188
epoch£º409	 i:6 	 global-step:8186	 l-p:0.1592789739370346
epoch£º409	 i:7 	 global-step:8187	 l-p:0.1371123045682907
epoch£º409	 i:8 	 global-step:8188	 l-p:0.14747540652751923
epoch£º409	 i:9 	 global-step:8189	 l-p:0.1170058622956276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4140, 3.3361, 3.3846],
        [3.4140, 3.2636, 3.3143],
        [3.4140, 3.4012, 3.4127],
        [3.4140, 3.4140, 3.4140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.13964912295341492 
model_pd.l_d.mean(): -23.392852783203125 
model_pd.lagr.mean(): -23.253204345703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2342], device='cuda:0')), ('power', tensor([-23.6271], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.13964912295341492
epoch£º410	 i:1 	 global-step:8201	 l-p:0.13342659175395966
epoch£º410	 i:2 	 global-step:8202	 l-p:0.1743340641260147
epoch£º410	 i:3 	 global-step:8203	 l-p:0.1323447823524475
epoch£º410	 i:4 	 global-step:8204	 l-p:0.1419868767261505
epoch£º410	 i:5 	 global-step:8205	 l-p:0.1272810399532318
epoch£º410	 i:6 	 global-step:8206	 l-p:0.15087640285491943
epoch£º410	 i:7 	 global-step:8207	 l-p:0.12845049798488617
epoch£º410	 i:8 	 global-step:8208	 l-p:0.14417071640491486
epoch£º410	 i:9 	 global-step:8209	 l-p:0.17582187056541443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3809, 3.3809, 3.3809],
        [3.3809, 3.3795, 3.3809],
        [3.3809, 3.3806, 3.3809],
        [3.3809, 3.3784, 3.3808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.32504433393478394 
model_pd.l_d.mean(): -23.298398971557617 
model_pd.lagr.mean(): -22.97335433959961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2571], device='cuda:0')), ('power', tensor([-23.5555], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.32504433393478394
epoch£º411	 i:1 	 global-step:8221	 l-p:0.1378326267004013
epoch£º411	 i:2 	 global-step:8222	 l-p:0.1425115317106247
epoch£º411	 i:3 	 global-step:8223	 l-p:0.15323753654956818
epoch£º411	 i:4 	 global-step:8224	 l-p:0.13475705683231354
epoch£º411	 i:5 	 global-step:8225	 l-p:0.13181941211223602
epoch£º411	 i:6 	 global-step:8226	 l-p:0.14296108484268188
epoch£º411	 i:7 	 global-step:8227	 l-p:0.1698276847600937
epoch£º411	 i:8 	 global-step:8228	 l-p:0.12950295209884644
epoch£º411	 i:9 	 global-step:8229	 l-p:0.14206811785697937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3888, 3.3762, 3.3875],
        [3.3888, 3.4352, 3.2122],
        [3.3888, 3.3888, 3.3888],
        [3.3888, 3.2387, 3.2909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.14571036398410797 
model_pd.l_d.mean(): -22.515256881713867 
model_pd.lagr.mean(): -22.36954689025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3137], device='cuda:0')), ('power', tensor([-22.8289], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.14571036398410797
epoch£º412	 i:1 	 global-step:8241	 l-p:0.14402852952480316
epoch£º412	 i:2 	 global-step:8242	 l-p:0.12814399600028992
epoch£º412	 i:3 	 global-step:8243	 l-p:0.13472817838191986
epoch£º412	 i:4 	 global-step:8244	 l-p:0.14425553381443024
epoch£º412	 i:5 	 global-step:8245	 l-p:0.13903477787971497
epoch£º412	 i:6 	 global-step:8246	 l-p:0.14698582887649536
epoch£º412	 i:7 	 global-step:8247	 l-p:0.7368387579917908
epoch£º412	 i:8 	 global-step:8248	 l-p:0.1425306499004364
epoch£º412	 i:9 	 global-step:8249	 l-p:0.15658995509147644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3761, 3.2605, 3.3167],
        [3.3761, 3.3761, 3.3761],
        [3.3761, 3.3788, 3.1339],
        [3.3761, 3.3761, 3.3761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.1365993618965149 
model_pd.l_d.mean(): -23.010616302490234 
model_pd.lagr.mean(): -22.8740177154541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2876], device='cuda:0')), ('power', tensor([-23.2982], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.1365993618965149
epoch£º413	 i:1 	 global-step:8261	 l-p:0.1325385570526123
epoch£º413	 i:2 	 global-step:8262	 l-p:0.13091705739498138
epoch£º413	 i:3 	 global-step:8263	 l-p:0.14978232979774475
epoch£º413	 i:4 	 global-step:8264	 l-p:0.11313862353563309
epoch£º413	 i:5 	 global-step:8265	 l-p:0.13074317574501038
epoch£º413	 i:6 	 global-step:8266	 l-p:0.14846014976501465
epoch£º413	 i:7 	 global-step:8267	 l-p:0.21823841333389282
epoch£º413	 i:8 	 global-step:8268	 l-p:0.18931858241558075
epoch£º413	 i:9 	 global-step:8269	 l-p:0.12445253878831863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4314, 3.4238, 3.4309],
        [3.4314, 3.1346, 2.8495],
        [3.4314, 3.4314, 3.4314],
        [3.4314, 3.3769, 3.4161]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.14595358073711395 
model_pd.l_d.mean(): -22.753110885620117 
model_pd.lagr.mean(): -22.60715675354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2809], device='cuda:0')), ('power', tensor([-23.0340], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.14595358073711395
epoch£º414	 i:1 	 global-step:8281	 l-p:0.13161715865135193
epoch£º414	 i:2 	 global-step:8282	 l-p:0.15127795934677124
epoch£º414	 i:3 	 global-step:8283	 l-p:0.17673972249031067
epoch£º414	 i:4 	 global-step:8284	 l-p:0.12369944155216217
epoch£º414	 i:5 	 global-step:8285	 l-p:0.11955180019140244
epoch£º414	 i:6 	 global-step:8286	 l-p:0.13928315043449402
epoch£º414	 i:7 	 global-step:8287	 l-p:0.12940068542957306
epoch£º414	 i:8 	 global-step:8288	 l-p:0.12019864469766617
epoch£º414	 i:9 	 global-step:8289	 l-p:0.13535645604133606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4108, 3.0987, 2.8286],
        [3.4108, 3.2791, 2.9808],
        [3.4108, 3.0904, 2.8601],
        [3.4108, 3.3803, 3.4053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.1319311559200287 
model_pd.l_d.mean(): -22.98370361328125 
model_pd.lagr.mean(): -22.85177230834961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3304], device='cuda:0')), ('power', tensor([-23.3141], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.1319311559200287
epoch£º415	 i:1 	 global-step:8301	 l-p:0.11939961463212967
epoch£º415	 i:2 	 global-step:8302	 l-p:0.13193538784980774
epoch£º415	 i:3 	 global-step:8303	 l-p:0.17513401806354523
epoch£º415	 i:4 	 global-step:8304	 l-p:0.139669731259346
epoch£º415	 i:5 	 global-step:8305	 l-p:0.1972961723804474
epoch£º415	 i:6 	 global-step:8306	 l-p:0.17650577425956726
epoch£º415	 i:7 	 global-step:8307	 l-p:0.06773995608091354
epoch£º415	 i:8 	 global-step:8308	 l-p:0.14316309988498688
epoch£º415	 i:9 	 global-step:8309	 l-p:0.12811918556690216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3800, 3.3059, 3.3540],
        [3.3800, 3.3799, 3.3800],
        [3.3800, 3.0570, 2.7952],
        [3.3800, 3.3800, 3.3800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.1506284922361374 
model_pd.l_d.mean(): -23.517719268798828 
model_pd.lagr.mean(): -23.367090225219727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2431], device='cuda:0')), ('power', tensor([-23.7608], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.1506284922361374
epoch£º416	 i:1 	 global-step:8321	 l-p:0.15120810270309448
epoch£º416	 i:2 	 global-step:8322	 l-p:-36.22938537597656
epoch£º416	 i:3 	 global-step:8323	 l-p:0.15588413178920746
epoch£º416	 i:4 	 global-step:8324	 l-p:0.11842643469572067
epoch£º416	 i:5 	 global-step:8325	 l-p:0.13534437119960785
epoch£º416	 i:6 	 global-step:8326	 l-p:0.13244521617889404
epoch£º416	 i:7 	 global-step:8327	 l-p:0.15243935585021973
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11730793863534927
epoch£º416	 i:9 	 global-step:8329	 l-p:0.12261384725570679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4627, 3.4446, 3.4604],
        [3.4627, 3.4237, 3.4542],
        [3.4627, 3.3467, 3.4029],
        [3.4627, 3.4627, 3.4627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.14510273933410645 
model_pd.l_d.mean(): -23.333072662353516 
model_pd.lagr.mean(): -23.187969207763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2117], device='cuda:0')), ('power', tensor([-23.5448], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.14510273933410645
epoch£º417	 i:1 	 global-step:8341	 l-p:0.129656121134758
epoch£º417	 i:2 	 global-step:8342	 l-p:0.12471921741962433
epoch£º417	 i:3 	 global-step:8343	 l-p:0.2188674658536911
epoch£º417	 i:4 	 global-step:8344	 l-p:0.12748242914676666
epoch£º417	 i:5 	 global-step:8345	 l-p:0.14575396478176117
epoch£º417	 i:6 	 global-step:8346	 l-p:0.17779456079006195
epoch£º417	 i:7 	 global-step:8347	 l-p:0.16762736439704895
epoch£º417	 i:8 	 global-step:8348	 l-p:0.19216421246528625
epoch£º417	 i:9 	 global-step:8349	 l-p:0.1327531635761261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3294, 3.3291, 3.3294],
        [3.3294, 3.3292, 3.3294],
        [3.3294, 3.0298, 2.7138],
        [3.3294, 3.1337, 3.1729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.1682071089744568 
model_pd.l_d.mean(): -22.840240478515625 
model_pd.lagr.mean(): -22.672033309936523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3157], device='cuda:0')), ('power', tensor([-23.1560], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.1682071089744568
epoch£º418	 i:1 	 global-step:8361	 l-p:0.1499168425798416
epoch£º418	 i:2 	 global-step:8362	 l-p:0.08460134267807007
epoch£º418	 i:3 	 global-step:8363	 l-p:0.1281738132238388
epoch£º418	 i:4 	 global-step:8364	 l-p:0.13579623401165009
epoch£º418	 i:5 	 global-step:8365	 l-p:0.15478147566318512
epoch£º418	 i:6 	 global-step:8366	 l-p:0.11863259226083755
epoch£º418	 i:7 	 global-step:8367	 l-p:0.1473923921585083
epoch£º418	 i:8 	 global-step:8368	 l-p:0.1319909244775772
epoch£º418	 i:9 	 global-step:8369	 l-p:0.15786609053611755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3968, 3.2042, 3.2429],
        [3.3968, 3.0664, 2.8364],
        [3.3968, 3.0818, 2.9465],
        [3.3968, 3.3771, 3.3942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.289330393075943 
model_pd.l_d.mean(): -23.288888931274414 
model_pd.lagr.mean(): -22.99955940246582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2809], device='cuda:0')), ('power', tensor([-23.5698], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.289330393075943
epoch£º419	 i:1 	 global-step:8381	 l-p:0.17851996421813965
epoch£º419	 i:2 	 global-step:8382	 l-p:0.15664859116077423
epoch£º419	 i:3 	 global-step:8383	 l-p:0.12829357385635376
epoch£º419	 i:4 	 global-step:8384	 l-p:0.1385369598865509
epoch£º419	 i:5 	 global-step:8385	 l-p:0.14896652102470398
epoch£º419	 i:6 	 global-step:8386	 l-p:0.13068637251853943
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12392938137054443
epoch£º419	 i:8 	 global-step:8388	 l-p:0.13464292883872986
epoch£º419	 i:9 	 global-step:8389	 l-p:0.13566023111343384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3211, 2.9982, 2.6932],
        [3.3211, 3.1469, 3.1977],
        [3.3211, 3.0716, 3.0692],
        [3.3211, 3.3201, 3.3211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.15443576872348785 
model_pd.l_d.mean(): -22.82503318786621 
model_pd.lagr.mean(): -22.670597076416016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3041], device='cuda:0')), ('power', tensor([-23.1291], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.15443576872348785
epoch£º420	 i:1 	 global-step:8401	 l-p:0.13518334925174713
epoch£º420	 i:2 	 global-step:8402	 l-p:0.13581396639347076
epoch£º420	 i:3 	 global-step:8403	 l-p:0.21414391696453094
epoch£º420	 i:4 	 global-step:8404	 l-p:0.13914605975151062
epoch£º420	 i:5 	 global-step:8405	 l-p:0.17450110614299774
epoch£º420	 i:6 	 global-step:8406	 l-p:0.22504009306430817
epoch£º420	 i:7 	 global-step:8407	 l-p:0.12518660724163055
epoch£º420	 i:8 	 global-step:8408	 l-p:-0.006149797234684229
epoch£º420	 i:9 	 global-step:8409	 l-p:0.15389153361320496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4077, 3.1430, 3.1142],
        [3.4077, 3.3348, 3.0500],
        [3.4077, 3.3202, 3.3730],
        [3.4077, 3.3943, 3.4064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.2155763953924179 
model_pd.l_d.mean(): -23.269153594970703 
model_pd.lagr.mean(): -23.053577423095703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2418], device='cuda:0')), ('power', tensor([-23.5109], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.2155763953924179
epoch£º421	 i:1 	 global-step:8421	 l-p:0.12445905059576035
epoch£º421	 i:2 	 global-step:8422	 l-p:0.10910533368587494
epoch£º421	 i:3 	 global-step:8423	 l-p:0.12993071973323822
epoch£º421	 i:4 	 global-step:8424	 l-p:0.13123998045921326
epoch£º421	 i:5 	 global-step:8425	 l-p:0.17020313441753387
epoch£º421	 i:6 	 global-step:8426	 l-p:0.14014004170894623
epoch£º421	 i:7 	 global-step:8427	 l-p:0.13766321539878845
epoch£º421	 i:8 	 global-step:8428	 l-p:0.1378832310438156
epoch£º421	 i:9 	 global-step:8429	 l-p:0.15920482575893402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4035, 3.3695, 3.3970],
        [3.4035, 3.4035, 3.4035],
        [3.4035, 3.4020, 3.4034],
        [3.4035, 3.0690, 2.8627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.1323283612728119 
model_pd.l_d.mean(): -23.257234573364258 
model_pd.lagr.mean(): -23.124906539916992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2615], device='cuda:0')), ('power', tensor([-23.5187], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.1323283612728119
epoch£º422	 i:1 	 global-step:8441	 l-p:0.13608728349208832
epoch£º422	 i:2 	 global-step:8442	 l-p:0.12367385625839233
epoch£º422	 i:3 	 global-step:8443	 l-p:0.10983431339263916
epoch£º422	 i:4 	 global-step:8444	 l-p:0.1609169989824295
epoch£º422	 i:5 	 global-step:8445	 l-p:0.3136604428291321
epoch£º422	 i:6 	 global-step:8446	 l-p:0.36003577709198
epoch£º422	 i:7 	 global-step:8447	 l-p:0.2073383331298828
epoch£º422	 i:8 	 global-step:8448	 l-p:0.15840362012386322
epoch£º422	 i:9 	 global-step:8449	 l-p:0.13466572761535645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3540, 3.3539, 3.3540],
        [3.3540, 3.0101, 2.7537],
        [3.3540, 3.3131, 3.3451],
        [3.3540, 3.0312, 2.9058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.05828297510743141 
model_pd.l_d.mean(): -23.2056941986084 
model_pd.lagr.mean(): -23.147411346435547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3209], device='cuda:0')), ('power', tensor([-23.5266], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.05828297510743141
epoch£º423	 i:1 	 global-step:8461	 l-p:0.16931721568107605
epoch£º423	 i:2 	 global-step:8462	 l-p:0.1468357890844345
epoch£º423	 i:3 	 global-step:8463	 l-p:0.1448562741279602
epoch£º423	 i:4 	 global-step:8464	 l-p:0.14495262503623962
epoch£º423	 i:5 	 global-step:8465	 l-p:0.11857008934020996
epoch£º423	 i:6 	 global-step:8466	 l-p:0.12048549205064774
epoch£º423	 i:7 	 global-step:8467	 l-p:0.11804193258285522
epoch£º423	 i:8 	 global-step:8468	 l-p:0.11939273774623871
epoch£º423	 i:9 	 global-step:8469	 l-p:0.12265966087579727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5472, 3.3646, 3.4061],
        [3.5472, 3.5424, 3.5469],
        [3.5472, 3.2996, 2.9799],
        [3.5472, 3.5472, 3.5472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.11524490267038345 
model_pd.l_d.mean(): -23.185070037841797 
model_pd.lagr.mean(): -23.06982421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2396], device='cuda:0')), ('power', tensor([-23.4246], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.11524490267038345
epoch£º424	 i:1 	 global-step:8481	 l-p:0.13162535429000854
epoch£º424	 i:2 	 global-step:8482	 l-p:0.1431506723165512
epoch£º424	 i:3 	 global-step:8483	 l-p:0.1493838131427765
epoch£º424	 i:4 	 global-step:8484	 l-p:0.13182291388511658
epoch£º424	 i:5 	 global-step:8485	 l-p:0.14034980535507202
epoch£º424	 i:6 	 global-step:8486	 l-p:0.13617484271526337
epoch£º424	 i:7 	 global-step:8487	 l-p:0.15861813724040985
epoch£º424	 i:8 	 global-step:8488	 l-p:0.15515626966953278
epoch£º424	 i:9 	 global-step:8489	 l-p:0.13279911875724792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3449, 3.1754, 3.2299],
        [3.3449, 3.0633, 3.0262],
        [3.3449, 3.0149, 2.7042],
        [3.3449, 3.3449, 3.3449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.16543956100940704 
model_pd.l_d.mean(): -22.774070739746094 
model_pd.lagr.mean(): -22.608631134033203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2987], device='cuda:0')), ('power', tensor([-23.0728], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.16543956100940704
epoch£º425	 i:1 	 global-step:8501	 l-p:0.14188694953918457
epoch£º425	 i:2 	 global-step:8502	 l-p:0.15859024226665497
epoch£º425	 i:3 	 global-step:8503	 l-p:0.15182006359100342
epoch£º425	 i:4 	 global-step:8504	 l-p:0.1739940345287323
epoch£º425	 i:5 	 global-step:8505	 l-p:0.04263549670577049
epoch£º425	 i:6 	 global-step:8506	 l-p:0.12227381765842438
epoch£º425	 i:7 	 global-step:8507	 l-p:0.13752520084381104
epoch£º425	 i:8 	 global-step:8508	 l-p:0.152460977435112
epoch£º425	 i:9 	 global-step:8509	 l-p:0.12887287139892578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4598, 3.4413, 3.4575],
        [3.4598, 3.4584, 3.4598],
        [3.4598, 3.4544, 3.4595],
        [3.4598, 3.2351, 2.9048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.16918151080608368 
model_pd.l_d.mean(): -22.456315994262695 
model_pd.lagr.mean(): -22.287134170532227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3286], device='cuda:0')), ('power', tensor([-22.7849], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.16918151080608368
epoch£º426	 i:1 	 global-step:8521	 l-p:0.1418730467557907
epoch£º426	 i:2 	 global-step:8522	 l-p:0.13369224965572357
epoch£º426	 i:3 	 global-step:8523	 l-p:0.13469868898391724
epoch£º426	 i:4 	 global-step:8524	 l-p:0.1230400800704956
epoch£º426	 i:5 	 global-step:8525	 l-p:0.1277836561203003
epoch£º426	 i:6 	 global-step:8526	 l-p:0.12390037626028061
epoch£º426	 i:7 	 global-step:8527	 l-p:0.15982083976268768
epoch£º426	 i:8 	 global-step:8528	 l-p:0.1405036896467209
epoch£º426	 i:9 	 global-step:8529	 l-p:0.11556426435709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4054, 3.3925, 3.4041],
        [3.4054, 3.3997, 3.4050],
        [3.4054, 3.2827, 3.3425],
        [3.4054, 3.0646, 2.8762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.1390082687139511 
model_pd.l_d.mean(): -22.94988441467285 
model_pd.lagr.mean(): -22.810876846313477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2283], device='cuda:0')), ('power', tensor([-23.1782], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.1390082687139511
epoch£º427	 i:1 	 global-step:8541	 l-p:0.16251756250858307
epoch£º427	 i:2 	 global-step:8542	 l-p:0.1370037943124771
epoch£º427	 i:3 	 global-step:8543	 l-p:0.11996205896139145
epoch£º427	 i:4 	 global-step:8544	 l-p:0.1266576051712036
epoch£º427	 i:5 	 global-step:8545	 l-p:0.133615642786026
epoch£º427	 i:6 	 global-step:8546	 l-p:0.17057299613952637
epoch£º427	 i:7 	 global-step:8547	 l-p:0.1296604722738266
epoch£º427	 i:8 	 global-step:8548	 l-p:0.1468142420053482
epoch£º427	 i:9 	 global-step:8549	 l-p:0.13770893216133118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[3.3630, 3.1806, 2.8510],
        [3.3630, 3.0786, 3.0414],
        [3.3630, 3.1077, 2.7668],
        [3.3630, 3.0555, 2.9800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.14825934171676636 
model_pd.l_d.mean(): -23.487573623657227 
model_pd.lagr.mean(): -23.339313507080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2432], device='cuda:0')), ('power', tensor([-23.7308], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.14825934171676636
epoch£º428	 i:1 	 global-step:8561	 l-p:0.17646002769470215
epoch£º428	 i:2 	 global-step:8562	 l-p:0.18014352023601532
epoch£º428	 i:3 	 global-step:8563	 l-p:0.1355186402797699
epoch£º428	 i:4 	 global-step:8564	 l-p:0.07315094023942947
epoch£º428	 i:5 	 global-step:8565	 l-p:0.15503817796707153
epoch£º428	 i:6 	 global-step:8566	 l-p:0.13757160305976868
epoch£º428	 i:7 	 global-step:8567	 l-p:0.1282014399766922
epoch£º428	 i:8 	 global-step:8568	 l-p:0.1524343639612198
epoch£º428	 i:9 	 global-step:8569	 l-p:0.1400192379951477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4184, 3.0924, 2.9669],
        [3.4184, 3.4183, 3.4184],
        [3.4184, 3.1091, 2.7815],
        [3.4184, 3.3926, 3.1169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.09980186074972153 
model_pd.l_d.mean(): -22.671628952026367 
model_pd.lagr.mean(): -22.571826934814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3599], device='cuda:0')), ('power', tensor([-23.0316], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.09980186074972153
epoch£º429	 i:1 	 global-step:8581	 l-p:0.13932175934314728
epoch£º429	 i:2 	 global-step:8582	 l-p:0.13414764404296875
epoch£º429	 i:3 	 global-step:8583	 l-p:0.13981746137142181
epoch£º429	 i:4 	 global-step:8584	 l-p:0.16784055531024933
epoch£º429	 i:5 	 global-step:8585	 l-p:0.1548255831003189
epoch£º429	 i:6 	 global-step:8586	 l-p:0.14948345720767975
epoch£º429	 i:7 	 global-step:8587	 l-p:0.14792822301387787
epoch£º429	 i:8 	 global-step:8588	 l-p:0.15029488503932953
epoch£º429	 i:9 	 global-step:8589	 l-p:-0.11757030338048935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3888, 3.3156, 3.3646],
        [3.3888, 3.3475, 3.0631],
        [3.3888, 3.2241, 3.2814],
        [3.3888, 3.3860, 3.3887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.17623111605644226 
model_pd.l_d.mean(): -22.63933753967285 
model_pd.lagr.mean(): -22.463106155395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3866], device='cuda:0')), ('power', tensor([-23.0259], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.17623111605644226
epoch£º430	 i:1 	 global-step:8601	 l-p:0.13994836807250977
epoch£º430	 i:2 	 global-step:8602	 l-p:0.13752636313438416
epoch£º430	 i:3 	 global-step:8603	 l-p:0.13969850540161133
epoch£º430	 i:4 	 global-step:8604	 l-p:0.13541187345981598
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12329547852277756
epoch£º430	 i:6 	 global-step:8606	 l-p:0.21102812886238098
epoch£º430	 i:7 	 global-step:8607	 l-p:0.19134250283241272
epoch£º430	 i:8 	 global-step:8608	 l-p:-0.930976152420044
epoch£º430	 i:9 	 global-step:8609	 l-p:0.12074533104896545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3012, 2.9436, 2.6304],
        [3.3012, 3.2370, 2.9402],
        [3.3012, 3.2744, 3.2971],
        [3.3012, 2.9701, 2.8694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.3442580997943878 
model_pd.l_d.mean(): -23.15220832824707 
model_pd.lagr.mean(): -22.807950973510742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3251], device='cuda:0')), ('power', tensor([-23.4773], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.3442580997943878
epoch£º431	 i:1 	 global-step:8621	 l-p:0.24033957719802856
epoch£º431	 i:2 	 global-step:8622	 l-p:0.1344946175813675
epoch£º431	 i:3 	 global-step:8623	 l-p:0.18118998408317566
epoch£º431	 i:4 	 global-step:8624	 l-p:0.14857502281665802
epoch£º431	 i:5 	 global-step:8625	 l-p:0.13990068435668945
epoch£º431	 i:6 	 global-step:8626	 l-p:0.1279706209897995
epoch£º431	 i:7 	 global-step:8627	 l-p:0.011244401335716248
epoch£º431	 i:8 	 global-step:8628	 l-p:0.12063004821538925
epoch£º431	 i:9 	 global-step:8629	 l-p:0.1469222754240036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3651, 3.2746, 2.9686],
        [3.3651, 3.3649, 3.3651],
        [3.3651, 3.3244, 3.3566],
        [3.3651, 3.2874, 3.3385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.06691280752420425 
model_pd.l_d.mean(): -23.198179244995117 
model_pd.lagr.mean(): -23.13126564025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2999], device='cuda:0')), ('power', tensor([-23.4981], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.06691280752420425
epoch£º432	 i:1 	 global-step:8641	 l-p:0.14753134548664093
epoch£º432	 i:2 	 global-step:8642	 l-p:0.15247568488121033
epoch£º432	 i:3 	 global-step:8643	 l-p:0.18299438059329987
epoch£º432	 i:4 	 global-step:8644	 l-p:0.17194345593452454
epoch£º432	 i:5 	 global-step:8645	 l-p:0.12390447407960892
epoch£º432	 i:6 	 global-step:8646	 l-p:0.13652198016643524
epoch£º432	 i:7 	 global-step:8647	 l-p:0.16989919543266296
epoch£º432	 i:8 	 global-step:8648	 l-p:0.1728755086660385
epoch£º432	 i:9 	 global-step:8649	 l-p:0.1294102817773819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4073, 3.1262, 3.0972],
        [3.4073, 3.0891, 2.7554],
        [3.4073, 3.4073, 3.4073],
        [3.4073, 3.3478, 3.3906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.1371159553527832 
model_pd.l_d.mean(): -23.148326873779297 
model_pd.lagr.mean(): -23.011211395263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2861], device='cuda:0')), ('power', tensor([-23.4344], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.1371159553527832
epoch£º433	 i:1 	 global-step:8661	 l-p:0.13444358110427856
epoch£º433	 i:2 	 global-step:8662	 l-p:0.12948191165924072
epoch£º433	 i:3 	 global-step:8663	 l-p:0.14350305497646332
epoch£º433	 i:4 	 global-step:8664	 l-p:-0.12979725003242493
epoch£º433	 i:5 	 global-step:8665	 l-p:0.16640955209732056
epoch£º433	 i:6 	 global-step:8666	 l-p:0.16545173525810242
epoch£º433	 i:7 	 global-step:8667	 l-p:0.15024283528327942
epoch£º433	 i:8 	 global-step:8668	 l-p:0.13687194883823395
epoch£º433	 i:9 	 global-step:8669	 l-p:0.14155730605125427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[3.4228, 3.0820, 2.9312],
        [3.4228, 3.1285, 2.7858],
        [3.4228, 3.2045, 3.2381],
        [3.4228, 3.1456, 3.1214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.1467549353837967 
model_pd.l_d.mean(): -23.702449798583984 
model_pd.lagr.mean(): -23.555694580078125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1914], device='cuda:0')), ('power', tensor([-23.8938], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.1467549353837967
epoch£º434	 i:1 	 global-step:8681	 l-p:0.14416278898715973
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11970319598913193
epoch£º434	 i:3 	 global-step:8683	 l-p:0.07581061869859695
epoch£º434	 i:4 	 global-step:8684	 l-p:0.1483604609966278
epoch£º434	 i:5 	 global-step:8685	 l-p:0.17996810376644135
epoch£º434	 i:6 	 global-step:8686	 l-p:0.1197671890258789
epoch£º434	 i:7 	 global-step:8687	 l-p:0.14761240780353546
epoch£º434	 i:8 	 global-step:8688	 l-p:0.14014244079589844
epoch£º434	 i:9 	 global-step:8689	 l-p:0.16298064589500427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3770, 3.3767, 3.3770],
        [3.3770, 3.3770, 3.3770],
        [3.3770, 3.3769, 3.3770],
        [3.3770, 3.2701, 2.9557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.14502505958080292 
model_pd.l_d.mean(): -23.623716354370117 
model_pd.lagr.mean(): -23.47869110107422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1972], device='cuda:0')), ('power', tensor([-23.8209], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.14502505958080292
epoch£º435	 i:1 	 global-step:8701	 l-p:0.15634484589099884
epoch£º435	 i:2 	 global-step:8702	 l-p:0.04547024518251419
epoch£º435	 i:3 	 global-step:8703	 l-p:0.12613604962825775
epoch£º435	 i:4 	 global-step:8704	 l-p:0.1276889443397522
epoch£º435	 i:5 	 global-step:8705	 l-p:0.15386803448200226
epoch£º435	 i:6 	 global-step:8706	 l-p:0.16784226894378662
epoch£º435	 i:7 	 global-step:8707	 l-p:0.1457582712173462
epoch£º435	 i:8 	 global-step:8708	 l-p:0.13591498136520386
epoch£º435	 i:9 	 global-step:8709	 l-p:0.2077852189540863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3828, 3.2619, 2.9413],
        [3.3828, 3.1167, 2.7645],
        [3.3828, 3.3656, 3.3809],
        [3.3828, 3.0233, 2.7227]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10055087506771088 
model_pd.l_d.mean(): -23.405174255371094 
model_pd.lagr.mean(): -23.304622650146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2877], device='cuda:0')), ('power', tensor([-23.6929], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10055087506771088
epoch£º436	 i:1 	 global-step:8721	 l-p:0.15167666971683502
epoch£º436	 i:2 	 global-step:8722	 l-p:0.12709856033325195
epoch£º436	 i:3 	 global-step:8723	 l-p:0.13649871945381165
epoch£º436	 i:4 	 global-step:8724	 l-p:0.11740606278181076
epoch£º436	 i:5 	 global-step:8725	 l-p:0.1356227546930313
epoch£º436	 i:6 	 global-step:8726	 l-p:0.14207611978054047
epoch£º436	 i:7 	 global-step:8727	 l-p:0.14826732873916626
epoch£º436	 i:8 	 global-step:8728	 l-p:0.13065208494663239
epoch£º436	 i:9 	 global-step:8729	 l-p:0.17090144753456116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3882, 3.3798, 3.3876],
        [3.3882, 3.3787, 3.1002],
        [3.3882, 3.0589, 2.7176],
        [3.3882, 3.0721, 2.7247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.15453572571277618 
model_pd.l_d.mean(): -22.734577178955078 
model_pd.lagr.mean(): -22.580041885375977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2714], device='cuda:0')), ('power', tensor([-23.0060], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.15453572571277618
epoch£º437	 i:1 	 global-step:8741	 l-p:0.14208701252937317
epoch£º437	 i:2 	 global-step:8742	 l-p:0.14544953405857086
epoch£º437	 i:3 	 global-step:8743	 l-p:0.14944234490394592
epoch£º437	 i:4 	 global-step:8744	 l-p:0.15937143564224243
epoch£º437	 i:5 	 global-step:8745	 l-p:0.3075702488422394
epoch£º437	 i:6 	 global-step:8746	 l-p:0.1450604349374771
epoch£º437	 i:7 	 global-step:8747	 l-p:0.13406462967395782
epoch£º437	 i:8 	 global-step:8748	 l-p:0.1459048092365265
epoch£º437	 i:9 	 global-step:8749	 l-p:0.11817511171102524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4478, 3.1667, 3.1421],
        [3.4478, 3.0862, 2.8289],
        [3.4478, 3.3875, 3.4310],
        [3.4478, 3.4478, 3.4478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.13444077968597412 
model_pd.l_d.mean(): -22.68515968322754 
model_pd.lagr.mean(): -22.550718307495117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2370], device='cuda:0')), ('power', tensor([-22.9221], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.13444077968597412
epoch£º438	 i:1 	 global-step:8761	 l-p:0.14200825989246368
epoch£º438	 i:2 	 global-step:8762	 l-p:0.179842010140419
epoch£º438	 i:3 	 global-step:8763	 l-p:0.11630313843488693
epoch£º438	 i:4 	 global-step:8764	 l-p:0.06614741683006287
epoch£º438	 i:5 	 global-step:8765	 l-p:0.15247246623039246
epoch£º438	 i:6 	 global-step:8766	 l-p:0.22770388424396515
epoch£º438	 i:7 	 global-step:8767	 l-p:0.1383354663848877
epoch£º438	 i:8 	 global-step:8768	 l-p:0.17412781715393066
epoch£º438	 i:9 	 global-step:8769	 l-p:0.137107715010643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3128, 2.9627, 2.8420],
        [3.3128, 2.9463, 2.7819],
        [3.3128, 2.9442, 2.7729],
        [3.3128, 3.3123, 3.3128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.1149120107293129 
model_pd.l_d.mean(): -23.656604766845703 
model_pd.lagr.mean(): -23.54169273376465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2258], device='cuda:0')), ('power', tensor([-23.8824], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.1149120107293129
epoch£º439	 i:1 	 global-step:8781	 l-p:0.13791827857494354
epoch£º439	 i:2 	 global-step:8782	 l-p:0.3976926803588867
epoch£º439	 i:3 	 global-step:8783	 l-p:0.1320677399635315
epoch£º439	 i:4 	 global-step:8784	 l-p:0.14616794884204865
epoch£º439	 i:5 	 global-step:8785	 l-p:0.30675560235977173
epoch£º439	 i:6 	 global-step:8786	 l-p:0.14794130623340607
epoch£º439	 i:7 	 global-step:8787	 l-p:0.17859043180942535
epoch£º439	 i:8 	 global-step:8788	 l-p:0.14704039692878723
epoch£º439	 i:9 	 global-step:8789	 l-p:0.1352873593568802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3573, 3.2955, 3.3400],
        [3.3573, 3.3086, 3.3459],
        [3.3573, 3.1222, 3.1534],
        [3.3573, 3.0046, 2.6657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.2026292234659195 
model_pd.l_d.mean(): -22.679866790771484 
model_pd.lagr.mean(): -22.477237701416016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2993], device='cuda:0')), ('power', tensor([-22.9791], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.2026292234659195
epoch£º440	 i:1 	 global-step:8801	 l-p:0.13498908281326294
epoch£º440	 i:2 	 global-step:8802	 l-p:0.051185645163059235
epoch£º440	 i:3 	 global-step:8803	 l-p:0.12613125145435333
epoch£º440	 i:4 	 global-step:8804	 l-p:0.14573365449905396
epoch£º440	 i:5 	 global-step:8805	 l-p:0.13210368156433105
epoch£º440	 i:6 	 global-step:8806	 l-p:0.12942741811275482
epoch£º440	 i:7 	 global-step:8807	 l-p:0.1530383825302124
epoch£º440	 i:8 	 global-step:8808	 l-p:0.1454080045223236
epoch£º440	 i:9 	 global-step:8809	 l-p:0.1502387523651123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4330, 3.4155, 3.4310],
        [3.4330, 3.0923, 2.7552],
        [3.4330, 3.4312, 3.4329],
        [3.4330, 3.1088, 3.0236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.1594316065311432 
model_pd.l_d.mean(): -22.529294967651367 
model_pd.lagr.mean(): -22.369863510131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3263], device='cuda:0')), ('power', tensor([-22.8556], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.1594316065311432
epoch£º441	 i:1 	 global-step:8821	 l-p:0.24572013318538666
epoch£º441	 i:2 	 global-step:8822	 l-p:0.13614550232887268
epoch£º441	 i:3 	 global-step:8823	 l-p:0.12638968229293823
epoch£º441	 i:4 	 global-step:8824	 l-p:0.144952654838562
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13459786772727966
epoch£º441	 i:6 	 global-step:8826	 l-p:0.13310213387012482
epoch£º441	 i:7 	 global-step:8827	 l-p:0.15258601307868958
epoch£º441	 i:8 	 global-step:8828	 l-p:0.12258173525333405
epoch£º441	 i:9 	 global-step:8829	 l-p:0.13521333038806915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3663, 3.3629, 3.3661],
        [3.3663, 3.1169, 2.7558],
        [3.3663, 2.9843, 2.7289],
        [3.3663, 3.3194, 3.3557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.13705411553382874 
model_pd.l_d.mean(): -23.310680389404297 
model_pd.lagr.mean(): -23.173625946044922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2500], device='cuda:0')), ('power', tensor([-23.5607], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.13705411553382874
epoch£º442	 i:1 	 global-step:8841	 l-p:0.14583280682563782
epoch£º442	 i:2 	 global-step:8842	 l-p:0.36357396841049194
epoch£º442	 i:3 	 global-step:8843	 l-p:0.1801009327173233
epoch£º442	 i:4 	 global-step:8844	 l-p:0.17745448648929596
epoch£º442	 i:5 	 global-step:8845	 l-p:-0.0036072300281375647
epoch£º442	 i:6 	 global-step:8846	 l-p:0.15938080847263336
epoch£º442	 i:7 	 global-step:8847	 l-p:0.15812397003173828
epoch£º442	 i:8 	 global-step:8848	 l-p:0.12188863754272461
epoch£º442	 i:9 	 global-step:8849	 l-p:0.16964519023895264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4055, 3.3132, 3.3703],
        [3.4055, 3.4039, 3.4055],
        [3.4055, 3.4033, 3.4054],
        [3.4055, 3.2790, 3.3430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.12144975364208221 
model_pd.l_d.mean(): -23.142948150634766 
model_pd.lagr.mean(): -23.02149772644043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2391], device='cuda:0')), ('power', tensor([-23.3821], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.12144975364208221
epoch£º443	 i:1 	 global-step:8861	 l-p:0.13763318955898285
epoch£º443	 i:2 	 global-step:8862	 l-p:0.12913894653320312
epoch£º443	 i:3 	 global-step:8863	 l-p:0.15150660276412964
epoch£º443	 i:4 	 global-step:8864	 l-p:0.1465625762939453
epoch£º443	 i:5 	 global-step:8865	 l-p:0.1400383561849594
epoch£º443	 i:6 	 global-step:8866	 l-p:0.1198299378156662
epoch£º443	 i:7 	 global-step:8867	 l-p:0.1461249440908432
epoch£º443	 i:8 	 global-step:8868	 l-p:0.1333620548248291
epoch£º443	 i:9 	 global-step:8869	 l-p:-0.11859320104122162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4103, 3.0538, 2.7165],
        [3.4103, 3.2865, 3.3504],
        [3.4103, 3.1297, 3.1193],
        [3.4103, 3.0379, 2.7286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): -0.005401573143899441 
model_pd.l_d.mean(): -22.423498153686523 
model_pd.lagr.mean(): -22.42889976501465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3395], device='cuda:0')), ('power', tensor([-22.7629], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:-0.005401573143899441
epoch£º444	 i:1 	 global-step:8881	 l-p:0.14164207875728607
epoch£º444	 i:2 	 global-step:8882	 l-p:0.13855105638504028
epoch£º444	 i:3 	 global-step:8883	 l-p:0.12870542705059052
epoch£º444	 i:4 	 global-step:8884	 l-p:0.14240621030330658
epoch£º444	 i:5 	 global-step:8885	 l-p:0.13122816383838654
epoch£º444	 i:6 	 global-step:8886	 l-p:0.15038403868675232
epoch£º444	 i:7 	 global-step:8887	 l-p:0.15216951072216034
epoch£º444	 i:8 	 global-step:8888	 l-p:0.1633405089378357
epoch£º444	 i:9 	 global-step:8889	 l-p:0.9867701530456543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3123, 3.3114, 3.3123],
        [3.3123, 2.9815, 2.9149],
        [3.3123, 3.2890, 3.3091],
        [3.3123, 3.1051, 3.1581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.22757141292095184 
model_pd.l_d.mean(): -23.533283233642578 
model_pd.lagr.mean(): -23.30571174621582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2758], device='cuda:0')), ('power', tensor([-23.8091], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.22757141292095184
epoch£º445	 i:1 	 global-step:8901	 l-p:0.1431657075881958
epoch£º445	 i:2 	 global-step:8902	 l-p:0.26599061489105225
epoch£º445	 i:3 	 global-step:8903	 l-p:0.12192172557115555
epoch£º445	 i:4 	 global-step:8904	 l-p:0.13563744723796844
epoch£º445	 i:5 	 global-step:8905	 l-p:0.15663684904575348
epoch£º445	 i:6 	 global-step:8906	 l-p:0.1771085262298584
epoch£º445	 i:7 	 global-step:8907	 l-p:0.17928364872932434
epoch£º445	 i:8 	 global-step:8908	 l-p:0.1250527948141098
epoch£º445	 i:9 	 global-step:8909	 l-p:0.12600278854370117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[3.4040, 3.0809, 3.0145],
        [3.4040, 3.2050, 2.8499],
        [3.4040, 3.0364, 2.8688],
        [3.4040, 3.2204, 2.8693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.058463938534259796 
model_pd.l_d.mean(): -23.225061416625977 
model_pd.lagr.mean(): -23.166597366333008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2829], device='cuda:0')), ('power', tensor([-23.5080], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.058463938534259796
epoch£º446	 i:1 	 global-step:8921	 l-p:0.12654559314250946
epoch£º446	 i:2 	 global-step:8922	 l-p:0.12267594784498215
epoch£º446	 i:3 	 global-step:8923	 l-p:0.1498391479253769
epoch£º446	 i:4 	 global-step:8924	 l-p:0.13727125525474548
epoch£º446	 i:5 	 global-step:8925	 l-p:0.16814862191677094
epoch£º446	 i:6 	 global-step:8926	 l-p:0.1434662938117981
epoch£º446	 i:7 	 global-step:8927	 l-p:0.13214652240276337
epoch£º446	 i:8 	 global-step:8928	 l-p:0.1572800576686859
epoch£º446	 i:9 	 global-step:8929	 l-p:0.20653857290744781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3807, 3.3700, 3.3798],
        [3.3807, 3.2875, 3.3455],
        [3.3807, 3.0332, 2.9278],
        [3.3807, 3.0293, 2.9156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.16056355834007263 
model_pd.l_d.mean(): -23.35500144958496 
model_pd.lagr.mean(): -23.19443702697754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2759], device='cuda:0')), ('power', tensor([-23.6309], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.16056355834007263
epoch£º447	 i:1 	 global-step:8941	 l-p:0.13640183210372925
epoch£º447	 i:2 	 global-step:8942	 l-p:0.13396282494068146
epoch£º447	 i:3 	 global-step:8943	 l-p:0.13167278468608856
epoch£º447	 i:4 	 global-step:8944	 l-p:0.1720762550830841
epoch£º447	 i:5 	 global-step:8945	 l-p:0.05514390021562576
epoch£º447	 i:6 	 global-step:8946	 l-p:0.1292141079902649
epoch£º447	 i:7 	 global-step:8947	 l-p:0.1403435617685318
epoch£º447	 i:8 	 global-step:8948	 l-p:0.14396832883358002
epoch£º447	 i:9 	 global-step:8949	 l-p:0.15563708543777466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4469, 3.3312, 2.9993],
        [3.4469, 3.2190, 2.8564],
        [3.4469, 3.3801, 3.4273],
        [3.4469, 3.4467, 3.4469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.12538698315620422 
model_pd.l_d.mean(): -22.9644718170166 
model_pd.lagr.mean(): -22.83908462524414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2951], device='cuda:0')), ('power', tensor([-23.2596], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.12538698315620422
epoch£º448	 i:1 	 global-step:8961	 l-p:0.13608945906162262
epoch£º448	 i:2 	 global-step:8962	 l-p:0.13843241333961487
epoch£º448	 i:3 	 global-step:8963	 l-p:0.1324155181646347
epoch£º448	 i:4 	 global-step:8964	 l-p:-0.13295401632785797
epoch£º448	 i:5 	 global-step:8965	 l-p:0.1979716420173645
epoch£º448	 i:6 	 global-step:8966	 l-p:0.1320669949054718
epoch£º448	 i:7 	 global-step:8967	 l-p:0.12173832952976227
epoch£º448	 i:8 	 global-step:8968	 l-p:0.1628861129283905
epoch£º448	 i:9 	 global-step:8969	 l-p:0.1358260065317154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3686, 3.3641, 3.3684],
        [3.3686, 3.1186, 3.1469],
        [3.3686, 3.2729, 3.3321],
        [3.3686, 3.2487, 3.3136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.1992279440164566 
model_pd.l_d.mean(): -23.60348892211914 
model_pd.lagr.mean(): -23.404260635375977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2413], device='cuda:0')), ('power', tensor([-23.8448], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.1992279440164566
epoch£º449	 i:1 	 global-step:8981	 l-p:0.16233649849891663
epoch£º449	 i:2 	 global-step:8982	 l-p:0.16272065043449402
epoch£º449	 i:3 	 global-step:8983	 l-p:0.14691658318042755
epoch£º449	 i:4 	 global-step:8984	 l-p:0.1408648043870926
epoch£º449	 i:5 	 global-step:8985	 l-p:0.5032442212104797
epoch£º449	 i:6 	 global-step:8986	 l-p:0.13310039043426514
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12447014451026917
epoch£º449	 i:8 	 global-step:8988	 l-p:0.14783146977424622
epoch£º449	 i:9 	 global-step:8989	 l-p:0.17236940562725067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4089, 3.4080, 3.4089],
        [3.4089, 3.4085, 3.4089],
        [3.4089, 3.4021, 3.4085],
        [3.4089, 3.0918, 2.7183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.15727591514587402 
model_pd.l_d.mean(): -23.553171157836914 
model_pd.lagr.mean(): -23.39589500427246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2418], device='cuda:0')), ('power', tensor([-23.7950], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.15727591514587402
epoch£º450	 i:1 	 global-step:9001	 l-p:0.14175710082054138
epoch£º450	 i:2 	 global-step:9002	 l-p:0.11780504137277603
epoch£º450	 i:3 	 global-step:9003	 l-p:0.12999722361564636
epoch£º450	 i:4 	 global-step:9004	 l-p:0.22542144358158112
epoch£º450	 i:5 	 global-step:9005	 l-p:0.13502290844917297
epoch£º450	 i:6 	 global-step:9006	 l-p:0.1480226218700409
epoch£º450	 i:7 	 global-step:9007	 l-p:0.14374840259552002
epoch£º450	 i:8 	 global-step:9008	 l-p:0.016733918339014053
epoch£º450	 i:9 	 global-step:9009	 l-p:0.11949929594993591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4313, 3.3789, 3.4188],
        [3.4313, 3.2957, 3.3621],
        [3.4313, 3.4312, 3.4314],
        [3.4313, 3.4308, 3.4313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.12566745281219482 
model_pd.l_d.mean(): -22.28997802734375 
model_pd.lagr.mean(): -22.164310455322266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3701], device='cuda:0')), ('power', tensor([-22.6601], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.12566745281219482
epoch£º451	 i:1 	 global-step:9021	 l-p:0.15292692184448242
epoch£º451	 i:2 	 global-step:9022	 l-p:0.13162393867969513
epoch£º451	 i:3 	 global-step:9023	 l-p:0.23656657338142395
epoch£º451	 i:4 	 global-step:9024	 l-p:0.1592283546924591
epoch£º451	 i:5 	 global-step:9025	 l-p:0.11584068834781647
epoch£º451	 i:6 	 global-step:9026	 l-p:0.14456449449062347
epoch£º451	 i:7 	 global-step:9027	 l-p:0.13173820078372955
epoch£º451	 i:8 	 global-step:9028	 l-p:0.1302156001329422
epoch£º451	 i:9 	 global-step:9029	 l-p:0.11692506819963455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4799, 3.3096, 3.3736],
        [3.4799, 3.4770, 3.4798],
        [3.4799, 3.4740, 3.4796],
        [3.4799, 3.4799, 3.4799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.1384621560573578 
model_pd.l_d.mean(): -23.144617080688477 
model_pd.lagr.mean(): -23.006155014038086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2624], device='cuda:0')), ('power', tensor([-23.4070], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.1384621560573578
epoch£º452	 i:1 	 global-step:9041	 l-p:0.17621786892414093
epoch£º452	 i:2 	 global-step:9042	 l-p:0.106531523168087
epoch£º452	 i:3 	 global-step:9043	 l-p:0.13058769702911377
epoch£º452	 i:4 	 global-step:9044	 l-p:0.12488390505313873
epoch£º452	 i:5 	 global-step:9045	 l-p:0.2735741138458252
epoch£º452	 i:6 	 global-step:9046	 l-p:0.10566694289445877
epoch£º452	 i:7 	 global-step:9047	 l-p:0.20130610466003418
epoch£º452	 i:8 	 global-step:9048	 l-p:0.14459367096424103
epoch£º452	 i:9 	 global-step:9049	 l-p:0.14337505400180817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3388, 3.3357, 3.3387],
        [3.3388, 3.3388, 3.3388],
        [3.3388, 3.3341, 3.3386],
        [3.3388, 3.2450, 3.3044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.18381141126155853 
model_pd.l_d.mean(): -23.593990325927734 
model_pd.lagr.mean(): -23.410179138183594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2594], device='cuda:0')), ('power', tensor([-23.8534], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.18381141126155853
epoch£º453	 i:1 	 global-step:9061	 l-p:0.15336556732654572
epoch£º453	 i:2 	 global-step:9062	 l-p:0.25005146861076355
epoch£º453	 i:3 	 global-step:9063	 l-p:0.22753913700580597
epoch£º453	 i:4 	 global-step:9064	 l-p:0.13639317452907562
epoch£º453	 i:5 	 global-step:9065	 l-p:0.1358497589826584
epoch£º453	 i:6 	 global-step:9066	 l-p:0.15796197950839996
epoch£º453	 i:7 	 global-step:9067	 l-p:0.17657290399074554
epoch£º453	 i:8 	 global-step:9068	 l-p:0.029082493856549263
epoch£º453	 i:9 	 global-step:9069	 l-p:0.14582248032093048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4622, 3.2500, 3.3023],
        [3.4622, 3.0852, 2.7386],
        [3.4622, 3.4560, 3.4618],
        [3.4622, 3.4349, 3.4581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.1345411241054535 
model_pd.l_d.mean(): -23.35240364074707 
model_pd.lagr.mean(): -23.217863082885742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1905], device='cuda:0')), ('power', tensor([-23.5429], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.1345411241054535
epoch£º454	 i:1 	 global-step:9081	 l-p:0.17855098843574524
epoch£º454	 i:2 	 global-step:9082	 l-p:0.14326591789722443
epoch£º454	 i:3 	 global-step:9083	 l-p:0.1431979387998581
epoch£º454	 i:4 	 global-step:9084	 l-p:0.15503308176994324
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12188374251127243
epoch£º454	 i:6 	 global-step:9086	 l-p:0.11856847256422043
epoch£º454	 i:7 	 global-step:9087	 l-p:0.043547265231609344
epoch£º454	 i:8 	 global-step:9088	 l-p:0.1659218817949295
epoch£º454	 i:9 	 global-step:9089	 l-p:0.1373554766178131
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3745, 3.0296, 2.9572],
        [3.3745, 3.3734, 3.3745],
        [3.3745, 3.3741, 3.3745],
        [3.3745, 3.3745, 3.3745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.12723608314990997 
model_pd.l_d.mean(): -23.252525329589844 
model_pd.lagr.mean(): -23.125289916992188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2966], device='cuda:0')), ('power', tensor([-23.5491], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.12723608314990997
epoch£º455	 i:1 	 global-step:9101	 l-p:0.1272028386592865
epoch£º455	 i:2 	 global-step:9102	 l-p:0.1493147760629654
epoch£º455	 i:3 	 global-step:9103	 l-p:0.12996917963027954
epoch£º455	 i:4 	 global-step:9104	 l-p:-0.12560750544071198
epoch£º455	 i:5 	 global-step:9105	 l-p:0.18744252622127533
epoch£º455	 i:6 	 global-step:9106	 l-p:0.21513736248016357
epoch£º455	 i:7 	 global-step:9107	 l-p:0.18629001080989838
epoch£º455	 i:8 	 global-step:9108	 l-p:0.1477968990802765
epoch£º455	 i:9 	 global-step:9109	 l-p:0.12443412840366364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3676, 3.3676, 3.3676],
        [3.3676, 3.1934, 3.2608],
        [3.3676, 3.3665, 3.3676],
        [3.3676, 3.3615, 3.3673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.19399455189704895 
model_pd.l_d.mean(): -23.3623046875 
model_pd.lagr.mean(): -23.168310165405273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2548], device='cuda:0')), ('power', tensor([-23.6171], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.19399455189704895
epoch£º456	 i:1 	 global-step:9121	 l-p:0.14109967648983002
epoch£º456	 i:2 	 global-step:9122	 l-p:0.12690038979053497
epoch£º456	 i:3 	 global-step:9123	 l-p:0.15897680819034576
epoch£º456	 i:4 	 global-step:9124	 l-p:0.12767504155635834
epoch£º456	 i:5 	 global-step:9125	 l-p:0.14348553121089935
epoch£º456	 i:6 	 global-step:9126	 l-p:0.1449107676744461
epoch£º456	 i:7 	 global-step:9127	 l-p:0.14619390666484833
epoch£º456	 i:8 	 global-step:9128	 l-p:0.14038412272930145
epoch£º456	 i:9 	 global-step:9129	 l-p:0.1703914999961853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4375, 3.0797, 2.7031],
        [3.4375, 3.2457, 3.3080],
        [3.4375, 3.0320, 2.7529],
        [3.4375, 3.4375, 3.4375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.15355157852172852 
model_pd.l_d.mean(): -23.66851043701172 
model_pd.lagr.mean(): -23.51495933532715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2022], device='cuda:0')), ('power', tensor([-23.8707], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.15355157852172852
epoch£º457	 i:1 	 global-step:9141	 l-p:0.12305498123168945
epoch£º457	 i:2 	 global-step:9142	 l-p:0.13353398442268372
epoch£º457	 i:3 	 global-step:9143	 l-p:0.13660508394241333
epoch£º457	 i:4 	 global-step:9144	 l-p:0.16516461968421936
epoch£º457	 i:5 	 global-step:9145	 l-p:0.14525814354419708
epoch£º457	 i:6 	 global-step:9146	 l-p:0.18165774643421173
epoch£º457	 i:7 	 global-step:9147	 l-p:0.12200898677110672
epoch£º457	 i:8 	 global-step:9148	 l-p:0.16423751413822174
epoch£º457	 i:9 	 global-step:9149	 l-p:0.1339181363582611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4038, 3.4036, 3.4038],
        [3.4038, 3.2977, 3.3611],
        [3.4038, 3.4038, 3.4038],
        [3.4038, 3.4036, 3.4038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.2008952647447586 
model_pd.l_d.mean(): -23.5938777923584 
model_pd.lagr.mean(): -23.392982482910156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2243], device='cuda:0')), ('power', tensor([-23.8182], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.2008952647447586
epoch£º458	 i:1 	 global-step:9161	 l-p:0.12284278869628906
epoch£º458	 i:2 	 global-step:9162	 l-p:0.13487043976783752
epoch£º458	 i:3 	 global-step:9163	 l-p:0.16030974686145782
epoch£º458	 i:4 	 global-step:9164	 l-p:0.12123997509479523
epoch£º458	 i:5 	 global-step:9165	 l-p:0.13371364772319794
epoch£º458	 i:6 	 global-step:9166	 l-p:0.3716355860233307
epoch£º458	 i:7 	 global-step:9167	 l-p:0.14551158249378204
epoch£º458	 i:8 	 global-step:9168	 l-p:0.12794898450374603
epoch£º458	 i:9 	 global-step:9169	 l-p:0.179162859916687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3656, 3.3656, 3.3656],
        [3.3656, 2.9609, 2.6041],
        [3.3656, 3.2849, 2.9462],
        [3.3656, 3.3656, 3.3656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.15113802254199982 
model_pd.l_d.mean(): -23.27228355407715 
model_pd.lagr.mean(): -23.121145248413086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2422], device='cuda:0')), ('power', tensor([-23.5144], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.15113802254199982
epoch£º459	 i:1 	 global-step:9181	 l-p:0.15382042527198792
epoch£º459	 i:2 	 global-step:9182	 l-p:0.11355943232774734
epoch£º459	 i:3 	 global-step:9183	 l-p:0.17150571942329407
epoch£º459	 i:4 	 global-step:9184	 l-p:0.17499466240406036
epoch£º459	 i:5 	 global-step:9185	 l-p:0.21391722559928894
epoch£º459	 i:6 	 global-step:9186	 l-p:0.13969728350639343
epoch£º459	 i:7 	 global-step:9187	 l-p:0.11403299123048782
epoch£º459	 i:8 	 global-step:9188	 l-p:0.14701710641384125
epoch£º459	 i:9 	 global-step:9189	 l-p:0.15219983458518982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4461, 3.4247, 3.4434],
        [3.4461, 3.1026, 3.0322],
        [3.4461, 3.3545, 3.0138],
        [3.4461, 3.0366, 2.7441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.11403980106115341 
model_pd.l_d.mean(): -22.313974380493164 
model_pd.lagr.mean(): -22.199934005737305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3554], device='cuda:0')), ('power', tensor([-22.6693], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.11403980106115341
epoch£º460	 i:1 	 global-step:9201	 l-p:0.1298017054796219
epoch£º460	 i:2 	 global-step:9202	 l-p:0.12916618585586548
epoch£º460	 i:3 	 global-step:9203	 l-p:0.18439806997776031
epoch£º460	 i:4 	 global-step:9204	 l-p:-0.008091382682323456
epoch£º460	 i:5 	 global-step:9205	 l-p:0.14338462054729462
epoch£º460	 i:6 	 global-step:9206	 l-p:0.13524268567562103
epoch£º460	 i:7 	 global-step:9207	 l-p:0.12952342629432678
epoch£º460	 i:8 	 global-step:9208	 l-p:0.19422553479671478
epoch£º460	 i:9 	 global-step:9209	 l-p:0.18839624524116516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4132, 3.4088, 3.4129],
        [3.4132, 3.0745, 3.0189],
        [3.4132, 3.3006, 3.3659],
        [3.4132, 3.0079, 2.6601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.13982924818992615 
model_pd.l_d.mean(): -22.17390251159668 
model_pd.lagr.mean(): -22.034072875976562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3485], device='cuda:0')), ('power', tensor([-22.5224], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.13982924818992615
epoch£º461	 i:1 	 global-step:9221	 l-p:0.1242893636226654
epoch£º461	 i:2 	 global-step:9222	 l-p:0.1558428555727005
epoch£º461	 i:3 	 global-step:9223	 l-p:0.13724778592586517
epoch£º461	 i:4 	 global-step:9224	 l-p:0.13505205512046814
epoch£º461	 i:5 	 global-step:9225	 l-p:0.1634654700756073
epoch£º461	 i:6 	 global-step:9226	 l-p:0.14865833520889282
epoch£º461	 i:7 	 global-step:9227	 l-p:0.15382976830005646
epoch£º461	 i:8 	 global-step:9228	 l-p:0.11298779398202896
epoch£º461	 i:9 	 global-step:9229	 l-p:0.17528223991394043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4102, 3.0056, 2.8102],
        [3.4102, 3.0323, 2.9088],
        [3.4102, 3.3098, 3.3719],
        [3.4102, 3.3112, 3.3729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.12110507488250732 
model_pd.l_d.mean(): -23.421335220336914 
model_pd.lagr.mean(): -23.300230026245117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2612], device='cuda:0')), ('power', tensor([-23.6825], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.12110507488250732
epoch£º462	 i:1 	 global-step:9241	 l-p:0.14312484860420227
epoch£º462	 i:2 	 global-step:9242	 l-p:0.14243865013122559
epoch£º462	 i:3 	 global-step:9243	 l-p:0.15004324913024902
epoch£º462	 i:4 	 global-step:9244	 l-p:0.1587669402360916
epoch£º462	 i:5 	 global-step:9245	 l-p:0.1183832511305809
epoch£º462	 i:6 	 global-step:9246	 l-p:0.13496316969394684
epoch£º462	 i:7 	 global-step:9247	 l-p:0.1463039517402649
epoch£º462	 i:8 	 global-step:9248	 l-p:0.14926749467849731
epoch£º462	 i:9 	 global-step:9249	 l-p:0.1344330906867981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4575, 3.4541, 3.4573],
        [3.4575, 3.4329, 3.4541],
        [3.4575, 3.4356, 3.4547],
        [3.4575, 3.4572, 3.4575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.16062122583389282 
model_pd.l_d.mean(): -21.94581413269043 
model_pd.lagr.mean(): -21.785192489624023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3888], device='cuda:0')), ('power', tensor([-22.3346], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.16062122583389282
epoch£º463	 i:1 	 global-step:9261	 l-p:0.1422051638364792
epoch£º463	 i:2 	 global-step:9262	 l-p:0.13365066051483154
epoch£º463	 i:3 	 global-step:9263	 l-p:0.1637313961982727
epoch£º463	 i:4 	 global-step:9264	 l-p:0.13334625959396362
epoch£º463	 i:5 	 global-step:9265	 l-p:0.12381438165903091
epoch£º463	 i:6 	 global-step:9266	 l-p:0.13111968338489532
epoch£º463	 i:7 	 global-step:9267	 l-p:0.1407964676618576
epoch£º463	 i:8 	 global-step:9268	 l-p:-24.778331756591797
epoch£º463	 i:9 	 global-step:9269	 l-p:0.13063648343086243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3023, 3.2941, 3.3018],
        [3.3023, 2.8651, 2.5241],
        [3.3023, 2.8979, 2.5068],
        [3.3023, 3.3023, 3.3023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.023225802928209305 
model_pd.l_d.mean(): -23.53183937072754 
model_pd.lagr.mean(): -23.50861358642578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2796], device='cuda:0')), ('power', tensor([-23.8114], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.023225802928209305
epoch£º464	 i:1 	 global-step:9281	 l-p:0.6497817635536194
epoch£º464	 i:2 	 global-step:9282	 l-p:0.11741504073143005
epoch£º464	 i:3 	 global-step:9283	 l-p:0.1608290821313858
epoch£º464	 i:4 	 global-step:9284	 l-p:0.1332971304655075
epoch£º464	 i:5 	 global-step:9285	 l-p:0.13946370780467987
epoch£º464	 i:6 	 global-step:9286	 l-p:0.4256608486175537
epoch£º464	 i:7 	 global-step:9287	 l-p:0.1613929718732834
epoch£º464	 i:8 	 global-step:9288	 l-p:0.17995372414588928
epoch£º464	 i:9 	 global-step:9289	 l-p:0.1439804583787918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3973, 3.3973, 3.3973],
        [3.3973, 3.3941, 3.3972],
        [3.3973, 3.3491, 3.3869],
        [3.3973, 3.0837, 3.0713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.1349552869796753 
model_pd.l_d.mean(): -23.212261199951172 
model_pd.lagr.mean(): -23.077306747436523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3120], device='cuda:0')), ('power', tensor([-23.5243], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.1349552869796753
epoch£º465	 i:1 	 global-step:9301	 l-p:0.22208040952682495
epoch£º465	 i:2 	 global-step:9302	 l-p:0.1295817792415619
epoch£º465	 i:3 	 global-step:9303	 l-p:0.1321069449186325
epoch£º465	 i:4 	 global-step:9304	 l-p:0.1242227628827095
epoch£º465	 i:5 	 global-step:9305	 l-p:0.12515802681446075
epoch£º465	 i:6 	 global-step:9306	 l-p:0.1169719323515892
epoch£º465	 i:7 	 global-step:9307	 l-p:0.1559913158416748
epoch£º465	 i:8 	 global-step:9308	 l-p:0.1451399177312851
epoch£º465	 i:9 	 global-step:9309	 l-p:0.1844472438097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3875, 3.1290, 3.1676],
        [3.3875, 3.0721, 3.0599],
        [3.3875, 3.3516, 3.3813],
        [3.3875, 2.9571, 2.6890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.11059337109327316 
model_pd.l_d.mean(): -22.466604232788086 
model_pd.lagr.mean(): -22.35601043701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3514], device='cuda:0')), ('power', tensor([-22.8180], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.11059337109327316
epoch£º466	 i:1 	 global-step:9321	 l-p:0.23365576565265656
epoch£º466	 i:2 	 global-step:9322	 l-p:0.1331779956817627
epoch£º466	 i:3 	 global-step:9323	 l-p:0.13227711617946625
epoch£º466	 i:4 	 global-step:9324	 l-p:0.14348287880420685
epoch£º466	 i:5 	 global-step:9325	 l-p:0.16149331629276276
epoch£º466	 i:6 	 global-step:9326	 l-p:0.1758534461259842
epoch£º466	 i:7 	 global-step:9327	 l-p:0.15590445697307587
epoch£º466	 i:8 	 global-step:9328	 l-p:0.1672336310148239
epoch£º466	 i:9 	 global-step:9329	 l-p:0.10186216235160828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4571, 3.4570, 3.4571],
        [3.4571, 3.0557, 2.6841],
        [3.4571, 3.4021, 3.4440],
        [3.4571, 3.1686, 3.1799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12559030950069427 
model_pd.l_d.mean(): -23.324119567871094 
model_pd.lagr.mean(): -23.198530197143555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2447], device='cuda:0')), ('power', tensor([-23.5688], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12559030950069427
epoch£º467	 i:1 	 global-step:9341	 l-p:0.1893729418516159
epoch£º467	 i:2 	 global-step:9342	 l-p:0.13483573496341705
epoch£º467	 i:3 	 global-step:9343	 l-p:-0.07329852133989334
epoch£º467	 i:4 	 global-step:9344	 l-p:0.13161569833755493
epoch£º467	 i:5 	 global-step:9345	 l-p:0.14058344066143036
epoch£º467	 i:6 	 global-step:9346	 l-p:0.14758355915546417
epoch£º467	 i:7 	 global-step:9347	 l-p:0.16075292229652405
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11776518821716309
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11604337394237518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4855, 3.4012, 3.4578],
        [3.4855, 3.0890, 2.7144],
        [3.4855, 3.3415, 3.4123],
        [3.4855, 3.3675, 3.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.16834811866283417 
model_pd.l_d.mean(): -23.48480224609375 
model_pd.lagr.mean(): -23.31645393371582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2253], device='cuda:0')), ('power', tensor([-23.7101], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.16834811866283417
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10929816216230392
epoch£º468	 i:2 	 global-step:9362	 l-p:0.13450965285301208
epoch£º468	 i:3 	 global-step:9363	 l-p:0.1768777072429657
epoch£º468	 i:4 	 global-step:9364	 l-p:0.1578836590051651
epoch£º468	 i:5 	 global-step:9365	 l-p:0.15562404692173004
epoch£º468	 i:6 	 global-step:9366	 l-p:0.14323483407497406
epoch£º468	 i:7 	 global-step:9367	 l-p:0.10260270535945892
epoch£º468	 i:8 	 global-step:9368	 l-p:0.1702340543270111
epoch£º468	 i:9 	 global-step:9369	 l-p:0.120857372879982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4354, 3.3251, 3.3910],
        [3.4354, 3.4354, 3.4354],
        [3.4354, 3.4289, 3.4350],
        [3.4354, 3.4354, 3.4354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.05180344358086586 
model_pd.l_d.mean(): -23.406003952026367 
model_pd.lagr.mean(): -23.35420036315918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1994], device='cuda:0')), ('power', tensor([-23.6054], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.05180344358086586
epoch£º469	 i:1 	 global-step:9381	 l-p:0.17750202119350433
epoch£º469	 i:2 	 global-step:9382	 l-p:0.1281806379556656
epoch£º469	 i:3 	 global-step:9383	 l-p:0.13552437722682953
epoch£º469	 i:4 	 global-step:9384	 l-p:0.1504078358411789
epoch£º469	 i:5 	 global-step:9385	 l-p:0.1505216658115387
epoch£º469	 i:6 	 global-step:9386	 l-p:0.17048422992229462
epoch£º469	 i:7 	 global-step:9387	 l-p:0.1365393102169037
epoch£º469	 i:8 	 global-step:9388	 l-p:0.1641319990158081
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11948157846927643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4284, 3.4279, 3.4284],
        [3.4284, 3.1589, 3.1919],
        [3.4284, 3.2870, 3.3589],
        [3.4284, 3.4281, 3.4284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.12295549362897873 
model_pd.l_d.mean(): -23.485624313354492 
model_pd.lagr.mean(): -23.362668991088867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2126], device='cuda:0')), ('power', tensor([-23.6982], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.12295549362897873
epoch£º470	 i:1 	 global-step:9401	 l-p:0.18998026847839355
epoch£º470	 i:2 	 global-step:9402	 l-p:0.21733829379081726
epoch£º470	 i:3 	 global-step:9403	 l-p:0.1679481714963913
epoch£º470	 i:4 	 global-step:9404	 l-p:0.14815402030944824
epoch£º470	 i:5 	 global-step:9405	 l-p:0.1529962420463562
epoch£º470	 i:6 	 global-step:9406	 l-p:0.14945098757743835
epoch£º470	 i:7 	 global-step:9407	 l-p:0.09151946753263474
epoch£º470	 i:8 	 global-step:9408	 l-p:0.1460229903459549
epoch£º470	 i:9 	 global-step:9409	 l-p:0.13086208701133728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4398, 3.4398, 3.4398],
        [3.4398, 3.4398, 3.4398],
        [3.4398, 3.4398, 3.4398],
        [3.4398, 3.4398, 3.4398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.13845443725585938 
model_pd.l_d.mean(): -23.146976470947266 
model_pd.lagr.mean(): -23.008522033691406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2897], device='cuda:0')), ('power', tensor([-23.4367], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.13845443725585938
epoch£º471	 i:1 	 global-step:9421	 l-p:0.13915961980819702
epoch£º471	 i:2 	 global-step:9422	 l-p:0.24504654109477997
epoch£º471	 i:3 	 global-step:9423	 l-p:0.14452910423278809
epoch£º471	 i:4 	 global-step:9424	 l-p:0.3625936806201935
epoch£º471	 i:5 	 global-step:9425	 l-p:0.1450602263212204
epoch£º471	 i:6 	 global-step:9426	 l-p:0.13868165016174316
epoch£º471	 i:7 	 global-step:9427	 l-p:0.14448653161525726
epoch£º471	 i:8 	 global-step:9428	 l-p:-0.021408366039395332
epoch£º471	 i:9 	 global-step:9429	 l-p:0.15249185264110565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3299, 2.9226, 2.7895],
        [3.3299, 3.2799, 3.3192],
        [3.3299, 2.9859, 2.5655],
        [3.3299, 3.3299, 3.3299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.14220619201660156 
model_pd.l_d.mean(): -23.556245803833008 
model_pd.lagr.mean(): -23.414039611816406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2678], device='cuda:0')), ('power', tensor([-23.8240], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.14220619201660156
epoch£º472	 i:1 	 global-step:9441	 l-p:0.23242899775505066
epoch£º472	 i:2 	 global-step:9442	 l-p:0.14078155159950256
epoch£º472	 i:3 	 global-step:9443	 l-p:0.15210872888565063
epoch£º472	 i:4 	 global-step:9444	 l-p:0.15512831509113312
epoch£º472	 i:5 	 global-step:9445	 l-p:0.1310122311115265
epoch£º472	 i:6 	 global-step:9446	 l-p:0.16284091770648956
epoch£º472	 i:7 	 global-step:9447	 l-p:-0.0013403367483988404
epoch£º472	 i:8 	 global-step:9448	 l-p:0.16045814752578735
epoch£º472	 i:9 	 global-step:9449	 l-p:0.15034952759742737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4424, 3.1069, 3.0774],
        [3.4424, 3.4424, 3.4424],
        [3.4424, 3.4423, 3.4424],
        [3.4424, 3.3904, 3.4308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.16137294471263885 
model_pd.l_d.mean(): -22.920902252197266 
model_pd.lagr.mean(): -22.75952911376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3773], device='cuda:0')), ('power', tensor([-23.2982], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.16137294471263885
epoch£º473	 i:1 	 global-step:9461	 l-p:0.12297844886779785
epoch£º473	 i:2 	 global-step:9462	 l-p:0.23868529498577118
epoch£º473	 i:3 	 global-step:9463	 l-p:0.1345948576927185
epoch£º473	 i:4 	 global-step:9464	 l-p:0.11858481913805008
epoch£º473	 i:5 	 global-step:9465	 l-p:0.15214750170707703
epoch£º473	 i:6 	 global-step:9466	 l-p:0.1179632917046547
epoch£º473	 i:7 	 global-step:9467	 l-p:0.1146380603313446
epoch£º473	 i:8 	 global-step:9468	 l-p:0.12843969464302063
epoch£º473	 i:9 	 global-step:9469	 l-p:0.1340951770544052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5635, 3.5591, 3.5633],
        [3.5635, 3.5635, 3.5635],
        [3.5635, 3.5635, 3.5635],
        [3.5635, 3.4069, 3.4786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.10401637107133865 
model_pd.l_d.mean(): -23.245758056640625 
model_pd.lagr.mean(): -23.141740798950195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2275], device='cuda:0')), ('power', tensor([-23.4733], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.10401637107133865
epoch£º474	 i:1 	 global-step:9481	 l-p:0.14917288720607758
epoch£º474	 i:2 	 global-step:9482	 l-p:0.15053518116474152
epoch£º474	 i:3 	 global-step:9483	 l-p:0.14776182174682617
epoch£º474	 i:4 	 global-step:9484	 l-p:0.08777698129415512
epoch£º474	 i:5 	 global-step:9485	 l-p:0.18154685199260712
epoch£º474	 i:6 	 global-step:9486	 l-p:0.12269236147403717
epoch£º474	 i:7 	 global-step:9487	 l-p:0.1486242711544037
epoch£º474	 i:8 	 global-step:9488	 l-p:0.18143901228904724
epoch£º474	 i:9 	 global-step:9489	 l-p:0.1749763786792755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3500, 3.1402, 3.2095],
        [3.3500, 3.0686, 3.1027],
        [3.3500, 3.3500, 3.3500],
        [3.3500, 3.3500, 3.3500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.1441580355167389 
model_pd.l_d.mean(): -23.177709579467773 
model_pd.lagr.mean(): -23.033552169799805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3489], device='cuda:0')), ('power', tensor([-23.5266], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.1441580355167389
epoch£º475	 i:1 	 global-step:9501	 l-p:0.1558503657579422
epoch£º475	 i:2 	 global-step:9502	 l-p:0.14674651622772217
epoch£º475	 i:3 	 global-step:9503	 l-p:0.06311777234077454
epoch£º475	 i:4 	 global-step:9504	 l-p:0.0900096669793129
epoch£º475	 i:5 	 global-step:9505	 l-p:0.13139697909355164
epoch£º475	 i:6 	 global-step:9506	 l-p:0.16147007048130035
epoch£º475	 i:7 	 global-step:9507	 l-p:0.16853640973567963
epoch£º475	 i:8 	 global-step:9508	 l-p:0.1904134303331375
epoch£º475	 i:9 	 global-step:9509	 l-p:0.09068499505519867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4367, 3.1191, 2.6981],
        [3.4367, 3.4367, 3.4367],
        [3.4367, 3.0831, 2.6622],
        [3.4367, 3.3039, 3.3760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.12763680517673492 
model_pd.l_d.mean(): -23.430919647216797 
model_pd.lagr.mean(): -23.30328369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2332], device='cuda:0')), ('power', tensor([-23.6641], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.12763680517673492
epoch£º476	 i:1 	 global-step:9521	 l-p:0.13603490591049194
epoch£º476	 i:2 	 global-step:9522	 l-p:0.1707245409488678
epoch£º476	 i:3 	 global-step:9523	 l-p:0.1408008337020874
epoch£º476	 i:4 	 global-step:9524	 l-p:0.14873552322387695
epoch£º476	 i:5 	 global-step:9525	 l-p:0.12608124315738678
epoch£º476	 i:6 	 global-step:9526	 l-p:0.03664328157901764
epoch£º476	 i:7 	 global-step:9527	 l-p:0.12792903184890747
epoch£º476	 i:8 	 global-step:9528	 l-p:0.163572758436203
epoch£º476	 i:9 	 global-step:9529	 l-p:0.12681272625923157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4593, 3.4572, 3.4592],
        [3.4593, 3.0241, 2.6528],
        [3.4593, 3.3466, 3.4142],
        [3.4593, 3.1234, 3.0991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.1284274160861969 
model_pd.l_d.mean(): -22.687410354614258 
model_pd.lagr.mean(): -22.558982849121094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2991], device='cuda:0')), ('power', tensor([-22.9865], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.1284274160861969
epoch£º477	 i:1 	 global-step:9541	 l-p:0.13939151167869568
epoch£º477	 i:2 	 global-step:9542	 l-p:0.11312690377235413
epoch£º477	 i:3 	 global-step:9543	 l-p:0.14215584099292755
epoch£º477	 i:4 	 global-step:9544	 l-p:0.11628169566392899
epoch£º477	 i:5 	 global-step:9545	 l-p:0.10943843424320221
epoch£º477	 i:6 	 global-step:9546	 l-p:-0.08492089807987213
epoch£º477	 i:7 	 global-step:9547	 l-p:0.11002177745103836
epoch£º477	 i:8 	 global-step:9548	 l-p:0.1691885143518448
epoch£º477	 i:9 	 global-step:9549	 l-p:0.16660523414611816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3181, 3.3181, 3.3181],
        [3.3181, 3.0138, 2.5843],
        [3.3181, 3.3181, 3.3181],
        [3.3181, 2.8855, 2.7227]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.0489148311316967 
model_pd.l_d.mean(): -22.341930389404297 
model_pd.lagr.mean(): -22.29301643371582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-22.7638], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.0489148311316967
epoch£º478	 i:1 	 global-step:9561	 l-p:0.1554691344499588
epoch£º478	 i:2 	 global-step:9562	 l-p:0.3824005722999573
epoch£º478	 i:3 	 global-step:9563	 l-p:0.14856895804405212
epoch£º478	 i:4 	 global-step:9564	 l-p:0.10885506123304367
epoch£º478	 i:5 	 global-step:9565	 l-p:0.1284232884645462
epoch£º478	 i:6 	 global-step:9566	 l-p:0.18909144401550293
epoch£º478	 i:7 	 global-step:9567	 l-p:0.1350102573633194
epoch£º478	 i:8 	 global-step:9568	 l-p:0.11181389540433884
epoch£º478	 i:9 	 global-step:9569	 l-p:0.1433420181274414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5244, 3.2669, 2.8530],
        [3.5244, 3.4872, 3.5179],
        [3.5244, 3.1132, 2.9511],
        [3.5244, 3.3421, 3.4147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.14047713577747345 
model_pd.l_d.mean(): -23.00619125366211 
model_pd.lagr.mean(): -22.86571502685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2651], device='cuda:0')), ('power', tensor([-23.2712], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.14047713577747345
epoch£º479	 i:1 	 global-step:9581	 l-p:0.14687544107437134
epoch£º479	 i:2 	 global-step:9582	 l-p:0.1307438611984253
epoch£º479	 i:3 	 global-step:9583	 l-p:0.03796377778053284
epoch£º479	 i:4 	 global-step:9584	 l-p:0.1364355832338333
epoch£º479	 i:5 	 global-step:9585	 l-p:0.13496820628643036
epoch£º479	 i:6 	 global-step:9586	 l-p:0.20675598084926605
epoch£º479	 i:7 	 global-step:9587	 l-p:0.1385287195444107
epoch£º479	 i:8 	 global-step:9588	 l-p:0.1487845927476883
epoch£º479	 i:9 	 global-step:9589	 l-p:0.1711670458316803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4048, 3.4047, 3.4048],
        [3.4048, 3.1287, 3.1688],
        [3.4048, 3.4047, 3.4048],
        [3.4048, 3.2473, 3.3234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.2266799807548523 
model_pd.l_d.mean(): -22.70677375793457 
model_pd.lagr.mean(): -22.480093002319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2988], device='cuda:0')), ('power', tensor([-23.0056], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.2266799807548523
epoch£º480	 i:1 	 global-step:9601	 l-p:0.13214059174060822
epoch£º480	 i:2 	 global-step:9602	 l-p:0.17161384224891663
epoch£º480	 i:3 	 global-step:9603	 l-p:0.09883684664964676
epoch£º480	 i:4 	 global-step:9604	 l-p:0.17153167724609375
epoch£º480	 i:5 	 global-step:9605	 l-p:0.14671291410923004
epoch£º480	 i:6 	 global-step:9606	 l-p:0.13246208429336548
epoch£º480	 i:7 	 global-step:9607	 l-p:0.13650193810462952
epoch£º480	 i:8 	 global-step:9608	 l-p:0.1537531167268753
epoch£º480	 i:9 	 global-step:9609	 l-p:0.14823810756206512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4917, 3.4082, 3.4656],
        [3.4917, 3.4917, 3.4918],
        [3.4917, 3.4823, 3.4911],
        [3.4917, 3.4917, 3.4918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): -0.10931514203548431 
model_pd.l_d.mean(): -23.660816192626953 
model_pd.lagr.mean(): -23.770132064819336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1939], device='cuda:0')), ('power', tensor([-23.8547], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:-0.10931514203548431
epoch£º481	 i:1 	 global-step:9621	 l-p:0.126502126455307
epoch£º481	 i:2 	 global-step:9622	 l-p:0.13196854293346405
epoch£º481	 i:3 	 global-step:9623	 l-p:0.13408084213733673
epoch£º481	 i:4 	 global-step:9624	 l-p:0.17395293712615967
epoch£º481	 i:5 	 global-step:9625	 l-p:0.13195982575416565
epoch£º481	 i:6 	 global-step:9626	 l-p:0.1167604997754097
epoch£º481	 i:7 	 global-step:9627	 l-p:0.12138919532299042
epoch£º481	 i:8 	 global-step:9628	 l-p:0.17404305934906006
epoch£º481	 i:9 	 global-step:9629	 l-p:0.17652152478694916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4455, 3.3930, 3.4340],
        [3.4455, 3.3236, 3.3945],
        [3.4455, 3.0428, 2.9286],
        [3.4455, 2.9948, 2.6191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.1620195209980011 
model_pd.l_d.mean(): -23.694419860839844 
model_pd.lagr.mean(): -23.532400131225586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1845], device='cuda:0')), ('power', tensor([-23.8789], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.1620195209980011
epoch£º482	 i:1 	 global-step:9641	 l-p:0.18014420568943024
epoch£º482	 i:2 	 global-step:9642	 l-p:0.22641994059085846
epoch£º482	 i:3 	 global-step:9643	 l-p:0.14250244200229645
epoch£º482	 i:4 	 global-step:9644	 l-p:0.15773043036460876
epoch£º482	 i:5 	 global-step:9645	 l-p:0.11707078665494919
epoch£º482	 i:6 	 global-step:9646	 l-p:-0.36818525195121765
epoch£º482	 i:7 	 global-step:9647	 l-p:0.12649816274642944
epoch£º482	 i:8 	 global-step:9648	 l-p:0.12093902379274368
epoch£º482	 i:9 	 global-step:9649	 l-p:0.1335935890674591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5397, 3.5397, 3.5397],
        [3.5397, 3.1045, 2.7307],
        [3.5397, 3.5387, 3.5397],
        [3.5397, 3.5397, 3.5397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.12663908302783966 
model_pd.l_d.mean(): -22.2818546295166 
model_pd.lagr.mean(): -22.155216217041016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3138], device='cuda:0')), ('power', tensor([-22.5957], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.12663908302783966
epoch£º483	 i:1 	 global-step:9661	 l-p:0.12953418493270874
epoch£º483	 i:2 	 global-step:9662	 l-p:0.14237001538276672
epoch£º483	 i:3 	 global-step:9663	 l-p:0.10989629477262497
epoch£º483	 i:4 	 global-step:9664	 l-p:0.131476029753685
epoch£º483	 i:5 	 global-step:9665	 l-p:0.14120200276374817
epoch£º483	 i:6 	 global-step:9666	 l-p:0.11879431456327438
epoch£º483	 i:7 	 global-step:9667	 l-p:0.3483339250087738
epoch£º483	 i:8 	 global-step:9668	 l-p:0.14533229172229767
epoch£º483	 i:9 	 global-step:9669	 l-p:0.17539946734905243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[3.4700, 3.0710, 2.6448],
        [3.4700, 3.0735, 2.9716],
        [3.4700, 3.0229, 2.7953],
        [3.4700, 3.0832, 2.6536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.12229606509208679 
model_pd.l_d.mean(): -22.88559913635254 
model_pd.lagr.mean(): -22.763303756713867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3420], device='cuda:0')), ('power', tensor([-23.2276], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.12229606509208679
epoch£º484	 i:1 	 global-step:9681	 l-p:0.14155380427837372
epoch£º484	 i:2 	 global-step:9682	 l-p:0.16123877465724945
epoch£º484	 i:3 	 global-step:9683	 l-p:0.1343715637922287
epoch£º484	 i:4 	 global-step:9684	 l-p:0.4127269685268402
epoch£º484	 i:5 	 global-step:9685	 l-p:0.32817569375038147
epoch£º484	 i:6 	 global-step:9686	 l-p:0.13281460106372833
epoch£º484	 i:7 	 global-step:9687	 l-p:0.13402274250984192
epoch£º484	 i:8 	 global-step:9688	 l-p:0.1466977298259735
epoch£º484	 i:9 	 global-step:9689	 l-p:0.13083148002624512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3936, 3.3155, 3.3709],
        [3.3936, 3.1678, 3.2364],
        [3.3936, 3.3936, 3.3936],
        [3.3936, 3.3936, 3.3936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.13744710385799408 
model_pd.l_d.mean(): -23.856779098510742 
model_pd.lagr.mean(): -23.719331741333008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1544], device='cuda:0')), ('power', tensor([-24.0112], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.13744710385799408
epoch£º485	 i:1 	 global-step:9701	 l-p:0.27131086587905884
epoch£º485	 i:2 	 global-step:9702	 l-p:0.125746950507164
epoch£º485	 i:3 	 global-step:9703	 l-p:0.13274815678596497
epoch£º485	 i:4 	 global-step:9704	 l-p:0.11388550698757172
epoch£º485	 i:5 	 global-step:9705	 l-p:0.1340366005897522
epoch£º485	 i:6 	 global-step:9706	 l-p:0.16600480675697327
epoch£º485	 i:7 	 global-step:9707	 l-p:0.20058920979499817
epoch£º485	 i:8 	 global-step:9708	 l-p:0.13634224236011505
epoch£º485	 i:9 	 global-step:9709	 l-p:0.15523779392242432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3027, 2.8249, 2.5694],
        [3.3027, 2.9682, 2.9727],
        [3.3027, 2.8570, 2.4256],
        [3.3027, 3.1524, 3.2303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.13661716878414154 
model_pd.l_d.mean(): -23.16474151611328 
model_pd.lagr.mean(): -23.02812385559082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3766], device='cuda:0')), ('power', tensor([-23.5414], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.13661716878414154
epoch£º486	 i:1 	 global-step:9721	 l-p:0.02319449931383133
epoch£º486	 i:2 	 global-step:9722	 l-p:0.13429689407348633
epoch£º486	 i:3 	 global-step:9723	 l-p:0.16234178841114044
epoch£º486	 i:4 	 global-step:9724	 l-p:0.252365380525589
epoch£º486	 i:5 	 global-step:9725	 l-p:0.11898873001337051
epoch£º486	 i:6 	 global-step:9726	 l-p:0.1466793566942215
epoch£º486	 i:7 	 global-step:9727	 l-p:0.16490574181079865
epoch£º486	 i:8 	 global-step:9728	 l-p:0.1816747635602951
epoch£º486	 i:9 	 global-step:9729	 l-p:0.1554010808467865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5188, 3.1964, 3.1962],
        [3.5188, 3.5184, 3.5188],
        [3.5188, 3.5188, 3.5188],
        [3.5188, 3.0636, 2.7124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.13579466938972473 
model_pd.l_d.mean(): -23.19205093383789 
model_pd.lagr.mean(): -23.056255340576172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2405], device='cuda:0')), ('power', tensor([-23.4326], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.13579466938972473
epoch£º487	 i:1 	 global-step:9741	 l-p:0.12913024425506592
epoch£º487	 i:2 	 global-step:9742	 l-p:0.13134099543094635
epoch£º487	 i:3 	 global-step:9743	 l-p:0.26113808155059814
epoch£º487	 i:4 	 global-step:9744	 l-p:0.1347784847021103
epoch£º487	 i:5 	 global-step:9745	 l-p:0.1421363651752472
epoch£º487	 i:6 	 global-step:9746	 l-p:0.13480642437934875
epoch£º487	 i:7 	 global-step:9747	 l-p:0.13878563046455383
epoch£º487	 i:8 	 global-step:9748	 l-p:0.13005225360393524
epoch£º487	 i:9 	 global-step:9749	 l-p:0.13193397223949432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4005, 3.3979, 3.4004],
        [3.4005, 2.9487, 2.7494],
        [3.4005, 3.3482, 3.3893],
        [3.4005, 3.0538, 2.6107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.13961361348628998 
model_pd.l_d.mean(): -23.401317596435547 
model_pd.lagr.mean(): -23.261703491210938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2833], device='cuda:0')), ('power', tensor([-23.6846], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.13961361348628998
epoch£º488	 i:1 	 global-step:9761	 l-p:0.7105550765991211
epoch£º488	 i:2 	 global-step:9762	 l-p:0.17226608097553253
epoch£º488	 i:3 	 global-step:9763	 l-p:0.12083712220191956
epoch£º488	 i:4 	 global-step:9764	 l-p:0.17054584622383118
epoch£º488	 i:5 	 global-step:9765	 l-p:-2.5794222354888916
epoch£º488	 i:6 	 global-step:9766	 l-p:-0.034007471054792404
epoch£º488	 i:7 	 global-step:9767	 l-p:0.08398318290710449
epoch£º488	 i:8 	 global-step:9768	 l-p:0.07620872557163239
epoch£º488	 i:9 	 global-step:9769	 l-p:0.15452909469604492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4182, 3.3396, 3.3954],
        [3.4182, 3.4175, 3.4182],
        [3.4182, 3.3318, 3.3913],
        [3.4182, 3.4182, 3.4182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.2714020907878876 
model_pd.l_d.mean(): -23.583913803100586 
model_pd.lagr.mean(): -23.312511444091797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2042], device='cuda:0')), ('power', tensor([-23.7881], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.2714020907878876
epoch£º489	 i:1 	 global-step:9781	 l-p:0.1177031472325325
epoch£º489	 i:2 	 global-step:9782	 l-p:0.21485969424247742
epoch£º489	 i:3 	 global-step:9783	 l-p:0.15108469128608704
epoch£º489	 i:4 	 global-step:9784	 l-p:0.1314457505941391
epoch£º489	 i:5 	 global-step:9785	 l-p:0.1391577571630478
epoch£º489	 i:6 	 global-step:9786	 l-p:0.12949016690254211
epoch£º489	 i:7 	 global-step:9787	 l-p:0.1345040649175644
epoch£º489	 i:8 	 global-step:9788	 l-p:0.15163695812225342
epoch£º489	 i:9 	 global-step:9789	 l-p:0.09834961593151093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4433, 3.4433, 3.4433],
        [3.4433, 3.4423, 3.4433],
        [3.4433, 3.4416, 3.4433],
        [3.4433, 3.4433, 3.4433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.1387101262807846 
model_pd.l_d.mean(): -23.65999984741211 
model_pd.lagr.mean(): -23.521289825439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1923], device='cuda:0')), ('power', tensor([-23.8523], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.1387101262807846
epoch£º490	 i:1 	 global-step:9801	 l-p:0.09594395756721497
epoch£º490	 i:2 	 global-step:9802	 l-p:0.14233942329883575
epoch£º490	 i:3 	 global-step:9803	 l-p:0.26542118191719055
epoch£º490	 i:4 	 global-step:9804	 l-p:0.18031981587409973
epoch£º490	 i:5 	 global-step:9805	 l-p:0.17986443638801575
epoch£º490	 i:6 	 global-step:9806	 l-p:0.19626380503177643
epoch£º490	 i:7 	 global-step:9807	 l-p:0.18205618858337402
epoch£º490	 i:8 	 global-step:9808	 l-p:0.15194037556648254
epoch£º490	 i:9 	 global-step:9809	 l-p:0.1262785941362381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5312, 3.2470, 2.8136],
        [3.5312, 3.2258, 3.2451],
        [3.5312, 3.4387, 3.5004],
        [3.5312, 3.1158, 2.6911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.1563069075345993 
model_pd.l_d.mean(): -22.758892059326172 
model_pd.lagr.mean(): -22.602584838867188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2508], device='cuda:0')), ('power', tensor([-23.0096], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.1563069075345993
epoch£º491	 i:1 	 global-step:9821	 l-p:0.12722942233085632
epoch£º491	 i:2 	 global-step:9822	 l-p:0.11660926789045334
epoch£º491	 i:3 	 global-step:9823	 l-p:0.129738911986351
epoch£º491	 i:4 	 global-step:9824	 l-p:0.14969487488269806
epoch£º491	 i:5 	 global-step:9825	 l-p:0.14638729393482208
epoch£º491	 i:6 	 global-step:9826	 l-p:0.1349431723356247
epoch£º491	 i:7 	 global-step:9827	 l-p:-0.015868782997131348
epoch£º491	 i:8 	 global-step:9828	 l-p:0.12662024796009064
epoch£º491	 i:9 	 global-step:9829	 l-p:0.12961514294147491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4431, 3.4430, 3.4431],
        [3.4431, 2.9969, 2.8169],
        [3.4431, 3.4259, 3.4414],
        [3.4431, 3.4305, 3.4420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.23706527054309845 
model_pd.l_d.mean(): -23.607770919799805 
model_pd.lagr.mean(): -23.37070655822754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2456], device='cuda:0')), ('power', tensor([-23.8534], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.23706527054309845
epoch£º492	 i:1 	 global-step:9841	 l-p:0.14162187278270721
epoch£º492	 i:2 	 global-step:9842	 l-p:0.1004304587841034
epoch£º492	 i:3 	 global-step:9843	 l-p:-0.03287544101476669
epoch£º492	 i:4 	 global-step:9844	 l-p:0.1462002694606781
epoch£º492	 i:5 	 global-step:9845	 l-p:-0.028813285753130913
epoch£º492	 i:6 	 global-step:9846	 l-p:0.16131706535816193
epoch£º492	 i:7 	 global-step:9847	 l-p:0.14033891260623932
epoch£º492	 i:8 	 global-step:9848	 l-p:0.20383307337760925
epoch£º492	 i:9 	 global-step:9849	 l-p:0.16744227707386017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4216, 3.2907, 2.8906],
        [3.4216, 3.4210, 3.4216],
        [3.4216, 3.1325, 3.1734],
        [3.4216, 3.1827, 2.7507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.10678866505622864 
model_pd.l_d.mean(): -21.573339462280273 
model_pd.lagr.mean(): -21.466550827026367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4555], device='cuda:0')), ('power', tensor([-22.0289], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.10678866505622864
epoch£º493	 i:1 	 global-step:9861	 l-p:0.12246590107679367
epoch£º493	 i:2 	 global-step:9862	 l-p:0.14189137518405914
epoch£º493	 i:3 	 global-step:9863	 l-p:0.12043371051549911
epoch£º493	 i:4 	 global-step:9864	 l-p:0.1341570019721985
epoch£º493	 i:5 	 global-step:9865	 l-p:0.25613585114479065
epoch£º493	 i:6 	 global-step:9866	 l-p:0.11899916082620621
epoch£º493	 i:7 	 global-step:9867	 l-p:0.13914713263511658
epoch£º493	 i:8 	 global-step:9868	 l-p:0.15182805061340332
epoch£º493	 i:9 	 global-step:9869	 l-p:0.15282993018627167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4416, 2.9925, 2.8118],
        [3.4416, 3.2708, 2.8575],
        [3.4416, 3.2114, 3.2813],
        [3.4416, 3.4008, 3.4344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.12335530668497086 
model_pd.l_d.mean(): -22.019559860229492 
model_pd.lagr.mean(): -21.896203994750977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-22.4486], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.12335530668497086
epoch£º494	 i:1 	 global-step:9881	 l-p:0.1517271101474762
epoch£º494	 i:2 	 global-step:9882	 l-p:0.19710037112236023
epoch£º494	 i:3 	 global-step:9883	 l-p:2.3622183799743652
epoch£º494	 i:4 	 global-step:9884	 l-p:0.1506505012512207
epoch£º494	 i:5 	 global-step:9885	 l-p:0.1765059381723404
epoch£º494	 i:6 	 global-step:9886	 l-p:-0.06660334765911102
epoch£º494	 i:7 	 global-step:9887	 l-p:0.14805588126182556
epoch£º494	 i:8 	 global-step:9888	 l-p:0.5828800201416016
epoch£º494	 i:9 	 global-step:9889	 l-p:0.1552691012620926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4277, 3.1511, 3.2011],
        [3.4277, 3.4264, 3.4277],
        [3.4277, 3.0508, 2.5984],
        [3.4277, 3.4017, 3.4243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.14026488363742828 
model_pd.l_d.mean(): -23.43014907836914 
model_pd.lagr.mean(): -23.289884567260742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2453], device='cuda:0')), ('power', tensor([-23.6755], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.14026488363742828
epoch£º495	 i:1 	 global-step:9901	 l-p:0.1366453766822815
epoch£º495	 i:2 	 global-step:9902	 l-p:0.14870314300060272
epoch£º495	 i:3 	 global-step:9903	 l-p:0.13914838433265686
epoch£º495	 i:4 	 global-step:9904	 l-p:-0.020438766106963158
epoch£º495	 i:5 	 global-step:9905	 l-p:0.13216708600521088
epoch£º495	 i:6 	 global-step:9906	 l-p:-0.10084579139947891
epoch£º495	 i:7 	 global-step:9907	 l-p:0.1370386779308319
epoch£º495	 i:8 	 global-step:9908	 l-p:0.158209890127182
epoch£º495	 i:9 	 global-step:9909	 l-p:0.13458296656608582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3150, 3.3150, 3.3150],
        [3.3150, 3.1548, 3.2357],
        [3.3150, 3.3150, 3.3150],
        [3.3150, 2.9093, 2.4489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.15552687644958496 
model_pd.l_d.mean(): -23.5390625 
model_pd.lagr.mean(): -23.383535385131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2810], device='cuda:0')), ('power', tensor([-23.8200], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.15552687644958496
epoch£º496	 i:1 	 global-step:9921	 l-p:0.14184671640396118
epoch£º496	 i:2 	 global-step:9922	 l-p:0.10774686932563782
epoch£º496	 i:3 	 global-step:9923	 l-p:-1.1263929605484009
epoch£º496	 i:4 	 global-step:9924	 l-p:-0.11822036653757095
epoch£º496	 i:5 	 global-step:9925	 l-p:0.11031407862901688
epoch£º496	 i:6 	 global-step:9926	 l-p:0.13879084587097168
epoch£º496	 i:7 	 global-step:9927	 l-p:-0.7369071841239929
epoch£º496	 i:8 	 global-step:9928	 l-p:0.15959583222866058
epoch£º496	 i:9 	 global-step:9929	 l-p:0.09613408893346786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4979, 3.0667, 2.9242],
        [3.4979, 3.4979, 3.4979],
        [3.4979, 3.4979, 3.4979],
        [3.4979, 3.4944, 3.4978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.13155387341976166 
model_pd.l_d.mean(): -23.332027435302734 
model_pd.lagr.mean(): -23.20047378540039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2647], device='cuda:0')), ('power', tensor([-23.5968], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.13155387341976166
epoch£º497	 i:1 	 global-step:9941	 l-p:0.16666725277900696
epoch£º497	 i:2 	 global-step:9942	 l-p:0.14423151314258575
epoch£º497	 i:3 	 global-step:9943	 l-p:0.14983800053596497
epoch£º497	 i:4 	 global-step:9944	 l-p:0.12283606082201004
epoch£º497	 i:5 	 global-step:9945	 l-p:0.12889981269836426
epoch£º497	 i:6 	 global-step:9946	 l-p:0.15922439098358154
epoch£º497	 i:7 	 global-step:9947	 l-p:0.13827133178710938
epoch£º497	 i:8 	 global-step:9948	 l-p:0.1468736231327057
epoch£º497	 i:9 	 global-step:9949	 l-p:0.08629091829061508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4861, 3.3758, 3.4450],
        [3.4861, 3.1744, 2.7249],
        [3.4861, 3.0410, 2.6076],
        [3.4861, 3.4695, 3.4845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.15649005770683289 
model_pd.l_d.mean(): -23.359750747680664 
model_pd.lagr.mean(): -23.20326042175293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2466], device='cuda:0')), ('power', tensor([-23.6064], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.15649005770683289
epoch£º498	 i:1 	 global-step:9961	 l-p:0.16875506937503815
epoch£º498	 i:2 	 global-step:9962	 l-p:0.11940442770719528
epoch£º498	 i:3 	 global-step:9963	 l-p:0.13521265983581543
epoch£º498	 i:4 	 global-step:9964	 l-p:0.10000012069940567
epoch£º498	 i:5 	 global-step:9965	 l-p:0.11868558824062347
epoch£º498	 i:6 	 global-step:9966	 l-p:0.14132004976272583
epoch£º498	 i:7 	 global-step:9967	 l-p:0.16506950557231903
epoch£º498	 i:8 	 global-step:9968	 l-p:0.379077285528183
epoch£º498	 i:9 	 global-step:9969	 l-p:0.2227591723203659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4489, 3.4472, 3.4488],
        [3.4489, 3.1075, 2.6518],
        [3.4489, 3.4489, 3.4489],
        [3.4489, 3.4489, 3.4489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.14997775852680206 
model_pd.l_d.mean(): -23.759157180786133 
model_pd.lagr.mean(): -23.60917854309082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1569], device='cuda:0')), ('power', tensor([-23.9161], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.14997775852680206
epoch£º499	 i:1 	 global-step:9981	 l-p:0.0835513323545456
epoch£º499	 i:2 	 global-step:9982	 l-p:0.16954579949378967
epoch£º499	 i:3 	 global-step:9983	 l-p:0.13987740874290466
epoch£º499	 i:4 	 global-step:9984	 l-p:0.1511036902666092
epoch£º499	 i:5 	 global-step:9985	 l-p:0.2124360054731369
epoch£º499	 i:6 	 global-step:9986	 l-p:0.12264695018529892
epoch£º499	 i:7 	 global-step:9987	 l-p:0.16526275873184204
epoch£º499	 i:8 	 global-step:9988	 l-p:0.13259239494800568
epoch£º499	 i:9 	 global-step:9989	 l-p:0.12829773128032684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4842, 3.2726, 3.3488],
        [3.4842, 3.0510, 2.9146],
        [3.4842, 3.3734, 3.4429],
        [3.4842, 3.0133, 2.6028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.18663573265075684 
model_pd.l_d.mean(): -23.682180404663086 
model_pd.lagr.mean(): -23.49554443359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2063], device='cuda:0')), ('power', tensor([-23.8885], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.18663573265075684
epoch£º500	 i:1 	 global-step:10001	 l-p:0.12944339215755463
epoch£º500	 i:2 	 global-step:10002	 l-p:0.20439402759075165
epoch£º500	 i:3 	 global-step:10003	 l-p:0.11929954588413239
epoch£º500	 i:4 	 global-step:10004	 l-p:0.11966563761234283
epoch£º500	 i:5 	 global-step:10005	 l-p:0.1271481066942215
epoch£º500	 i:6 	 global-step:10006	 l-p:0.12331560999155045
epoch£º500	 i:7 	 global-step:10007	 l-p:0.1448855698108673
epoch£º500	 i:8 	 global-step:10008	 l-p:0.23758482933044434
epoch£º500	 i:9 	 global-step:10009	 l-p:0.1408577263355255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4279, 3.1599, 3.2184],
        [3.4279, 2.9794, 2.8259],
        [3.4279, 2.9581, 2.7524],
        [3.4279, 3.3652, 3.4130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.4719289541244507 
model_pd.l_d.mean(): -23.132667541503906 
model_pd.lagr.mean(): -22.660737991333008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3021], device='cuda:0')), ('power', tensor([-23.4348], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.4719289541244507
epoch£º501	 i:1 	 global-step:10021	 l-p:0.16260875761508942
epoch£º501	 i:2 	 global-step:10022	 l-p:-0.10826772451400757
epoch£º501	 i:3 	 global-step:10023	 l-p:0.11437242478132248
epoch£º501	 i:4 	 global-step:10024	 l-p:0.14604778587818146
epoch£º501	 i:5 	 global-step:10025	 l-p:-0.0806654691696167
epoch£º501	 i:6 	 global-step:10026	 l-p:0.12142189592123032
epoch£º501	 i:7 	 global-step:10027	 l-p:0.14187172055244446
epoch£º501	 i:8 	 global-step:10028	 l-p:-0.8712847828865051
epoch£º501	 i:9 	 global-step:10029	 l-p:0.1429293304681778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3560, 2.9535, 2.8949],
        [3.3560, 3.3560, 3.3560],
        [3.3560, 2.9796, 2.5125],
        [3.3560, 3.3555, 3.3560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.1511608511209488 
model_pd.l_d.mean(): -22.871326446533203 
model_pd.lagr.mean(): -22.720165252685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3331], device='cuda:0')), ('power', tensor([-23.2045], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.1511608511209488
epoch£º502	 i:1 	 global-step:10041	 l-p:0.1886458694934845
epoch£º502	 i:2 	 global-step:10042	 l-p:0.1782040148973465
epoch£º502	 i:3 	 global-step:10043	 l-p:0.15035615861415863
epoch£º502	 i:4 	 global-step:10044	 l-p:0.08319582790136337
epoch£º502	 i:5 	 global-step:10045	 l-p:0.14377252757549286
epoch£º502	 i:6 	 global-step:10046	 l-p:0.1541341245174408
epoch£º502	 i:7 	 global-step:10047	 l-p:0.3045758605003357
epoch£º502	 i:8 	 global-step:10048	 l-p:0.1330772042274475
epoch£º502	 i:9 	 global-step:10049	 l-p:0.090679831802845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4925, 3.4924, 3.4925],
        [3.4925, 3.1365, 2.6762],
        [3.4925, 3.4926, 3.4926],
        [3.4925, 3.4384, 3.4809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.0854964628815651 
model_pd.l_d.mean(): -22.971609115600586 
model_pd.lagr.mean(): -22.886112213134766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2404], device='cuda:0')), ('power', tensor([-23.2120], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.0854964628815651
epoch£º503	 i:1 	 global-step:10061	 l-p:0.17098844051361084
epoch£º503	 i:2 	 global-step:10062	 l-p:0.1435966044664383
epoch£º503	 i:3 	 global-step:10063	 l-p:0.12160850316286087
epoch£º503	 i:4 	 global-step:10064	 l-p:0.12028850615024567
epoch£º503	 i:5 	 global-step:10065	 l-p:0.13281244039535522
epoch£º503	 i:6 	 global-step:10066	 l-p:0.12814700603485107
epoch£º503	 i:7 	 global-step:10067	 l-p:0.1410737782716751
epoch£º503	 i:8 	 global-step:10068	 l-p:0.12077274918556213
epoch£º503	 i:9 	 global-step:10069	 l-p:0.16003254055976868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4866, 3.4619, 3.4835],
        [3.4866, 3.0390, 2.8841],
        [3.4866, 3.4174, 3.4689],
        [3.4866, 3.2230, 3.2835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.18446725606918335 
model_pd.l_d.mean(): -23.386476516723633 
model_pd.lagr.mean(): -23.202009201049805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2537], device='cuda:0')), ('power', tensor([-23.6402], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.18446725606918335
epoch£º504	 i:1 	 global-step:10081	 l-p:0.10868530720472336
epoch£º504	 i:2 	 global-step:10082	 l-p:0.1441129893064499
epoch£º504	 i:3 	 global-step:10083	 l-p:0.5129427313804626
epoch£º504	 i:4 	 global-step:10084	 l-p:0.15276236832141876
epoch£º504	 i:5 	 global-step:10085	 l-p:0.1782151758670807
epoch£º504	 i:6 	 global-step:10086	 l-p:0.13327616453170776
epoch£º504	 i:7 	 global-step:10087	 l-p:0.1987045258283615
epoch£º504	 i:8 	 global-step:10088	 l-p:0.15494433045387268
epoch£º504	 i:9 	 global-step:10089	 l-p:0.14553573727607727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3888, 3.0714, 3.1038],
        [3.3888, 3.2675, 3.3415],
        [3.3888, 3.3789, 3.3881],
        [3.3888, 3.2178, 3.3005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.17169837653636932 
model_pd.l_d.mean(): -23.54684066772461 
model_pd.lagr.mean(): -23.37514305114746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2580], device='cuda:0')), ('power', tensor([-23.8048], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.17169837653636932
epoch£º505	 i:1 	 global-step:10101	 l-p:0.06625460833311081
epoch£º505	 i:2 	 global-step:10102	 l-p:-0.05054950714111328
epoch£º505	 i:3 	 global-step:10103	 l-p:0.15129509568214417
epoch£º505	 i:4 	 global-step:10104	 l-p:0.14775502681732178
epoch£º505	 i:5 	 global-step:10105	 l-p:0.13233226537704468
epoch£º505	 i:6 	 global-step:10106	 l-p:0.14701707661151886
epoch£º505	 i:7 	 global-step:10107	 l-p:0.13692344725131989
epoch£º505	 i:8 	 global-step:10108	 l-p:-0.1860971748828888
epoch£º505	 i:9 	 global-step:10109	 l-p:0.14046765863895416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5903, 3.5658, 3.5873],
        [3.5903, 3.4682, 3.0638],
        [3.5903, 3.3690, 2.9336],
        [3.5903, 3.1344, 2.9350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.12292293459177017 
model_pd.l_d.mean(): -22.924646377563477 
model_pd.lagr.mean(): -22.80172348022461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2586], device='cuda:0')), ('power', tensor([-23.1833], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.12292293459177017
epoch£º506	 i:1 	 global-step:10121	 l-p:0.14566516876220703
epoch£º506	 i:2 	 global-step:10122	 l-p:0.1395656168460846
epoch£º506	 i:3 	 global-step:10123	 l-p:0.17307601869106293
epoch£º506	 i:4 	 global-step:10124	 l-p:0.15491743385791779
epoch£º506	 i:5 	 global-step:10125	 l-p:0.10831408202648163
epoch£º506	 i:6 	 global-step:10126	 l-p:0.1373641937971115
epoch£º506	 i:7 	 global-step:10127	 l-p:0.1086442843079567
epoch£º506	 i:8 	 global-step:10128	 l-p:0.12401412427425385
epoch£º506	 i:9 	 global-step:10129	 l-p:0.14185145497322083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4593, 3.3490, 3.4193],
        [3.4593, 3.0968, 3.0845],
        [3.4593, 3.0407, 2.9531],
        [3.4593, 3.2482, 2.8090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.1559390425682068 
model_pd.l_d.mean(): -23.418289184570312 
model_pd.lagr.mean(): -23.26235008239746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2701], device='cuda:0')), ('power', tensor([-23.6884], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.1559390425682068
epoch£º507	 i:1 	 global-step:10141	 l-p:0.1541813313961029
epoch£º507	 i:2 	 global-step:10142	 l-p:0.14140430092811584
epoch£º507	 i:3 	 global-step:10143	 l-p:0.045685119926929474
epoch£º507	 i:4 	 global-step:10144	 l-p:0.15776507556438446
epoch£º507	 i:5 	 global-step:10145	 l-p:0.11934128403663635
epoch£º507	 i:6 	 global-step:10146	 l-p:0.10086670517921448
epoch£º507	 i:7 	 global-step:10147	 l-p:0.24360224604606628
epoch£º507	 i:8 	 global-step:10148	 l-p:-0.09874832630157471
epoch£º507	 i:9 	 global-step:10149	 l-p:0.1318330317735672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4710, 3.4157, 3.4591],
        [3.4710, 3.4660, 3.4708],
        [3.4710, 3.4697, 3.4710],
        [3.4710, 2.9662, 2.6196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.0879800096154213 
model_pd.l_d.mean(): -22.81934356689453 
model_pd.lagr.mean(): -22.73136329650879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3157], device='cuda:0')), ('power', tensor([-23.1350], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.0879800096154213
epoch£º508	 i:1 	 global-step:10161	 l-p:0.13890869915485382
epoch£º508	 i:2 	 global-step:10162	 l-p:0.15699982643127441
epoch£º508	 i:3 	 global-step:10163	 l-p:0.15679524838924408
epoch£º508	 i:4 	 global-step:10164	 l-p:0.15257743000984192
epoch£º508	 i:5 	 global-step:10165	 l-p:0.12878946959972382
epoch£º508	 i:6 	 global-step:10166	 l-p:0.152339369058609
epoch£º508	 i:7 	 global-step:10167	 l-p:0.11408872157335281
epoch£º508	 i:8 	 global-step:10168	 l-p:0.15042239427566528
epoch£º508	 i:9 	 global-step:10169	 l-p:0.13187694549560547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5040, 3.0718, 2.6100],
        [3.5040, 3.5030, 3.5040],
        [3.5040, 3.3609, 2.9421],
        [3.5040, 3.4077, 3.4725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.18303810060024261 
model_pd.l_d.mean(): -23.0871524810791 
model_pd.lagr.mean(): -22.90411376953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2628], device='cuda:0')), ('power', tensor([-23.3500], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.18303810060024261
epoch£º509	 i:1 	 global-step:10181	 l-p:0.18026107549667358
epoch£º509	 i:2 	 global-step:10182	 l-p:0.1276559680700302
epoch£º509	 i:3 	 global-step:10183	 l-p:0.14255304634571075
epoch£º509	 i:4 	 global-step:10184	 l-p:0.0729418620467186
epoch£º509	 i:5 	 global-step:10185	 l-p:0.13105781376361847
epoch£º509	 i:6 	 global-step:10186	 l-p:0.12265588343143463
epoch£º509	 i:7 	 global-step:10187	 l-p:0.24933692812919617
epoch£º509	 i:8 	 global-step:10188	 l-p:0.11478172242641449
epoch£º509	 i:9 	 global-step:10189	 l-p:0.15248502790927887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4257, 3.4226, 3.4256],
        [3.4257, 2.9342, 2.4932],
        [3.4257, 2.9481, 2.4945],
        [3.4257, 2.9099, 2.5520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.1258804053068161 
model_pd.l_d.mean(): -22.306808471679688 
model_pd.lagr.mean(): -22.180927276611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2775], device='cuda:0')), ('power', tensor([-22.5843], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.1258804053068161
epoch£º510	 i:1 	 global-step:10201	 l-p:-0.09656284004449844
epoch£º510	 i:2 	 global-step:10202	 l-p:0.14188458025455475
epoch£º510	 i:3 	 global-step:10203	 l-p:0.28320619463920593
epoch£º510	 i:4 	 global-step:10204	 l-p:0.13624612987041473
epoch£º510	 i:5 	 global-step:10205	 l-p:0.14635774493217468
epoch£º510	 i:6 	 global-step:10206	 l-p:0.15559789538383484
epoch£º510	 i:7 	 global-step:10207	 l-p:0.13667353987693787
epoch£º510	 i:8 	 global-step:10208	 l-p:0.16507388651371002
epoch£º510	 i:9 	 global-step:10209	 l-p:-7.142081260681152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3476, 3.1621, 3.2473],
        [3.3476, 2.8264, 2.4037],
        [3.3476, 3.2678, 3.3256],
        [3.3476, 3.3476, 3.3476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.04431072995066643 
model_pd.l_d.mean(): -23.24002456665039 
model_pd.lagr.mean(): -23.19571304321289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3715], device='cuda:0')), ('power', tensor([-23.6115], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.04431072995066643
epoch£º511	 i:1 	 global-step:10221	 l-p:0.09874306619167328
epoch£º511	 i:2 	 global-step:10222	 l-p:0.12507884204387665
epoch£º511	 i:3 	 global-step:10223	 l-p:0.13559874892234802
epoch£º511	 i:4 	 global-step:10224	 l-p:0.13279147446155548
epoch£º511	 i:5 	 global-step:10225	 l-p:0.15172803401947021
epoch£º511	 i:6 	 global-step:10226	 l-p:0.12778015434741974
epoch£º511	 i:7 	 global-step:10227	 l-p:0.13816116750240326
epoch£º511	 i:8 	 global-step:10228	 l-p:0.1648053377866745
epoch£º511	 i:9 	 global-step:10229	 l-p:0.057285528630018234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4374, 3.4116, 3.4342],
        [3.4374, 3.3392, 3.4053],
        [3.4374, 2.9254, 2.5154],
        [3.4374, 3.1617, 3.2235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.14833533763885498 
model_pd.l_d.mean(): -23.553668975830078 
model_pd.lagr.mean(): -23.40533447265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2239], device='cuda:0')), ('power', tensor([-23.7776], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.14833533763885498
epoch£º512	 i:1 	 global-step:10241	 l-p:-1.310820460319519
epoch£º512	 i:2 	 global-step:10242	 l-p:0.15223386883735657
epoch£º512	 i:3 	 global-step:10243	 l-p:0.1381005495786667
epoch£º512	 i:4 	 global-step:10244	 l-p:0.14850327372550964
epoch£º512	 i:5 	 global-step:10245	 l-p:0.19878670573234558
epoch£º512	 i:6 	 global-step:10246	 l-p:0.1211940199136734
epoch£º512	 i:7 	 global-step:10247	 l-p:0.13453949987888336
epoch£º512	 i:8 	 global-step:10248	 l-p:0.23150917887687683
epoch£º512	 i:9 	 global-step:10249	 l-p:0.1634751409292221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4882, 3.1307, 3.1292],
        [3.4882, 3.1227, 2.6474],
        [3.4882, 3.1680, 3.1999],
        [3.4882, 3.4835, 3.4880]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.17322146892547607 
model_pd.l_d.mean(): -23.42455291748047 
model_pd.lagr.mean(): -23.251331329345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2476], device='cuda:0')), ('power', tensor([-23.6721], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.17322146892547607
epoch£º513	 i:1 	 global-step:10261	 l-p:0.17804482579231262
epoch£º513	 i:2 	 global-step:10262	 l-p:0.1244901716709137
epoch£º513	 i:3 	 global-step:10263	 l-p:0.15935185551643372
epoch£º513	 i:4 	 global-step:10264	 l-p:0.12801580131053925
epoch£º513	 i:5 	 global-step:10265	 l-p:0.14439022541046143
epoch£º513	 i:6 	 global-step:10266	 l-p:-1.8808023929595947
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12307466566562653
epoch£º513	 i:8 	 global-step:10268	 l-p:0.12514585256576538
epoch£º513	 i:9 	 global-step:10269	 l-p:0.1403999924659729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5369, 3.3098, 2.8607],
        [3.5369, 3.1694, 2.6958],
        [3.5369, 3.2488, 3.3006],
        [3.5369, 3.5364, 3.5369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.12795518338680267 
model_pd.l_d.mean(): -23.388517379760742 
model_pd.lagr.mean(): -23.260562896728516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1968], device='cuda:0')), ('power', tensor([-23.5853], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.12795518338680267
epoch£º514	 i:1 	 global-step:10281	 l-p:0.1431392878293991
epoch£º514	 i:2 	 global-step:10282	 l-p:0.15561506152153015
epoch£º514	 i:3 	 global-step:10283	 l-p:2.6414151191711426
epoch£º514	 i:4 	 global-step:10284	 l-p:0.19519977271556854
epoch£º514	 i:5 	 global-step:10285	 l-p:0.13532090187072754
epoch£º514	 i:6 	 global-step:10286	 l-p:0.12070301920175552
epoch£º514	 i:7 	 global-step:10287	 l-p:0.1475646197795868
epoch£º514	 i:8 	 global-step:10288	 l-p:0.1116948276758194
epoch£º514	 i:9 	 global-step:10289	 l-p:0.1447964459657669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3644, 2.9348, 2.4499],
        [3.3644, 2.9090, 2.4273],
        [3.3644, 3.2359, 3.3134],
        [3.3644, 2.8318, 2.4982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): -0.2895638346672058 
model_pd.l_d.mean(): -22.58209800720215 
model_pd.lagr.mean(): -22.871662139892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4023], device='cuda:0')), ('power', tensor([-22.9844], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:-0.2895638346672058
epoch£º515	 i:1 	 global-step:10301	 l-p:0.10471823811531067
epoch£º515	 i:2 	 global-step:10302	 l-p:0.12417546659708023
epoch£º515	 i:3 	 global-step:10303	 l-p:0.16682781279087067
epoch£º515	 i:4 	 global-step:10304	 l-p:0.1417529135942459
epoch£º515	 i:5 	 global-step:10305	 l-p:0.14209769666194916
epoch£º515	 i:6 	 global-step:10306	 l-p:-0.10654766112565994
epoch£º515	 i:7 	 global-step:10307	 l-p:0.18486925959587097
epoch£º515	 i:8 	 global-step:10308	 l-p:0.1914418488740921
epoch£º515	 i:9 	 global-step:10309	 l-p:0.10134490579366684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3791, 3.3582, 3.3768],
        [3.3791, 3.0585, 3.0994],
        [3.3791, 2.9487, 2.4627],
        [3.3791, 3.3400, 3.3727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.6039265394210815 
model_pd.l_d.mean(): -23.169076919555664 
model_pd.lagr.mean(): -22.56515121459961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3622], device='cuda:0')), ('power', tensor([-23.5312], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.6039265394210815
epoch£º516	 i:1 	 global-step:10321	 l-p:0.15675540268421173
epoch£º516	 i:2 	 global-step:10322	 l-p:0.13700200617313385
epoch£º516	 i:3 	 global-step:10323	 l-p:0.47745245695114136
epoch£º516	 i:4 	 global-step:10324	 l-p:0.2902694046497345
epoch£º516	 i:5 	 global-step:10325	 l-p:0.12836484611034393
epoch£º516	 i:6 	 global-step:10326	 l-p:0.17644640803337097
epoch£º516	 i:7 	 global-step:10327	 l-p:0.027079002931714058
epoch£º516	 i:8 	 global-step:10328	 l-p:0.12443750351667404
epoch£º516	 i:9 	 global-step:10329	 l-p:0.12905332446098328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5923, 3.5764, 3.5909],
        [3.5923, 3.1314, 2.9615],
        [3.5923, 3.1516, 3.0251],
        [3.5923, 3.5923, 3.5923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.1362892985343933 
model_pd.l_d.mean(): -23.12972068786621 
model_pd.lagr.mean(): -22.993431091308594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2038], device='cuda:0')), ('power', tensor([-23.3335], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.1362892985343933
epoch£º517	 i:1 	 global-step:10341	 l-p:0.12247676402330399
epoch£º517	 i:2 	 global-step:10342	 l-p:0.13602040708065033
epoch£º517	 i:3 	 global-step:10343	 l-p:0.16025440394878387
epoch£º517	 i:4 	 global-step:10344	 l-p:0.15888507664203644
epoch£º517	 i:5 	 global-step:10345	 l-p:0.12285419553518295
epoch£º517	 i:6 	 global-step:10346	 l-p:1.5718642473220825
epoch£º517	 i:7 	 global-step:10347	 l-p:0.17110615968704224
epoch£º517	 i:8 	 global-step:10348	 l-p:0.044386252760887146
epoch£º517	 i:9 	 global-step:10349	 l-p:0.15389809012413025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[3.4234, 2.9289, 2.4650],
        [3.4234, 2.9046, 2.6261],
        [3.4234, 3.2130, 3.2974],
        [3.4234, 3.0468, 3.0367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.12640054523944855 
model_pd.l_d.mean(): -23.374893188476562 
model_pd.lagr.mean(): -23.248493194580078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2990], device='cuda:0')), ('power', tensor([-23.6739], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.12640054523944855
epoch£º518	 i:1 	 global-step:10361	 l-p:0.038789767771959305
epoch£º518	 i:2 	 global-step:10362	 l-p:0.037161074578762054
epoch£º518	 i:3 	 global-step:10363	 l-p:0.1347225457429886
epoch£º518	 i:4 	 global-step:10364	 l-p:0.21943289041519165
epoch£º518	 i:5 	 global-step:10365	 l-p:0.15138468146324158
epoch£º518	 i:6 	 global-step:10366	 l-p:0.09132061898708344
epoch£º518	 i:7 	 global-step:10367	 l-p:0.13630937039852142
epoch£º518	 i:8 	 global-step:10368	 l-p:0.13040044903755188
epoch£º518	 i:9 	 global-step:10369	 l-p:0.19099071621894836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5132, 3.5070, 3.5129],
        [3.5132, 3.3181, 2.8727],
        [3.5132, 3.5065, 3.5129],
        [3.5132, 3.3436, 3.4276]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.13610394299030304 
model_pd.l_d.mean(): -23.431867599487305 
model_pd.lagr.mean(): -23.29576301574707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2155], device='cuda:0')), ('power', tensor([-23.6473], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.13610394299030304
epoch£º519	 i:1 	 global-step:10381	 l-p:0.17066419124603271
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11427059024572372
epoch£º519	 i:3 	 global-step:10383	 l-p:0.15763680636882782
epoch£º519	 i:4 	 global-step:10384	 l-p:0.30502378940582275
epoch£º519	 i:5 	 global-step:10385	 l-p:0.33025291562080383
epoch£º519	 i:6 	 global-step:10386	 l-p:0.13066913187503815
epoch£º519	 i:7 	 global-step:10387	 l-p:-0.10114067792892456
epoch£º519	 i:8 	 global-step:10388	 l-p:0.12319337576627731
epoch£º519	 i:9 	 global-step:10389	 l-p:0.12476714700460434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4425, 3.4425, 3.4425],
        [3.4425, 3.1474, 2.6745],
        [3.4425, 3.4425, 3.4425],
        [3.4425, 3.4262, 3.4411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.14624691009521484 
model_pd.l_d.mean(): -22.893230438232422 
model_pd.lagr.mean(): -22.74698257446289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2638], device='cuda:0')), ('power', tensor([-23.1570], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.14624691009521484
epoch£º520	 i:1 	 global-step:10401	 l-p:-0.07905731350183487
epoch£º520	 i:2 	 global-step:10402	 l-p:0.12769708037376404
epoch£º520	 i:3 	 global-step:10403	 l-p:0.17247645556926727
epoch£º520	 i:4 	 global-step:10404	 l-p:0.12100361287593842
epoch£º520	 i:5 	 global-step:10405	 l-p:0.2879617214202881
epoch£º520	 i:6 	 global-step:10406	 l-p:0.1350046843290329
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12911128997802734
epoch£º520	 i:8 	 global-step:10408	 l-p:0.13732537627220154
epoch£º520	 i:9 	 global-step:10409	 l-p:0.15547198057174683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5104, 3.0955, 3.0317],
        [3.5104, 3.4472, 3.4957],
        [3.5104, 3.2885, 2.8338],
        [3.5104, 3.1542, 3.1605]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.13715913891792297 
model_pd.l_d.mean(): -23.60100555419922 
model_pd.lagr.mean(): -23.46384620666504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1825], device='cuda:0')), ('power', tensor([-23.7836], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.13715913891792297
epoch£º521	 i:1 	 global-step:10421	 l-p:0.1115267351269722
epoch£º521	 i:2 	 global-step:10422	 l-p:2.7720067501068115
epoch£º521	 i:3 	 global-step:10423	 l-p:0.1489858627319336
epoch£º521	 i:4 	 global-step:10424	 l-p:0.8059892654418945
epoch£º521	 i:5 	 global-step:10425	 l-p:0.07248902320861816
epoch£º521	 i:6 	 global-step:10426	 l-p:0.12226676940917969
epoch£º521	 i:7 	 global-step:10427	 l-p:0.1439305692911148
epoch£º521	 i:8 	 global-step:10428	 l-p:0.2694453001022339
epoch£º521	 i:9 	 global-step:10429	 l-p:0.16389913856983185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4154, 3.4075, 3.4149],
        [3.4154, 3.4154, 3.4154],
        [3.4154, 3.3967, 3.4135],
        [3.4154, 3.3944, 3.4131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.1299942135810852 
model_pd.l_d.mean(): -23.62861442565918 
model_pd.lagr.mean(): -23.498620986938477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2525], device='cuda:0')), ('power', tensor([-23.8812], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.1299942135810852
epoch£º522	 i:1 	 global-step:10441	 l-p:0.1477353423833847
epoch£º522	 i:2 	 global-step:10442	 l-p:0.200709268450737
epoch£º522	 i:3 	 global-step:10443	 l-p:0.10422968864440918
epoch£º522	 i:4 	 global-step:10444	 l-p:0.13748706877231598
epoch£º522	 i:5 	 global-step:10445	 l-p:0.14133909344673157
epoch£º522	 i:6 	 global-step:10446	 l-p:0.12731800973415375
epoch£º522	 i:7 	 global-step:10447	 l-p:0.15126462280750275
epoch£º522	 i:8 	 global-step:10448	 l-p:0.12328580766916275
epoch£º522	 i:9 	 global-step:10449	 l-p:0.15628165006637573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5612, 3.5214, 3.5546],
        [3.5612, 3.5612, 3.5612],
        [3.5612, 3.3513, 2.8991],
        [3.5612, 3.5609, 3.5612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.1377025991678238 
model_pd.l_d.mean(): -22.06959342956543 
model_pd.lagr.mean(): -21.9318904876709 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3747], device='cuda:0')), ('power', tensor([-22.4443], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.1377025991678238
epoch£º523	 i:1 	 global-step:10461	 l-p:0.12827737629413605
epoch£º523	 i:2 	 global-step:10462	 l-p:0.14942726492881775
epoch£º523	 i:3 	 global-step:10463	 l-p:0.13441438972949982
epoch£º523	 i:4 	 global-step:10464	 l-p:0.12021169066429138
epoch£º523	 i:5 	 global-step:10465	 l-p:0.14662347733974457
epoch£º523	 i:6 	 global-step:10466	 l-p:0.13600873947143555
epoch£º523	 i:7 	 global-step:10467	 l-p:0.13512088358402252
epoch£º523	 i:8 	 global-step:10468	 l-p:0.13978339731693268
epoch£º523	 i:9 	 global-step:10469	 l-p:0.10522279143333435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4651, 3.4408, 3.4623],
        [3.4651, 3.0411, 2.5499],
        [3.4651, 3.4645, 3.4651],
        [3.4651, 3.4421, 3.4625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.16809217631816864 
model_pd.l_d.mean(): -23.35364532470703 
model_pd.lagr.mean(): -23.1855525970459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2598], device='cuda:0')), ('power', tensor([-23.6135], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.16809217631816864
epoch£º524	 i:1 	 global-step:10481	 l-p:0.054221246391534805
epoch£º524	 i:2 	 global-step:10482	 l-p:0.6513701677322388
epoch£º524	 i:3 	 global-step:10483	 l-p:0.20495973527431488
epoch£º524	 i:4 	 global-step:10484	 l-p:0.1376085728406906
epoch£º524	 i:5 	 global-step:10485	 l-p:0.13788068294525146
epoch£º524	 i:6 	 global-step:10486	 l-p:0.13155364990234375
epoch£º524	 i:7 	 global-step:10487	 l-p:0.1301705688238144
epoch£º524	 i:8 	 global-step:10488	 l-p:0.1477857530117035
epoch£º524	 i:9 	 global-step:10489	 l-p:0.09558276832103729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4010, 3.3959, 3.4007],
        [3.4010, 3.4009, 3.4010],
        [3.4010, 3.3941, 3.4006],
        [3.4010, 3.0036, 2.9809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16218963265419006 
model_pd.l_d.mean(): -22.99881935119629 
model_pd.lagr.mean(): -22.83662986755371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3225], device='cuda:0')), ('power', tensor([-23.3213], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16218963265419006
epoch£º525	 i:1 	 global-step:10501	 l-p:0.14717727899551392
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12464715540409088
epoch£º525	 i:3 	 global-step:10503	 l-p:0.17428122460842133
epoch£º525	 i:4 	 global-step:10504	 l-p:0.12344253063201904
epoch£º525	 i:5 	 global-step:10505	 l-p:0.12094621360301971
epoch£º525	 i:6 	 global-step:10506	 l-p:0.1561601161956787
epoch£º525	 i:7 	 global-step:10507	 l-p:0.14362689852714539
epoch£º525	 i:8 	 global-step:10508	 l-p:-1.3128793239593506
epoch£º525	 i:9 	 global-step:10509	 l-p:-1.3034335374832153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4774, 3.4534, 3.4746],
        [3.4774, 3.1558, 3.1979],
        [3.4774, 2.9573, 2.5084],
        [3.4774, 2.9861, 2.5094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.3070330321788788 
model_pd.l_d.mean(): -23.575109481811523 
model_pd.lagr.mean(): -23.268075942993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2464], device='cuda:0')), ('power', tensor([-23.8215], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.3070330321788788
epoch£º526	 i:1 	 global-step:10521	 l-p:0.13935936987400055
epoch£º526	 i:2 	 global-step:10522	 l-p:0.13499099016189575
epoch£º526	 i:3 	 global-step:10523	 l-p:0.11523931473493576
epoch£º526	 i:4 	 global-step:10524	 l-p:0.1523367315530777
epoch£º526	 i:5 	 global-step:10525	 l-p:0.13435788452625275
epoch£º526	 i:6 	 global-step:10526	 l-p:0.1300114244222641
epoch£º526	 i:7 	 global-step:10527	 l-p:0.13014453649520874
epoch£º526	 i:8 	 global-step:10528	 l-p:0.20083238184452057
epoch£º526	 i:9 	 global-step:10529	 l-p:0.1295001059770584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6219, 3.6219, 3.6219],
        [3.6219, 3.1586, 3.0062],
        [3.6219, 3.6219, 3.6219],
        [3.6219, 3.6206, 3.6219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.12501615285873413 
model_pd.l_d.mean(): -23.44269371032715 
model_pd.lagr.mean(): -23.317678451538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1619], device='cuda:0')), ('power', tensor([-23.6046], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.12501615285873413
epoch£º527	 i:1 	 global-step:10541	 l-p:0.13274560868740082
epoch£º527	 i:2 	 global-step:10542	 l-p:0.1334763467311859
epoch£º527	 i:3 	 global-step:10543	 l-p:0.1355711966753006
epoch£º527	 i:4 	 global-step:10544	 l-p:0.15167446434497833
epoch£º527	 i:5 	 global-step:10545	 l-p:0.13291053473949432
epoch£º527	 i:6 	 global-step:10546	 l-p:0.14001375436782837
epoch£º527	 i:7 	 global-step:10547	 l-p:-0.0090316291898489
epoch£º527	 i:8 	 global-step:10548	 l-p:0.1353805959224701
epoch£º527	 i:9 	 global-step:10549	 l-p:0.14852342009544373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3770, 3.2709, 3.3418],
        [3.3770, 3.3625, 3.3758],
        [3.3770, 2.9414, 2.4409],
        [3.3770, 2.9415, 2.4409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): -0.06039274111390114 
model_pd.l_d.mean(): -23.038061141967773 
model_pd.lagr.mean(): -23.098453521728516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3566], device='cuda:0')), ('power', tensor([-23.3947], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:-0.06039274111390114
epoch£º528	 i:1 	 global-step:10561	 l-p:0.1310989111661911
epoch£º528	 i:2 	 global-step:10562	 l-p:0.14267048239707947
epoch£º528	 i:3 	 global-step:10563	 l-p:0.17019081115722656
epoch£º528	 i:4 	 global-step:10564	 l-p:0.13517995178699493
epoch£º528	 i:5 	 global-step:10565	 l-p:0.1212436631321907
epoch£º528	 i:6 	 global-step:10566	 l-p:0.17130839824676514
epoch£º528	 i:7 	 global-step:10567	 l-p:0.14303533732891083
epoch£º528	 i:8 	 global-step:10568	 l-p:0.1371212899684906
epoch£º528	 i:9 	 global-step:10569	 l-p:0.15210546553134918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[3.4077, 2.9039, 2.7203],
        [3.4077, 2.8538, 2.4947],
        [3.4077, 3.2168, 3.3058],
        [3.4077, 2.9353, 2.4363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.10712538659572601 
model_pd.l_d.mean(): -23.602542877197266 
model_pd.lagr.mean(): -23.49541664123535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2690], device='cuda:0')), ('power', tensor([-23.8715], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.10712538659572601
epoch£º529	 i:1 	 global-step:10581	 l-p:0.016006050631403923
epoch£º529	 i:2 	 global-step:10582	 l-p:0.14136090874671936
epoch£º529	 i:3 	 global-step:10583	 l-p:0.13163504004478455
epoch£º529	 i:4 	 global-step:10584	 l-p:0.16009074449539185
epoch£º529	 i:5 	 global-step:10585	 l-p:0.313037246465683
epoch£º529	 i:6 	 global-step:10586	 l-p:0.1372283548116684
epoch£º529	 i:7 	 global-step:10587	 l-p:0.13960525393486023
epoch£º529	 i:8 	 global-step:10588	 l-p:0.12816523015499115
epoch£º529	 i:9 	 global-step:10589	 l-p:0.014136667363345623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4490, 3.4490, 3.4490],
        [3.4490, 3.0161, 2.9522],
        [3.4490, 3.4490, 3.4490],
        [3.4490, 2.9318, 2.4557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.13643330335617065 
model_pd.l_d.mean(): -23.373544692993164 
model_pd.lagr.mean(): -23.237112045288086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2823], device='cuda:0')), ('power', tensor([-23.6558], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.13643330335617065
epoch£º530	 i:1 	 global-step:10601	 l-p:0.22272904217243195
epoch£º530	 i:2 	 global-step:10602	 l-p:0.1283850222826004
epoch£º530	 i:3 	 global-step:10603	 l-p:0.042596787214279175
epoch£º530	 i:4 	 global-step:10604	 l-p:0.18883483111858368
epoch£º530	 i:5 	 global-step:10605	 l-p:0.13781389594078064
epoch£º530	 i:6 	 global-step:10606	 l-p:0.16751305758953094
epoch£º530	 i:7 	 global-step:10607	 l-p:0.05281449854373932
epoch£º530	 i:8 	 global-step:10608	 l-p:0.13763003051280975
epoch£º530	 i:9 	 global-step:10609	 l-p:0.1580987274646759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5129, 3.5019, 3.5121],
        [3.5129, 3.5129, 3.5129],
        [3.5129, 3.5129, 3.5129],
        [3.5129, 3.5129, 3.5129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.1418108195066452 
model_pd.l_d.mean(): -22.79212188720703 
model_pd.lagr.mean(): -22.650310516357422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2524], device='cuda:0')), ('power', tensor([-23.0445], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.1418108195066452
epoch£º531	 i:1 	 global-step:10621	 l-p:0.12417256832122803
epoch£º531	 i:2 	 global-step:10622	 l-p:0.11716775596141815
epoch£º531	 i:3 	 global-step:10623	 l-p:0.12915372848510742
epoch£º531	 i:4 	 global-step:10624	 l-p:0.13606034219264984
epoch£º531	 i:5 	 global-step:10625	 l-p:0.15565717220306396
epoch£º531	 i:6 	 global-step:10626	 l-p:0.14140568673610687
epoch£º531	 i:7 	 global-step:10627	 l-p:0.13361883163452148
epoch£º531	 i:8 	 global-step:10628	 l-p:0.12863346934318542
epoch£º531	 i:9 	 global-step:10629	 l-p:0.1371178925037384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5332, 2.9926, 2.5923],
        [3.5332, 3.4236, 3.4955],
        [3.5332, 3.0074, 2.5557],
        [3.5332, 3.5272, 3.5329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.17412066459655762 
model_pd.l_d.mean(): -22.812191009521484 
model_pd.lagr.mean(): -22.638071060180664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2758], device='cuda:0')), ('power', tensor([-23.0880], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.17412066459655762
epoch£º532	 i:1 	 global-step:10641	 l-p:0.13074864447116852
epoch£º532	 i:2 	 global-step:10642	 l-p:0.19234509766101837
epoch£º532	 i:3 	 global-step:10643	 l-p:0.13338057696819305
epoch£º532	 i:4 	 global-step:10644	 l-p:0.18698182702064514
epoch£º532	 i:5 	 global-step:10645	 l-p:0.14060217142105103
epoch£º532	 i:6 	 global-step:10646	 l-p:0.12314808368682861
epoch£º532	 i:7 	 global-step:10647	 l-p:0.12246450781822205
epoch£º532	 i:8 	 global-step:10648	 l-p:0.08267582952976227
epoch£º532	 i:9 	 global-step:10649	 l-p:0.16377055644989014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5626, 3.1409, 2.6406],
        [3.5626, 3.5556, 3.5622],
        [3.5626, 3.5068, 3.5510],
        [3.5626, 3.1790, 3.1683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.13213874399662018 
model_pd.l_d.mean(): -23.00840950012207 
model_pd.lagr.mean(): -22.876270294189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2357], device='cuda:0')), ('power', tensor([-23.2441], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.13213874399662018
epoch£º533	 i:1 	 global-step:10661	 l-p:0.12100289016962051
epoch£º533	 i:2 	 global-step:10662	 l-p:0.12183043360710144
epoch£º533	 i:3 	 global-step:10663	 l-p:0.15022897720336914
epoch£º533	 i:4 	 global-step:10664	 l-p:0.194350928068161
epoch£º533	 i:5 	 global-step:10665	 l-p:0.14499476552009583
epoch£º533	 i:6 	 global-step:10666	 l-p:0.41233253479003906
epoch£º533	 i:7 	 global-step:10667	 l-p:-0.28617557883262634
epoch£º533	 i:8 	 global-step:10668	 l-p:0.14744141697883606
epoch£º533	 i:9 	 global-step:10669	 l-p:0.19636087119579315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4712, 3.0952, 3.1008],
        [3.4712, 2.9354, 2.6638],
        [3.4712, 3.4535, 3.4695],
        [3.4712, 3.0254, 2.9465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): -0.14674727618694305 
model_pd.l_d.mean(): -23.16382598876953 
model_pd.lagr.mean(): -23.31057357788086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3264], device='cuda:0')), ('power', tensor([-23.4902], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:-0.14674727618694305
epoch£º534	 i:1 	 global-step:10681	 l-p:0.14531081914901733
epoch£º534	 i:2 	 global-step:10682	 l-p:0.1331513375043869
epoch£º534	 i:3 	 global-step:10683	 l-p:0.1367896944284439
epoch£º534	 i:4 	 global-step:10684	 l-p:0.11839540302753448
epoch£º534	 i:5 	 global-step:10685	 l-p:0.15950053930282593
epoch£º534	 i:6 	 global-step:10686	 l-p:-0.12648333609104156
epoch£º534	 i:7 	 global-step:10687	 l-p:0.12436561286449432
epoch£º534	 i:8 	 global-step:10688	 l-p:0.15753591060638428
epoch£º534	 i:9 	 global-step:10689	 l-p:0.17037536203861237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3887, 3.3882, 3.3887],
        [3.3887, 3.3419, 3.3803],
        [3.3887, 3.3886, 3.3887],
        [3.3887, 3.2836, 3.3545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.34290987253189087 
model_pd.l_d.mean(): -23.35350799560547 
model_pd.lagr.mean(): -23.010597229003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2635], device='cuda:0')), ('power', tensor([-23.6170], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.34290987253189087
epoch£º535	 i:1 	 global-step:10701	 l-p:0.15421584248542786
epoch£º535	 i:2 	 global-step:10702	 l-p:0.16181355714797974
epoch£º535	 i:3 	 global-step:10703	 l-p:0.16755793988704681
epoch£º535	 i:4 	 global-step:10704	 l-p:0.12636637687683105
epoch£º535	 i:5 	 global-step:10705	 l-p:0.14749819040298462
epoch£º535	 i:6 	 global-step:10706	 l-p:0.09664563834667206
epoch£º535	 i:7 	 global-step:10707	 l-p:-0.05821450054645538
epoch£º535	 i:8 	 global-step:10708	 l-p:0.43454432487487793
epoch£º535	 i:9 	 global-step:10709	 l-p:0.15061412751674652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5704, 3.5469, 3.5678],
        [3.5704, 3.5590, 3.5696],
        [3.5704, 3.5490, 3.5681],
        [3.5704, 3.0660, 2.5878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.1318439096212387 
model_pd.l_d.mean(): -22.889118194580078 
model_pd.lagr.mean(): -22.757274627685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2757], device='cuda:0')), ('power', tensor([-23.1648], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.1318439096212387
epoch£º536	 i:1 	 global-step:10721	 l-p:0.4350016415119171
epoch£º536	 i:2 	 global-step:10722	 l-p:0.1331648975610733
epoch£º536	 i:3 	 global-step:10723	 l-p:0.1328738033771515
epoch£º536	 i:4 	 global-step:10724	 l-p:0.12028604000806808
epoch£º536	 i:5 	 global-step:10725	 l-p:0.1376650184392929
epoch£º536	 i:6 	 global-step:10726	 l-p:0.11796325445175171
epoch£º536	 i:7 	 global-step:10727	 l-p:0.13123537600040436
epoch£º536	 i:8 	 global-step:10728	 l-p:0.12378467619419098
epoch£º536	 i:9 	 global-step:10729	 l-p:0.15414606034755707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5579, 3.5578, 3.5579],
        [3.5579, 3.1525, 3.1212],
        [3.5579, 3.1687, 3.1562],
        [3.5579, 3.5579, 3.5579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.15765556693077087 
model_pd.l_d.mean(): -23.63412094116211 
model_pd.lagr.mean(): -23.476465225219727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1274], device='cuda:0')), ('power', tensor([-23.7615], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.15765556693077087
epoch£º537	 i:1 	 global-step:10741	 l-p:0.12668195366859436
epoch£º537	 i:2 	 global-step:10742	 l-p:0.16915710270404816
epoch£º537	 i:3 	 global-step:10743	 l-p:0.18390648066997528
epoch£º537	 i:4 	 global-step:10744	 l-p:0.1414121836423874
epoch£º537	 i:5 	 global-step:10745	 l-p:0.2045474648475647
epoch£º537	 i:6 	 global-step:10746	 l-p:0.12962743639945984
epoch£º537	 i:7 	 global-step:10747	 l-p:0.09320861846208572
epoch£º537	 i:8 	 global-step:10748	 l-p:0.13923606276512146
epoch£º537	 i:9 	 global-step:10749	 l-p:0.15031328797340393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5260, 3.4505, 3.5067],
        [3.5260, 3.5249, 3.5260],
        [3.5260, 3.5260, 3.5260],
        [3.5260, 3.4010, 3.4789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.13572588562965393 
model_pd.l_d.mean(): -23.566221237182617 
model_pd.lagr.mean(): -23.430496215820312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1960], device='cuda:0')), ('power', tensor([-23.7622], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.13572588562965393
epoch£º538	 i:1 	 global-step:10761	 l-p:0.18679428100585938
epoch£º538	 i:2 	 global-step:10762	 l-p:0.14156801998615265
epoch£º538	 i:3 	 global-step:10763	 l-p:0.1178247407078743
epoch£º538	 i:4 	 global-step:10764	 l-p:0.21923217177391052
epoch£º538	 i:5 	 global-step:10765	 l-p:0.1598164588212967
epoch£º538	 i:6 	 global-step:10766	 l-p:0.14089734852313995
epoch£º538	 i:7 	 global-step:10767	 l-p:0.16610242426395416
epoch£º538	 i:8 	 global-step:10768	 l-p:0.1398027092218399
epoch£º538	 i:9 	 global-step:10769	 l-p:0.16532599925994873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5573, 3.3404, 3.4279],
        [3.5573, 3.5572, 3.5573],
        [3.5573, 3.5285, 3.5535],
        [3.5573, 3.5571, 3.5573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.06916195154190063 
model_pd.l_d.mean(): -23.465763092041016 
model_pd.lagr.mean(): -23.3966007232666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2108], device='cuda:0')), ('power', tensor([-23.6766], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.06916195154190063
epoch£º539	 i:1 	 global-step:10781	 l-p:0.17935676872730255
epoch£º539	 i:2 	 global-step:10782	 l-p:0.12899824976921082
epoch£º539	 i:3 	 global-step:10783	 l-p:0.13184984028339386
epoch£º539	 i:4 	 global-step:10784	 l-p:0.1119716465473175
epoch£º539	 i:5 	 global-step:10785	 l-p:0.12084802240133286
epoch£º539	 i:6 	 global-step:10786	 l-p:0.11105211824178696
epoch£º539	 i:7 	 global-step:10787	 l-p:0.11613480001688004
epoch£º539	 i:8 	 global-step:10788	 l-p:0.15290774405002594
epoch£º539	 i:9 	 global-step:10789	 l-p:0.14603355526924133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5600, 3.0451, 2.8245],
        [3.5600, 3.5600, 3.5600],
        [3.5600, 3.2665, 3.3309],
        [3.5600, 3.0483, 2.5654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.1417200118303299 
model_pd.l_d.mean(): -22.717411041259766 
model_pd.lagr.mean(): -22.57569122314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3427], device='cuda:0')), ('power', tensor([-23.0601], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.1417200118303299
epoch£º540	 i:1 	 global-step:10801	 l-p:0.15789230167865753
epoch£º540	 i:2 	 global-step:10802	 l-p:0.14124402403831482
epoch£º540	 i:3 	 global-step:10803	 l-p:0.13354714214801788
epoch£º540	 i:4 	 global-step:10804	 l-p:0.032682761549949646
epoch£º540	 i:5 	 global-step:10805	 l-p:0.13198481500148773
epoch£º540	 i:6 	 global-step:10806	 l-p:0.14356912672519684
epoch£º540	 i:7 	 global-step:10807	 l-p:0.16687576472759247
epoch£º540	 i:8 	 global-step:10808	 l-p:0.1707407385110855
epoch£º540	 i:9 	 global-step:10809	 l-p:0.07818584144115448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3620, 3.2760, 3.3383],
        [3.3620, 3.2943, 3.3464],
        [3.3620, 3.0059, 2.5073],
        [3.3620, 3.3620, 3.3620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.12779808044433594 
model_pd.l_d.mean(): -22.66120147705078 
model_pd.lagr.mean(): -22.533403396606445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3205], device='cuda:0')), ('power', tensor([-22.9817], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.12779808044433594
epoch£º541	 i:1 	 global-step:10821	 l-p:0.16035108268260956
epoch£º541	 i:2 	 global-step:10822	 l-p:0.8425059914588928
epoch£º541	 i:3 	 global-step:10823	 l-p:0.6036379933357239
epoch£º541	 i:4 	 global-step:10824	 l-p:0.15075501799583435
epoch£º541	 i:5 	 global-step:10825	 l-p:-0.041744496673345566
epoch£º541	 i:6 	 global-step:10826	 l-p:0.23608408868312836
epoch£º541	 i:7 	 global-step:10827	 l-p:0.14976200461387634
epoch£º541	 i:8 	 global-step:10828	 l-p:-0.010808663442730904
epoch£º541	 i:9 	 global-step:10829	 l-p:0.14623519778251648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6653, 3.6653, 3.6653],
        [3.6653, 3.1807, 3.0125],
        [3.6653, 3.5261, 3.0805],
        [3.6653, 3.1430, 2.8686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.10969027876853943 
model_pd.l_d.mean(): -22.23685646057129 
model_pd.lagr.mean(): -22.127166748046875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2631], device='cuda:0')), ('power', tensor([-22.4999], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.10969027876853943
epoch£º542	 i:1 	 global-step:10841	 l-p:0.12137860804796219
epoch£º542	 i:2 	 global-step:10842	 l-p:0.11827096343040466
epoch£º542	 i:3 	 global-step:10843	 l-p:0.15074166655540466
epoch£º542	 i:4 	 global-step:10844	 l-p:0.11885249614715576
epoch£º542	 i:5 	 global-step:10845	 l-p:0.11803675442934036
epoch£º542	 i:6 	 global-step:10846	 l-p:0.14014646410942078
epoch£º542	 i:7 	 global-step:10847	 l-p:0.16497117280960083
epoch£º542	 i:8 	 global-step:10848	 l-p:0.13481518626213074
epoch£º542	 i:9 	 global-step:10849	 l-p:0.11972847580909729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5649, 3.0323, 2.7712],
        [3.5649, 3.3469, 2.8742],
        [3.5649, 3.3797, 3.4692],
        [3.5649, 3.5621, 3.5648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.12410680204629898 
model_pd.l_d.mean(): -23.18609046936035 
model_pd.lagr.mean(): -23.061983108520508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2371], device='cuda:0')), ('power', tensor([-23.4232], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.12410680204629898
epoch£º543	 i:1 	 global-step:10861	 l-p:0.236815944314003
epoch£º543	 i:2 	 global-step:10862	 l-p:0.19268092513084412
epoch£º543	 i:3 	 global-step:10863	 l-p:-0.2718670070171356
epoch£º543	 i:4 	 global-step:10864	 l-p:0.12577521800994873
epoch£º543	 i:5 	 global-step:10865	 l-p:0.11949639022350311
epoch£º543	 i:6 	 global-step:10866	 l-p:0.21358026564121246
epoch£º543	 i:7 	 global-step:10867	 l-p:0.18799293041229248
epoch£º543	 i:8 	 global-step:10868	 l-p:0.14622820913791656
epoch£º543	 i:9 	 global-step:10869	 l-p:0.1547563672065735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4481, 3.4457, 3.4480],
        [3.4481, 2.8838, 2.5633],
        [3.4481, 3.4481, 3.4481],
        [3.4481, 3.0466, 2.5340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.06364675611257553 
model_pd.l_d.mean(): -23.523479461669922 
model_pd.lagr.mean(): -23.4598331451416 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2287], device='cuda:0')), ('power', tensor([-23.7522], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.06364675611257553
epoch£º544	 i:1 	 global-step:10881	 l-p:0.13568519055843353
epoch£º544	 i:2 	 global-step:10882	 l-p:0.16905254125595093
epoch£º544	 i:3 	 global-step:10883	 l-p:0.09257184714078903
epoch£º544	 i:4 	 global-step:10884	 l-p:0.12961676716804504
epoch£º544	 i:5 	 global-step:10885	 l-p:0.16800221800804138
epoch£º544	 i:6 	 global-step:10886	 l-p:0.13624337315559387
epoch£º544	 i:7 	 global-step:10887	 l-p:0.15197452902793884
epoch£º544	 i:8 	 global-step:10888	 l-p:0.15620921552181244
epoch£º544	 i:9 	 global-step:10889	 l-p:0.1185639351606369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6081, 3.5827, 3.6051],
        [3.6081, 3.5035, 3.5739],
        [3.6081, 3.2746, 3.3157],
        [3.6081, 3.2845, 3.3326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.14341267943382263 
model_pd.l_d.mean(): -23.61144256591797 
model_pd.lagr.mean(): -23.468029022216797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1716], device='cuda:0')), ('power', tensor([-23.7830], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.14341267943382263
epoch£º545	 i:1 	 global-step:10901	 l-p:0.13159677386283875
epoch£º545	 i:2 	 global-step:10902	 l-p:0.13087892532348633
epoch£º545	 i:3 	 global-step:10903	 l-p:0.07954972237348557
epoch£º545	 i:4 	 global-step:10904	 l-p:0.21316318213939667
epoch£º545	 i:5 	 global-step:10905	 l-p:0.154961496591568
epoch£º545	 i:6 	 global-step:10906	 l-p:0.14521871507167816
epoch£º545	 i:7 	 global-step:10907	 l-p:0.17253005504608154
epoch£º545	 i:8 	 global-step:10908	 l-p:0.12627990543842316
epoch£º545	 i:9 	 global-step:10909	 l-p:0.1156889870762825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5448, 3.4844, 3.5318],
        [3.5448, 3.4872, 3.5328],
        [3.5448, 3.5444, 3.5448],
        [3.5448, 3.0509, 2.9007]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.13286112248897552 
model_pd.l_d.mean(): -23.040119171142578 
model_pd.lagr.mean(): -22.907258987426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1767], device='cuda:0')), ('power', tensor([-23.2168], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.13286112248897552
epoch£º546	 i:1 	 global-step:10921	 l-p:0.14059853553771973
epoch£º546	 i:2 	 global-step:10922	 l-p:0.13823816180229187
epoch£º546	 i:3 	 global-step:10923	 l-p:0.09004337340593338
epoch£º546	 i:4 	 global-step:10924	 l-p:0.07045028358697891
epoch£º546	 i:5 	 global-step:10925	 l-p:-1.0160382986068726
epoch£º546	 i:6 	 global-step:10926	 l-p:0.165095254778862
epoch£º546	 i:7 	 global-step:10927	 l-p:0.17434361577033997
epoch£º546	 i:8 	 global-step:10928	 l-p:0.14270621538162231
epoch£º546	 i:9 	 global-step:10929	 l-p:0.20664146542549133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4435, 3.1874, 3.2733],
        [3.4435, 3.1124, 2.6106],
        [3.4435, 3.4413, 3.4434],
        [3.4435, 3.4435, 3.4435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.13644185662269592 
model_pd.l_d.mean(): -23.504121780395508 
model_pd.lagr.mean(): -23.367679595947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2548], device='cuda:0')), ('power', tensor([-23.7590], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.13644185662269592
epoch£º547	 i:1 	 global-step:10941	 l-p:0.0800216868519783
epoch£º547	 i:2 	 global-step:10942	 l-p:0.11883903294801712
epoch£º547	 i:3 	 global-step:10943	 l-p:0.1877231001853943
epoch£º547	 i:4 	 global-step:10944	 l-p:0.13252970576286316
epoch£º547	 i:5 	 global-step:10945	 l-p:0.14040414988994598
epoch£º547	 i:6 	 global-step:10946	 l-p:0.12837593257427216
epoch£º547	 i:7 	 global-step:10947	 l-p:0.13617920875549316
epoch£º547	 i:8 	 global-step:10948	 l-p:0.13316500186920166
epoch£º547	 i:9 	 global-step:10949	 l-p:0.18483340740203857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5369, 3.5364, 3.5369],
        [3.5369, 3.3127, 3.4027],
        [3.5369, 3.4657, 3.5198],
        [3.5369, 3.4305, 3.5022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.1886281967163086 
model_pd.l_d.mean(): -22.792600631713867 
model_pd.lagr.mean(): -22.603973388671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2853], device='cuda:0')), ('power', tensor([-23.0779], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.1886281967163086
epoch£º548	 i:1 	 global-step:10961	 l-p:0.12962621450424194
epoch£º548	 i:2 	 global-step:10962	 l-p:0.12052150815725327
epoch£º548	 i:3 	 global-step:10963	 l-p:0.12189970910549164
epoch£º548	 i:4 	 global-step:10964	 l-p:0.14231418073177338
epoch£º548	 i:5 	 global-step:10965	 l-p:0.506485104560852
epoch£º548	 i:6 	 global-step:10966	 l-p:0.12854041159152985
epoch£º548	 i:7 	 global-step:10967	 l-p:-0.5842066407203674
epoch£º548	 i:8 	 global-step:10968	 l-p:0.11670099198818207
epoch£º548	 i:9 	 global-step:10969	 l-p:0.15813881158828735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5065, 3.0994, 2.5803],
        [3.5065, 3.4956, 3.5057],
        [3.5065, 3.4893, 3.5049],
        [3.5065, 3.1064, 3.1009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.16829639673233032 
model_pd.l_d.mean(): -23.494718551635742 
model_pd.lagr.mean(): -23.3264217376709 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2023], device='cuda:0')), ('power', tensor([-23.6970], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.16829639673233032
epoch£º549	 i:1 	 global-step:10981	 l-p:0.10672958940267563
epoch£º549	 i:2 	 global-step:10982	 l-p:0.1414918601512909
epoch£º549	 i:3 	 global-step:10983	 l-p:0.19684536755084991
epoch£º549	 i:4 	 global-step:10984	 l-p:0.1644611358642578
epoch£º549	 i:5 	 global-step:10985	 l-p:0.10579126328229904
epoch£º549	 i:6 	 global-step:10986	 l-p:0.1536601185798645
epoch£º549	 i:7 	 global-step:10987	 l-p:0.13502977788448334
epoch£º549	 i:8 	 global-step:10988	 l-p:0.16844117641448975
epoch£º549	 i:9 	 global-step:10989	 l-p:0.13827815651893616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5870, 3.5121, 3.5682],
        [3.5870, 3.0809, 2.9068],
        [3.5870, 3.1435, 3.0782],
        [3.5870, 3.4609, 3.5399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.1280175745487213 
model_pd.l_d.mean(): -22.257844924926758 
model_pd.lagr.mean(): -22.12982749938965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3224], device='cuda:0')), ('power', tensor([-22.5802], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.1280175745487213
epoch£º550	 i:1 	 global-step:11001	 l-p:0.0872580036520958
epoch£º550	 i:2 	 global-step:11002	 l-p:0.1378430277109146
epoch£º550	 i:3 	 global-step:11003	 l-p:0.17258906364440918
epoch£º550	 i:4 	 global-step:11004	 l-p:0.11579091101884842
epoch£º550	 i:5 	 global-step:11005	 l-p:0.12977813184261322
epoch£º550	 i:6 	 global-step:11006	 l-p:0.15771254897117615
epoch£º550	 i:7 	 global-step:11007	 l-p:0.12849511206150055
epoch£º550	 i:8 	 global-step:11008	 l-p:0.1280253529548645
epoch£º550	 i:9 	 global-step:11009	 l-p:0.10643956810235977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5402, 3.2526, 3.3270],
        [3.5402, 3.0023, 2.5052],
        [3.5402, 3.2935, 2.8069],
        [3.5402, 3.5402, 3.5402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.12145207077264786 
model_pd.l_d.mean(): -22.02107810974121 
model_pd.lagr.mean(): -21.899625778198242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3445], device='cuda:0')), ('power', tensor([-22.3655], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.12145207077264786
epoch£º551	 i:1 	 global-step:11021	 l-p:0.13454611599445343
epoch£º551	 i:2 	 global-step:11022	 l-p:0.17044709622859955
epoch£º551	 i:3 	 global-step:11023	 l-p:0.14853782951831818
epoch£º551	 i:4 	 global-step:11024	 l-p:0.11481045186519623
epoch£º551	 i:5 	 global-step:11025	 l-p:0.14167454838752747
epoch£º551	 i:6 	 global-step:11026	 l-p:0.10596492886543274
epoch£º551	 i:7 	 global-step:11027	 l-p:0.10275200009346008
epoch£º551	 i:8 	 global-step:11028	 l-p:0.19270305335521698
epoch£º551	 i:9 	 global-step:11029	 l-p:0.13204528391361237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4700, 2.9444, 2.7623],
        [3.4700, 3.4699, 3.4700],
        [3.4700, 3.4700, 3.4700],
        [3.4700, 3.4700, 3.4700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.2706044614315033 
model_pd.l_d.mean(): -23.103986740112305 
model_pd.lagr.mean(): -22.83338165283203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2841], device='cuda:0')), ('power', tensor([-23.3881], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.2706044614315033
epoch£º552	 i:1 	 global-step:11041	 l-p:0.16442175209522247
epoch£º552	 i:2 	 global-step:11042	 l-p:0.4976178705692291
epoch£º552	 i:3 	 global-step:11043	 l-p:0.10998710244894028
epoch£º552	 i:4 	 global-step:11044	 l-p:0.13214325904846191
epoch£º552	 i:5 	 global-step:11045	 l-p:0.15604567527770996
epoch£º552	 i:6 	 global-step:11046	 l-p:0.1508256047964096
epoch£º552	 i:7 	 global-step:11047	 l-p:0.13742290437221527
epoch£º552	 i:8 	 global-step:11048	 l-p:0.12280791252851486
epoch£º552	 i:9 	 global-step:11049	 l-p:0.12072297930717468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6720, 3.6720, 3.6720],
        [3.6720, 3.1633, 2.9722],
        [3.6720, 3.4363, 2.9495],
        [3.6720, 3.5721, 3.6408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.13425077497959137 
model_pd.l_d.mean(): -23.284440994262695 
model_pd.lagr.mean(): -23.150190353393555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2337], device='cuda:0')), ('power', tensor([-23.5181], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.13425077497959137
epoch£º553	 i:1 	 global-step:11061	 l-p:0.12588486075401306
epoch£º553	 i:2 	 global-step:11062	 l-p:0.26053285598754883
epoch£º553	 i:3 	 global-step:11063	 l-p:0.13581424951553345
epoch£º553	 i:4 	 global-step:11064	 l-p:0.12177297472953796
epoch£º553	 i:5 	 global-step:11065	 l-p:0.16832251846790314
epoch£º553	 i:6 	 global-step:11066	 l-p:0.13209523260593414
epoch£º553	 i:7 	 global-step:11067	 l-p:0.12251323461532593
epoch£º553	 i:8 	 global-step:11068	 l-p:0.1729709953069687
epoch£º553	 i:9 	 global-step:11069	 l-p:0.1631413847208023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4720, 3.4243, 3.4636],
        [3.4720, 3.4701, 3.4720],
        [3.4720, 3.4125, 3.4597],
        [3.4720, 2.8853, 2.5070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.1881115734577179 
model_pd.l_d.mean(): -23.157283782958984 
model_pd.lagr.mean(): -22.96917152404785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2281], device='cuda:0')), ('power', tensor([-23.3854], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.1881115734577179
epoch£º554	 i:1 	 global-step:11081	 l-p:0.13418303430080414
epoch£º554	 i:2 	 global-step:11082	 l-p:0.005066203884780407
epoch£º554	 i:3 	 global-step:11083	 l-p:0.10728086531162262
epoch£º554	 i:4 	 global-step:11084	 l-p:0.14338932931423187
epoch£º554	 i:5 	 global-step:11085	 l-p:0.19266757369041443
epoch£º554	 i:6 	 global-step:11086	 l-p:0.1290712207555771
epoch£º554	 i:7 	 global-step:11087	 l-p:0.05144653096795082
epoch£º554	 i:8 	 global-step:11088	 l-p:0.1289384514093399
epoch£º554	 i:9 	 global-step:11089	 l-p:0.14300809800624847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5160, 3.5159, 3.5160],
        [3.5160, 3.3295, 3.4222],
        [3.5160, 3.4875, 3.5125],
        [3.5160, 3.2858, 3.3775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.1425812691450119 
model_pd.l_d.mean(): -23.427095413208008 
model_pd.lagr.mean(): -23.284513473510742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2185], device='cuda:0')), ('power', tensor([-23.6456], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.1425812691450119
epoch£º555	 i:1 	 global-step:11101	 l-p:0.24432209134101868
epoch£º555	 i:2 	 global-step:11102	 l-p:0.1103062778711319
epoch£º555	 i:3 	 global-step:11103	 l-p:0.13122807443141937
epoch£º555	 i:4 	 global-step:11104	 l-p:0.2038160264492035
epoch£º555	 i:5 	 global-step:11105	 l-p:0.15517333149909973
epoch£º555	 i:6 	 global-step:11106	 l-p:0.14608632028102875
epoch£º555	 i:7 	 global-step:11107	 l-p:0.2682134509086609
epoch£º555	 i:8 	 global-step:11108	 l-p:0.11393960565328598
epoch£º555	 i:9 	 global-step:11109	 l-p:0.12584637105464935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6778, 3.2071, 3.0968],
        [3.6778, 3.6778, 3.6778],
        [3.6778, 3.1343, 2.6660],
        [3.6778, 3.6739, 3.6777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.14488916099071503 
model_pd.l_d.mean(): -23.509231567382812 
model_pd.lagr.mean(): -23.364341735839844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1636], device='cuda:0')), ('power', tensor([-23.6728], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.14488916099071503
epoch£º556	 i:1 	 global-step:11121	 l-p:0.12653478980064392
epoch£º556	 i:2 	 global-step:11122	 l-p:0.1325923353433609
epoch£º556	 i:3 	 global-step:11123	 l-p:0.12199202924966812
epoch£º556	 i:4 	 global-step:11124	 l-p:0.1435275673866272
epoch£º556	 i:5 	 global-step:11125	 l-p:0.1557563990354538
epoch£º556	 i:6 	 global-step:11126	 l-p:0.0503356046974659
epoch£º556	 i:7 	 global-step:11127	 l-p:0.14357131719589233
epoch£º556	 i:8 	 global-step:11128	 l-p:0.1300818920135498
epoch£º556	 i:9 	 global-step:11129	 l-p:0.1822291761636734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3400, 3.3169, 3.3375],
        [3.3400, 2.7527, 2.4674],
        [3.3400, 3.1957, 3.2827],
        [3.3400, 2.7365, 2.2549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.10178545117378235 
model_pd.l_d.mean(): -23.226892471313477 
model_pd.lagr.mean(): -23.125106811523438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3991], device='cuda:0')), ('power', tensor([-23.6260], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.10178545117378235
epoch£º557	 i:1 	 global-step:11141	 l-p:0.1646628975868225
epoch£º557	 i:2 	 global-step:11142	 l-p:0.17909936606884003
epoch£º557	 i:3 	 global-step:11143	 l-p:0.16658709943294525
epoch£º557	 i:4 	 global-step:11144	 l-p:0.18433649837970734
epoch£º557	 i:5 	 global-step:11145	 l-p:0.19662265479564667
epoch£º557	 i:6 	 global-step:11146	 l-p:-0.09134241938591003
epoch£º557	 i:7 	 global-step:11147	 l-p:0.14233796298503876
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11956106126308441
epoch£º557	 i:9 	 global-step:11149	 l-p:0.12203759700059891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5377, 3.0678, 2.9836],
        [3.5377, 2.9588, 2.5025],
        [3.5377, 3.4722, 3.5231],
        [3.5377, 3.5229, 3.5365]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.28140872716903687 
model_pd.l_d.mean(): -23.10620880126953 
model_pd.lagr.mean(): -22.824800491333008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2446], device='cuda:0')), ('power', tensor([-23.3508], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.28140872716903687
epoch£º558	 i:1 	 global-step:11161	 l-p:0.17320342361927032
epoch£º558	 i:2 	 global-step:11162	 l-p:0.11686564981937408
epoch£º558	 i:3 	 global-step:11163	 l-p:0.1220860481262207
epoch£º558	 i:4 	 global-step:11164	 l-p:0.14482365548610687
epoch£º558	 i:5 	 global-step:11165	 l-p:0.12356404215097427
epoch£º558	 i:6 	 global-step:11166	 l-p:0.11988870054483414
epoch£º558	 i:7 	 global-step:11167	 l-p:0.13254518806934357
epoch£º558	 i:8 	 global-step:11168	 l-p:0.1250765174627304
epoch£º558	 i:9 	 global-step:11169	 l-p:0.13609354197978973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5204, 2.9336, 2.5555],
        [3.5204, 3.5006, 3.5185],
        [3.5204, 3.5203, 3.5204],
        [3.5204, 3.5189, 3.5204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.1278468817472458 
model_pd.l_d.mean(): -23.110633850097656 
model_pd.lagr.mean(): -22.982786178588867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3451], device='cuda:0')), ('power', tensor([-23.4558], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.1278468817472458
epoch£º559	 i:1 	 global-step:11181	 l-p:0.14432723820209503
epoch£º559	 i:2 	 global-step:11182	 l-p:0.16362445056438446
epoch£º559	 i:3 	 global-step:11183	 l-p:0.157705157995224
epoch£º559	 i:4 	 global-step:11184	 l-p:0.16151031851768494
epoch£º559	 i:5 	 global-step:11185	 l-p:0.12205850332975388
epoch£º559	 i:6 	 global-step:11186	 l-p:0.14398591220378876
epoch£º559	 i:7 	 global-step:11187	 l-p:0.15519708395004272
epoch£º559	 i:8 	 global-step:11188	 l-p:-0.11051425337791443
epoch£º559	 i:9 	 global-step:11189	 l-p:0.12407412379980087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[3.4560, 3.2070, 2.7184],
        [3.4560, 2.9078, 2.6996],
        [3.4560, 3.2115, 2.7243],
        [3.4560, 2.8649, 2.3880]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.11824565380811691 
model_pd.l_d.mean(): -22.35501480102539 
model_pd.lagr.mean(): -22.23676872253418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3467], device='cuda:0')), ('power', tensor([-22.7017], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.11824565380811691
epoch£º560	 i:1 	 global-step:11201	 l-p:0.1294574737548828
epoch£º560	 i:2 	 global-step:11202	 l-p:3.399109125137329
epoch£º560	 i:3 	 global-step:11203	 l-p:0.15161600708961487
epoch£º560	 i:4 	 global-step:11204	 l-p:0.14066524803638458
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11473330855369568
epoch£º560	 i:6 	 global-step:11206	 l-p:0.13854628801345825
epoch£º560	 i:7 	 global-step:11207	 l-p:0.14831265807151794
epoch£º560	 i:8 	 global-step:11208	 l-p:0.1526387631893158
epoch£º560	 i:9 	 global-step:11209	 l-p:0.12840650975704193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5827, 3.5827, 3.5827],
        [3.5827, 3.0646, 2.8941],
        [3.5827, 3.5625, 3.5807],
        [3.5827, 3.0171, 2.5332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.1711641401052475 
model_pd.l_d.mean(): -22.981311798095703 
model_pd.lagr.mean(): -22.810148239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2197], device='cuda:0')), ('power', tensor([-23.2010], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.1711641401052475
epoch£º561	 i:1 	 global-step:11221	 l-p:0.15760590136051178
epoch£º561	 i:2 	 global-step:11222	 l-p:0.15625891089439392
epoch£º561	 i:3 	 global-step:11223	 l-p:0.12383203953504562
epoch£º561	 i:4 	 global-step:11224	 l-p:0.1451713889837265
epoch£º561	 i:5 	 global-step:11225	 l-p:0.13629651069641113
epoch£º561	 i:6 	 global-step:11226	 l-p:0.11354481428861618
epoch£º561	 i:7 	 global-step:11227	 l-p:0.16300472617149353
epoch£º561	 i:8 	 global-step:11228	 l-p:-0.46755221486091614
epoch£º561	 i:9 	 global-step:11229	 l-p:0.1292254775762558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3683, 3.2780, 3.3435],
        [3.3683, 3.3671, 3.3683],
        [3.3683, 3.3682, 3.3683],
        [3.3683, 2.7601, 2.4105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.21921664476394653 
model_pd.l_d.mean(): -23.144716262817383 
model_pd.lagr.mean(): -22.925498962402344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2841], device='cuda:0')), ('power', tensor([-23.4288], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.21921664476394653
epoch£º562	 i:1 	 global-step:11241	 l-p:0.15300603210926056
epoch£º562	 i:2 	 global-step:11242	 l-p:0.11352010816335678
epoch£º562	 i:3 	 global-step:11243	 l-p:0.12521930038928986
epoch£º562	 i:4 	 global-step:11244	 l-p:0.1466924548149109
epoch£º562	 i:5 	 global-step:11245	 l-p:0.1692374050617218
epoch£º562	 i:6 	 global-step:11246	 l-p:0.17000791430473328
epoch£º562	 i:7 	 global-step:11247	 l-p:0.1474609076976776
epoch£º562	 i:8 	 global-step:11248	 l-p:0.07066118717193604
epoch£º562	 i:9 	 global-step:11249	 l-p:0.27734121680259705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5991, 3.5837, 3.5978],
        [3.5991, 3.3503, 3.4397],
        [3.5991, 3.4322, 3.5229],
        [3.5991, 3.5254, 3.5813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.09087146818637848 
model_pd.l_d.mean(): -23.512052536010742 
model_pd.lagr.mean(): -23.421180725097656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2025], device='cuda:0')), ('power', tensor([-23.7146], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.09087146818637848
epoch£º563	 i:1 	 global-step:11261	 l-p:0.1258867233991623
epoch£º563	 i:2 	 global-step:11262	 l-p:0.12532977759838104
epoch£º563	 i:3 	 global-step:11263	 l-p:0.12463724613189697
epoch£º563	 i:4 	 global-step:11264	 l-p:0.12353608012199402
epoch£º563	 i:5 	 global-step:11265	 l-p:0.13266852498054504
epoch£º563	 i:6 	 global-step:11266	 l-p:0.11841730773448944
epoch£º563	 i:7 	 global-step:11267	 l-p:0.1261787712574005
epoch£º563	 i:8 	 global-step:11268	 l-p:0.12088379263877869
epoch£º563	 i:9 	 global-step:11269	 l-p:0.10651644319295883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[3.6284, 3.0629, 2.7600],
        [3.6284, 3.0618, 2.5845],
        [3.6284, 3.1595, 3.0736],
        [3.6284, 3.3437, 3.4223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.13400551676750183 
model_pd.l_d.mean(): -22.94845962524414 
model_pd.lagr.mean(): -22.814455032348633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1995], device='cuda:0')), ('power', tensor([-23.1480], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.13400551676750183
epoch£º564	 i:1 	 global-step:11281	 l-p:0.1431763917207718
epoch£º564	 i:2 	 global-step:11282	 l-p:0.13320112228393555
epoch£º564	 i:3 	 global-step:11283	 l-p:0.13472427427768707
epoch£º564	 i:4 	 global-step:11284	 l-p:0.17769969999790192
epoch£º564	 i:5 	 global-step:11285	 l-p:0.1131027415394783
epoch£º564	 i:6 	 global-step:11286	 l-p:0.14774389564990997
epoch£º564	 i:7 	 global-step:11287	 l-p:0.1408667415380478
epoch£º564	 i:8 	 global-step:11288	 l-p:0.17310462892055511
epoch£º564	 i:9 	 global-step:11289	 l-p:0.20739562809467316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2136, 3.2026, 3.2129],
        [3.2136, 2.9514, 3.0481],
        [3.2136, 2.8359, 2.3353],
        [3.2136, 2.8392, 2.3392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.13980214297771454 
model_pd.l_d.mean(): -23.29945945739746 
model_pd.lagr.mean(): -23.159656524658203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4109], device='cuda:0')), ('power', tensor([-23.7104], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.13980214297771454
epoch£º565	 i:1 	 global-step:11301	 l-p:0.10619033873081207
epoch£º565	 i:2 	 global-step:11302	 l-p:0.13850893080234528
epoch£º565	 i:3 	 global-step:11303	 l-p:0.35527870059013367
epoch£º565	 i:4 	 global-step:11304	 l-p:0.06496859341859818
epoch£º565	 i:5 	 global-step:11305	 l-p:0.2612127959728241
epoch£º565	 i:6 	 global-step:11306	 l-p:0.17192718386650085
epoch£º565	 i:7 	 global-step:11307	 l-p:0.16066646575927734
epoch£º565	 i:8 	 global-step:11308	 l-p:0.05774153396487236
epoch£º565	 i:9 	 global-step:11309	 l-p:0.1639380156993866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5812, 3.5526, 3.5777],
        [3.5812, 3.5777, 3.5811],
        [3.5812, 3.3418, 3.4341],
        [3.5812, 3.4689, 3.5441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.13642622530460358 
model_pd.l_d.mean(): -23.345287322998047 
model_pd.lagr.mean(): -23.208860397338867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2168], device='cuda:0')), ('power', tensor([-23.5621], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.13642622530460358
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11245663464069366
epoch£º566	 i:2 	 global-step:11322	 l-p:0.1319548785686493
epoch£º566	 i:3 	 global-step:11323	 l-p:0.12719948589801788
epoch£º566	 i:4 	 global-step:11324	 l-p:0.12059985846281052
epoch£º566	 i:5 	 global-step:11325	 l-p:0.09654513746500015
epoch£º566	 i:6 	 global-step:11326	 l-p:0.124147929251194
epoch£º566	 i:7 	 global-step:11327	 l-p:0.13144466280937195
epoch£º566	 i:8 	 global-step:11328	 l-p:0.12829633057117462
epoch£º566	 i:9 	 global-step:11329	 l-p:0.12456820905208588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[3.6634, 3.4359, 2.9420],
        [3.6634, 3.3889, 2.8814],
        [3.6634, 3.3531, 3.4202],
        [3.6634, 3.2622, 2.7284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.12274116277694702 
model_pd.l_d.mean(): -22.24081039428711 
model_pd.lagr.mean(): -22.11806869506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3058], device='cuda:0')), ('power', tensor([-22.5467], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.12274116277694702
epoch£º567	 i:1 	 global-step:11341	 l-p:0.13456875085830688
epoch£º567	 i:2 	 global-step:11342	 l-p:0.1479005366563797
epoch£º567	 i:3 	 global-step:11343	 l-p:0.1411518156528473
epoch£º567	 i:4 	 global-step:11344	 l-p:0.06444870680570602
epoch£º567	 i:5 	 global-step:11345	 l-p:0.1475105881690979
epoch£º567	 i:6 	 global-step:11346	 l-p:0.07572344690561295
epoch£º567	 i:7 	 global-step:11347	 l-p:0.18956689536571503
epoch£º567	 i:8 	 global-step:11348	 l-p:0.1614518165588379
epoch£º567	 i:9 	 global-step:11349	 l-p:0.1064099594950676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4016, 2.8439, 2.6467],
        [3.4016, 3.3816, 3.3997],
        [3.4016, 2.8506, 2.3159],
        [3.4016, 2.8265, 2.5896]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.16071268916130066 
model_pd.l_d.mean(): -23.274572372436523 
model_pd.lagr.mean(): -23.113859176635742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3493], device='cuda:0')), ('power', tensor([-23.6239], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.16071268916130066
epoch£º568	 i:1 	 global-step:11361	 l-p:0.189567431807518
epoch£º568	 i:2 	 global-step:11362	 l-p:0.12630298733711243
epoch£º568	 i:3 	 global-step:11363	 l-p:0.1234835535287857
epoch£º568	 i:4 	 global-step:11364	 l-p:-1.0111974477767944
epoch£º568	 i:5 	 global-step:11365	 l-p:0.1276632398366928
epoch£º568	 i:6 	 global-step:11366	 l-p:0.12310351431369781
epoch£º568	 i:7 	 global-step:11367	 l-p:0.04250119626522064
epoch£º568	 i:8 	 global-step:11368	 l-p:0.18821540474891663
epoch£º568	 i:9 	 global-step:11369	 l-p:0.24469466507434845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5660, 3.5660, 3.5661],
        [3.5660, 3.4949, 3.5495],
        [3.5660, 3.2909, 2.7848],
        [3.5660, 3.5660, 3.5661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.12653055787086487 
model_pd.l_d.mean(): -22.893417358398438 
model_pd.lagr.mean(): -22.766887664794922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2577], device='cuda:0')), ('power', tensor([-23.1512], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.12653055787086487
epoch£º569	 i:1 	 global-step:11381	 l-p:0.1533195525407791
epoch£º569	 i:2 	 global-step:11382	 l-p:0.16603852808475494
epoch£º569	 i:3 	 global-step:11383	 l-p:0.11945857107639313
epoch£º569	 i:4 	 global-step:11384	 l-p:0.13064856827259064
epoch£º569	 i:5 	 global-step:11385	 l-p:0.12468843162059784
epoch£º569	 i:6 	 global-step:11386	 l-p:0.1263991892337799
epoch£º569	 i:7 	 global-step:11387	 l-p:0.13507194817066193
epoch£º569	 i:8 	 global-step:11388	 l-p:0.13808448612689972
epoch£º569	 i:9 	 global-step:11389	 l-p:0.12321799993515015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5573, 3.5350, 3.5550],
        [3.5573, 3.5573, 3.5573],
        [3.5573, 3.0297, 2.8652],
        [3.5573, 3.0996, 2.5573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.13149458169937134 
model_pd.l_d.mean(): -23.184812545776367 
model_pd.lagr.mean(): -23.05331802368164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2187], device='cuda:0')), ('power', tensor([-23.4035], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.13149458169937134
epoch£º570	 i:1 	 global-step:11401	 l-p:0.14170366525650024
epoch£º570	 i:2 	 global-step:11402	 l-p:0.18201187252998352
epoch£º570	 i:3 	 global-step:11403	 l-p:-0.016855351626873016
epoch£º570	 i:4 	 global-step:11404	 l-p:0.14733755588531494
epoch£º570	 i:5 	 global-step:11405	 l-p:0.20418429374694824
epoch£º570	 i:6 	 global-step:11406	 l-p:0.16385650634765625
epoch£º570	 i:7 	 global-step:11407	 l-p:0.1589972823858261
epoch£º570	 i:8 	 global-step:11408	 l-p:0.11690032482147217
epoch£º570	 i:9 	 global-step:11409	 l-p:0.21126475930213928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4009, 2.9545, 2.9328],
        [3.4009, 3.4008, 3.4009],
        [3.4009, 3.3327, 3.3858],
        [3.4009, 3.3893, 3.4001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.199294313788414 
model_pd.l_d.mean(): -22.789932250976562 
model_pd.lagr.mean(): -22.59063720703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3224], device='cuda:0')), ('power', tensor([-23.1123], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.199294313788414
epoch£º571	 i:1 	 global-step:11421	 l-p:-0.057751234620809555
epoch£º571	 i:2 	 global-step:11422	 l-p:0.11657173186540604
epoch£º571	 i:3 	 global-step:11423	 l-p:0.06287414580583572
epoch£º571	 i:4 	 global-step:11424	 l-p:0.17302004992961884
epoch£º571	 i:5 	 global-step:11425	 l-p:0.12165063619613647
epoch£º571	 i:6 	 global-step:11426	 l-p:0.13325856626033783
epoch£º571	 i:7 	 global-step:11427	 l-p:0.1416558176279068
epoch£º571	 i:8 	 global-step:11428	 l-p:0.12642168998718262
epoch£º571	 i:9 	 global-step:11429	 l-p:0.11575236916542053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6356, 3.6336, 3.6356],
        [3.6356, 3.4277, 2.9376],
        [3.6356, 3.3585, 2.8477],
        [3.6356, 3.6087, 3.6324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.11929985135793686 
model_pd.l_d.mean(): -22.882373809814453 
model_pd.lagr.mean(): -22.76307487487793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2568], device='cuda:0')), ('power', tensor([-23.1392], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.11929985135793686
epoch£º572	 i:1 	 global-step:11441	 l-p:0.1482807844877243
epoch£º572	 i:2 	 global-step:11442	 l-p:0.1470327377319336
epoch£º572	 i:3 	 global-step:11443	 l-p:0.1682669073343277
epoch£º572	 i:4 	 global-step:11444	 l-p:0.12054421752691269
epoch£º572	 i:5 	 global-step:11445	 l-p:0.12742173671722412
epoch£º572	 i:6 	 global-step:11446	 l-p:0.14704380929470062
epoch£º572	 i:7 	 global-step:11447	 l-p:0.1302291452884674
epoch£º572	 i:8 	 global-step:11448	 l-p:-0.11388629674911499
epoch£º572	 i:9 	 global-step:11449	 l-p:0.16029728949069977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4658, 3.3362, 3.4190],
        [3.4658, 3.0834, 3.1179],
        [3.4658, 3.2814, 3.3774],
        [3.4658, 3.4653, 3.4658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.1528095155954361 
model_pd.l_d.mean(): -22.927539825439453 
model_pd.lagr.mean(): -22.774730682373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2617], device='cuda:0')), ('power', tensor([-23.1893], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.1528095155954361
epoch£º573	 i:1 	 global-step:11461	 l-p:0.14803774654865265
epoch£º573	 i:2 	 global-step:11462	 l-p:0.32435503602027893
epoch£º573	 i:3 	 global-step:11463	 l-p:-0.3933205008506775
epoch£º573	 i:4 	 global-step:11464	 l-p:0.4101090133190155
epoch£º573	 i:5 	 global-step:11465	 l-p:0.14050428569316864
epoch£º573	 i:6 	 global-step:11466	 l-p:0.15327922999858856
epoch£º573	 i:7 	 global-step:11467	 l-p:0.12709647417068481
epoch£º573	 i:8 	 global-step:11468	 l-p:0.1368703842163086
epoch£º573	 i:9 	 global-step:11469	 l-p:0.09313607215881348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6634, 3.1567, 3.0195],
        [3.6634, 3.1022, 2.5937],
        [3.6634, 3.6634, 3.6634],
        [3.6634, 3.6552, 3.6629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.12681908905506134 
model_pd.l_d.mean(): -23.336078643798828 
model_pd.lagr.mean(): -23.209259033203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1745], device='cuda:0')), ('power', tensor([-23.5106], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.12681908905506134
epoch£º574	 i:1 	 global-step:11481	 l-p:0.12861238420009613
epoch£º574	 i:2 	 global-step:11482	 l-p:0.12965844571590424
epoch£º574	 i:3 	 global-step:11483	 l-p:0.13221070170402527
epoch£º574	 i:4 	 global-step:11484	 l-p:0.15089616179466248
epoch£º574	 i:5 	 global-step:11485	 l-p:0.14718836545944214
epoch£º574	 i:6 	 global-step:11486	 l-p:0.6388059854507446
epoch£º574	 i:7 	 global-step:11487	 l-p:0.476847380399704
epoch£º574	 i:8 	 global-step:11488	 l-p:0.1480790674686432
epoch£º574	 i:9 	 global-step:11489	 l-p:0.11743495613336563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5164, 3.4906, 3.5135],
        [3.5164, 3.1538, 3.2017],
        [3.5164, 3.5164, 3.5164],
        [3.5164, 3.4871, 3.5128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): -0.22800204157829285 
model_pd.l_d.mean(): -23.416236877441406 
model_pd.lagr.mean(): -23.64423942565918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2890], device='cuda:0')), ('power', tensor([-23.7052], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:-0.22800204157829285
epoch£º575	 i:1 	 global-step:11501	 l-p:0.005718765314668417
epoch£º575	 i:2 	 global-step:11502	 l-p:-0.03502959758043289
epoch£º575	 i:3 	 global-step:11503	 l-p:0.116744264960289
epoch£º575	 i:4 	 global-step:11504	 l-p:0.15525799989700317
epoch£º575	 i:5 	 global-step:11505	 l-p:0.13115432858467102
epoch£º575	 i:6 	 global-step:11506	 l-p:0.12800325453281403
epoch£º575	 i:7 	 global-step:11507	 l-p:0.1393376737833023
epoch£º575	 i:8 	 global-step:11508	 l-p:0.16998590528964996
epoch£º575	 i:9 	 global-step:11509	 l-p:0.1416344791650772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5803, 3.5796, 3.5803],
        [3.5803, 3.5729, 3.5799],
        [3.5803, 3.2542, 2.7302],
        [3.5803, 2.9751, 2.5741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.17133203148841858 
model_pd.l_d.mean(): -23.352663040161133 
model_pd.lagr.mean(): -23.181331634521484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2348], device='cuda:0')), ('power', tensor([-23.5875], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.17133203148841858
epoch£º576	 i:1 	 global-step:11521	 l-p:0.1374945342540741
epoch£º576	 i:2 	 global-step:11522	 l-p:0.13457027077674866
epoch£º576	 i:3 	 global-step:11523	 l-p:0.21196149289608002
epoch£º576	 i:4 	 global-step:11524	 l-p:0.12156296521425247
epoch£º576	 i:5 	 global-step:11525	 l-p:0.12584395706653595
epoch£º576	 i:6 	 global-step:11526	 l-p:0.24026170372962952
epoch£º576	 i:7 	 global-step:11527	 l-p:0.10764673352241516
epoch£º576	 i:8 	 global-step:11528	 l-p:0.15231233835220337
epoch£º576	 i:9 	 global-step:11529	 l-p:0.13947556912899017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5804, 3.3392, 3.4344],
        [3.5804, 3.5804, 3.5804],
        [3.5804, 3.0072, 2.4845],
        [3.5804, 3.5804, 3.5804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.2286403924226761 
model_pd.l_d.mean(): -23.590665817260742 
model_pd.lagr.mean(): -23.36202621459961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1994], device='cuda:0')), ('power', tensor([-23.7901], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.2286403924226761
epoch£º577	 i:1 	 global-step:11541	 l-p:0.14622829854488373
epoch£º577	 i:2 	 global-step:11542	 l-p:0.08897152543067932
epoch£º577	 i:3 	 global-step:11543	 l-p:0.11223363876342773
epoch£º577	 i:4 	 global-step:11544	 l-p:0.13447560369968414
epoch£º577	 i:5 	 global-step:11545	 l-p:0.13512784242630005
epoch£º577	 i:6 	 global-step:11546	 l-p:0.19162975251674652
epoch£º577	 i:7 	 global-step:11547	 l-p:0.12512822449207306
epoch£º577	 i:8 	 global-step:11548	 l-p:0.13106730580329895
epoch£º577	 i:9 	 global-step:11549	 l-p:0.17145255208015442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5664, 3.5664, 3.5664],
        [3.5664, 2.9797, 2.4645],
        [3.5664, 3.1537, 3.1596],
        [3.5664, 3.0017, 2.7746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.16832469403743744 
model_pd.l_d.mean(): -23.494348526000977 
model_pd.lagr.mean(): -23.32602310180664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2074], device='cuda:0')), ('power', tensor([-23.7017], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.16832469403743744
epoch£º578	 i:1 	 global-step:11561	 l-p:0.19949647784233093
epoch£º578	 i:2 	 global-step:11562	 l-p:0.11726431548595428
epoch£º578	 i:3 	 global-step:11563	 l-p:0.10279053449630737
epoch£º578	 i:4 	 global-step:11564	 l-p:0.301334947347641
epoch£º578	 i:5 	 global-step:11565	 l-p:0.14370344579219818
epoch£º578	 i:6 	 global-step:11566	 l-p:0.12068330496549606
epoch£º578	 i:7 	 global-step:11567	 l-p:0.11960748583078384
epoch£º578	 i:8 	 global-step:11568	 l-p:0.1261805295944214
epoch£º578	 i:9 	 global-step:11569	 l-p:0.140825092792511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6652, 3.4188, 2.9113],
        [3.6652, 3.4940, 3.5870],
        [3.6652, 3.1214, 2.9198],
        [3.6652, 3.5437, 3.6230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.12822997570037842 
model_pd.l_d.mean(): -23.413408279418945 
model_pd.lagr.mean(): -23.285179138183594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1748], device='cuda:0')), ('power', tensor([-23.5882], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.12822997570037842
epoch£º579	 i:1 	 global-step:11581	 l-p:0.1506880223751068
epoch£º579	 i:2 	 global-step:11582	 l-p:0.11676456034183502
epoch£º579	 i:3 	 global-step:11583	 l-p:0.15067735314369202
epoch£º579	 i:4 	 global-step:11584	 l-p:0.24141593277454376
epoch£º579	 i:5 	 global-step:11585	 l-p:0.1476799100637436
epoch£º579	 i:6 	 global-step:11586	 l-p:0.17475676536560059
epoch£º579	 i:7 	 global-step:11587	 l-p:0.10960354655981064
epoch£º579	 i:8 	 global-step:11588	 l-p:0.14968544244766235
epoch£º579	 i:9 	 global-step:11589	 l-p:0.19500084221363068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5091, 3.4645, 3.5018],
        [3.5091, 3.4968, 3.5082],
        [3.5091, 2.8857, 2.4663],
        [3.5091, 3.4519, 3.4980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.06393353641033173 
model_pd.l_d.mean(): -23.773204803466797 
model_pd.lagr.mean(): -23.709270477294922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1739], device='cuda:0')), ('power', tensor([-23.9471], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.06393353641033173
epoch£º580	 i:1 	 global-step:11601	 l-p:0.1457432359457016
epoch£º580	 i:2 	 global-step:11602	 l-p:0.13495118916034698
epoch£º580	 i:3 	 global-step:11603	 l-p:0.07595887780189514
epoch£º580	 i:4 	 global-step:11604	 l-p:-0.11679007858037949
epoch£º580	 i:5 	 global-step:11605	 l-p:0.1617850512266159
epoch£º580	 i:6 	 global-step:11606	 l-p:0.15242822468280792
epoch£º580	 i:7 	 global-step:11607	 l-p:-0.2683267295360565
epoch£º580	 i:8 	 global-step:11608	 l-p:0.10045015066862106
epoch£º580	 i:9 	 global-step:11609	 l-p:0.1191931664943695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6369, 3.6369, 3.6369],
        [3.6369, 3.1372, 2.5852],
        [3.6369, 3.0978, 2.9178],
        [3.6369, 3.4106, 3.5072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.13549913465976715 
model_pd.l_d.mean(): -23.30596923828125 
model_pd.lagr.mean(): -23.170469284057617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2218], device='cuda:0')), ('power', tensor([-23.5278], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.13549913465976715
epoch£º581	 i:1 	 global-step:11621	 l-p:0.11923255026340485
epoch£º581	 i:2 	 global-step:11622	 l-p:0.12109702825546265
epoch£º581	 i:3 	 global-step:11623	 l-p:0.12274815142154694
epoch£º581	 i:4 	 global-step:11624	 l-p:0.1418628692626953
epoch£º581	 i:5 	 global-step:11625	 l-p:0.1724584549665451
epoch£º581	 i:6 	 global-step:11626	 l-p:0.17536091804504395
epoch£º581	 i:7 	 global-step:11627	 l-p:0.13049954175949097
epoch£º581	 i:8 	 global-step:11628	 l-p:0.060066670179367065
epoch£º581	 i:9 	 global-step:11629	 l-p:0.12901632487773895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6181, 3.5061, 3.5820],
        [3.6181, 3.3210, 2.7994],
        [3.6181, 3.6173, 3.6180],
        [3.6181, 3.5953, 3.6157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.128890722990036 
model_pd.l_d.mean(): -22.235965728759766 
model_pd.lagr.mean(): -22.107074737548828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3007], device='cuda:0')), ('power', tensor([-22.5366], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.128890722990036
epoch£º582	 i:1 	 global-step:11641	 l-p:0.1781434565782547
epoch£º582	 i:2 	 global-step:11642	 l-p:0.11708308011293411
epoch£º582	 i:3 	 global-step:11643	 l-p:0.12992939352989197
epoch£º582	 i:4 	 global-step:11644	 l-p:0.1267433762550354
epoch£º582	 i:5 	 global-step:11645	 l-p:-0.6369466781616211
epoch£º582	 i:6 	 global-step:11646	 l-p:0.1850554645061493
epoch£º582	 i:7 	 global-step:11647	 l-p:0.07258133590221405
epoch£º582	 i:8 	 global-step:11648	 l-p:0.2552230656147003
epoch£º582	 i:9 	 global-step:11649	 l-p:0.13986723124980927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5344, 3.4595, 3.5168],
        [3.5344, 2.9603, 2.4175],
        [3.5344, 2.9712, 2.4238],
        [3.5344, 2.9484, 2.6917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.1331436187028885 
model_pd.l_d.mean(): -23.461727142333984 
model_pd.lagr.mean(): -23.328582763671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2657], device='cuda:0')), ('power', tensor([-23.7274], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.1331436187028885
epoch£º583	 i:1 	 global-step:11661	 l-p:0.13434472680091858
epoch£º583	 i:2 	 global-step:11662	 l-p:0.1993211954832077
epoch£º583	 i:3 	 global-step:11663	 l-p:0.3348672091960907
epoch£º583	 i:4 	 global-step:11664	 l-p:0.03847286477684975
epoch£º583	 i:5 	 global-step:11665	 l-p:0.14421159029006958
epoch£º583	 i:6 	 global-step:11666	 l-p:0.13175518810749054
epoch£º583	 i:7 	 global-step:11667	 l-p:0.5227944850921631
epoch£º583	 i:8 	 global-step:11668	 l-p:0.22684608399868011
epoch£º583	 i:9 	 global-step:11669	 l-p:0.1282111555337906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6259, 3.5816, 3.6187],
        [3.6259, 3.6259, 3.6259],
        [3.6259, 3.5042, 3.5842],
        [3.6259, 3.4786, 3.5671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.07885786890983582 
model_pd.l_d.mean(): -23.539487838745117 
model_pd.lagr.mean(): -23.460630416870117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1736], device='cuda:0')), ('power', tensor([-23.7130], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.07885786890983582
epoch£º584	 i:1 	 global-step:11681	 l-p:0.13316334784030914
epoch£º584	 i:2 	 global-step:11682	 l-p:0.14840808510780334
epoch£º584	 i:3 	 global-step:11683	 l-p:0.14250756800174713
epoch£º584	 i:4 	 global-step:11684	 l-p:0.1039135754108429
epoch£º584	 i:5 	 global-step:11685	 l-p:0.13909471035003662
epoch£º584	 i:6 	 global-step:11686	 l-p:0.12502239644527435
epoch£º584	 i:7 	 global-step:11687	 l-p:0.11516929417848587
epoch£º584	 i:8 	 global-step:11688	 l-p:0.14453551173210144
epoch£º584	 i:9 	 global-step:11689	 l-p:0.10792865604162216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5643, 3.3918, 3.4868],
        [3.5643, 3.5488, 3.5631],
        [3.5643, 3.5611, 3.5642],
        [3.5643, 3.5439, 3.5624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.16110773384571075 
model_pd.l_d.mean(): -23.427490234375 
model_pd.lagr.mean(): -23.266382217407227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1880], device='cuda:0')), ('power', tensor([-23.6155], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.16110773384571075
epoch£º585	 i:1 	 global-step:11701	 l-p:0.13087081909179688
epoch£º585	 i:2 	 global-step:11702	 l-p:0.29210901260375977
epoch£º585	 i:3 	 global-step:11703	 l-p:0.06222892925143242
epoch£º585	 i:4 	 global-step:11704	 l-p:0.14052437245845795
epoch£º585	 i:5 	 global-step:11705	 l-p:0.14689065515995026
epoch£º585	 i:6 	 global-step:11706	 l-p:0.17049571871757507
epoch£º585	 i:7 	 global-step:11707	 l-p:0.12849044799804688
epoch£º585	 i:8 	 global-step:11708	 l-p:0.15054339170455933
epoch£º585	 i:9 	 global-step:11709	 l-p:0.06348728388547897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5152, 3.5139, 3.5151],
        [3.5152, 3.1791, 2.6522],
        [3.5152, 3.1699, 3.2351],
        [3.5152, 3.4451, 3.4995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.1467500478029251 
model_pd.l_d.mean(): -23.314180374145508 
model_pd.lagr.mean(): -23.167430877685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2491], device='cuda:0')), ('power', tensor([-23.5632], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.1467500478029251
epoch£º586	 i:1 	 global-step:11721	 l-p:0.15780793130397797
epoch£º586	 i:2 	 global-step:11722	 l-p:0.019925322383642197
epoch£º586	 i:3 	 global-step:11723	 l-p:0.15977106988430023
epoch£º586	 i:4 	 global-step:11724	 l-p:0.3641047477722168
epoch£º586	 i:5 	 global-step:11725	 l-p:0.11049703508615494
epoch£º586	 i:6 	 global-step:11726	 l-p:0.14885187149047852
epoch£º586	 i:7 	 global-step:11727	 l-p:0.1276828795671463
epoch£º586	 i:8 	 global-step:11728	 l-p:0.033623140305280685
epoch£º586	 i:9 	 global-step:11729	 l-p:0.13226450979709625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6385, 3.0911, 2.9059],
        [3.6385, 3.0651, 2.5312],
        [3.6385, 3.0408, 2.5361],
        [3.6385, 3.5186, 3.5980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.11518628150224686 
model_pd.l_d.mean(): -23.259300231933594 
model_pd.lagr.mean(): -23.144113540649414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2360], device='cuda:0')), ('power', tensor([-23.4953], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.11518628150224686
epoch£º587	 i:1 	 global-step:11741	 l-p:0.16068023443222046
epoch£º587	 i:2 	 global-step:11742	 l-p:0.17762528359889984
epoch£º587	 i:3 	 global-step:11743	 l-p:0.12115087360143661
epoch£º587	 i:4 	 global-step:11744	 l-p:0.12819413840770721
epoch£º587	 i:5 	 global-step:11745	 l-p:0.14882051944732666
epoch£º587	 i:6 	 global-step:11746	 l-p:0.1500697135925293
epoch£º587	 i:7 	 global-step:11747	 l-p:0.3507382869720459
epoch£º587	 i:8 	 global-step:11748	 l-p:0.12037230283021927
epoch£º587	 i:9 	 global-step:11749	 l-p:0.13881154358386993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5070, 3.5070, 3.5070],
        [3.5070, 3.3112, 3.4105],
        [3.5070, 3.0339, 2.9892],
        [3.5070, 3.3416, 3.4359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.06993009895086288 
model_pd.l_d.mean(): -23.444828033447266 
model_pd.lagr.mean(): -23.374897003173828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2477], device='cuda:0')), ('power', tensor([-23.6925], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.06993009895086288
epoch£º588	 i:1 	 global-step:11761	 l-p:0.20763324201107025
epoch£º588	 i:2 	 global-step:11762	 l-p:-0.05920766666531563
epoch£º588	 i:3 	 global-step:11763	 l-p:0.10265641659498215
epoch£º588	 i:4 	 global-step:11764	 l-p:0.1916515827178955
epoch£º588	 i:5 	 global-step:11765	 l-p:0.144477978348732
epoch£º588	 i:6 	 global-step:11766	 l-p:0.13408006727695465
epoch£º588	 i:7 	 global-step:11767	 l-p:0.12311694771051407
epoch£º588	 i:8 	 global-step:11768	 l-p:0.1310732662677765
epoch£º588	 i:9 	 global-step:11769	 l-p:0.17552338540554047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5514, 3.5210, 3.5476],
        [3.5514, 3.5497, 3.5514],
        [3.5514, 3.4581, 3.5257],
        [3.5514, 3.5513, 3.5514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.914497435092926 
model_pd.l_d.mean(): -23.286705017089844 
model_pd.lagr.mean(): -22.372207641601562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2676], device='cuda:0')), ('power', tensor([-23.5543], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.914497435092926
epoch£º589	 i:1 	 global-step:11781	 l-p:0.12121318280696869
epoch£º589	 i:2 	 global-step:11782	 l-p:0.25278303027153015
epoch£º589	 i:3 	 global-step:11783	 l-p:0.16624316573143005
epoch£º589	 i:4 	 global-step:11784	 l-p:0.13312987983226776
epoch£º589	 i:5 	 global-step:11785	 l-p:0.14086784422397614
epoch£º589	 i:6 	 global-step:11786	 l-p:0.12924326956272125
epoch£º589	 i:7 	 global-step:11787	 l-p:0.13269133865833282
epoch£º589	 i:8 	 global-step:11788	 l-p:0.13366518914699554
epoch£º589	 i:9 	 global-step:11789	 l-p:0.028916392475366592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5072, 3.4332, 3.4901],
        [3.5072, 2.8716, 2.3933],
        [3.5072, 3.3814, 3.4639],
        [3.5072, 3.4864, 3.5052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.15672935545444489 
model_pd.l_d.mean(): -23.41169548034668 
model_pd.lagr.mean(): -23.254966735839844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2725], device='cuda:0')), ('power', tensor([-23.6842], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.15672935545444489
epoch£º590	 i:1 	 global-step:11801	 l-p:0.041088033467531204
epoch£º590	 i:2 	 global-step:11802	 l-p:0.13202747702598572
epoch£º590	 i:3 	 global-step:11803	 l-p:0.1463373899459839
epoch£º590	 i:4 	 global-step:11804	 l-p:0.17265647649765015
epoch£º590	 i:5 	 global-step:11805	 l-p:0.149940624833107
epoch£º590	 i:6 	 global-step:11806	 l-p:0.14809224009513855
epoch£º590	 i:7 	 global-step:11807	 l-p:0.15645040571689606
epoch£º590	 i:8 	 global-step:11808	 l-p:0.07134714722633362
epoch£º590	 i:9 	 global-step:11809	 l-p:0.16018614172935486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4298, 2.8150, 2.2731],
        [3.4298, 2.8594, 2.6813],
        [3.4298, 3.4296, 3.4298],
        [3.4298, 3.4291, 3.4298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.13347111642360687 
model_pd.l_d.mean(): -22.455352783203125 
model_pd.lagr.mean(): -22.321882247924805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3369], device='cuda:0')), ('power', tensor([-22.7922], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.13347111642360687
epoch£º591	 i:1 	 global-step:11821	 l-p:0.16004733741283417
epoch£º591	 i:2 	 global-step:11822	 l-p:0.11630631983280182
epoch£º591	 i:3 	 global-step:11823	 l-p:0.1848660558462143
epoch£º591	 i:4 	 global-step:11824	 l-p:0.15030677616596222
epoch£º591	 i:5 	 global-step:11825	 l-p:0.15582214295864105
epoch£º591	 i:6 	 global-step:11826	 l-p:0.14804108440876007
epoch£º591	 i:7 	 global-step:11827	 l-p:0.14122220873832703
epoch£º591	 i:8 	 global-step:11828	 l-p:0.12545445561408997
epoch£º591	 i:9 	 global-step:11829	 l-p:0.12456700950860977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[3.6880, 3.2666, 3.2669],
        [3.6880, 3.2662, 3.2662],
        [3.6880, 3.4616, 2.9527],
        [3.6880, 3.2248, 3.1778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.12204810231924057 
model_pd.l_d.mean(): -23.235673904418945 
model_pd.lagr.mean(): -23.11362648010254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1618], device='cuda:0')), ('power', tensor([-23.3975], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.12204810231924057
epoch£º592	 i:1 	 global-step:11841	 l-p:0.15337565541267395
epoch£º592	 i:2 	 global-step:11842	 l-p:0.12394515424966812
epoch£º592	 i:3 	 global-step:11843	 l-p:0.1263388991355896
epoch£º592	 i:4 	 global-step:11844	 l-p:0.2134658396244049
epoch£º592	 i:5 	 global-step:11845	 l-p:0.10435953736305237
epoch£º592	 i:6 	 global-step:11846	 l-p:0.13693390786647797
epoch£º592	 i:7 	 global-step:11847	 l-p:0.13090810179710388
epoch£º592	 i:8 	 global-step:11848	 l-p:0.1656169444322586
epoch£º592	 i:9 	 global-step:11849	 l-p:0.10142020881175995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5003, 3.3401, 3.4339],
        [3.5003, 3.2410, 3.3399],
        [3.5003, 3.5003, 3.5003],
        [3.5003, 3.5003, 3.5003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.18887943029403687 
model_pd.l_d.mean(): -23.675643920898438 
model_pd.lagr.mean(): -23.486764907836914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2210], device='cuda:0')), ('power', tensor([-23.8967], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.18887943029403687
epoch£º593	 i:1 	 global-step:11861	 l-p:0.07535471022129059
epoch£º593	 i:2 	 global-step:11862	 l-p:0.23160777986049652
epoch£º593	 i:3 	 global-step:11863	 l-p:0.15525752305984497
epoch£º593	 i:4 	 global-step:11864	 l-p:0.13285072147846222
epoch£º593	 i:5 	 global-step:11865	 l-p:0.16561150550842285
epoch£º593	 i:6 	 global-step:11866	 l-p:0.10926246643066406
epoch£º593	 i:7 	 global-step:11867	 l-p:0.10220877081155777
epoch£º593	 i:8 	 global-step:11868	 l-p:0.1305006891489029
epoch£º593	 i:9 	 global-step:11869	 l-p:0.21686308085918427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6170, 3.3822, 3.4817],
        [3.6170, 3.6169, 3.6170],
        [3.6170, 3.4831, 3.5683],
        [3.6170, 3.1348, 2.5712]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.1904204934835434 
model_pd.l_d.mean(): -23.37560272216797 
model_pd.lagr.mean(): -23.185182571411133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2596], device='cuda:0')), ('power', tensor([-23.6352], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.1904204934835434
epoch£º594	 i:1 	 global-step:11881	 l-p:0.12643855810165405
epoch£º594	 i:2 	 global-step:11882	 l-p:0.15300379693508148
epoch£º594	 i:3 	 global-step:11883	 l-p:0.12278691679239273
epoch£º594	 i:4 	 global-step:11884	 l-p:0.13419528305530548
epoch£º594	 i:5 	 global-step:11885	 l-p:0.14843450486660004
epoch£º594	 i:6 	 global-step:11886	 l-p:0.1321144551038742
epoch£º594	 i:7 	 global-step:11887	 l-p:0.13574759662151337
epoch£º594	 i:8 	 global-step:11888	 l-p:0.0105279590934515
epoch£º594	 i:9 	 global-step:11889	 l-p:0.1259744018316269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6283, 3.1501, 2.5858],
        [3.6283, 3.2001, 3.2013],
        [3.6283, 3.6282, 3.6283],
        [3.6283, 3.6250, 3.6282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.1295069009065628 
model_pd.l_d.mean(): -23.519386291503906 
model_pd.lagr.mean(): -23.38987922668457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1751], device='cuda:0')), ('power', tensor([-23.6945], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.1295069009065628
epoch£º595	 i:1 	 global-step:11901	 l-p:0.11976933479309082
epoch£º595	 i:2 	 global-step:11902	 l-p:0.1419709026813507
epoch£º595	 i:3 	 global-step:11903	 l-p:0.18274623155593872
epoch£º595	 i:4 	 global-step:11904	 l-p:0.27399080991744995
epoch£º595	 i:5 	 global-step:11905	 l-p:0.4544207751750946
epoch£º595	 i:6 	 global-step:11906	 l-p:0.21835722029209137
epoch£º595	 i:7 	 global-step:11907	 l-p:0.1414773166179657
epoch£º595	 i:8 	 global-step:11908	 l-p:0.11326401680707932
epoch£º595	 i:9 	 global-step:11909	 l-p:0.1052044928073883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6683, 3.5903, 3.6495],
        [3.6683, 3.6667, 3.6683],
        [3.6683, 3.0474, 2.6338],
        [3.6683, 3.6683, 3.6683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): -0.3797198534011841 
model_pd.l_d.mean(): -23.463191986083984 
model_pd.lagr.mean(): -23.842912673950195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1583], device='cuda:0')), ('power', tensor([-23.6215], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:-0.3797198534011841
epoch£º596	 i:1 	 global-step:11921	 l-p:0.1218358501791954
epoch£º596	 i:2 	 global-step:11922	 l-p:0.1256168931722641
epoch£º596	 i:3 	 global-step:11923	 l-p:0.11710052937269211
epoch£º596	 i:4 	 global-step:11924	 l-p:0.1310107260942459
epoch£º596	 i:5 	 global-step:11925	 l-p:0.18583182990550995
epoch£º596	 i:6 	 global-step:11926	 l-p:0.15902778506278992
epoch£º596	 i:7 	 global-step:11927	 l-p:0.15070444345474243
epoch£º596	 i:8 	 global-step:11928	 l-p:0.1373026818037033
epoch£º596	 i:9 	 global-step:11929	 l-p:0.12355327606201172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5522, 3.1742, 3.2237],
        [3.5522, 3.5007, 3.5431],
        [3.5522, 3.5521, 3.5522],
        [3.5522, 3.5488, 3.5521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): -0.308480441570282 
model_pd.l_d.mean(): -23.416776657104492 
model_pd.lagr.mean(): -23.725257873535156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2080], device='cuda:0')), ('power', tensor([-23.6247], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:-0.308480441570282
epoch£º597	 i:1 	 global-step:11941	 l-p:0.13345274329185486
epoch£º597	 i:2 	 global-step:11942	 l-p:0.07867196202278137
epoch£º597	 i:3 	 global-step:11943	 l-p:0.13900186121463776
epoch£º597	 i:4 	 global-step:11944	 l-p:0.23261383175849915
epoch£º597	 i:5 	 global-step:11945	 l-p:0.16685351729393005
epoch£º597	 i:6 	 global-step:11946	 l-p:0.09314366430044174
epoch£º597	 i:7 	 global-step:11947	 l-p:0.14172276854515076
epoch£º597	 i:8 	 global-step:11948	 l-p:0.12317754328250885
epoch£º597	 i:9 	 global-step:11949	 l-p:0.12728357315063477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5560, 3.5544, 3.5560],
        [3.5560, 2.9874, 2.4247],
        [3.5560, 3.2419, 3.3260],
        [3.5560, 3.1784, 2.6338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.11136938631534576 
model_pd.l_d.mean(): -22.72210121154785 
model_pd.lagr.mean(): -22.61073112487793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2861], device='cuda:0')), ('power', tensor([-23.0082], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.11136938631534576
epoch£º598	 i:1 	 global-step:11961	 l-p:0.26552167534828186
epoch£º598	 i:2 	 global-step:11962	 l-p:0.036315612494945526
epoch£º598	 i:3 	 global-step:11963	 l-p:0.13188806176185608
epoch£º598	 i:4 	 global-step:11964	 l-p:-0.04151856154203415
epoch£º598	 i:5 	 global-step:11965	 l-p:0.1971052885055542
epoch£º598	 i:6 	 global-step:11966	 l-p:0.13422133028507233
epoch£º598	 i:7 	 global-step:11967	 l-p:0.13827186822891235
epoch£º598	 i:8 	 global-step:11968	 l-p:0.15073375403881073
epoch£º598	 i:9 	 global-step:11969	 l-p:0.13548234105110168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4995, 3.4995, 3.4995],
        [3.4995, 3.1639, 3.2413],
        [3.4995, 2.9837, 2.4206],
        [3.4995, 3.4917, 3.4991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.11403428763151169 
model_pd.l_d.mean(): -23.068593978881836 
model_pd.lagr.mean(): -22.954559326171875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3462], device='cuda:0')), ('power', tensor([-23.4148], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.11403428763151169
epoch£º599	 i:1 	 global-step:11981	 l-p:0.16130797564983368
epoch£º599	 i:2 	 global-step:11982	 l-p:0.12898936867713928
epoch£º599	 i:3 	 global-step:11983	 l-p:0.17495177686214447
epoch£º599	 i:4 	 global-step:11984	 l-p:0.08706748485565186
epoch£º599	 i:5 	 global-step:11985	 l-p:0.06816183775663376
epoch£º599	 i:6 	 global-step:11986	 l-p:0.14321200549602509
epoch£º599	 i:7 	 global-step:11987	 l-p:0.14070193469524384
epoch£º599	 i:8 	 global-step:11988	 l-p:-0.25666695833206177
epoch£º599	 i:9 	 global-step:11989	 l-p:0.1290154606103897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4525, 2.8256, 2.2780],
        [3.4525, 3.4525, 3.4525],
        [3.4525, 3.4525, 3.4525],
        [3.4525, 3.4525, 3.4525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.1396559327840805 
model_pd.l_d.mean(): -22.965808868408203 
model_pd.lagr.mean(): -22.826152801513672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2742], device='cuda:0')), ('power', tensor([-23.2400], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.1396559327840805
epoch£º600	 i:1 	 global-step:12001	 l-p:0.14560401439666748
epoch£º600	 i:2 	 global-step:12002	 l-p:0.16054950654506683
epoch£º600	 i:3 	 global-step:12003	 l-p:0.047026995569467545
epoch£º600	 i:4 	 global-step:12004	 l-p:-0.04407542943954468
epoch£º600	 i:5 	 global-step:12005	 l-p:0.11524106562137604
epoch£º600	 i:6 	 global-step:12006	 l-p:0.20068477094173431
epoch£º600	 i:7 	 global-step:12007	 l-p:0.10012952238321304
epoch£º600	 i:8 	 global-step:12008	 l-p:0.12478963285684586
epoch£º600	 i:9 	 global-step:12009	 l-p:0.1297285407781601
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5269, 2.8829, 2.3777],
        [3.5269, 2.9419, 2.7339],
        [3.5269, 3.4846, 3.5205],
        [3.5269, 3.5257, 3.5269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.17818042635917664 
model_pd.l_d.mean(): -23.56452178955078 
model_pd.lagr.mean(): -23.386341094970703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1929], device='cuda:0')), ('power', tensor([-23.7574], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.17818042635917664
epoch£º601	 i:1 	 global-step:12021	 l-p:0.12684187293052673
epoch£º601	 i:2 	 global-step:12022	 l-p:0.030364055186510086
epoch£º601	 i:3 	 global-step:12023	 l-p:0.1398746818304062
epoch£º601	 i:4 	 global-step:12024	 l-p:0.12990951538085938
epoch£º601	 i:5 	 global-step:12025	 l-p:0.1651943176984787
epoch£º601	 i:6 	 global-step:12026	 l-p:0.14964324235916138
epoch£º601	 i:7 	 global-step:12027	 l-p:-0.15460853278636932
epoch£º601	 i:8 	 global-step:12028	 l-p:0.49987563490867615
epoch£º601	 i:9 	 global-step:12029	 l-p:0.1005072370171547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5813, 3.5813, 3.5813],
        [3.5813, 2.9696, 2.4260],
        [3.5813, 3.5312, 3.5726],
        [3.5813, 2.9527, 2.4322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.1739092618227005 
model_pd.l_d.mean(): -23.409496307373047 
model_pd.lagr.mean(): -23.235586166381836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2308], device='cuda:0')), ('power', tensor([-23.6403], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.1739092618227005
epoch£º602	 i:1 	 global-step:12041	 l-p:0.13807477056980133
epoch£º602	 i:2 	 global-step:12042	 l-p:0.11386489868164062
epoch£º602	 i:3 	 global-step:12043	 l-p:0.10782541334629059
epoch£º602	 i:4 	 global-step:12044	 l-p:0.16602161526679993
epoch£º602	 i:5 	 global-step:12045	 l-p:0.14221739768981934
epoch£º602	 i:6 	 global-step:12046	 l-p:0.16438418626785278
epoch£º602	 i:7 	 global-step:12047	 l-p:0.14001116156578064
epoch£º602	 i:8 	 global-step:12048	 l-p:0.13624341785907745
epoch£º602	 i:9 	 global-step:12049	 l-p:0.13970939815044403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6309, 3.3507, 3.4447],
        [3.6309, 3.6086, 3.6287],
        [3.6309, 3.6308, 3.6309],
        [3.6309, 3.3719, 3.4702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.14685919880867004 
model_pd.l_d.mean(): -23.61614227294922 
model_pd.lagr.mean(): -23.469282150268555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1450], device='cuda:0')), ('power', tensor([-23.7612], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.14685919880867004
epoch£º603	 i:1 	 global-step:12061	 l-p:0.14133433997631073
epoch£º603	 i:2 	 global-step:12062	 l-p:0.12303247302770615
epoch£º603	 i:3 	 global-step:12063	 l-p:0.12967976927757263
epoch£º603	 i:4 	 global-step:12064	 l-p:0.17270416021347046
epoch£º603	 i:5 	 global-step:12065	 l-p:-5.226682662963867
epoch£º603	 i:6 	 global-step:12066	 l-p:0.14410090446472168
epoch£º603	 i:7 	 global-step:12067	 l-p:0.0790397971868515
epoch£º603	 i:8 	 global-step:12068	 l-p:0.1175878494977951
epoch£º603	 i:9 	 global-step:12069	 l-p:0.1256682127714157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5210, 3.0273, 2.4633],
        [3.5210, 3.5211, 3.5210],
        [3.5210, 3.3778, 3.4673],
        [3.5210, 3.0530, 3.0283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.1624440848827362 
model_pd.l_d.mean(): -23.501182556152344 
model_pd.lagr.mean(): -23.3387393951416 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2232], device='cuda:0')), ('power', tensor([-23.7244], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.1624440848827362
epoch£º604	 i:1 	 global-step:12081	 l-p:0.15667124092578888
epoch£º604	 i:2 	 global-step:12082	 l-p:0.06466162949800491
epoch£º604	 i:3 	 global-step:12083	 l-p:0.36531105637550354
epoch£º604	 i:4 	 global-step:12084	 l-p:0.06556057929992676
epoch£º604	 i:5 	 global-step:12085	 l-p:0.11390836536884308
epoch£º604	 i:6 	 global-step:12086	 l-p:0.15458711981773376
epoch£º604	 i:7 	 global-step:12087	 l-p:-0.6480609774589539
epoch£º604	 i:8 	 global-step:12088	 l-p:0.14146281778812408
epoch£º604	 i:9 	 global-step:12089	 l-p:0.12270542234182358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5920, 2.9530, 2.4594],
        [3.5920, 3.3441, 3.4449],
        [3.5920, 3.4738, 3.5536],
        [3.5920, 3.4967, 3.5657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.09651151299476624 
model_pd.l_d.mean(): -23.432401657104492 
model_pd.lagr.mean(): -23.33588981628418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2003], device='cuda:0')), ('power', tensor([-23.6327], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.09651151299476624
epoch£º605	 i:1 	 global-step:12101	 l-p:0.2029808759689331
epoch£º605	 i:2 	 global-step:12102	 l-p:0.12988200783729553
epoch£º605	 i:3 	 global-step:12103	 l-p:0.12509727478027344
epoch£º605	 i:4 	 global-step:12104	 l-p:0.15031209588050842
epoch£º605	 i:5 	 global-step:12105	 l-p:0.1360941380262375
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11133107542991638
epoch£º605	 i:7 	 global-step:12107	 l-p:0.15426577627658844
epoch£º605	 i:8 	 global-step:12108	 l-p:0.12601923942565918
epoch£º605	 i:9 	 global-step:12109	 l-p:0.3263248801231384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5903, 3.5903, 3.5903],
        [3.5903, 3.0942, 2.5249],
        [3.5903, 3.1278, 3.1035],
        [3.5903, 3.5896, 3.5903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.17364951968193054 
model_pd.l_d.mean(): -23.59586524963379 
model_pd.lagr.mean(): -23.422216415405273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1809], device='cuda:0')), ('power', tensor([-23.7768], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.17364951968193054
epoch£º606	 i:1 	 global-step:12121	 l-p:0.28303077816963196
epoch£º606	 i:2 	 global-step:12122	 l-p:0.12717781960964203
epoch£º606	 i:3 	 global-step:12123	 l-p:0.20134851336479187
epoch£º606	 i:4 	 global-step:12124	 l-p:0.10233398526906967
epoch£º606	 i:5 	 global-step:12125	 l-p:0.2069919854402542
epoch£º606	 i:6 	 global-step:12126	 l-p:0.1381697952747345
epoch£º606	 i:7 	 global-step:12127	 l-p:0.13211476802825928
epoch£º606	 i:8 	 global-step:12128	 l-p:0.1238696426153183
epoch£º606	 i:9 	 global-step:12129	 l-p:0.13590802252292633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6105, 3.5915, 3.6088],
        [3.6105, 3.1132, 3.0441],
        [3.6105, 3.6078, 3.6104],
        [3.6105, 3.1748, 3.1773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.13691887259483337 
model_pd.l_d.mean(): -23.443580627441406 
model_pd.lagr.mean(): -23.30666160583496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2083], device='cuda:0')), ('power', tensor([-23.6519], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.13691887259483337
epoch£º607	 i:1 	 global-step:12141	 l-p:0.12649068236351013
epoch£º607	 i:2 	 global-step:12142	 l-p:0.3049752712249756
epoch£º607	 i:3 	 global-step:12143	 l-p:0.12822915613651276
epoch£º607	 i:4 	 global-step:12144	 l-p:0.18063680827617645
epoch£º607	 i:5 	 global-step:12145	 l-p:0.41808637976646423
epoch£º607	 i:6 	 global-step:12146	 l-p:0.16308456659317017
epoch£º607	 i:7 	 global-step:12147	 l-p:0.106624074280262
epoch£º607	 i:8 	 global-step:12148	 l-p:0.3276841640472412
epoch£º607	 i:9 	 global-step:12149	 l-p:0.13507701456546783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5972, 3.5943, 3.5971],
        [3.5972, 3.5859, 3.5965],
        [3.5972, 3.4617, 3.5483],
        [3.5972, 3.0863, 2.5150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.1365174651145935 
model_pd.l_d.mean(): -22.917098999023438 
model_pd.lagr.mean(): -22.780582427978516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3044], device='cuda:0')), ('power', tensor([-23.2215], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.1365174651145935
epoch£º608	 i:1 	 global-step:12161	 l-p:0.13290120661258698
epoch£º608	 i:2 	 global-step:12162	 l-p:0.1667226403951645
epoch£º608	 i:3 	 global-step:12163	 l-p:0.13694016635417938
epoch£º608	 i:4 	 global-step:12164	 l-p:0.2601724863052368
epoch£º608	 i:5 	 global-step:12165	 l-p:0.22714152932167053
epoch£º608	 i:6 	 global-step:12166	 l-p:0.13300321996212006
epoch£º608	 i:7 	 global-step:12167	 l-p:0.17831102013587952
epoch£º608	 i:8 	 global-step:12168	 l-p:0.13145780563354492
epoch£º608	 i:9 	 global-step:12169	 l-p:0.08455467224121094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6142, 3.6142, 3.6142],
        [3.6142, 3.5784, 3.6093],
        [3.6142, 3.1068, 2.5348],
        [3.6142, 3.3609, 3.4611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.24121633172035217 
model_pd.l_d.mean(): -22.68666648864746 
model_pd.lagr.mean(): -22.445449829101562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3020], device='cuda:0')), ('power', tensor([-22.9887], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.24121633172035217
epoch£º609	 i:1 	 global-step:12181	 l-p:0.13185875117778778
epoch£º609	 i:2 	 global-step:12182	 l-p:0.11994544416666031
epoch£º609	 i:3 	 global-step:12183	 l-p:0.12508687376976013
epoch£º609	 i:4 	 global-step:12184	 l-p:0.0782323032617569
epoch£º609	 i:5 	 global-step:12185	 l-p:0.13381116092205048
epoch£º609	 i:6 	 global-step:12186	 l-p:0.133991077542305
epoch£º609	 i:7 	 global-step:12187	 l-p:0.128889799118042
epoch£º609	 i:8 	 global-step:12188	 l-p:0.19069205224514008
epoch£º609	 i:9 	 global-step:12189	 l-p:0.24484089016914368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5851, 3.3308, 3.4315],
        [3.5851, 3.5844, 3.5851],
        [3.5851, 3.5850, 3.5851],
        [3.5851, 3.5138, 3.5693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.18907062709331512 
model_pd.l_d.mean(): -23.395906448364258 
model_pd.lagr.mean(): -23.206836700439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2109], device='cuda:0')), ('power', tensor([-23.6068], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.18907062709331512
epoch£º610	 i:1 	 global-step:12201	 l-p:0.13921856880187988
epoch£º610	 i:2 	 global-step:12202	 l-p:0.09386670589447021
epoch£º610	 i:3 	 global-step:12203	 l-p:0.2701261043548584
epoch£º610	 i:4 	 global-step:12204	 l-p:0.18051967024803162
epoch£º610	 i:5 	 global-step:12205	 l-p:0.192449152469635
epoch£º610	 i:6 	 global-step:12206	 l-p:0.1233595758676529
epoch£º610	 i:7 	 global-step:12207	 l-p:0.14709439873695374
epoch£º610	 i:8 	 global-step:12208	 l-p:0.1427278369665146
epoch£º610	 i:9 	 global-step:12209	 l-p:0.13405349850654602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6273, 3.1556, 3.1190],
        [3.6273, 3.1648, 2.5962],
        [3.6273, 3.3249, 2.7912],
        [3.6273, 3.6252, 3.6273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.14242614805698395 
model_pd.l_d.mean(): -23.467618942260742 
model_pd.lagr.mean(): -23.325193405151367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1297], device='cuda:0')), ('power', tensor([-23.5973], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.14242614805698395
epoch£º611	 i:1 	 global-step:12221	 l-p:0.11703069508075714
epoch£º611	 i:2 	 global-step:12222	 l-p:0.11907149851322174
epoch£º611	 i:3 	 global-step:12223	 l-p:0.14196673035621643
epoch£º611	 i:4 	 global-step:12224	 l-p:0.13152249157428741
epoch£º611	 i:5 	 global-step:12225	 l-p:0.279908150434494
epoch£º611	 i:6 	 global-step:12226	 l-p:0.1304084211587906
epoch£º611	 i:7 	 global-step:12227	 l-p:0.12883798778057098
epoch£º611	 i:8 	 global-step:12228	 l-p:0.014572820626199245
epoch£º611	 i:9 	 global-step:12229	 l-p:0.15403978526592255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5185, 2.8632, 2.3673],
        [3.5185, 3.5184, 3.5185],
        [3.5185, 3.0838, 3.0961],
        [3.5185, 3.5167, 3.5184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14739516377449036 
model_pd.l_d.mean(): -22.890201568603516 
model_pd.lagr.mean(): -22.74280548095703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2795], device='cuda:0')), ('power', tensor([-23.1697], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14739516377449036
epoch£º612	 i:1 	 global-step:12241	 l-p:0.13659892976284027
epoch£º612	 i:2 	 global-step:12242	 l-p:0.09061282873153687
epoch£º612	 i:3 	 global-step:12243	 l-p:0.13689598441123962
epoch£º612	 i:4 	 global-step:12244	 l-p:0.0877806544303894
epoch£º612	 i:5 	 global-step:12245	 l-p:0.14371861517429352
epoch£º612	 i:6 	 global-step:12246	 l-p:0.1446167230606079
epoch£º612	 i:7 	 global-step:12247	 l-p:0.14461755752563477
epoch£º612	 i:8 	 global-step:12248	 l-p:0.1945219337940216
epoch£º612	 i:9 	 global-step:12249	 l-p:-0.24946030974388123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4737, 3.4734, 3.4737],
        [3.4737, 3.4226, 3.4648],
        [3.4737, 3.4672, 3.4734],
        [3.4737, 3.3509, 3.4333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.19509391486644745 
model_pd.l_d.mean(): -23.557077407836914 
model_pd.lagr.mean(): -23.361984252929688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2543], device='cuda:0')), ('power', tensor([-23.8114], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.19509391486644745
epoch£º613	 i:1 	 global-step:12261	 l-p:0.16103792190551758
epoch£º613	 i:2 	 global-step:12262	 l-p:0.12369423359632492
epoch£º613	 i:3 	 global-step:12263	 l-p:0.15710091590881348
epoch£º613	 i:4 	 global-step:12264	 l-p:0.07689741998910904
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12374759465456009
epoch£º613	 i:6 	 global-step:12266	 l-p:0.14114761352539062
epoch£º613	 i:7 	 global-step:12267	 l-p:0.27565908432006836
epoch£º613	 i:8 	 global-step:12268	 l-p:-0.04560307785868645
epoch£º613	 i:9 	 global-step:12269	 l-p:0.10620947927236557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5902, 2.9420, 2.5087],
        [3.5902, 3.0649, 2.9622],
        [3.5902, 3.5834, 3.5899],
        [3.5902, 3.5868, 3.5901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13020335137844086 
model_pd.l_d.mean(): -23.430866241455078 
model_pd.lagr.mean(): -23.300662994384766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2235], device='cuda:0')), ('power', tensor([-23.6544], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13020335137844086
epoch£º614	 i:1 	 global-step:12281	 l-p:0.1375994086265564
epoch£º614	 i:2 	 global-step:12282	 l-p:0.21196162700653076
epoch£º614	 i:3 	 global-step:12283	 l-p:0.12522836029529572
epoch£º614	 i:4 	 global-step:12284	 l-p:0.1202058494091034
epoch£º614	 i:5 	 global-step:12285	 l-p:0.12880153954029083
epoch£º614	 i:6 	 global-step:12286	 l-p:0.14471279084682465
epoch£º614	 i:7 	 global-step:12287	 l-p:0.15069352090358734
epoch£º614	 i:8 	 global-step:12288	 l-p:0.24754635989665985
epoch£º614	 i:9 	 global-step:12289	 l-p:0.2421332150697708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5837, 3.0335, 2.8922],
        [3.5837, 3.0660, 2.9762],
        [3.5837, 2.9717, 2.4169],
        [3.5837, 3.5837, 3.5837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.13009782135486603 
model_pd.l_d.mean(): -22.631793975830078 
model_pd.lagr.mean(): -22.50169563293457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2640], device='cuda:0')), ('power', tensor([-22.8958], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.13009782135486603
epoch£º615	 i:1 	 global-step:12301	 l-p:0.14501447975635529
epoch£º615	 i:2 	 global-step:12302	 l-p:0.2861979603767395
epoch£º615	 i:3 	 global-step:12303	 l-p:0.3178597390651703
epoch£º615	 i:4 	 global-step:12304	 l-p:0.12988269329071045
epoch£º615	 i:5 	 global-step:12305	 l-p:0.12973038852214813
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14120016992092133
epoch£º615	 i:7 	 global-step:12307	 l-p:0.14823172986507416
epoch£º615	 i:8 	 global-step:12308	 l-p:0.21497103571891785
epoch£º615	 i:9 	 global-step:12309	 l-p:-0.2487412542104721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5682, 3.5641, 3.5681],
        [3.5682, 3.4999, 3.5536],
        [3.5682, 3.0129, 2.4402],
        [3.5682, 2.9293, 2.5772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.12372814863920212 
model_pd.l_d.mean(): -22.396678924560547 
model_pd.lagr.mean(): -22.272951126098633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3704], device='cuda:0')), ('power', tensor([-22.7671], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.12372814863920212
epoch£º616	 i:1 	 global-step:12321	 l-p:0.13903549313545227
epoch£º616	 i:2 	 global-step:12322	 l-p:0.12998199462890625
epoch£º616	 i:3 	 global-step:12323	 l-p:0.19388793408870697
epoch£º616	 i:4 	 global-step:12324	 l-p:0.025981787592172623
epoch£º616	 i:5 	 global-step:12325	 l-p:0.18971920013427734
epoch£º616	 i:6 	 global-step:12326	 l-p:0.11965037137269974
epoch£º616	 i:7 	 global-step:12327	 l-p:0.41461238265037537
epoch£º616	 i:8 	 global-step:12328	 l-p:0.23563054203987122
epoch£º616	 i:9 	 global-step:12329	 l-p:0.10126058012247086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6165, 3.4892, 3.5729],
        [3.6165, 3.2771, 3.3520],
        [3.6165, 3.3297, 2.7999],
        [3.6165, 3.4916, 3.5743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.12573657929897308 
model_pd.l_d.mean(): -23.309640884399414 
model_pd.lagr.mean(): -23.18390464782715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2126], device='cuda:0')), ('power', tensor([-23.5222], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.12573657929897308
epoch£º617	 i:1 	 global-step:12341	 l-p:0.14638064801692963
epoch£º617	 i:2 	 global-step:12342	 l-p:0.13587994873523712
epoch£º617	 i:3 	 global-step:12343	 l-p:0.1525851935148239
epoch£º617	 i:4 	 global-step:12344	 l-p:0.11750470846891403
epoch£º617	 i:5 	 global-step:12345	 l-p:0.19244323670864105
epoch£º617	 i:6 	 global-step:12346	 l-p:0.14614950120449066
epoch£º617	 i:7 	 global-step:12347	 l-p:0.1398402899503708
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11258804798126221
epoch£º617	 i:9 	 global-step:12349	 l-p:0.08024564385414124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6473, 3.1106, 2.9839],
        [3.6473, 3.5173, 3.6019],
        [3.6473, 3.3527, 2.8185],
        [3.6473, 3.2158, 3.2235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.06325172632932663 
model_pd.l_d.mean(): -23.319774627685547 
model_pd.lagr.mean(): -23.25652313232422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2016], device='cuda:0')), ('power', tensor([-23.5214], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.06325172632932663
epoch£º618	 i:1 	 global-step:12361	 l-p:0.146529883146286
epoch£º618	 i:2 	 global-step:12362	 l-p:0.14000196754932404
epoch£º618	 i:3 	 global-step:12363	 l-p:0.1490352749824524
epoch£º618	 i:4 	 global-step:12364	 l-p:0.15808139741420746
epoch£º618	 i:5 	 global-step:12365	 l-p:0.11785349249839783
epoch£º618	 i:6 	 global-step:12366	 l-p:0.16416455805301666
epoch£º618	 i:7 	 global-step:12367	 l-p:0.14068014919757843
epoch£º618	 i:8 	 global-step:12368	 l-p:0.11456824839115143
epoch£º618	 i:9 	 global-step:12369	 l-p:0.1459508091211319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6512, 3.5265, 3.6091],
        [3.6512, 3.3930, 3.4929],
        [3.6512, 3.3825, 3.4806],
        [3.6512, 3.0762, 2.5067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.16876719892024994 
model_pd.l_d.mean(): -23.376033782958984 
model_pd.lagr.mean(): -23.207265853881836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2353], device='cuda:0')), ('power', tensor([-23.6113], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.16876719892024994
epoch£º619	 i:1 	 global-step:12381	 l-p:0.04106336459517479
epoch£º619	 i:2 	 global-step:12382	 l-p:0.11256473511457443
epoch£º619	 i:3 	 global-step:12383	 l-p:0.11369077116250992
epoch£º619	 i:4 	 global-step:12384	 l-p:0.14535470306873322
epoch£º619	 i:5 	 global-step:12385	 l-p:0.18964706361293793
epoch£º619	 i:6 	 global-step:12386	 l-p:0.12357356399297714
epoch£º619	 i:7 	 global-step:12387	 l-p:0.14962999522686005
epoch£º619	 i:8 	 global-step:12388	 l-p:0.15370719134807587
epoch£º619	 i:9 	 global-step:12389	 l-p:0.13477198779582977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6032, 3.5567, 3.5956],
        [3.6032, 3.2698, 3.3488],
        [3.6032, 3.6032, 3.6032],
        [3.6032, 2.9622, 2.4451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.2370041161775589 
model_pd.l_d.mean(): -23.71623420715332 
model_pd.lagr.mean(): -23.479230880737305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1833], device='cuda:0')), ('power', tensor([-23.8995], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.2370041161775589
epoch£º620	 i:1 	 global-step:12401	 l-p:0.2602245509624481
epoch£º620	 i:2 	 global-step:12402	 l-p:0.12493577599525452
epoch£º620	 i:3 	 global-step:12403	 l-p:0.15171591937541962
epoch£º620	 i:4 	 global-step:12404	 l-p:0.12284296005964279
epoch£º620	 i:5 	 global-step:12405	 l-p:0.1652091145515442
epoch£º620	 i:6 	 global-step:12406	 l-p:0.13497567176818848
epoch£º620	 i:7 	 global-step:12407	 l-p:0.11129289120435715
epoch£º620	 i:8 	 global-step:12408	 l-p:0.13071110844612122
epoch£º620	 i:9 	 global-step:12409	 l-p:-0.3256041705608368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5579, 2.9406, 2.3810],
        [3.5579, 3.5561, 3.5579],
        [3.5579, 3.5579, 3.5579],
        [3.5579, 3.5579, 3.5579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.13873939216136932 
model_pd.l_d.mean(): -23.695119857788086 
model_pd.lagr.mean(): -23.556381225585938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1567], device='cuda:0')), ('power', tensor([-23.8518], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.13873939216136932
epoch£º621	 i:1 	 global-step:12421	 l-p:-0.09145307540893555
epoch£º621	 i:2 	 global-step:12422	 l-p:0.22981877624988556
epoch£º621	 i:3 	 global-step:12423	 l-p:0.04557815566658974
epoch£º621	 i:4 	 global-step:12424	 l-p:0.1511985808610916
epoch£º621	 i:5 	 global-step:12425	 l-p:0.1421404480934143
epoch£º621	 i:6 	 global-step:12426	 l-p:0.10995304584503174
epoch£º621	 i:7 	 global-step:12427	 l-p:0.12623830139636993
epoch£º621	 i:8 	 global-step:12428	 l-p:0.14056862890720367
epoch£º621	 i:9 	 global-step:12429	 l-p:-0.23046405613422394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5643, 3.5643, 3.5643],
        [3.5643, 3.0924, 3.0672],
        [3.5643, 2.9071, 2.4708],
        [3.5643, 3.1894, 3.2480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.14795637130737305 
model_pd.l_d.mean(): -23.642982482910156 
model_pd.lagr.mean(): -23.495025634765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1864], device='cuda:0')), ('power', tensor([-23.8294], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.14795637130737305
epoch£º622	 i:1 	 global-step:12441	 l-p:0.12992791831493378
epoch£º622	 i:2 	 global-step:12442	 l-p:-0.02870107628405094
epoch£º622	 i:3 	 global-step:12443	 l-p:0.18430466949939728
epoch£º622	 i:4 	 global-step:12444	 l-p:0.12054130434989929
epoch£º622	 i:5 	 global-step:12445	 l-p:0.1925504207611084
epoch£º622	 i:6 	 global-step:12446	 l-p:0.1364278793334961
epoch£º622	 i:7 	 global-step:12447	 l-p:0.1406698226928711
epoch£º622	 i:8 	 global-step:12448	 l-p:0.3247954249382019
epoch£º622	 i:9 	 global-step:12449	 l-p:2.1764910221099854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5858, 3.5156, 3.5705],
        [3.5858, 3.3288, 3.4307],
        [3.5858, 3.5730, 3.5849],
        [3.5858, 3.5855, 3.5858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.13994424045085907 
model_pd.l_d.mean(): -23.32340431213379 
model_pd.lagr.mean(): -23.183460235595703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1724], device='cuda:0')), ('power', tensor([-23.4958], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.13994424045085907
epoch£º623	 i:1 	 global-step:12461	 l-p:0.4342974126338959
epoch£º623	 i:2 	 global-step:12462	 l-p:0.14343945682048798
epoch£º623	 i:3 	 global-step:12463	 l-p:0.12255333364009857
epoch£º623	 i:4 	 global-step:12464	 l-p:0.20442430675029755
epoch£º623	 i:5 	 global-step:12465	 l-p:0.15682974457740784
epoch£º623	 i:6 	 global-step:12466	 l-p:0.11326824128627777
epoch£º623	 i:7 	 global-step:12467	 l-p:0.14668937027454376
epoch£º623	 i:8 	 global-step:12468	 l-p:0.13308677077293396
epoch£º623	 i:9 	 global-step:12469	 l-p:0.19357192516326904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6157, 3.5989, 3.6143],
        [3.6157, 3.2054, 2.6440],
        [3.6157, 3.6123, 3.6156],
        [3.6157, 3.6157, 3.6157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.13992084562778473 
model_pd.l_d.mean(): -23.44435691833496 
model_pd.lagr.mean(): -23.30443572998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1980], device='cuda:0')), ('power', tensor([-23.6424], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.13992084562778473
epoch£º624	 i:1 	 global-step:12481	 l-p:0.12466784566640854
epoch£º624	 i:2 	 global-step:12482	 l-p:0.1371452361345291
epoch£º624	 i:3 	 global-step:12483	 l-p:0.25455641746520996
epoch£º624	 i:4 	 global-step:12484	 l-p:0.11386134475469589
epoch£º624	 i:5 	 global-step:12485	 l-p:0.12239911407232285
epoch£º624	 i:6 	 global-step:12486	 l-p:0.13054895401000977
epoch£º624	 i:7 	 global-step:12487	 l-p:0.1566515415906906
epoch£º624	 i:8 	 global-step:12488	 l-p:0.4704422950744629
epoch£º624	 i:9 	 global-step:12489	 l-p:0.7577351927757263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5898, 3.5898, 3.5898],
        [3.5898, 3.5649, 3.5872],
        [3.5898, 3.5874, 3.5898],
        [3.5898, 2.9928, 2.7670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.17872905731201172 
model_pd.l_d.mean(): -23.364002227783203 
model_pd.lagr.mean(): -23.185272216796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2375], device='cuda:0')), ('power', tensor([-23.6015], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.17872905731201172
epoch£º625	 i:1 	 global-step:12501	 l-p:0.07310300320386887
epoch£º625	 i:2 	 global-step:12502	 l-p:0.1530316323041916
epoch£º625	 i:3 	 global-step:12503	 l-p:0.1333988755941391
epoch£º625	 i:4 	 global-step:12504	 l-p:0.18439610302448273
epoch£º625	 i:5 	 global-step:12505	 l-p:0.1345449686050415
epoch£º625	 i:6 	 global-step:12506	 l-p:0.19703926146030426
epoch£º625	 i:7 	 global-step:12507	 l-p:0.11573466658592224
epoch£º625	 i:8 	 global-step:12508	 l-p:0.12623384594917297
epoch£º625	 i:9 	 global-step:12509	 l-p:0.16545848548412323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6390, 3.5522, 3.6168],
        [3.6390, 3.1657, 3.1330],
        [3.6390, 3.6390, 3.6390],
        [3.6390, 3.2483, 2.6892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.1395910680294037 
model_pd.l_d.mean(): -23.648075103759766 
model_pd.lagr.mean(): -23.50848388671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1343], device='cuda:0')), ('power', tensor([-23.7824], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.1395910680294037
epoch£º626	 i:1 	 global-step:12521	 l-p:0.1667461395263672
epoch£º626	 i:2 	 global-step:12522	 l-p:0.11197398602962494
epoch£º626	 i:3 	 global-step:12523	 l-p:0.17405302822589874
epoch£º626	 i:4 	 global-step:12524	 l-p:0.15104123950004578
epoch£º626	 i:5 	 global-step:12525	 l-p:0.10311976820230484
epoch£º626	 i:6 	 global-step:12526	 l-p:0.13625623285770416
epoch£º626	 i:7 	 global-step:12527	 l-p:0.13727517426013947
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11583998054265976
epoch£º626	 i:9 	 global-step:12529	 l-p:0.12054763734340668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6893, 3.4796, 3.5816],
        [3.6893, 3.0996, 2.8662],
        [3.6893, 3.4973, 3.5977],
        [3.6893, 3.6242, 3.6758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.1343686431646347 
model_pd.l_d.mean(): -23.56573486328125 
model_pd.lagr.mean(): -23.431365966796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1078], device='cuda:0')), ('power', tensor([-23.6735], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.1343686431646347
epoch£º627	 i:1 	 global-step:12541	 l-p:0.1330384463071823
epoch£º627	 i:2 	 global-step:12542	 l-p:0.1405055969953537
epoch£º627	 i:3 	 global-step:12543	 l-p:0.051597945392131805
epoch£º627	 i:4 	 global-step:12544	 l-p:0.1153169795870781
epoch£º627	 i:5 	 global-step:12545	 l-p:0.1699977070093155
epoch£º627	 i:6 	 global-step:12546	 l-p:0.13398931920528412
epoch£º627	 i:7 	 global-step:12547	 l-p:0.23476001620292664
epoch£º627	 i:8 	 global-step:12548	 l-p:0.13707008957862854
epoch£º627	 i:9 	 global-step:12549	 l-p:0.13578158617019653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5903, 3.0563, 2.9488],
        [3.5903, 3.5897, 3.5903],
        [3.5903, 3.5886, 3.5903],
        [3.5903, 3.4516, 3.5399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.1743078976869583 
model_pd.l_d.mean(): -23.61068344116211 
model_pd.lagr.mean(): -23.43637466430664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1563], device='cuda:0')), ('power', tensor([-23.7670], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.1743078976869583
epoch£º628	 i:1 	 global-step:12561	 l-p:0.13101893663406372
epoch£º628	 i:2 	 global-step:12562	 l-p:0.15564891695976257
epoch£º628	 i:3 	 global-step:12563	 l-p:0.14856956899166107
epoch£º628	 i:4 	 global-step:12564	 l-p:0.01584598422050476
epoch£º628	 i:5 	 global-step:12565	 l-p:0.1375846564769745
epoch£º628	 i:6 	 global-step:12566	 l-p:0.13190297782421112
epoch£º628	 i:7 	 global-step:12567	 l-p:0.10374544560909271
epoch£º628	 i:8 	 global-step:12568	 l-p:0.11824695020914078
epoch£º628	 i:9 	 global-step:12569	 l-p:-0.003803682280704379
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4830, 3.4813, 3.4830],
        [3.4830, 3.0038, 2.9807],
        [3.4830, 3.4740, 3.4825],
        [3.4830, 2.9320, 2.3611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.14847710728645325 
model_pd.l_d.mean(): -23.83240509033203 
model_pd.lagr.mean(): -23.683927536010742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1653], device='cuda:0')), ('power', tensor([-23.9977], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.14847710728645325
epoch£º629	 i:1 	 global-step:12581	 l-p:0.04839358851313591
epoch£º629	 i:2 	 global-step:12582	 l-p:0.1354919672012329
epoch£º629	 i:3 	 global-step:12583	 l-p:0.13388194143772125
epoch£º629	 i:4 	 global-step:12584	 l-p:0.1714065968990326
epoch£º629	 i:5 	 global-step:12585	 l-p:0.08234822750091553
epoch£º629	 i:6 	 global-step:12586	 l-p:0.07219827175140381
epoch£º629	 i:7 	 global-step:12587	 l-p:0.12367086112499237
epoch£º629	 i:8 	 global-step:12588	 l-p:0.1822250336408615
epoch£º629	 i:9 	 global-step:12589	 l-p:1.0973381996154785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4878, 3.4878, 3.4878],
        [3.4878, 3.0090, 2.9864],
        [3.4878, 2.9220, 2.7802],
        [3.4878, 3.4878, 3.4878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.15669015049934387 
model_pd.l_d.mean(): -23.235963821411133 
model_pd.lagr.mean(): -23.079273223876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2851], device='cuda:0')), ('power', tensor([-23.5210], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.15669015049934387
epoch£º630	 i:1 	 global-step:12601	 l-p:-0.19277143478393555
epoch£º630	 i:2 	 global-step:12602	 l-p:0.18295250833034515
epoch£º630	 i:3 	 global-step:12603	 l-p:0.0851205587387085
epoch£º630	 i:4 	 global-step:12604	 l-p:0.19044773280620575
epoch£º630	 i:5 	 global-step:12605	 l-p:0.13709811866283417
epoch£º630	 i:6 	 global-step:12606	 l-p:-0.1864132136106491
epoch£º630	 i:7 	 global-step:12607	 l-p:0.12054560333490372
epoch£º630	 i:8 	 global-step:12608	 l-p:0.13786111772060394
epoch£º630	 i:9 	 global-step:12609	 l-p:0.34373000264167786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5960, 3.5627, 3.5917],
        [3.5960, 2.9424, 2.4411],
        [3.5960, 3.5960, 3.5960],
        [3.5960, 3.3086, 3.4053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.2201654314994812 
model_pd.l_d.mean(): -23.742921829223633 
model_pd.lagr.mean(): -23.522756576538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1196], device='cuda:0')), ('power', tensor([-23.8625], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.2201654314994812
epoch£º631	 i:1 	 global-step:12621	 l-p:0.14605756103992462
epoch£º631	 i:2 	 global-step:12622	 l-p:0.1567385047674179
epoch£º631	 i:3 	 global-step:12623	 l-p:0.12169399112462997
epoch£º631	 i:4 	 global-step:12624	 l-p:0.1779044270515442
epoch£º631	 i:5 	 global-step:12625	 l-p:0.1273239254951477
epoch£º631	 i:6 	 global-step:12626	 l-p:0.11613248288631439
epoch£º631	 i:7 	 global-step:12627	 l-p:-0.11853979527950287
epoch£º631	 i:8 	 global-step:12628	 l-p:0.12250947952270508
epoch£º631	 i:9 	 global-step:12629	 l-p:0.14937688410282135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6921, 3.5798, 3.6572],
        [3.6921, 3.6586, 3.6877],
        [3.6921, 3.6819, 3.6915],
        [3.6921, 3.6841, 3.6917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.16077472269535065 
model_pd.l_d.mean(): -22.827287673950195 
model_pd.lagr.mean(): -22.666513442993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1977], device='cuda:0')), ('power', tensor([-23.0250], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.16077472269535065
epoch£º632	 i:1 	 global-step:12641	 l-p:0.12096463143825531
epoch£º632	 i:2 	 global-step:12642	 l-p:0.12430998682975769
epoch£º632	 i:3 	 global-step:12643	 l-p:0.12782150506973267
epoch£º632	 i:4 	 global-step:12644	 l-p:-0.01826118491590023
epoch£º632	 i:5 	 global-step:12645	 l-p:0.1254320740699768
epoch£º632	 i:6 	 global-step:12646	 l-p:0.11337169259786606
epoch£º632	 i:7 	 global-step:12647	 l-p:0.20138107240200043
epoch£º632	 i:8 	 global-step:12648	 l-p:0.14166443049907684
epoch£º632	 i:9 	 global-step:12649	 l-p:0.13682259619235992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6202, 3.5917, 3.6169],
        [3.6202, 3.0401, 2.8495],
        [3.6202, 3.3767, 3.4801],
        [3.6202, 3.3453, 2.8162]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.12605097889900208 
model_pd.l_d.mean(): -23.083356857299805 
model_pd.lagr.mean(): -22.957305908203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2395], device='cuda:0')), ('power', tensor([-23.3229], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.12605097889900208
epoch£º633	 i:1 	 global-step:12661	 l-p:0.09483370929956436
epoch£º633	 i:2 	 global-step:12662	 l-p:0.15645460784435272
epoch£º633	 i:3 	 global-step:12663	 l-p:-0.48642778396606445
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12285353243350983
epoch£º633	 i:5 	 global-step:12665	 l-p:0.13590925931930542
epoch£º633	 i:6 	 global-step:12666	 l-p:0.14022623002529144
epoch£º633	 i:7 	 global-step:12667	 l-p:0.15238429605960846
epoch£º633	 i:8 	 global-step:12668	 l-p:0.08040433377027512
epoch£º633	 i:9 	 global-step:12669	 l-p:-0.5089544653892517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4938, 3.4938, 3.4938],
        [3.4938, 3.4862, 3.4935],
        [3.4938, 3.2678, 3.3737],
        [3.4938, 3.2916, 3.3958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.126902237534523 
model_pd.l_d.mean(): -23.21006965637207 
model_pd.lagr.mean(): -23.083168029785156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2778], device='cuda:0')), ('power', tensor([-23.4879], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.126902237534523
epoch£º634	 i:1 	 global-step:12681	 l-p:0.17060688138008118
epoch£º634	 i:2 	 global-step:12682	 l-p:0.16365337371826172
epoch£º634	 i:3 	 global-step:12683	 l-p:0.17273513972759247
epoch£º634	 i:4 	 global-step:12684	 l-p:1.4005444049835205
epoch£º634	 i:5 	 global-step:12685	 l-p:0.14855241775512695
epoch£º634	 i:6 	 global-step:12686	 l-p:0.10301941633224487
epoch£º634	 i:7 	 global-step:12687	 l-p:-0.004886593669652939
epoch£º634	 i:8 	 global-step:12688	 l-p:0.10396541655063629
epoch£º634	 i:9 	 global-step:12689	 l-p:0.1479070484638214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5010, 3.1518, 3.2301],
        [3.5010, 3.1809, 3.2713],
        [3.5010, 3.4992, 3.5010],
        [3.5010, 2.8326, 2.4296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.15036392211914062 
model_pd.l_d.mean(): -23.584718704223633 
model_pd.lagr.mean(): -23.434354782104492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2516], device='cuda:0')), ('power', tensor([-23.8363], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.15036392211914062
epoch£º635	 i:1 	 global-step:12701	 l-p:0.13951744139194489
epoch£º635	 i:2 	 global-step:12702	 l-p:0.10457243770360947
epoch£º635	 i:3 	 global-step:12703	 l-p:0.1452629715204239
epoch£º635	 i:4 	 global-step:12704	 l-p:0.31249484419822693
epoch£º635	 i:5 	 global-step:12705	 l-p:-0.33974435925483704
epoch£º635	 i:6 	 global-step:12706	 l-p:0.08380185812711716
epoch£º635	 i:7 	 global-step:12707	 l-p:0.12845522165298462
epoch£º635	 i:8 	 global-step:12708	 l-p:0.00811021775007248
epoch£º635	 i:9 	 global-step:12709	 l-p:0.16105270385742188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5864, 3.4594, 3.5436],
        [3.5864, 3.5508, 3.5816],
        [3.5864, 2.9283, 2.4249],
        [3.5864, 3.5862, 3.5864]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.10210628062486649 
model_pd.l_d.mean(): -23.445148468017578 
model_pd.lagr.mean(): -23.343042373657227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1699], device='cuda:0')), ('power', tensor([-23.6150], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.10210628062486649
epoch£º636	 i:1 	 global-step:12721	 l-p:0.1208038330078125
epoch£º636	 i:2 	 global-step:12722	 l-p:0.22176383435726166
epoch£º636	 i:3 	 global-step:12723	 l-p:0.11711335927248001
epoch£º636	 i:4 	 global-step:12724	 l-p:0.19697465002536774
epoch£º636	 i:5 	 global-step:12725	 l-p:0.18910594284534454
epoch£º636	 i:6 	 global-step:12726	 l-p:0.1281944364309311
epoch£º636	 i:7 	 global-step:12727	 l-p:0.138870507478714
epoch£º636	 i:8 	 global-step:12728	 l-p:0.14114625751972198
epoch£º636	 i:9 	 global-step:12729	 l-p:0.14165158569812775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6531, 3.2157, 3.2234],
        [3.6531, 3.3826, 2.8520],
        [3.6531, 3.3630, 3.4583],
        [3.6531, 3.6194, 3.6487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.13628022372722626 
model_pd.l_d.mean(): -22.834125518798828 
model_pd.lagr.mean(): -22.697845458984375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2192], device='cuda:0')), ('power', tensor([-23.0533], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.13628022372722626
epoch£º637	 i:1 	 global-step:12741	 l-p:0.11446422338485718
epoch£º637	 i:2 	 global-step:12742	 l-p:0.1370897889137268
epoch£º637	 i:3 	 global-step:12743	 l-p:0.13177309930324554
epoch£º637	 i:4 	 global-step:12744	 l-p:0.11627426743507385
epoch£º637	 i:5 	 global-step:12745	 l-p:0.12351623177528381
epoch£º637	 i:6 	 global-step:12746	 l-p:-1.486686110496521
epoch£º637	 i:7 	 global-step:12747	 l-p:0.21330080926418304
epoch£º637	 i:8 	 global-step:12748	 l-p:0.15489976108074188
epoch£º637	 i:9 	 global-step:12749	 l-p:0.07974505424499512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5463, 3.5457, 3.5463],
        [3.5463, 3.5463, 3.5463],
        [3.5463, 3.0052, 2.4281],
        [3.5463, 3.1380, 2.5811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.05316799134016037 
model_pd.l_d.mean(): -23.482280731201172 
model_pd.lagr.mean(): -23.429113388061523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3079], device='cuda:0')), ('power', tensor([-23.7901], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.05316799134016037
epoch£º638	 i:1 	 global-step:12761	 l-p:0.20049068331718445
epoch£º638	 i:2 	 global-step:12762	 l-p:0.15812358260154724
epoch£º638	 i:3 	 global-step:12763	 l-p:0.12435216456651688
epoch£º638	 i:4 	 global-step:12764	 l-p:0.13710567355155945
epoch£º638	 i:5 	 global-step:12765	 l-p:0.17861789464950562
epoch£º638	 i:6 	 global-step:12766	 l-p:0.12633538246154785
epoch£º638	 i:7 	 global-step:12767	 l-p:0.13914674520492554
epoch£º638	 i:8 	 global-step:12768	 l-p:-0.024808168411254883
epoch£º638	 i:9 	 global-step:12769	 l-p:0.11784867942333221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5713, 3.1944, 2.6420],
        [3.5713, 3.5637, 3.5709],
        [3.5713, 3.0762, 3.0298],
        [3.5713, 3.1147, 3.1111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.09070372581481934 
model_pd.l_d.mean(): -22.160837173461914 
model_pd.lagr.mean(): -22.070133209228516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4129], device='cuda:0')), ('power', tensor([-22.5737], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.09070372581481934
epoch£º639	 i:1 	 global-step:12781	 l-p:0.15900330245494843
epoch£º639	 i:2 	 global-step:12782	 l-p:0.19983676075935364
epoch£º639	 i:3 	 global-step:12783	 l-p:0.1661759912967682
epoch£º639	 i:4 	 global-step:12784	 l-p:0.13975854218006134
epoch£º639	 i:5 	 global-step:12785	 l-p:-23.908246994018555
epoch£º639	 i:6 	 global-step:12786	 l-p:0.1333199143409729
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14194846153259277
epoch£º639	 i:8 	 global-step:12788	 l-p:0.1485535353422165
epoch£º639	 i:9 	 global-step:12789	 l-p:0.13136586546897888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5874, 3.1113, 3.0857],
        [3.5874, 3.0020, 2.8129],
        [3.5874, 3.5874, 3.5874],
        [3.5874, 3.5517, 3.5826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.27632537484169006 
model_pd.l_d.mean(): -23.217252731323242 
model_pd.lagr.mean(): -22.940927505493164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3101], device='cuda:0')), ('power', tensor([-23.5274], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.27632537484169006
epoch£º640	 i:1 	 global-step:12801	 l-p:0.13826027512550354
epoch£º640	 i:2 	 global-step:12802	 l-p:0.1834069788455963
epoch£º640	 i:3 	 global-step:12803	 l-p:0.1710117757320404
epoch£º640	 i:4 	 global-step:12804	 l-p:0.3445523679256439
epoch£º640	 i:5 	 global-step:12805	 l-p:0.11851104348897934
epoch£º640	 i:6 	 global-step:12806	 l-p:0.09904102236032486
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12090158462524414
epoch£º640	 i:8 	 global-step:12808	 l-p:0.20582036674022675
epoch£º640	 i:9 	 global-step:12809	 l-p:0.1298275589942932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6310, 3.6283, 3.6310],
        [3.6310, 2.9831, 2.4624],
        [3.6310, 3.6310, 3.6310],
        [3.6310, 3.5043, 3.5883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.12932296097278595 
model_pd.l_d.mean(): -23.568199157714844 
model_pd.lagr.mean(): -23.43887710571289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2052], device='cuda:0')), ('power', tensor([-23.7734], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.12932296097278595
epoch£º641	 i:1 	 global-step:12821	 l-p:0.15141692757606506
epoch£º641	 i:2 	 global-step:12822	 l-p:0.0981529951095581
epoch£º641	 i:3 	 global-step:12823	 l-p:0.15882715582847595
epoch£º641	 i:4 	 global-step:12824	 l-p:0.13169077038764954
epoch£º641	 i:5 	 global-step:12825	 l-p:0.14380189776420593
epoch£º641	 i:6 	 global-step:12826	 l-p:0.12796224653720856
epoch£º641	 i:7 	 global-step:12827	 l-p:0.1504267305135727
epoch£º641	 i:8 	 global-step:12828	 l-p:0.17501315474510193
epoch£º641	 i:9 	 global-step:12829	 l-p:0.12873083353042603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6378, 3.6373, 3.6378],
        [3.6378, 3.1851, 3.1804],
        [3.6378, 3.6378, 3.6378],
        [3.6378, 3.2962, 2.7467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.16605666279792786 
model_pd.l_d.mean(): -22.687759399414062 
model_pd.lagr.mean(): -22.52170181274414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3274], device='cuda:0')), ('power', tensor([-23.0152], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.16605666279792786
epoch£º642	 i:1 	 global-step:12841	 l-p:0.14294996857643127
epoch£º642	 i:2 	 global-step:12842	 l-p:0.21167334914207458
epoch£º642	 i:3 	 global-step:12843	 l-p:0.12855906784534454
epoch£º642	 i:4 	 global-step:12844	 l-p:0.1465950906276703
epoch£º642	 i:5 	 global-step:12845	 l-p:0.10081914067268372
epoch£º642	 i:6 	 global-step:12846	 l-p:0.12484470754861832
epoch£º642	 i:7 	 global-step:12847	 l-p:0.10664345324039459
epoch£º642	 i:8 	 global-step:12848	 l-p:0.22922402620315552
epoch£º642	 i:9 	 global-step:12849	 l-p:0.13874226808547974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6117, 3.4719, 3.5609],
        [3.6117, 3.6117, 3.6117],
        [3.6117, 2.9520, 2.5223],
        [3.6117, 3.6117, 3.6117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.14481398463249207 
model_pd.l_d.mean(): -22.56629180908203 
model_pd.lagr.mean(): -22.421478271484375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3008], device='cuda:0')), ('power', tensor([-22.8671], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.14481398463249207
epoch£º643	 i:1 	 global-step:12861	 l-p:0.11706346273422241
epoch£º643	 i:2 	 global-step:12862	 l-p:0.12806810438632965
epoch£º643	 i:3 	 global-step:12863	 l-p:0.17142540216445923
epoch£º643	 i:4 	 global-step:12864	 l-p:0.16725358366966248
epoch£º643	 i:5 	 global-step:12865	 l-p:0.4121135473251343
epoch£º643	 i:6 	 global-step:12866	 l-p:0.3244001567363739
epoch£º643	 i:7 	 global-step:12867	 l-p:0.7478861808776855
epoch£º643	 i:8 	 global-step:12868	 l-p:0.11809029430150986
epoch£º643	 i:9 	 global-step:12869	 l-p:0.13146835565567017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[3.5969, 3.0603, 2.9561],
        [3.5969, 2.9345, 2.5042],
        [3.5969, 3.1344, 3.1240],
        [3.5969, 3.3822, 3.4868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.3061384856700897 
model_pd.l_d.mean(): -23.34933090209961 
model_pd.lagr.mean(): -23.04319190979004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2582], device='cuda:0')), ('power', tensor([-23.6076], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.3061384856700897
epoch£º644	 i:1 	 global-step:12881	 l-p:0.14571601152420044
epoch£º644	 i:2 	 global-step:12882	 l-p:0.131805881857872
epoch£º644	 i:3 	 global-step:12883	 l-p:0.12484582513570786
epoch£º644	 i:4 	 global-step:12884	 l-p:0.1506006270647049
epoch£º644	 i:5 	 global-step:12885	 l-p:0.1392754465341568
epoch£º644	 i:6 	 global-step:12886	 l-p:0.13655030727386475
epoch£º644	 i:7 	 global-step:12887	 l-p:0.18125590682029724
epoch£º644	 i:8 	 global-step:12888	 l-p:-0.05074325576424599
epoch£º644	 i:9 	 global-step:12889	 l-p:-3.3768162727355957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5041, 3.3814, 3.4643],
        [3.5041, 3.0844, 3.1201],
        [3.5041, 2.8366, 2.3018],
        [3.5041, 3.4977, 3.5038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.15337324142456055 
model_pd.l_d.mean(): -23.234516143798828 
model_pd.lagr.mean(): -23.08114242553711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3062], device='cuda:0')), ('power', tensor([-23.5407], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.15337324142456055
epoch£º645	 i:1 	 global-step:12901	 l-p:0.27998003363609314
epoch£º645	 i:2 	 global-step:12902	 l-p:0.11524426937103271
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11771682649850845
epoch£º645	 i:4 	 global-step:12904	 l-p:0.13064663112163544
epoch£º645	 i:5 	 global-step:12905	 l-p:-0.3052043914794922
epoch£º645	 i:6 	 global-step:12906	 l-p:0.15254130959510803
epoch£º645	 i:7 	 global-step:12907	 l-p:-0.052129413932561874
epoch£º645	 i:8 	 global-step:12908	 l-p:0.0816478356719017
epoch£º645	 i:9 	 global-step:12909	 l-p:0.13244430720806122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[3.5655, 2.8957, 2.4130],
        [3.5655, 3.1967, 2.6459],
        [3.5655, 3.2243, 2.6802],
        [3.5655, 3.1202, 3.1300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): -0.014252413995563984 
model_pd.l_d.mean(): -22.81414794921875 
model_pd.lagr.mean(): -22.828399658203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3261], device='cuda:0')), ('power', tensor([-23.1403], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:-0.014252413995563984
epoch£º646	 i:1 	 global-step:12921	 l-p:0.16293933987617493
epoch£º646	 i:2 	 global-step:12922	 l-p:0.1265115737915039
epoch£º646	 i:3 	 global-step:12923	 l-p:0.1352383941411972
epoch£º646	 i:4 	 global-step:12924	 l-p:0.14008335769176483
epoch£º646	 i:5 	 global-step:12925	 l-p:0.5190978646278381
epoch£º646	 i:6 	 global-step:12926	 l-p:0.12306707352399826
epoch£º646	 i:7 	 global-step:12927	 l-p:0.12768222391605377
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13611939549446106
epoch£º646	 i:9 	 global-step:12929	 l-p:0.2448706179857254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5942, 3.5942, 3.5942],
        [3.5942, 3.5937, 3.5942],
        [3.5942, 2.9310, 2.5096],
        [3.5942, 3.5936, 3.5942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.5482646226882935 
model_pd.l_d.mean(): -23.22404670715332 
model_pd.lagr.mean(): -22.67578125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2892], device='cuda:0')), ('power', tensor([-23.5132], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.5482646226882935
epoch£º647	 i:1 	 global-step:12941	 l-p:0.1280129998922348
epoch£º647	 i:2 	 global-step:12942	 l-p:0.12180721759796143
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12243577092885971
epoch£º647	 i:4 	 global-step:12944	 l-p:-0.7143587470054626
epoch£º647	 i:5 	 global-step:12945	 l-p:2.3472414016723633
epoch£º647	 i:6 	 global-step:12946	 l-p:0.15567432343959808
epoch£º647	 i:7 	 global-step:12947	 l-p:0.18106679618358612
epoch£º647	 i:8 	 global-step:12948	 l-p:0.13534973561763763
epoch£º647	 i:9 	 global-step:12949	 l-p:0.13570484519004822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6026, 2.9530, 2.4146],
        [3.6026, 3.6025, 3.6026],
        [3.6026, 3.5254, 3.5848],
        [3.6026, 3.5936, 3.6021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.4457803964614868 
model_pd.l_d.mean(): -23.65753173828125 
model_pd.lagr.mean(): -23.21175193786621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1914], device='cuda:0')), ('power', tensor([-23.8490], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.4457803964614868
epoch£º648	 i:1 	 global-step:12961	 l-p:0.09844039380550385
epoch£º648	 i:2 	 global-step:12962	 l-p:0.13667650520801544
epoch£º648	 i:3 	 global-step:12963	 l-p:0.11957784742116928
epoch£º648	 i:4 	 global-step:12964	 l-p:0.14784277975559235
epoch£º648	 i:5 	 global-step:12965	 l-p:0.14759323000907898
epoch£º648	 i:6 	 global-step:12966	 l-p:0.12639136612415314
epoch£º648	 i:7 	 global-step:12967	 l-p:0.1481669694185257
epoch£º648	 i:8 	 global-step:12968	 l-p:0.10987871140241623
epoch£º648	 i:9 	 global-step:12969	 l-p:0.1829650104045868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6240, 3.5486, 3.6069],
        [3.6240, 3.6182, 3.6238],
        [3.6240, 3.6195, 3.6239],
        [3.6240, 3.1620, 3.1512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.11823342740535736 
model_pd.l_d.mean(): -23.208431243896484 
model_pd.lagr.mean(): -23.090198516845703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2093], device='cuda:0')), ('power', tensor([-23.4178], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.11823342740535736
epoch£º649	 i:1 	 global-step:12981	 l-p:0.11705589294433594
epoch£º649	 i:2 	 global-step:12982	 l-p:0.2241133600473404
epoch£º649	 i:3 	 global-step:12983	 l-p:0.13870161771774292
epoch£º649	 i:4 	 global-step:12984	 l-p:0.14290808141231537
epoch£º649	 i:5 	 global-step:12985	 l-p:-0.11572948098182678
epoch£º649	 i:6 	 global-step:12986	 l-p:0.12538401782512665
epoch£º649	 i:7 	 global-step:12987	 l-p:0.12040146440267563
epoch£º649	 i:8 	 global-step:12988	 l-p:0.7188822031021118
epoch£º649	 i:9 	 global-step:12989	 l-p:0.1172836646437645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5278, 3.3244, 3.4292],
        [3.5278, 3.4285, 3.5004],
        [3.5278, 2.8667, 2.5074],
        [3.5278, 3.1783, 3.2577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.33020785450935364 
model_pd.l_d.mean(): -23.620237350463867 
model_pd.lagr.mean(): -23.290029525756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2158], device='cuda:0')), ('power', tensor([-23.8361], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.33020785450935364
epoch£º650	 i:1 	 global-step:13001	 l-p:0.04880751669406891
epoch£º650	 i:2 	 global-step:13002	 l-p:0.15369297564029694
epoch£º650	 i:3 	 global-step:13003	 l-p:1.4626227617263794
epoch£º650	 i:4 	 global-step:13004	 l-p:0.13799405097961426
epoch£º650	 i:5 	 global-step:13005	 l-p:0.13923723995685577
epoch£º650	 i:6 	 global-step:13006	 l-p:0.14019155502319336
epoch£º650	 i:7 	 global-step:13007	 l-p:0.15293090045452118
epoch£º650	 i:8 	 global-step:13008	 l-p:0.15059934556484222
epoch£º650	 i:9 	 global-step:13009	 l-p:0.14042474329471588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4929, 3.4929, 3.4929],
        [3.4929, 3.0900, 2.5382],
        [3.4929, 3.4929, 3.4929],
        [3.4929, 3.0711, 3.1072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.15889227390289307 
model_pd.l_d.mean(): -23.384613037109375 
model_pd.lagr.mean(): -23.22572135925293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2970], device='cuda:0')), ('power', tensor([-23.6816], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.15889227390289307
epoch£º651	 i:1 	 global-step:13021	 l-p:0.10680100321769714
epoch£º651	 i:2 	 global-step:13022	 l-p:-0.44355690479278564
epoch£º651	 i:3 	 global-step:13023	 l-p:0.111238032579422
epoch£º651	 i:4 	 global-step:13024	 l-p:0.12035064399242401
epoch£º651	 i:5 	 global-step:13025	 l-p:0.14181379973888397
epoch£º651	 i:6 	 global-step:13026	 l-p:0.096154123544693
epoch£º651	 i:7 	 global-step:13027	 l-p:0.15189632773399353
epoch£º651	 i:8 	 global-step:13028	 l-p:0.15635785460472107
epoch£º651	 i:9 	 global-step:13029	 l-p:0.31527870893478394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5354, 3.2887, 3.3949],
        [3.5354, 3.5121, 3.5330],
        [3.5354, 3.5354, 3.5354],
        [3.5354, 3.5328, 3.5353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.2242848128080368 
model_pd.l_d.mean(): -23.492212295532227 
model_pd.lagr.mean(): -23.267927169799805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1971], device='cuda:0')), ('power', tensor([-23.6893], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.2242848128080368
epoch£º652	 i:1 	 global-step:13041	 l-p:0.13763436675071716
epoch£º652	 i:2 	 global-step:13042	 l-p:0.1781349927186966
epoch£º652	 i:3 	 global-step:13043	 l-p:-0.12007905542850494
epoch£º652	 i:4 	 global-step:13044	 l-p:1.3368592262268066
epoch£º652	 i:5 	 global-step:13045	 l-p:0.1423707753419876
epoch£º652	 i:6 	 global-step:13046	 l-p:0.1323784738779068
epoch£º652	 i:7 	 global-step:13047	 l-p:0.1334654837846756
epoch£º652	 i:8 	 global-step:13048	 l-p:0.08380954712629318
epoch£º652	 i:9 	 global-step:13049	 l-p:0.1582331359386444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6517, 3.0815, 2.4974],
        [3.6517, 2.9919, 2.5218],
        [3.6517, 3.0860, 2.5014],
        [3.6517, 3.5117, 3.6008]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.13444694876670837 
model_pd.l_d.mean(): -23.24701690673828 
model_pd.lagr.mean(): -23.11256980895996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2094], device='cuda:0')), ('power', tensor([-23.4564], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.13444694876670837
epoch£º653	 i:1 	 global-step:13061	 l-p:0.11427594721317291
epoch£º653	 i:2 	 global-step:13062	 l-p:0.1271420568227768
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12366693466901779
epoch£º653	 i:4 	 global-step:13064	 l-p:0.13242913782596588
epoch£º653	 i:5 	 global-step:13065	 l-p:0.15431417524814606
epoch£º653	 i:6 	 global-step:13066	 l-p:0.3021014332771301
epoch£º653	 i:7 	 global-step:13067	 l-p:0.20358802378177643
epoch£º653	 i:8 	 global-step:13068	 l-p:0.18174099922180176
epoch£º653	 i:9 	 global-step:13069	 l-p:0.14601027965545654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6074, 3.0152, 2.4340],
        [3.6074, 3.2300, 3.2914],
        [3.6074, 3.6073, 3.6074],
        [3.6074, 3.1657, 3.1778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.2530362010002136 
model_pd.l_d.mean(): -22.978302001953125 
model_pd.lagr.mean(): -22.725265502929688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2585], device='cuda:0')), ('power', tensor([-23.2368], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.2530362010002136
epoch£º654	 i:1 	 global-step:13081	 l-p:0.14958932995796204
epoch£º654	 i:2 	 global-step:13082	 l-p:0.13515901565551758
epoch£º654	 i:3 	 global-step:13083	 l-p:0.12333754450082779
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13074301183223724
epoch£º654	 i:5 	 global-step:13085	 l-p:0.14406558871269226
epoch£º654	 i:6 	 global-step:13086	 l-p:0.12240085005760193
epoch£º654	 i:7 	 global-step:13087	 l-p:0.15682050585746765
epoch£º654	 i:8 	 global-step:13088	 l-p:0.14229725301265717
epoch£º654	 i:9 	 global-step:13089	 l-p:0.11976579576730728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[3.6772, 3.0935, 2.8986],
        [3.6772, 3.0384, 2.6979],
        [3.6772, 3.2184, 3.2081],
        [3.6772, 3.3883, 3.4853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.15801620483398438 
model_pd.l_d.mean(): -23.505905151367188 
model_pd.lagr.mean(): -23.347888946533203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1765], device='cuda:0')), ('power', tensor([-23.6824], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.15801620483398438
epoch£º655	 i:1 	 global-step:13101	 l-p:0.17801669239997864
epoch£º655	 i:2 	 global-step:13102	 l-p:0.12699177861213684
epoch£º655	 i:3 	 global-step:13103	 l-p:0.14510409533977509
epoch£º655	 i:4 	 global-step:13104	 l-p:0.13435621559619904
epoch£º655	 i:5 	 global-step:13105	 l-p:0.12044574320316315
epoch£º655	 i:6 	 global-step:13106	 l-p:0.12443289160728455
epoch£º655	 i:7 	 global-step:13107	 l-p:0.24102240800857544
epoch£º655	 i:8 	 global-step:13108	 l-p:0.1265110820531845
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12382255494594574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5787, 3.5373, 3.5725],
        [3.5787, 3.5120, 3.5649],
        [3.5787, 3.5490, 3.5752],
        [3.5787, 2.9057, 2.4479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.11894121021032333 
model_pd.l_d.mean(): -23.527673721313477 
model_pd.lagr.mean(): -23.408733367919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1915], device='cuda:0')), ('power', tensor([-23.7191], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.11894121021032333
epoch£º656	 i:1 	 global-step:13121	 l-p:0.1289178729057312
epoch£º656	 i:2 	 global-step:13122	 l-p:0.16790473461151123
epoch£º656	 i:3 	 global-step:13123	 l-p:0.13205048441886902
epoch£º656	 i:4 	 global-step:13124	 l-p:0.13956867158412933
epoch£º656	 i:5 	 global-step:13125	 l-p:0.1952817440032959
epoch£º656	 i:6 	 global-step:13126	 l-p:0.14269620180130005
epoch£º656	 i:7 	 global-step:13127	 l-p:-4.360934734344482
epoch£º656	 i:8 	 global-step:13128	 l-p:0.10793250799179077
epoch£º656	 i:9 	 global-step:13129	 l-p:0.10454308986663818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[3.5179, 3.0662, 3.0771],
        [3.5179, 2.9344, 2.7734],
        [3.5179, 2.8536, 2.3038],
        [3.5179, 2.8347, 2.3532]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.14689764380455017 
model_pd.l_d.mean(): -23.490882873535156 
model_pd.lagr.mean(): -23.343984603881836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2833], device='cuda:0')), ('power', tensor([-23.7742], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.14689764380455017
epoch£º657	 i:1 	 global-step:13141	 l-p:0.42081767320632935
epoch£º657	 i:2 	 global-step:13142	 l-p:0.15948854386806488
epoch£º657	 i:3 	 global-step:13143	 l-p:0.13949325680732727
epoch£º657	 i:4 	 global-step:13144	 l-p:-0.07971344888210297
epoch£º657	 i:5 	 global-step:13145	 l-p:0.07825062423944473
epoch£º657	 i:6 	 global-step:13146	 l-p:0.10068148374557495
epoch£º657	 i:7 	 global-step:13147	 l-p:0.1195964440703392
epoch£º657	 i:8 	 global-step:13148	 l-p:0.1455686092376709
epoch£º657	 i:9 	 global-step:13149	 l-p:0.16515876352787018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5585, 3.5584, 3.5585],
        [3.5585, 2.9519, 2.7390],
        [3.5585, 3.0651, 3.0293],
        [3.5585, 3.0379, 2.9682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): -0.05033526197075844 
model_pd.l_d.mean(): -23.295486450195312 
model_pd.lagr.mean(): -23.345821380615234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2519], device='cuda:0')), ('power', tensor([-23.5474], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:-0.05033526197075844
epoch£º658	 i:1 	 global-step:13161	 l-p:0.14537054300308228
epoch£º658	 i:2 	 global-step:13162	 l-p:0.14740963280200958
epoch£º658	 i:3 	 global-step:13163	 l-p:0.18452292680740356
epoch£º658	 i:4 	 global-step:13164	 l-p:0.21762420237064362
epoch£º658	 i:5 	 global-step:13165	 l-p:1.416846752166748
epoch£º658	 i:6 	 global-step:13166	 l-p:0.1454460173845291
epoch£º658	 i:7 	 global-step:13167	 l-p:0.12904198467731476
epoch£º658	 i:8 	 global-step:13168	 l-p:0.1450728327035904
epoch£º658	 i:9 	 global-step:13169	 l-p:0.10284403711557388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5996, 2.9436, 2.4019],
        [3.5996, 3.5990, 3.5996],
        [3.5996, 3.0320, 2.8857],
        [3.5996, 3.5978, 3.5996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12285125255584717 
model_pd.l_d.mean(): -23.448486328125 
model_pd.lagr.mean(): -23.32563591003418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1676], device='cuda:0')), ('power', tensor([-23.6161], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12285125255584717
epoch£º659	 i:1 	 global-step:13181	 l-p:0.3944992125034332
epoch£º659	 i:2 	 global-step:13182	 l-p:0.14427140355110168
epoch£º659	 i:3 	 global-step:13183	 l-p:0.11207570880651474
epoch£º659	 i:4 	 global-step:13184	 l-p:-3.0199711322784424
epoch£º659	 i:5 	 global-step:13185	 l-p:0.12980474531650543
epoch£º659	 i:6 	 global-step:13186	 l-p:0.17060433328151703
epoch£º659	 i:7 	 global-step:13187	 l-p:0.19895042479038239
epoch£º659	 i:8 	 global-step:13188	 l-p:0.2893936038017273
epoch£º659	 i:9 	 global-step:13189	 l-p:0.12931494414806366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6106, 3.0732, 2.4879],
        [3.6106, 3.3501, 3.4544],
        [3.6106, 2.9636, 2.4102],
        [3.6106, 3.6104, 3.6106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.2756762206554413 
model_pd.l_d.mean(): -22.99078941345215 
model_pd.lagr.mean(): -22.715112686157227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3266], device='cuda:0')), ('power', tensor([-23.3174], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.2756762206554413
epoch£º660	 i:1 	 global-step:13201	 l-p:0.1285100132226944
epoch£º660	 i:2 	 global-step:13202	 l-p:0.13033777475357056
epoch£º660	 i:3 	 global-step:13203	 l-p:0.19343683123588562
epoch£º660	 i:4 	 global-step:13204	 l-p:0.26999664306640625
epoch£º660	 i:5 	 global-step:13205	 l-p:0.0807451605796814
epoch£º660	 i:6 	 global-step:13206	 l-p:0.14682534337043762
epoch£º660	 i:7 	 global-step:13207	 l-p:0.11649704724550247
epoch£º660	 i:8 	 global-step:13208	 l-p:0.12107183039188385
epoch£º660	 i:9 	 global-step:13209	 l-p:0.13617490231990814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6347, 3.6329, 3.6347],
        [3.6347, 3.4182, 3.5234],
        [3.6347, 3.5812, 3.6253],
        [3.6347, 3.3025, 3.3872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.13119493424892426 
model_pd.l_d.mean(): -23.150739669799805 
model_pd.lagr.mean(): -23.01954460144043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2228], device='cuda:0')), ('power', tensor([-23.3736], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.13119493424892426
epoch£º661	 i:1 	 global-step:13221	 l-p:0.12794390320777893
epoch£º661	 i:2 	 global-step:13222	 l-p:0.2683008015155792
epoch£º661	 i:3 	 global-step:13223	 l-p:0.152534618973732
epoch£º661	 i:4 	 global-step:13224	 l-p:0.6163541078567505
epoch£º661	 i:5 	 global-step:13225	 l-p:0.10326399654150009
epoch£º661	 i:6 	 global-step:13226	 l-p:0.1335170865058899
epoch£º661	 i:7 	 global-step:13227	 l-p:0.40008264780044556
epoch£º661	 i:8 	 global-step:13228	 l-p:0.1693379133939743
epoch£º661	 i:9 	 global-step:13229	 l-p:0.10469962656497955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5433, 3.5433, 3.5433],
        [3.5433, 3.5433, 3.5433],
        [3.5433, 3.4994, 3.5366],
        [3.5433, 3.1214, 3.1570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.13466733694076538 
model_pd.l_d.mean(): -23.68999481201172 
model_pd.lagr.mean(): -23.555328369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1441], device='cuda:0')), ('power', tensor([-23.8341], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.13466733694076538
epoch£º662	 i:1 	 global-step:13241	 l-p:0.18171551823616028
epoch£º662	 i:2 	 global-step:13242	 l-p:0.13483120501041412
epoch£º662	 i:3 	 global-step:13243	 l-p:0.3671557903289795
epoch£º662	 i:4 	 global-step:13244	 l-p:-0.08931007981300354
epoch£º662	 i:5 	 global-step:13245	 l-p:0.12288299202919006
epoch£º662	 i:6 	 global-step:13246	 l-p:-0.06499310582876205
epoch£º662	 i:7 	 global-step:13247	 l-p:0.11565332114696503
epoch£º662	 i:8 	 global-step:13248	 l-p:0.1351754069328308
epoch£º662	 i:9 	 global-step:13249	 l-p:0.17584584653377533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6274, 3.0512, 2.4650],
        [3.6274, 3.5027, 3.5863],
        [3.6274, 3.0521, 2.8891],
        [3.6274, 3.6019, 3.6247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.12468717992305756 
model_pd.l_d.mean(): -23.486160278320312 
model_pd.lagr.mean(): -23.361473083496094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1500], device='cuda:0')), ('power', tensor([-23.6362], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.12468717992305756
epoch£º663	 i:1 	 global-step:13261	 l-p:0.11179269850254059
epoch£º663	 i:2 	 global-step:13262	 l-p:0.14461372792720795
epoch£º663	 i:3 	 global-step:13263	 l-p:0.15355737507343292
epoch£º663	 i:4 	 global-step:13264	 l-p:0.2208946943283081
epoch£º663	 i:5 	 global-step:13265	 l-p:0.12695972621440887
epoch£º663	 i:6 	 global-step:13266	 l-p:0.13300035893917084
epoch£º663	 i:7 	 global-step:13267	 l-p:0.1561407744884491
epoch£º663	 i:8 	 global-step:13268	 l-p:0.18138839304447174
epoch£º663	 i:9 	 global-step:13269	 l-p:0.19166339933872223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6400, 3.6400, 3.6400],
        [3.6400, 3.0400, 2.8264],
        [3.6400, 3.6366, 3.6399],
        [3.6400, 3.2478, 3.3000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.1655024141073227 
model_pd.l_d.mean(): -23.37925910949707 
model_pd.lagr.mean(): -23.213756561279297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2065], device='cuda:0')), ('power', tensor([-23.5858], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.1655024141073227
epoch£º664	 i:1 	 global-step:13281	 l-p:0.14009976387023926
epoch£º664	 i:2 	 global-step:13282	 l-p:0.12020386755466461
epoch£º664	 i:3 	 global-step:13283	 l-p:0.1475064903497696
epoch£º664	 i:4 	 global-step:13284	 l-p:0.12278503179550171
epoch£º664	 i:5 	 global-step:13285	 l-p:0.12393061071634293
epoch£º664	 i:6 	 global-step:13286	 l-p:0.13649721443653107
epoch£º664	 i:7 	 global-step:13287	 l-p:0.08163828402757645
epoch£º664	 i:8 	 global-step:13288	 l-p:0.21694868803024292
epoch£º664	 i:9 	 global-step:13289	 l-p:0.24887442588806152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6255, 3.6210, 3.6253],
        [3.6255, 3.6209, 3.6253],
        [3.6255, 3.0322, 2.8366],
        [3.6255, 3.4715, 3.5657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.26435500383377075 
model_pd.l_d.mean(): -23.4248104095459 
model_pd.lagr.mean(): -23.16045570373535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2467], device='cuda:0')), ('power', tensor([-23.6715], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.26435500383377075
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13128633797168732
epoch£º665	 i:2 	 global-step:13302	 l-p:0.14803431928157806
epoch£º665	 i:3 	 global-step:13303	 l-p:0.17081648111343384
epoch£º665	 i:4 	 global-step:13304	 l-p:0.18537352979183197
epoch£º665	 i:5 	 global-step:13305	 l-p:0.12457849085330963
epoch£º665	 i:6 	 global-step:13306	 l-p:0.123745858669281
epoch£º665	 i:7 	 global-step:13307	 l-p:0.12875384092330933
epoch£º665	 i:8 	 global-step:13308	 l-p:0.1202068030834198
epoch£º665	 i:9 	 global-step:13309	 l-p:0.11775610595941544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6189, 3.6189, 3.6189],
        [3.6189, 2.9511, 2.4422],
        [3.6189, 2.9550, 2.4312],
        [3.6189, 3.2447, 3.3098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12100265175104141 
model_pd.l_d.mean(): -22.489900588989258 
model_pd.lagr.mean(): -22.368898391723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2971], device='cuda:0')), ('power', tensor([-22.7870], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12100265175104141
epoch£º666	 i:1 	 global-step:13321	 l-p:0.19015972316265106
epoch£º666	 i:2 	 global-step:13322	 l-p:0.2532498240470886
epoch£º666	 i:3 	 global-step:13323	 l-p:0.12931369245052338
epoch£º666	 i:4 	 global-step:13324	 l-p:0.38746377825737
epoch£º666	 i:5 	 global-step:13325	 l-p:0.13224636018276215
epoch£º666	 i:6 	 global-step:13326	 l-p:0.15754012763500214
epoch£º666	 i:7 	 global-step:13327	 l-p:1.6330657005310059
epoch£º666	 i:8 	 global-step:13328	 l-p:0.1126738116145134
epoch£º666	 i:9 	 global-step:13329	 l-p:0.15013748407363892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[3.6151, 3.2860, 2.7385],
        [3.6151, 3.1086, 3.0538],
        [3.6151, 3.1188, 2.5363],
        [3.6151, 3.0035, 2.7728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.13981643319129944 
model_pd.l_d.mean(): -23.353199005126953 
model_pd.lagr.mean(): -23.213382720947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2691], device='cuda:0')), ('power', tensor([-23.6223], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.13981643319129944
epoch£º667	 i:1 	 global-step:13341	 l-p:0.10921955108642578
epoch£º667	 i:2 	 global-step:13342	 l-p:0.12565740942955017
epoch£º667	 i:3 	 global-step:13343	 l-p:0.22423619031906128
epoch£º667	 i:4 	 global-step:13344	 l-p:0.1388244777917862
epoch£º667	 i:5 	 global-step:13345	 l-p:0.144536554813385
epoch£º667	 i:6 	 global-step:13346	 l-p:0.20105944573879242
epoch£º667	 i:7 	 global-step:13347	 l-p:0.2506222724914551
epoch£º667	 i:8 	 global-step:13348	 l-p:0.11777864396572113
epoch£º667	 i:9 	 global-step:13349	 l-p:0.17546619474887848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6204, 3.5615, 3.6093],
        [3.6204, 3.6204, 3.6204],
        [3.6204, 3.1665, 3.1700],
        [3.6204, 2.9489, 2.4613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.08899063616991043 
model_pd.l_d.mean(): -23.114870071411133 
model_pd.lagr.mean(): -23.02587890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2517], device='cuda:0')), ('power', tensor([-23.3666], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.08899063616991043
epoch£º668	 i:1 	 global-step:13361	 l-p:0.12848684191703796
epoch£º668	 i:2 	 global-step:13362	 l-p:0.15559501945972443
epoch£º668	 i:3 	 global-step:13363	 l-p:0.13581398129463196
epoch£º668	 i:4 	 global-step:13364	 l-p:0.1525212675333023
epoch£º668	 i:5 	 global-step:13365	 l-p:0.24701127409934998
epoch£º668	 i:6 	 global-step:13366	 l-p:0.24314181506633759
epoch£º668	 i:7 	 global-step:13367	 l-p:0.11627151072025299
epoch£º668	 i:8 	 global-step:13368	 l-p:0.1964656561613083
epoch£º668	 i:9 	 global-step:13369	 l-p:0.14229637384414673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6283, 3.2759, 3.3530],
        [3.6283, 3.6281, 3.6283],
        [3.6283, 3.6283, 3.6283],
        [3.6283, 3.0051, 2.7456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.1296013444662094 
model_pd.l_d.mean(): -23.194108963012695 
model_pd.lagr.mean(): -23.06450843811035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1379], device='cuda:0')), ('power', tensor([-23.3320], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.1296013444662094
epoch£º669	 i:1 	 global-step:13381	 l-p:0.13814496994018555
epoch£º669	 i:2 	 global-step:13382	 l-p:0.2549511790275574
epoch£º669	 i:3 	 global-step:13383	 l-p:0.24708914756774902
epoch£º669	 i:4 	 global-step:13384	 l-p:0.13991700112819672
epoch£º669	 i:5 	 global-step:13385	 l-p:0.1794942021369934
epoch£º669	 i:6 	 global-step:13386	 l-p:0.14083990454673767
epoch£º669	 i:7 	 global-step:13387	 l-p:0.14353659749031067
epoch£º669	 i:8 	 global-step:13388	 l-p:0.1382514238357544
epoch£º669	 i:9 	 global-step:13389	 l-p:0.22405467927455902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6094, 3.6094, 3.6094],
        [3.6094, 3.4641, 3.5557],
        [3.6094, 3.6066, 3.6094],
        [3.6094, 3.6094, 3.6094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.35448768734931946 
model_pd.l_d.mean(): -23.55207633972168 
model_pd.lagr.mean(): -23.197587966918945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2328], device='cuda:0')), ('power', tensor([-23.7849], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.35448768734931946
epoch£º670	 i:1 	 global-step:13401	 l-p:0.10781525075435638
epoch£º670	 i:2 	 global-step:13402	 l-p:0.13077174127101898
epoch£º670	 i:3 	 global-step:13403	 l-p:0.12272892892360687
epoch£º670	 i:4 	 global-step:13404	 l-p:0.17106840014457703
epoch£º670	 i:5 	 global-step:13405	 l-p:0.1540585160255432
epoch£º670	 i:6 	 global-step:13406	 l-p:0.24418872594833374
epoch£º670	 i:7 	 global-step:13407	 l-p:0.24773043394088745
epoch£º670	 i:8 	 global-step:13408	 l-p:0.1216522827744484
epoch£º670	 i:9 	 global-step:13409	 l-p:0.12968814373016357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6471, 3.6263, 3.6452],
        [3.6471, 3.6402, 3.6468],
        [3.6471, 3.1658, 3.1394],
        [3.6471, 3.6399, 3.6468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.13160665333271027 
model_pd.l_d.mean(): -23.590465545654297 
model_pd.lagr.mean(): -23.458858489990234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1446], device='cuda:0')), ('power', tensor([-23.7350], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.13160665333271027
epoch£º671	 i:1 	 global-step:13421	 l-p:0.1520911604166031
epoch£º671	 i:2 	 global-step:13422	 l-p:0.15292038023471832
epoch£º671	 i:3 	 global-step:13423	 l-p:0.17409324645996094
epoch£º671	 i:4 	 global-step:13424	 l-p:0.14144852757453918
epoch£º671	 i:5 	 global-step:13425	 l-p:0.16451141238212585
epoch£º671	 i:6 	 global-step:13426	 l-p:0.12308268994092941
epoch£º671	 i:7 	 global-step:13427	 l-p:0.029053373262286186
epoch£º671	 i:8 	 global-step:13428	 l-p:0.13014371693134308
epoch£º671	 i:9 	 global-step:13429	 l-p:0.13500168919563293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6859, 3.6446, 3.6798],
        [3.6859, 3.0227, 2.5763],
        [3.6859, 3.6859, 3.6859],
        [3.6859, 3.2411, 3.2494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.12880189716815948 
model_pd.l_d.mean(): -23.208023071289062 
model_pd.lagr.mean(): -23.079221725463867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2199], device='cuda:0')), ('power', tensor([-23.4279], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.12880189716815948
epoch£º672	 i:1 	 global-step:13441	 l-p:0.12778112292289734
epoch£º672	 i:2 	 global-step:13442	 l-p:0.12561161816120148
epoch£º672	 i:3 	 global-step:13443	 l-p:0.22307860851287842
epoch£º672	 i:4 	 global-step:13444	 l-p:0.14160259068012238
epoch£º672	 i:5 	 global-step:13445	 l-p:0.12125091254711151
epoch£º672	 i:6 	 global-step:13446	 l-p:0.1560329645872116
epoch£º672	 i:7 	 global-step:13447	 l-p:0.12108292430639267
epoch£º672	 i:8 	 global-step:13448	 l-p:0.11325463652610779
epoch£º672	 i:9 	 global-step:13449	 l-p:4.682806968688965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5738, 3.5733, 3.5738],
        [3.5738, 3.5738, 3.5738],
        [3.5738, 2.9223, 2.3566],
        [3.5738, 3.5533, 3.5719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.12911100685596466 
model_pd.l_d.mean(): -23.31048583984375 
model_pd.lagr.mean(): -23.18137550354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2056], device='cuda:0')), ('power', tensor([-23.5161], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.12911100685596466
epoch£º673	 i:1 	 global-step:13461	 l-p:0.13899178802967072
epoch£º673	 i:2 	 global-step:13462	 l-p:0.15508270263671875
epoch£º673	 i:3 	 global-step:13463	 l-p:0.17049863934516907
epoch£º673	 i:4 	 global-step:13464	 l-p:0.07895658165216446
epoch£º673	 i:5 	 global-step:13465	 l-p:0.15484540164470673
epoch£º673	 i:6 	 global-step:13466	 l-p:0.00854447390884161
epoch£º673	 i:7 	 global-step:13467	 l-p:0.0861349105834961
epoch£º673	 i:8 	 global-step:13468	 l-p:0.10243182629346848
epoch£º673	 i:9 	 global-step:13469	 l-p:0.09060825407505035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4667, 3.2246, 3.3337],
        [3.4667, 3.2588, 3.3661],
        [3.4667, 3.1310, 3.2222],
        [3.4667, 2.8789, 2.7291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.16480261087417603 
model_pd.l_d.mean(): -22.933256149291992 
model_pd.lagr.mean(): -22.76845359802246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3686], device='cuda:0')), ('power', tensor([-23.3018], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.16480261087417603
epoch£º674	 i:1 	 global-step:13481	 l-p:0.1365077793598175
epoch£º674	 i:2 	 global-step:13482	 l-p:-0.0010713672963902354
epoch£º674	 i:3 	 global-step:13483	 l-p:0.13516102731227875
epoch£º674	 i:4 	 global-step:13484	 l-p:-0.013648347929120064
epoch£º674	 i:5 	 global-step:13485	 l-p:0.07852423936128616
epoch£º674	 i:6 	 global-step:13486	 l-p:0.1761551946401596
epoch£º674	 i:7 	 global-step:13487	 l-p:0.14834100008010864
epoch£º674	 i:8 	 global-step:13488	 l-p:0.05836061015725136
epoch£º674	 i:9 	 global-step:13489	 l-p:0.13697853684425354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5835, 3.1936, 3.2528],
        [3.5835, 3.0448, 2.9550],
        [3.5835, 3.3832, 3.4882],
        [3.5835, 3.4335, 3.5270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): -1.2327243089675903 
model_pd.l_d.mean(): -23.25391960144043 
model_pd.lagr.mean(): -24.486644744873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2149], device='cuda:0')), ('power', tensor([-23.4688], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:-1.2327243089675903
epoch£º675	 i:1 	 global-step:13501	 l-p:0.7955560088157654
epoch£º675	 i:2 	 global-step:13502	 l-p:0.15920455753803253
epoch£º675	 i:3 	 global-step:13503	 l-p:0.16205160319805145
epoch£º675	 i:4 	 global-step:13504	 l-p:0.13180027902126312
epoch£º675	 i:5 	 global-step:13505	 l-p:0.13442741334438324
epoch£º675	 i:6 	 global-step:13506	 l-p:0.02000882662832737
epoch£º675	 i:7 	 global-step:13507	 l-p:0.1178668662905693
epoch£º675	 i:8 	 global-step:13508	 l-p:0.13458000123500824
epoch£º675	 i:9 	 global-step:13509	 l-p:0.15466459095478058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7050, 3.7050, 3.7051],
        [3.7050, 3.0809, 2.8040],
        [3.7050, 3.6818, 3.7027],
        [3.7050, 3.6974, 3.7047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.13290251791477203 
model_pd.l_d.mean(): -23.38135528564453 
model_pd.lagr.mean(): -23.24845314025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1416], device='cuda:0')), ('power', tensor([-23.5230], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.13290251791477203
epoch£º676	 i:1 	 global-step:13521	 l-p:0.13799352943897247
epoch£º676	 i:2 	 global-step:13522	 l-p:0.14151519536972046
epoch£º676	 i:3 	 global-step:13523	 l-p:0.11878474801778793
epoch£º676	 i:4 	 global-step:13524	 l-p:0.12784847617149353
epoch£º676	 i:5 	 global-step:13525	 l-p:0.16090713441371918
epoch£º676	 i:6 	 global-step:13526	 l-p:0.13677804172039032
epoch£º676	 i:7 	 global-step:13527	 l-p:0.885269284248352
epoch£º676	 i:8 	 global-step:13528	 l-p:0.10259250551462173
epoch£º676	 i:9 	 global-step:13529	 l-p:0.12412627786397934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5800, 3.5800, 3.5800],
        [3.5800, 3.3705, 3.4768],
        [3.5800, 3.1084, 3.1011],
        [3.5800, 3.5479, 3.5761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): -0.25907930731773376 
model_pd.l_d.mean(): -23.52515411376953 
model_pd.lagr.mean(): -23.78423309326172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2073], device='cuda:0')), ('power', tensor([-23.7324], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:-0.25907930731773376
epoch£º677	 i:1 	 global-step:13541	 l-p:0.13479942083358765
epoch£º677	 i:2 	 global-step:13542	 l-p:0.028983496129512787
epoch£º677	 i:3 	 global-step:13543	 l-p:0.13277649879455566
epoch£º677	 i:4 	 global-step:13544	 l-p:0.1488703191280365
epoch£º677	 i:5 	 global-step:13545	 l-p:0.12866531312465668
epoch£º677	 i:6 	 global-step:13546	 l-p:0.18216657638549805
epoch£º677	 i:7 	 global-step:13547	 l-p:0.204914391040802
epoch£º677	 i:8 	 global-step:13548	 l-p:0.5161933898925781
epoch£º677	 i:9 	 global-step:13549	 l-p:0.08618205785751343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5342, 3.4719, 3.5221],
        [3.5342, 3.5312, 3.5341],
        [3.5342, 3.4796, 3.5246],
        [3.5342, 3.4551, 3.5160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.3420567214488983 
model_pd.l_d.mean(): -23.118831634521484 
model_pd.lagr.mean(): -22.776775360107422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3327], device='cuda:0')), ('power', tensor([-23.4515], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:0.3420567214488983
epoch£º678	 i:1 	 global-step:13561	 l-p:0.08811869472265244
epoch£º678	 i:2 	 global-step:13562	 l-p:0.06259258836507797
epoch£º678	 i:3 	 global-step:13563	 l-p:0.09261714667081833
epoch£º678	 i:4 	 global-step:13564	 l-p:0.12446622550487518
epoch£º678	 i:5 	 global-step:13565	 l-p:0.12126483023166656
epoch£º678	 i:6 	 global-step:13566	 l-p:0.10777807235717773
epoch£º678	 i:7 	 global-step:13567	 l-p:0.16291679441928864
epoch£º678	 i:8 	 global-step:13568	 l-p:0.1346505731344223
epoch£º678	 i:9 	 global-step:13569	 l-p:0.12205828726291656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6328, 3.6327, 3.6328],
        [3.6328, 2.9797, 2.4209],
        [3.6328, 3.5806, 3.6238],
        [3.6328, 3.5332, 3.6054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.09893470257520676 
model_pd.l_d.mean(): -23.399351119995117 
model_pd.lagr.mean(): -23.300416946411133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1998], device='cuda:0')), ('power', tensor([-23.5992], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.09893470257520676
epoch£º679	 i:1 	 global-step:13581	 l-p:0.13863128423690796
epoch£º679	 i:2 	 global-step:13582	 l-p:0.15747691690921783
epoch£º679	 i:3 	 global-step:13583	 l-p:0.1610332876443863
epoch£º679	 i:4 	 global-step:13584	 l-p:0.20596595108509064
epoch£º679	 i:5 	 global-step:13585	 l-p:0.10684888809919357
epoch£º679	 i:6 	 global-step:13586	 l-p:-0.302307665348053
epoch£º679	 i:7 	 global-step:13587	 l-p:0.8024662733078003
epoch£º679	 i:8 	 global-step:13588	 l-p:-0.5151346325874329
epoch£º679	 i:9 	 global-step:13589	 l-p:0.11052016168832779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5975, 3.5844, 3.5966],
        [3.5975, 3.5928, 3.5973],
        [3.5975, 3.5230, 3.5810],
        [3.5975, 3.0623, 2.4744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.15269708633422852 
model_pd.l_d.mean(): -23.746004104614258 
model_pd.lagr.mean(): -23.593307495117188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1604], device='cuda:0')), ('power', tensor([-23.9064], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.15269708633422852
epoch£º680	 i:1 	 global-step:13601	 l-p:0.5770798921585083
epoch£º680	 i:2 	 global-step:13602	 l-p:0.19324713945388794
epoch£º680	 i:3 	 global-step:13603	 l-p:0.13574109971523285
epoch£º680	 i:4 	 global-step:13604	 l-p:-0.4165772795677185
epoch£º680	 i:5 	 global-step:13605	 l-p:0.10915987938642502
epoch£º680	 i:6 	 global-step:13606	 l-p:0.12216565757989883
epoch£º680	 i:7 	 global-step:13607	 l-p:0.15709657967090607
epoch£º680	 i:8 	 global-step:13608	 l-p:0.11604135483503342
epoch£º680	 i:9 	 global-step:13609	 l-p:0.11065715551376343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6422, 3.2192, 3.2526],
        [3.6422, 3.2421, 2.6730],
        [3.6422, 3.6405, 3.6422],
        [3.6422, 3.6404, 3.6422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.1413283348083496 
model_pd.l_d.mean(): -23.352405548095703 
model_pd.lagr.mean(): -23.211076736450195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1839], device='cuda:0')), ('power', tensor([-23.5363], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.1413283348083496
epoch£º681	 i:1 	 global-step:13621	 l-p:0.10078981518745422
epoch£º681	 i:2 	 global-step:13622	 l-p:0.12884578108787537
epoch£º681	 i:3 	 global-step:13623	 l-p:0.12891839444637299
epoch£º681	 i:4 	 global-step:13624	 l-p:0.12745413184165955
epoch£º681	 i:5 	 global-step:13625	 l-p:0.4369482696056366
epoch£º681	 i:6 	 global-step:13626	 l-p:0.13522998988628387
epoch£º681	 i:7 	 global-step:13627	 l-p:0.14243444800376892
epoch£º681	 i:8 	 global-step:13628	 l-p:0.14297696948051453
epoch£º681	 i:9 	 global-step:13629	 l-p:0.17255987226963043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6005, 3.3994, 3.5048],
        [3.6005, 3.3333, 3.4391],
        [3.6005, 3.5958, 3.6003],
        [3.6005, 3.5710, 3.5970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.12700724601745605 
model_pd.l_d.mean(): -23.491703033447266 
model_pd.lagr.mean(): -23.364696502685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2103], device='cuda:0')), ('power', tensor([-23.7020], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.12700724601745605
epoch£º682	 i:1 	 global-step:13641	 l-p:-0.4384016990661621
epoch£º682	 i:2 	 global-step:13642	 l-p:0.10206925868988037
epoch£º682	 i:3 	 global-step:13643	 l-p:-0.008001575246453285
epoch£º682	 i:4 	 global-step:13644	 l-p:0.1331179141998291
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12258768081665039
epoch£º682	 i:6 	 global-step:13646	 l-p:0.1340697854757309
epoch£º682	 i:7 	 global-step:13647	 l-p:0.13574212789535522
epoch£º682	 i:8 	 global-step:13648	 l-p:0.2794804275035858
epoch£º682	 i:9 	 global-step:13649	 l-p:0.02717171609401703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5276, 3.2732, 3.3817],
        [3.5276, 3.4559, 3.5123],
        [3.5276, 3.5276, 3.5276],
        [3.5276, 3.1890, 3.2787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.1426841765642166 
model_pd.l_d.mean(): -23.724273681640625 
model_pd.lagr.mean(): -23.581588745117188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2216], device='cuda:0')), ('power', tensor([-23.9459], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.1426841765642166
epoch£º683	 i:1 	 global-step:13661	 l-p:0.1965099722146988
epoch£º683	 i:2 	 global-step:13662	 l-p:0.08564327657222748
epoch£º683	 i:3 	 global-step:13663	 l-p:0.09365259855985641
epoch£º683	 i:4 	 global-step:13664	 l-p:0.02689867466688156
epoch£º683	 i:5 	 global-step:13665	 l-p:0.11899591982364655
epoch£º683	 i:6 	 global-step:13666	 l-p:0.3104976713657379
epoch£º683	 i:7 	 global-step:13667	 l-p:0.08698720484972
epoch£º683	 i:8 	 global-step:13668	 l-p:0.14190621674060822
epoch£º683	 i:9 	 global-step:13669	 l-p:0.1637106090784073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5797, 2.8935, 2.4530],
        [3.5797, 3.5797, 3.5797],
        [3.5797, 3.0711, 2.4870],
        [3.5797, 3.5754, 3.5795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): -0.0119831133633852 
model_pd.l_d.mean(): -23.36058807373047 
model_pd.lagr.mean(): -23.37257194519043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2284], device='cuda:0')), ('power', tensor([-23.5890], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:-0.0119831133633852
epoch£º684	 i:1 	 global-step:13681	 l-p:0.1840941458940506
epoch£º684	 i:2 	 global-step:13682	 l-p:0.12358328700065613
epoch£º684	 i:3 	 global-step:13683	 l-p:0.13359226286411285
epoch£º684	 i:4 	 global-step:13684	 l-p:0.14120611548423767
epoch£º684	 i:5 	 global-step:13685	 l-p:0.13553842902183533
epoch£º684	 i:6 	 global-step:13686	 l-p:0.10157766938209534
epoch£º684	 i:7 	 global-step:13687	 l-p:0.23666594922542572
epoch£º684	 i:8 	 global-step:13688	 l-p:0.11727024614810944
epoch£º684	 i:9 	 global-step:13689	 l-p:0.1455683559179306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6709, 3.5106, 3.6071],
        [3.6709, 3.2730, 2.7012],
        [3.6709, 3.0899, 2.9247],
        [3.6709, 3.6708, 3.6709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.1333109736442566 
model_pd.l_d.mean(): -23.518543243408203 
model_pd.lagr.mean(): -23.38523292541504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1879], device='cuda:0')), ('power', tensor([-23.7064], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.1333109736442566
epoch£º685	 i:1 	 global-step:13701	 l-p:0.18324507772922516
epoch£º685	 i:2 	 global-step:13702	 l-p:0.14640890061855316
epoch£º685	 i:3 	 global-step:13703	 l-p:0.14522285759449005
epoch£º685	 i:4 	 global-step:13704	 l-p:0.07630448043346405
epoch£º685	 i:5 	 global-step:13705	 l-p:0.16345588862895966
epoch£º685	 i:6 	 global-step:13706	 l-p:0.16152557730674744
epoch£º685	 i:7 	 global-step:13707	 l-p:0.12031606584787369
epoch£º685	 i:8 	 global-step:13708	 l-p:0.14923357963562012
epoch£º685	 i:9 	 global-step:13709	 l-p:0.1139608845114708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6621, 3.6615, 3.6621],
        [3.6621, 3.6611, 3.6621],
        [3.6621, 3.6013, 3.6505],
        [3.6621, 3.1575, 2.5665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.14113005995750427 
model_pd.l_d.mean(): -22.566112518310547 
model_pd.lagr.mean(): -22.42498207092285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3290], device='cuda:0')), ('power', tensor([-22.8952], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.14113005995750427
epoch£º686	 i:1 	 global-step:13721	 l-p:0.08299367129802704
epoch£º686	 i:2 	 global-step:13722	 l-p:0.14309974014759064
epoch£º686	 i:3 	 global-step:13723	 l-p:0.13474656641483307
epoch£º686	 i:4 	 global-step:13724	 l-p:0.16942252218723297
epoch£º686	 i:5 	 global-step:13725	 l-p:0.15571856498718262
epoch£º686	 i:6 	 global-step:13726	 l-p:0.16150222718715668
epoch£º686	 i:7 	 global-step:13727	 l-p:0.17763522267341614
epoch£º686	 i:8 	 global-step:13728	 l-p:0.1274310201406479
epoch£º686	 i:9 	 global-step:13729	 l-p:0.1331280618906021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[3.6745, 3.1471, 2.5524],
        [3.6745, 3.0175, 2.6676],
        [3.6745, 3.2042, 3.1944],
        [3.6745, 3.0771, 2.4841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.07086081802845001 
model_pd.l_d.mean(): -22.998653411865234 
model_pd.lagr.mean(): -22.927793502807617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2974], device='cuda:0')), ('power', tensor([-23.2960], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.07086081802845001
epoch£º687	 i:1 	 global-step:13741	 l-p:0.14835762977600098
epoch£º687	 i:2 	 global-step:13742	 l-p:0.12647324800491333
epoch£º687	 i:3 	 global-step:13743	 l-p:0.12849603593349457
epoch£º687	 i:4 	 global-step:13744	 l-p:0.1373821198940277
epoch£º687	 i:5 	 global-step:13745	 l-p:0.09985317289829254
epoch£º687	 i:6 	 global-step:13746	 l-p:0.22440490126609802
epoch£º687	 i:7 	 global-step:13747	 l-p:0.2088983803987503
epoch£º687	 i:8 	 global-step:13748	 l-p:0.1380573958158493
epoch£º687	 i:9 	 global-step:13749	 l-p:0.44178739190101624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6111, 3.6111, 3.6111],
        [3.6111, 2.9757, 2.7131],
        [3.6111, 3.2541, 3.3336],
        [3.6111, 3.6110, 3.6111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.7307776212692261 
model_pd.l_d.mean(): -22.850351333618164 
model_pd.lagr.mean(): -22.11957359313965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2927], device='cuda:0')), ('power', tensor([-23.1430], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.7307776212692261
epoch£º688	 i:1 	 global-step:13761	 l-p:0.11950378119945526
epoch£º688	 i:2 	 global-step:13762	 l-p:0.13827228546142578
epoch£º688	 i:3 	 global-step:13763	 l-p:0.15912938117980957
epoch£º688	 i:4 	 global-step:13764	 l-p:0.1431441456079483
epoch£º688	 i:5 	 global-step:13765	 l-p:0.23248526453971863
epoch£º688	 i:6 	 global-step:13766	 l-p:0.15388751029968262
epoch£º688	 i:7 	 global-step:13767	 l-p:0.12933911383152008
epoch£º688	 i:8 	 global-step:13768	 l-p:0.1251896172761917
epoch£º688	 i:9 	 global-step:13769	 l-p:0.09724313765764236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5438, 3.4091, 3.4978],
        [3.5438, 3.5298, 3.5428],
        [3.5438, 3.5437, 3.5437],
        [3.5438, 3.5438, 3.5438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.15359272062778473 
model_pd.l_d.mean(): -23.588611602783203 
model_pd.lagr.mean(): -23.43501853942871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2169], device='cuda:0')), ('power', tensor([-23.8055], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.15359272062778473
epoch£º689	 i:1 	 global-step:13781	 l-p:0.13422757387161255
epoch£º689	 i:2 	 global-step:13782	 l-p:0.12363281846046448
epoch£º689	 i:3 	 global-step:13783	 l-p:0.10387087613344193
epoch£º689	 i:4 	 global-step:13784	 l-p:0.015575055964291096
epoch£º689	 i:5 	 global-step:13785	 l-p:0.10210055857896805
epoch£º689	 i:6 	 global-step:13786	 l-p:0.4416901767253876
epoch£º689	 i:7 	 global-step:13787	 l-p:0.1679721623659134
epoch£º689	 i:8 	 global-step:13788	 l-p:0.15173527598381042
epoch£º689	 i:9 	 global-step:13789	 l-p:0.12560978531837463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5697, 3.5696, 3.5698],
        [3.5697, 3.1783, 2.6166],
        [3.5697, 3.5687, 3.5697],
        [3.5697, 3.5698, 3.5698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.13596093654632568 
model_pd.l_d.mean(): -23.55788230895996 
model_pd.lagr.mean(): -23.421920776367188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2322], device='cuda:0')), ('power', tensor([-23.7901], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.13596093654632568
epoch£º690	 i:1 	 global-step:13801	 l-p:0.1284732073545456
epoch£º690	 i:2 	 global-step:13802	 l-p:0.12242832779884338
epoch£º690	 i:3 	 global-step:13803	 l-p:0.05557028204202652
epoch£º690	 i:4 	 global-step:13804	 l-p:0.12623783946037292
epoch£º690	 i:5 	 global-step:13805	 l-p:0.12993431091308594
epoch£º690	 i:6 	 global-step:13806	 l-p:0.16373710334300995
epoch£º690	 i:7 	 global-step:13807	 l-p:0.22084495425224304
epoch£º690	 i:8 	 global-step:13808	 l-p:0.13451741635799408
epoch£º690	 i:9 	 global-step:13809	 l-p:0.13152170181274414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5635, 3.5634, 3.5635],
        [3.5635, 3.3413, 3.4499],
        [3.5635, 2.8707, 2.3533],
        [3.5635, 3.5592, 3.5633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.17757554352283478 
model_pd.l_d.mean(): -22.928142547607422 
model_pd.lagr.mean(): -22.750566482543945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2831], device='cuda:0')), ('power', tensor([-23.2112], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.17757554352283478
epoch£º691	 i:1 	 global-step:13821	 l-p:-0.014227981679141521
epoch£º691	 i:2 	 global-step:13822	 l-p:0.14644284546375275
epoch£º691	 i:3 	 global-step:13823	 l-p:0.14862516522407532
epoch£º691	 i:4 	 global-step:13824	 l-p:0.16058574616909027
epoch£º691	 i:5 	 global-step:13825	 l-p:0.22917021811008453
epoch£º691	 i:6 	 global-step:13826	 l-p:0.1605394333600998
epoch£º691	 i:7 	 global-step:13827	 l-p:-0.2639847695827484
epoch£º691	 i:8 	 global-step:13828	 l-p:0.0798550620675087
epoch£º691	 i:9 	 global-step:13829	 l-p:0.11898717284202576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5664, 3.5658, 3.5664],
        [3.5664, 3.5653, 3.5663],
        [3.5664, 3.4372, 3.5236],
        [3.5664, 3.5664, 3.5664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.17115184664726257 
model_pd.l_d.mean(): -23.67270851135254 
model_pd.lagr.mean(): -23.501556396484375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1559], device='cuda:0')), ('power', tensor([-23.8286], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.17115184664726257
epoch£º692	 i:1 	 global-step:13841	 l-p:0.002255878411233425
epoch£º692	 i:2 	 global-step:13842	 l-p:-0.015130030922591686
epoch£º692	 i:3 	 global-step:13843	 l-p:0.13061322271823883
epoch£º692	 i:4 	 global-step:13844	 l-p:0.16817748546600342
epoch£º692	 i:5 	 global-step:13845	 l-p:0.26718881726264954
epoch£º692	 i:6 	 global-step:13846	 l-p:0.13475990295410156
epoch£º692	 i:7 	 global-step:13847	 l-p:0.11540962755680084
epoch£º692	 i:8 	 global-step:13848	 l-p:0.1328888088464737
epoch£º692	 i:9 	 global-step:13849	 l-p:0.11492986232042313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6611, 3.6540, 3.6608],
        [3.6611, 3.0824, 2.4853],
        [3.6611, 3.2551, 2.6809],
        [3.6611, 3.2844, 2.7171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.13191939890384674 
model_pd.l_d.mean(): -22.914236068725586 
model_pd.lagr.mean(): -22.782316207885742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3326], device='cuda:0')), ('power', tensor([-23.2468], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.13191939890384674
epoch£º693	 i:1 	 global-step:13861	 l-p:0.1296038180589676
epoch£º693	 i:2 	 global-step:13862	 l-p:0.13450182974338531
epoch£º693	 i:3 	 global-step:13863	 l-p:0.13626515865325928
epoch£º693	 i:4 	 global-step:13864	 l-p:0.1281280368566513
epoch£º693	 i:5 	 global-step:13865	 l-p:0.12740445137023926
epoch£º693	 i:6 	 global-step:13866	 l-p:0.2038346379995346
epoch£º693	 i:7 	 global-step:13867	 l-p:0.15202350914478302
epoch£º693	 i:8 	 global-step:13868	 l-p:0.2708922028541565
epoch£º693	 i:9 	 global-step:13869	 l-p:0.1647213101387024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6176, 3.4884, 3.5747],
        [3.6176, 2.9861, 2.7378],
        [3.6176, 3.6175, 3.6176],
        [3.6176, 3.6176, 3.6176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.14796103537082672 
model_pd.l_d.mean(): -22.660263061523438 
model_pd.lagr.mean(): -22.51230239868164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2940], device='cuda:0')), ('power', tensor([-22.9543], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.14796103537082672
epoch£º694	 i:1 	 global-step:13881	 l-p:0.19667363166809082
epoch£º694	 i:2 	 global-step:13882	 l-p:0.23533985018730164
epoch£º694	 i:3 	 global-step:13883	 l-p:0.12171678245067596
epoch£º694	 i:4 	 global-step:13884	 l-p:0.10360334068536758
epoch£º694	 i:5 	 global-step:13885	 l-p:0.13247376680374146
epoch£º694	 i:6 	 global-step:13886	 l-p:0.12480389326810837
epoch£º694	 i:7 	 global-step:13887	 l-p:0.15671654045581818
epoch£º694	 i:8 	 global-step:13888	 l-p:0.12326616048812866
epoch£º694	 i:9 	 global-step:13889	 l-p:0.1554434597492218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6790, 3.1395, 3.0476],
        [3.6790, 3.0007, 2.5589],
        [3.6790, 2.9995, 2.5418],
        [3.6790, 3.6784, 3.6790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.11876804381608963 
model_pd.l_d.mean(): -22.893739700317383 
model_pd.lagr.mean(): -22.77497100830078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2198], device='cuda:0')), ('power', tensor([-23.1135], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.11876804381608963
epoch£º695	 i:1 	 global-step:13901	 l-p:0.13408377766609192
epoch£º695	 i:2 	 global-step:13902	 l-p:0.1710672676563263
epoch£º695	 i:3 	 global-step:13903	 l-p:0.07442789524793625
epoch£º695	 i:4 	 global-step:13904	 l-p:0.1577029973268509
epoch£º695	 i:5 	 global-step:13905	 l-p:0.1150047779083252
epoch£º695	 i:6 	 global-step:13906	 l-p:0.12015283107757568
epoch£º695	 i:7 	 global-step:13907	 l-p:0.18559439480304718
epoch£º695	 i:8 	 global-step:13908	 l-p:0.12874126434326172
epoch£º695	 i:9 	 global-step:13909	 l-p:0.19081756472587585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6495, 3.6325, 3.6481],
        [3.6495, 2.9663, 2.5239],
        [3.6495, 3.5614, 3.6275],
        [3.6495, 3.2231, 2.6451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.14191800355911255 
model_pd.l_d.mean(): -23.646183013916016 
model_pd.lagr.mean(): -23.50426483154297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1587], device='cuda:0')), ('power', tensor([-23.8048], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.14191800355911255
epoch£º696	 i:1 	 global-step:13921	 l-p:0.08762061595916748
epoch£º696	 i:2 	 global-step:13922	 l-p:0.1421724557876587
epoch£º696	 i:3 	 global-step:13923	 l-p:0.1146598756313324
epoch£º696	 i:4 	 global-step:13924	 l-p:0.13278505206108093
epoch£º696	 i:5 	 global-step:13925	 l-p:0.5454122424125671
epoch£º696	 i:6 	 global-step:13926	 l-p:0.154746413230896
epoch£º696	 i:7 	 global-step:13927	 l-p:-0.196511909365654
epoch£º696	 i:8 	 global-step:13928	 l-p:0.13339003920555115
epoch£º696	 i:9 	 global-step:13929	 l-p:0.5387374758720398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6053, 3.6053, 3.6053],
        [3.6053, 3.6052, 3.6053],
        [3.6053, 3.0029, 2.4083],
        [3.6053, 3.6036, 3.6052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.12683379650115967 
model_pd.l_d.mean(): -23.208759307861328 
model_pd.lagr.mean(): -23.081926345825195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1822], device='cuda:0')), ('power', tensor([-23.3910], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.12683379650115967
epoch£º697	 i:1 	 global-step:13941	 l-p:0.14639754593372345
epoch£º697	 i:2 	 global-step:13942	 l-p:0.11312827467918396
epoch£º697	 i:3 	 global-step:13943	 l-p:0.1647871732711792
epoch£º697	 i:4 	 global-step:13944	 l-p:0.235107421875
epoch£º697	 i:5 	 global-step:13945	 l-p:0.13485459983348846
epoch£º697	 i:6 	 global-step:13946	 l-p:0.13756296038627625
epoch£º697	 i:7 	 global-step:13947	 l-p:0.03381188213825226
epoch£º697	 i:8 	 global-step:13948	 l-p:0.13006921112537384
epoch£º697	 i:9 	 global-step:13949	 l-p:0.12837684154510498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5572, 3.4852, 3.5419],
        [3.5572, 3.5524, 3.5570],
        [3.5572, 3.1247, 3.1615],
        [3.5572, 2.9665, 2.8142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.13744305074214935 
model_pd.l_d.mean(): -23.518613815307617 
model_pd.lagr.mean(): -23.38117027282715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2110], device='cuda:0')), ('power', tensor([-23.7296], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.13744305074214935
epoch£º698	 i:1 	 global-step:13961	 l-p:0.08724919706583023
epoch£º698	 i:2 	 global-step:13962	 l-p:0.29520025849342346
epoch£º698	 i:3 	 global-step:13963	 l-p:0.13123179972171783
epoch£º698	 i:4 	 global-step:13964	 l-p:0.12753960490226746
epoch£º698	 i:5 	 global-step:13965	 l-p:0.14252176880836487
epoch£º698	 i:6 	 global-step:13966	 l-p:0.1896323412656784
epoch£º698	 i:7 	 global-step:13967	 l-p:0.07235556095838547
epoch£º698	 i:8 	 global-step:13968	 l-p:0.21322578191757202
epoch£º698	 i:9 	 global-step:13969	 l-p:0.11003225296735764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5877, 3.5742, 3.5868],
        [3.5877, 3.4592, 3.5454],
        [3.5877, 2.9762, 2.7810],
        [3.5877, 3.5877, 3.5877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): -0.14402355253696442 
model_pd.l_d.mean(): -23.43936538696289 
model_pd.lagr.mean(): -23.583389282226562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2338], device='cuda:0')), ('power', tensor([-23.6731], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:-0.14402355253696442
epoch£º699	 i:1 	 global-step:13981	 l-p:0.12008733302354813
epoch£º699	 i:2 	 global-step:13982	 l-p:0.13364602625370026
epoch£º699	 i:3 	 global-step:13983	 l-p:-0.09579332172870636
epoch£º699	 i:4 	 global-step:13984	 l-p:0.1451193243265152
epoch£º699	 i:5 	 global-step:13985	 l-p:0.1930769681930542
epoch£º699	 i:6 	 global-step:13986	 l-p:0.42558759450912476
epoch£º699	 i:7 	 global-step:13987	 l-p:0.13912872970104218
epoch£º699	 i:8 	 global-step:13988	 l-p:0.10647813230752945
epoch£º699	 i:9 	 global-step:13989	 l-p:0.12611231207847595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6480, 3.6385, 3.6475],
        [3.6480, 3.1178, 3.0455],
        [3.6480, 3.1899, 3.1992],
        [3.6480, 3.5163, 3.6036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.08354616165161133 
model_pd.l_d.mean(): -23.414030075073242 
model_pd.lagr.mean(): -23.33048439025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1586], device='cuda:0')), ('power', tensor([-23.5726], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.08354616165161133
epoch£º700	 i:1 	 global-step:14001	 l-p:0.1732296645641327
epoch£º700	 i:2 	 global-step:14002	 l-p:0.12472178786993027
epoch£º700	 i:3 	 global-step:14003	 l-p:0.11827903240919113
epoch£º700	 i:4 	 global-step:14004	 l-p:0.15361280739307404
epoch£º700	 i:5 	 global-step:14005	 l-p:0.13840171694755554
epoch£º700	 i:6 	 global-step:14006	 l-p:0.15241739153862
epoch£º700	 i:7 	 global-step:14007	 l-p:0.16055679321289062
epoch£º700	 i:8 	 global-step:14008	 l-p:0.15911248326301575
epoch£º700	 i:9 	 global-step:14009	 l-p:0.1389012336730957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6617, 3.6478, 3.6607],
        [3.6617, 3.4992, 3.5970],
        [3.6617, 3.6617, 3.6617],
        [3.6617, 3.2869, 3.3577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.17853248119354248 
model_pd.l_d.mean(): -23.216182708740234 
model_pd.lagr.mean(): -23.03765106201172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2166], device='cuda:0')), ('power', tensor([-23.4328], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.17853248119354248
epoch£º701	 i:1 	 global-step:14021	 l-p:0.1330936998128891
epoch£º701	 i:2 	 global-step:14022	 l-p:0.08997737616300583
epoch£º701	 i:3 	 global-step:14023	 l-p:0.12685535848140717
epoch£º701	 i:4 	 global-step:14024	 l-p:0.20770633220672607
epoch£º701	 i:5 	 global-step:14025	 l-p:0.15506450831890106
epoch£º701	 i:6 	 global-step:14026	 l-p:0.12455346435308456
epoch£º701	 i:7 	 global-step:14027	 l-p:0.24069640040397644
epoch£º701	 i:8 	 global-step:14028	 l-p:0.12565036118030548
epoch£º701	 i:9 	 global-step:14029	 l-p:0.13297711312770844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6118, 3.4832, 3.5694],
        [3.6118, 3.0492, 2.4528],
        [3.6118, 3.4873, 3.5718],
        [3.6118, 3.6118, 3.6118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.10478896647691727 
model_pd.l_d.mean(): -23.339967727661133 
model_pd.lagr.mean(): -23.235177993774414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2310], device='cuda:0')), ('power', tensor([-23.5709], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.10478896647691727
epoch£º702	 i:1 	 global-step:14041	 l-p:0.14878752827644348
epoch£º702	 i:2 	 global-step:14042	 l-p:0.19323322176933289
epoch£º702	 i:3 	 global-step:14043	 l-p:-0.13600972294807434
epoch£º702	 i:4 	 global-step:14044	 l-p:0.1346236765384674
epoch£º702	 i:5 	 global-step:14045	 l-p:0.14048486948013306
epoch£º702	 i:6 	 global-step:14046	 l-p:0.45909327268600464
epoch£º702	 i:7 	 global-step:14047	 l-p:0.15961381793022156
epoch£º702	 i:8 	 global-step:14048	 l-p:0.10739602148532867
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12380426377058029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5468, 3.5413, 3.5466],
        [3.5468, 3.0170, 2.4302],
        [3.5468, 3.5227, 3.5444],
        [3.5468, 3.0333, 2.4488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.09648691862821579 
model_pd.l_d.mean(): -23.46918487548828 
model_pd.lagr.mean(): -23.372697830200195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2711], device='cuda:0')), ('power', tensor([-23.7403], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.09648691862821579
epoch£º703	 i:1 	 global-step:14061	 l-p:0.08386249095201492
epoch£º703	 i:2 	 global-step:14062	 l-p:-0.42951440811157227
epoch£º703	 i:3 	 global-step:14063	 l-p:0.13730815052986145
epoch£º703	 i:4 	 global-step:14064	 l-p:0.16211867332458496
epoch£º703	 i:5 	 global-step:14065	 l-p:0.16028007864952087
epoch£º703	 i:6 	 global-step:14066	 l-p:0.08007124066352844
epoch£º703	 i:7 	 global-step:14067	 l-p:0.22921769320964813
epoch£º703	 i:8 	 global-step:14068	 l-p:0.15327276289463043
epoch£º703	 i:9 	 global-step:14069	 l-p:0.14983078837394714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5801, 3.5802, 3.5802],
        [3.5801, 3.4908, 3.5579],
        [3.5801, 3.4138, 3.5135],
        [3.5801, 2.8883, 2.4849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.1343771517276764 
model_pd.l_d.mean(): -23.814268112182617 
model_pd.lagr.mean(): -23.67989158630371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1160], device='cuda:0')), ('power', tensor([-23.9303], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.1343771517276764
epoch£º704	 i:1 	 global-step:14081	 l-p:0.1740446388721466
epoch£º704	 i:2 	 global-step:14082	 l-p:0.14945939183235168
epoch£º704	 i:3 	 global-step:14083	 l-p:0.17127658426761627
epoch£º704	 i:4 	 global-step:14084	 l-p:0.13393127918243408
epoch£º704	 i:5 	 global-step:14085	 l-p:0.19035251438617706
epoch£º704	 i:6 	 global-step:14086	 l-p:-0.1407512128353119
epoch£º704	 i:7 	 global-step:14087	 l-p:0.13939712941646576
epoch£º704	 i:8 	 global-step:14088	 l-p:-0.16136550903320312
epoch£º704	 i:9 	 global-step:14089	 l-p:0.11262522637844086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6275, 3.6276, 3.6276],
        [3.6275, 3.6191, 3.6271],
        [3.6275, 2.9372, 2.5108],
        [3.6275, 3.0190, 2.4206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.1838679015636444 
model_pd.l_d.mean(): -23.279579162597656 
model_pd.lagr.mean(): -23.09571075439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2826], device='cuda:0')), ('power', tensor([-23.5622], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:0.1838679015636444
epoch£º705	 i:1 	 global-step:14101	 l-p:0.1393342912197113
epoch£º705	 i:2 	 global-step:14102	 l-p:0.2032724916934967
epoch£º705	 i:3 	 global-step:14103	 l-p:0.12924714386463165
epoch£º705	 i:4 	 global-step:14104	 l-p:0.167928084731102
epoch£º705	 i:5 	 global-step:14105	 l-p:0.10902654379606247
epoch£º705	 i:6 	 global-step:14106	 l-p:0.12130212783813477
epoch£º705	 i:7 	 global-step:14107	 l-p:0.1279647946357727
epoch£º705	 i:8 	 global-step:14108	 l-p:0.16754527390003204
epoch£º705	 i:9 	 global-step:14109	 l-p:0.05504186451435089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6948, 3.5428, 3.6374],
        [3.6948, 3.1582, 2.5565],
        [3.6948, 3.6232, 3.6795],
        [3.6948, 3.1020, 2.9292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.12287599593400955 
model_pd.l_d.mean(): -23.45651626586914 
model_pd.lagr.mean(): -23.333641052246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1919], device='cuda:0')), ('power', tensor([-23.6484], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.12287599593400955
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12049888074398041
epoch£º706	 i:2 	 global-step:14122	 l-p:0.13602092862129211
epoch£º706	 i:3 	 global-step:14123	 l-p:0.14996293187141418
epoch£º706	 i:4 	 global-step:14124	 l-p:0.17165899276733398
epoch£º706	 i:5 	 global-step:14125	 l-p:0.13793911039829254
epoch£º706	 i:6 	 global-step:14126	 l-p:0.06466104835271835
epoch£º706	 i:7 	 global-step:14127	 l-p:0.1669146865606308
epoch£º706	 i:8 	 global-step:14128	 l-p:0.1262870728969574
epoch£º706	 i:9 	 global-step:14129	 l-p:0.1598200500011444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6938, 3.6933, 3.6938],
        [3.6938, 3.6938, 3.6939],
        [3.6938, 3.6938, 3.6939],
        [3.6938, 3.4717, 3.5798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.14594964683055878 
model_pd.l_d.mean(): -23.686216354370117 
model_pd.lagr.mean(): -23.540266036987305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1319], device='cuda:0')), ('power', tensor([-23.8181], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.14594964683055878
epoch£º707	 i:1 	 global-step:14141	 l-p:0.14396433532238007
epoch£º707	 i:2 	 global-step:14142	 l-p:0.13425502181053162
epoch£º707	 i:3 	 global-step:14143	 l-p:0.15427443385124207
epoch£º707	 i:4 	 global-step:14144	 l-p:0.11977815628051758
epoch£º707	 i:5 	 global-step:14145	 l-p:0.1301734447479248
epoch£º707	 i:6 	 global-step:14146	 l-p:0.15559273958206177
epoch£º707	 i:7 	 global-step:14147	 l-p:0.11976059526205063
epoch£º707	 i:8 	 global-step:14148	 l-p:0.2090081125497818
epoch£º707	 i:9 	 global-step:14149	 l-p:0.08140125125646591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6519, 3.5037, 3.5974],
        [3.6519, 3.2743, 3.3459],
        [3.6519, 2.9827, 2.4118],
        [3.6519, 3.5020, 3.5962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.12421537935733795 
model_pd.l_d.mean(): -23.386919021606445 
model_pd.lagr.mean(): -23.26270294189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2241], device='cuda:0')), ('power', tensor([-23.6110], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.12421537935733795
epoch£º708	 i:1 	 global-step:14161	 l-p:0.1385226547718048
epoch£º708	 i:2 	 global-step:14162	 l-p:0.1336694061756134
epoch£º708	 i:3 	 global-step:14163	 l-p:0.2173098772764206
epoch£º708	 i:4 	 global-step:14164	 l-p:0.09680274873971939
epoch£º708	 i:5 	 global-step:14165	 l-p:0.27951905131340027
epoch£º708	 i:6 	 global-step:14166	 l-p:0.13579803705215454
epoch£º708	 i:7 	 global-step:14167	 l-p:0.13336333632469177
epoch£º708	 i:8 	 global-step:14168	 l-p:0.12560299038887024
epoch£º708	 i:9 	 global-step:14169	 l-p:0.13560184836387634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6497, 3.1398, 3.0982],
        [3.6497, 3.5757, 3.6336],
        [3.6497, 3.3046, 2.7436],
        [3.6497, 2.9625, 2.5463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.14845070242881775 
model_pd.l_d.mean(): -23.268022537231445 
model_pd.lagr.mean(): -23.119571685791016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2074], device='cuda:0')), ('power', tensor([-23.4754], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.14845070242881775
epoch£º709	 i:1 	 global-step:14181	 l-p:0.14009974896907806
epoch£º709	 i:2 	 global-step:14182	 l-p:0.14012351632118225
epoch£º709	 i:3 	 global-step:14183	 l-p:-0.12058640271425247
epoch£º709	 i:4 	 global-step:14184	 l-p:0.14139501750469208
epoch£º709	 i:5 	 global-step:14185	 l-p:0.41343724727630615
epoch£º709	 i:6 	 global-step:14186	 l-p:0.08994321525096893
epoch£º709	 i:7 	 global-step:14187	 l-p:0.10622106492519379
epoch£º709	 i:8 	 global-step:14188	 l-p:0.1174452006816864
epoch£º709	 i:9 	 global-step:14189	 l-p:0.15915849804878235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5547, 3.5212, 3.5505],
        [3.5547, 3.0448, 3.0132],
        [3.5547, 3.1584, 3.2241],
        [3.5547, 3.5008, 3.5454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.11739322543144226 
model_pd.l_d.mean(): -23.459726333618164 
model_pd.lagr.mean(): -23.34233283996582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1892], device='cuda:0')), ('power', tensor([-23.6489], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.11739322543144226
epoch£º710	 i:1 	 global-step:14201	 l-p:0.12505638599395752
epoch£º710	 i:2 	 global-step:14202	 l-p:0.130994975566864
epoch£º710	 i:3 	 global-step:14203	 l-p:0.4251418709754944
epoch£º710	 i:4 	 global-step:14204	 l-p:0.15966254472732544
epoch£º710	 i:5 	 global-step:14205	 l-p:0.053331587463617325
epoch£º710	 i:6 	 global-step:14206	 l-p:0.20200568437576294
epoch£º710	 i:7 	 global-step:14207	 l-p:0.15256869792938232
epoch£º710	 i:8 	 global-step:14208	 l-p:0.14998218417167664
epoch£º710	 i:9 	 global-step:14209	 l-p:0.11337022483348846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5114, 3.4903, 3.5095],
        [3.5114, 2.8659, 2.2762],
        [3.5114, 2.8293, 2.2505],
        [3.5114, 3.5114, 3.5114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.144110769033432 
model_pd.l_d.mean(): -23.015321731567383 
model_pd.lagr.mean(): -22.8712100982666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3303], device='cuda:0')), ('power', tensor([-23.3456], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.144110769033432
epoch£º711	 i:1 	 global-step:14221	 l-p:0.16778463125228882
epoch£º711	 i:2 	 global-step:14222	 l-p:0.21562917530536652
epoch£º711	 i:3 	 global-step:14223	 l-p:0.14526624977588654
epoch£º711	 i:4 	 global-step:14224	 l-p:0.13857728242874146
epoch£º711	 i:5 	 global-step:14225	 l-p:0.14825740456581116
epoch£º711	 i:6 	 global-step:14226	 l-p:-0.7155801057815552
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12291397154331207
epoch£º711	 i:8 	 global-step:14228	 l-p:0.29860720038414
epoch£º711	 i:9 	 global-step:14229	 l-p:0.11625610291957855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6391, 2.9564, 2.3964],
        [3.6391, 3.0285, 2.4275],
        [3.6391, 3.4902, 3.5843],
        [3.6391, 3.1105, 2.5128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.1567341834306717 
model_pd.l_d.mean(): -23.340648651123047 
model_pd.lagr.mean(): -23.183914184570312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3011], device='cuda:0')), ('power', tensor([-23.6418], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:0.1567341834306717
epoch£º712	 i:1 	 global-step:14241	 l-p:0.115867018699646
epoch£º712	 i:2 	 global-step:14242	 l-p:0.13214188814163208
epoch£º712	 i:3 	 global-step:14243	 l-p:0.38870444893836975
epoch£º712	 i:4 	 global-step:14244	 l-p:0.10952558368444443
epoch£º712	 i:5 	 global-step:14245	 l-p:0.12630100548267365
epoch£º712	 i:6 	 global-step:14246	 l-p:0.13992983102798462
epoch£º712	 i:7 	 global-step:14247	 l-p:0.15982183814048767
epoch£º712	 i:8 	 global-step:14248	 l-p:0.16178902983665466
epoch£º712	 i:9 	 global-step:14249	 l-p:0.12630002200603485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5622, 3.5487, 3.5613],
        [3.5622, 2.9374, 2.7361],
        [3.5622, 3.5622, 3.5622],
        [3.5622, 2.8655, 2.4749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): -1.0491334199905396 
model_pd.l_d.mean(): -23.715221405029297 
model_pd.lagr.mean(): -24.764354705810547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1503], device='cuda:0')), ('power', tensor([-23.8655], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:-1.0491334199905396
epoch£º713	 i:1 	 global-step:14261	 l-p:0.18271155655384064
epoch£º713	 i:2 	 global-step:14262	 l-p:0.1416098028421402
epoch£º713	 i:3 	 global-step:14263	 l-p:0.13009819388389587
epoch£º713	 i:4 	 global-step:14264	 l-p:0.16064196825027466
epoch£º713	 i:5 	 global-step:14265	 l-p:0.14694719016551971
epoch£º713	 i:6 	 global-step:14266	 l-p:0.12634852528572083
epoch£º713	 i:7 	 global-step:14267	 l-p:0.12223833799362183
epoch£º713	 i:8 	 global-step:14268	 l-p:-0.044186897575855255
epoch£º713	 i:9 	 global-step:14269	 l-p:0.18663902580738068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4946, 3.4946, 3.4946],
        [3.4946, 2.7845, 2.2274],
        [3.4946, 2.7795, 2.3492],
        [3.4946, 3.4866, 3.4942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.1617899239063263 
model_pd.l_d.mean(): -23.584327697753906 
model_pd.lagr.mean(): -23.422536849975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2559], device='cuda:0')), ('power', tensor([-23.8402], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.1617899239063263
epoch£º714	 i:1 	 global-step:14281	 l-p:0.17760398983955383
epoch£º714	 i:2 	 global-step:14282	 l-p:0.13865447044372559
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14869114756584167
epoch£º714	 i:4 	 global-step:14284	 l-p:0.13829173147678375
epoch£º714	 i:5 	 global-step:14285	 l-p:0.1483612358570099
epoch£º714	 i:6 	 global-step:14286	 l-p:-0.06421805918216705
epoch£º714	 i:7 	 global-step:14287	 l-p:0.2095130831003189
epoch£º714	 i:8 	 global-step:14288	 l-p:0.12347375601530075
epoch£º714	 i:9 	 global-step:14289	 l-p:-0.011974990367889404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5962, 3.5731, 3.5940],
        [3.5962, 3.1703, 2.5947],
        [3.5962, 2.9777, 2.7842],
        [3.5962, 3.5962, 3.5962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): -0.12839868664741516 
model_pd.l_d.mean(): -23.392383575439453 
model_pd.lagr.mean(): -23.520782470703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2248], device='cuda:0')), ('power', tensor([-23.6172], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:-0.12839868664741516
epoch£º715	 i:1 	 global-step:14301	 l-p:0.14236032962799072
epoch£º715	 i:2 	 global-step:14302	 l-p:0.16718776524066925
epoch£º715	 i:3 	 global-step:14303	 l-p:0.13424280285835266
epoch£º715	 i:4 	 global-step:14304	 l-p:0.1385365128517151
epoch£º715	 i:5 	 global-step:14305	 l-p:0.1267426311969757
epoch£º715	 i:6 	 global-step:14306	 l-p:0.12058021128177643
epoch£º715	 i:7 	 global-step:14307	 l-p:0.19022709131240845
epoch£º715	 i:8 	 global-step:14308	 l-p:0.15261930227279663
epoch£º715	 i:9 	 global-step:14309	 l-p:0.18931663036346436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6712, 2.9785, 2.4526],
        [3.6712, 3.6665, 3.6711],
        [3.6712, 3.6713, 3.6712],
        [3.6712, 3.2526, 2.6707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.14842763543128967 
model_pd.l_d.mean(): -23.359708786010742 
model_pd.lagr.mean(): -23.211280822753906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2216], device='cuda:0')), ('power', tensor([-23.5813], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.14842763543128967
epoch£º716	 i:1 	 global-step:14321	 l-p:0.12062135338783264
epoch£º716	 i:2 	 global-step:14322	 l-p:0.12651276588439941
epoch£º716	 i:3 	 global-step:14323	 l-p:0.12333688139915466
epoch£º716	 i:4 	 global-step:14324	 l-p:0.139882430434227
epoch£º716	 i:5 	 global-step:14325	 l-p:0.2407873421907425
epoch£º716	 i:6 	 global-step:14326	 l-p:0.1006302535533905
epoch£º716	 i:7 	 global-step:14327	 l-p:0.13931818306446075
epoch£º716	 i:8 	 global-step:14328	 l-p:0.27699989080429077
epoch£º716	 i:9 	 global-step:14329	 l-p:0.6518973112106323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6299, 3.4220, 3.5304],
        [3.6299, 3.3582, 3.4672],
        [3.6299, 3.6199, 3.6293],
        [3.6299, 3.0382, 2.4350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.13540850579738617 
model_pd.l_d.mean(): -23.675750732421875 
model_pd.lagr.mean(): -23.540342330932617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1443], device='cuda:0')), ('power', tensor([-23.8201], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.13540850579738617
epoch£º717	 i:1 	 global-step:14341	 l-p:0.10430299490690231
epoch£º717	 i:2 	 global-step:14342	 l-p:0.14616641402244568
epoch£º717	 i:3 	 global-step:14343	 l-p:0.372266948223114
epoch£º717	 i:4 	 global-step:14344	 l-p:0.19806496798992157
epoch£º717	 i:5 	 global-step:14345	 l-p:-0.16945567727088928
epoch£º717	 i:6 	 global-step:14346	 l-p:0.1367727369070053
epoch£º717	 i:7 	 global-step:14347	 l-p:0.14009705185890198
epoch£º717	 i:8 	 global-step:14348	 l-p:0.13158929347991943
epoch£º717	 i:9 	 global-step:14349	 l-p:0.11519195884466171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[3.6081, 3.1336, 3.1386],
        [3.6081, 3.2419, 3.3238],
        [3.6081, 3.2093, 2.6379],
        [3.6081, 3.3129, 3.4188]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.12944746017456055 
model_pd.l_d.mean(): -23.6938533782959 
model_pd.lagr.mean(): -23.56440544128418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1476], device='cuda:0')), ('power', tensor([-23.8415], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.12944746017456055
epoch£º718	 i:1 	 global-step:14361	 l-p:0.16637448966503143
epoch£º718	 i:2 	 global-step:14362	 l-p:0.03813904523849487
epoch£º718	 i:3 	 global-step:14363	 l-p:0.12465506047010422
epoch£º718	 i:4 	 global-step:14364	 l-p:-0.05997482314705849
epoch£º718	 i:5 	 global-step:14365	 l-p:0.19627830386161804
epoch£º718	 i:6 	 global-step:14366	 l-p:0.12119151651859283
epoch£º718	 i:7 	 global-step:14367	 l-p:0.15306158363819122
epoch£º718	 i:8 	 global-step:14368	 l-p:0.1682719886302948
epoch£º718	 i:9 	 global-step:14369	 l-p:-0.8727735877037048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6199, 3.2992, 3.3986],
        [3.6199, 2.9223, 2.3733],
        [3.6199, 3.0042, 2.4017],
        [3.6199, 3.5767, 3.6135]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.1344422549009323 
model_pd.l_d.mean(): -22.371562957763672 
model_pd.lagr.mean(): -22.23712158203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3629], device='cuda:0')), ('power', tensor([-22.7344], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.1344422549009323
epoch£º719	 i:1 	 global-step:14381	 l-p:0.4757711589336395
epoch£º719	 i:2 	 global-step:14382	 l-p:0.3027033805847168
epoch£º719	 i:3 	 global-step:14383	 l-p:0.14509302377700806
epoch£º719	 i:4 	 global-step:14384	 l-p:0.1310792714357376
epoch£º719	 i:5 	 global-step:14385	 l-p:0.13580021262168884
epoch£º719	 i:6 	 global-step:14386	 l-p:0.12787745893001556
epoch£º719	 i:7 	 global-step:14387	 l-p:0.13295751810073853
epoch£º719	 i:8 	 global-step:14388	 l-p:0.1246938705444336
epoch£º719	 i:9 	 global-step:14389	 l-p:0.126643106341362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6537, 3.5140, 3.6050],
        [3.6537, 3.1810, 3.1846],
        [3.6537, 2.9592, 2.5396],
        [3.6537, 3.0860, 2.9729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.1558043658733368 
model_pd.l_d.mean(): -23.383758544921875 
model_pd.lagr.mean(): -23.227954864501953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2017], device='cuda:0')), ('power', tensor([-23.5855], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.1558043658733368
epoch£º720	 i:1 	 global-step:14401	 l-p:0.209183007478714
epoch£º720	 i:2 	 global-step:14402	 l-p:0.16836971044540405
epoch£º720	 i:3 	 global-step:14403	 l-p:0.24180427193641663
epoch£º720	 i:4 	 global-step:14404	 l-p:0.12137424200773239
epoch£º720	 i:5 	 global-step:14405	 l-p:0.1251816302537918
epoch£º720	 i:6 	 global-step:14406	 l-p:0.13705599308013916
epoch£º720	 i:7 	 global-step:14407	 l-p:0.1291096806526184
epoch£º720	 i:8 	 global-step:14408	 l-p:0.13748322427272797
epoch£º720	 i:9 	 global-step:14409	 l-p:0.20906750857830048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[3.6336, 2.9289, 2.4344],
        [3.6336, 3.2408, 2.6679],
        [3.6336, 3.0408, 2.8910],
        [3.6336, 2.9885, 2.7358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.09778197109699249 
model_pd.l_d.mean(): -22.94928550720215 
model_pd.lagr.mean(): -22.851503372192383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2654], device='cuda:0')), ('power', tensor([-23.2147], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.09778197109699249
epoch£º721	 i:1 	 global-step:14421	 l-p:0.141153946518898
epoch£º721	 i:2 	 global-step:14422	 l-p:0.275726318359375
epoch£º721	 i:3 	 global-step:14423	 l-p:0.12179868668317795
epoch£º721	 i:4 	 global-step:14424	 l-p:0.16315414011478424
epoch£º721	 i:5 	 global-step:14425	 l-p:0.15858960151672363
epoch£º721	 i:6 	 global-step:14426	 l-p:0.14233346283435822
epoch£º721	 i:7 	 global-step:14427	 l-p:0.27770209312438965
epoch£º721	 i:8 	 global-step:14428	 l-p:0.12327014654874802
epoch£º721	 i:9 	 global-step:14429	 l-p:0.13098841905593872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6574, 3.6574, 3.6574],
        [3.6574, 3.6527, 3.6572],
        [3.6574, 3.6574, 3.6574],
        [3.6574, 3.5652, 3.6340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.11484302580356598 
model_pd.l_d.mean(): -23.277013778686523 
model_pd.lagr.mean(): -23.16217041015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2283], device='cuda:0')), ('power', tensor([-23.5054], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.11484302580356598
epoch£º722	 i:1 	 global-step:14441	 l-p:0.19338832795619965
epoch£º722	 i:2 	 global-step:14442	 l-p:0.19810378551483154
epoch£º722	 i:3 	 global-step:14443	 l-p:0.12668846547603607
epoch£º722	 i:4 	 global-step:14444	 l-p:0.12193753570318222
epoch£º722	 i:5 	 global-step:14445	 l-p:0.13371609151363373
epoch£º722	 i:6 	 global-step:14446	 l-p:0.1626160442829132
epoch£º722	 i:7 	 global-step:14447	 l-p:0.12503720819950104
epoch£º722	 i:8 	 global-step:14448	 l-p:0.14988261461257935
epoch£º722	 i:9 	 global-step:14449	 l-p:0.1386471539735794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6490, 3.5211, 3.6075],
        [3.6490, 2.9568, 2.4016],
        [3.6490, 3.6487, 3.6490],
        [3.6490, 3.6491, 3.6491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.1993282288312912 
model_pd.l_d.mean(): -23.472501754760742 
model_pd.lagr.mean(): -23.273174285888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2063], device='cuda:0')), ('power', tensor([-23.6788], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.1993282288312912
epoch£º723	 i:1 	 global-step:14461	 l-p:0.13129839301109314
epoch£º723	 i:2 	 global-step:14462	 l-p:0.14125894010066986
epoch£º723	 i:3 	 global-step:14463	 l-p:0.1303098052740097
epoch£º723	 i:4 	 global-step:14464	 l-p:0.1434403359889984
epoch£º723	 i:5 	 global-step:14465	 l-p:0.14892305433750153
epoch£º723	 i:6 	 global-step:14466	 l-p:0.18423737585544586
epoch£º723	 i:7 	 global-step:14467	 l-p:0.12531423568725586
epoch£º723	 i:8 	 global-step:14468	 l-p:0.16819457709789276
epoch£º723	 i:9 	 global-step:14469	 l-p:-0.03450136259198189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5328, 3.5276, 3.5326],
        [3.5328, 2.8309, 2.2589],
        [3.5328, 3.5101, 3.5307],
        [3.5328, 3.0076, 2.9675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.09864003956317902 
model_pd.l_d.mean(): -23.065967559814453 
model_pd.lagr.mean(): -22.967327117919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2793], device='cuda:0')), ('power', tensor([-23.3452], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.09864003956317902
epoch£º724	 i:1 	 global-step:14481	 l-p:0.35023102164268494
epoch£º724	 i:2 	 global-step:14482	 l-p:0.1591375768184662
epoch£º724	 i:3 	 global-step:14483	 l-p:0.13534864783287048
epoch£º724	 i:4 	 global-step:14484	 l-p:0.11593936383724213
epoch£º724	 i:5 	 global-step:14485	 l-p:0.14772021770477295
epoch£º724	 i:6 	 global-step:14486	 l-p:0.12547889351844788
epoch£º724	 i:7 	 global-step:14487	 l-p:0.06708206981420517
epoch£º724	 i:8 	 global-step:14488	 l-p:0.18323686718940735
epoch£º724	 i:9 	 global-step:14489	 l-p:0.07692588865756989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5293, 3.5213, 3.5289],
        [3.5293, 2.8060, 2.3103],
        [3.5293, 2.8141, 2.3884],
        [3.5293, 3.2524, 3.3636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.13409291207790375 
model_pd.l_d.mean(): -23.502378463745117 
model_pd.lagr.mean(): -23.3682861328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1883], device='cuda:0')), ('power', tensor([-23.6907], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.13409291207790375
epoch£º725	 i:1 	 global-step:14501	 l-p:0.12490960955619812
epoch£º725	 i:2 	 global-step:14502	 l-p:0.13418789207935333
epoch£º725	 i:3 	 global-step:14503	 l-p:0.0442928746342659
epoch£º725	 i:4 	 global-step:14504	 l-p:0.12952753901481628
epoch£º725	 i:5 	 global-step:14505	 l-p:0.1646168828010559
epoch£º725	 i:6 	 global-step:14506	 l-p:-2.1825008392333984
epoch£º725	 i:7 	 global-step:14507	 l-p:0.06675107032060623
epoch£º725	 i:8 	 global-step:14508	 l-p:0.09782785177230835
epoch£º725	 i:9 	 global-step:14509	 l-p:0.1269707828760147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[3.5514, 3.2913, 3.4035],
        [3.5514, 2.9343, 2.7602],
        [3.5514, 2.8588, 2.2780],
        [3.5514, 2.9998, 2.4062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): -0.0038708827923983335 
model_pd.l_d.mean(): -22.6912899017334 
model_pd.lagr.mean(): -22.695159912109375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3365], device='cuda:0')), ('power', tensor([-23.0277], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:-0.0038708827923983335
epoch£º726	 i:1 	 global-step:14521	 l-p:0.0695643350481987
epoch£º726	 i:2 	 global-step:14522	 l-p:0.14512859284877777
epoch£º726	 i:3 	 global-step:14523	 l-p:0.14667806029319763
epoch£º726	 i:4 	 global-step:14524	 l-p:0.20329013466835022
epoch£º726	 i:5 	 global-step:14525	 l-p:0.12930317223072052
epoch£º726	 i:6 	 global-step:14526	 l-p:0.14049744606018066
epoch£º726	 i:7 	 global-step:14527	 l-p:-0.008403330110013485
epoch£º726	 i:8 	 global-step:14528	 l-p:0.14430463314056396
epoch£º726	 i:9 	 global-step:14529	 l-p:0.12192131578922272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5871, 3.5871, 3.5871],
        [3.5871, 3.0011, 2.4000],
        [3.5871, 3.1796, 2.6076],
        [3.5871, 3.5871, 3.5871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.14844870567321777 
model_pd.l_d.mean(): -23.516557693481445 
model_pd.lagr.mean(): -23.36810874938965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2410], device='cuda:0')), ('power', tensor([-23.7576], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.14844870567321777
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12335948646068573
epoch£º727	 i:2 	 global-step:14542	 l-p:0.31167352199554443
epoch£º727	 i:3 	 global-step:14543	 l-p:0.09126400947570801
epoch£º727	 i:4 	 global-step:14544	 l-p:0.0010736525291576982
epoch£º727	 i:5 	 global-step:14545	 l-p:0.16425883769989014
epoch£º727	 i:6 	 global-step:14546	 l-p:0.1095927283167839
epoch£º727	 i:7 	 global-step:14547	 l-p:0.09756115078926086
epoch£º727	 i:8 	 global-step:14548	 l-p:0.14270177483558655
epoch£º727	 i:9 	 global-step:14549	 l-p:0.15791985392570496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5918, 3.5660, 3.5891],
        [3.5918, 3.5903, 3.5917],
        [3.5918, 2.8839, 2.4615],
        [3.5918, 3.2067, 2.6391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.25731316208839417 
model_pd.l_d.mean(): -23.44905662536621 
model_pd.lagr.mean(): -23.191743850708008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2196], device='cuda:0')), ('power', tensor([-23.6687], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.25731316208839417
epoch£º728	 i:1 	 global-step:14561	 l-p:0.1725311577320099
epoch£º728	 i:2 	 global-step:14562	 l-p:0.12199456989765167
epoch£º728	 i:3 	 global-step:14563	 l-p:0.13073772192001343
epoch£º728	 i:4 	 global-step:14564	 l-p:0.23999911546707153
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13343022763729095
epoch£º728	 i:6 	 global-step:14566	 l-p:0.14209912717342377
epoch£º728	 i:7 	 global-step:14567	 l-p:0.1334782987833023
epoch£º728	 i:8 	 global-step:14568	 l-p:0.29333457350730896
epoch£º728	 i:9 	 global-step:14569	 l-p:0.25188156962394714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6453, 3.3529, 3.4598],
        [3.6453, 3.4970, 3.5915],
        [3.6453, 3.2796, 3.3624],
        [3.6453, 3.6453, 3.6453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.15518388152122498 
model_pd.l_d.mean(): -22.776765823364258 
model_pd.lagr.mean(): -22.62158203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3571], device='cuda:0')), ('power', tensor([-23.1339], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.15518388152122498
epoch£º729	 i:1 	 global-step:14581	 l-p:0.22356730699539185
epoch£º729	 i:2 	 global-step:14582	 l-p:0.14155997335910797
epoch£º729	 i:3 	 global-step:14583	 l-p:0.12660332024097443
epoch£º729	 i:4 	 global-step:14584	 l-p:0.12141170352697372
epoch£º729	 i:5 	 global-step:14585	 l-p:0.1633056104183197
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12138108909130096
epoch£º729	 i:7 	 global-step:14587	 l-p:0.17694377899169922
epoch£º729	 i:8 	 global-step:14588	 l-p:0.12686583399772644
epoch£º729	 i:9 	 global-step:14589	 l-p:0.0019469261169433594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[3.7232, 3.0410, 2.4794],
        [3.7232, 3.2036, 2.5963],
        [3.7232, 3.0309, 2.5982],
        [3.7232, 3.0284, 2.5760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.1473807990550995 
model_pd.l_d.mean(): -22.75847625732422 
model_pd.lagr.mean(): -22.611095428466797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2928], device='cuda:0')), ('power', tensor([-23.0513], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.1473807990550995
epoch£º730	 i:1 	 global-step:14601	 l-p:0.11913122981786728
epoch£º730	 i:2 	 global-step:14602	 l-p:0.14819489419460297
epoch£º730	 i:3 	 global-step:14603	 l-p:0.14268991351127625
epoch£º730	 i:4 	 global-step:14604	 l-p:0.11590202897787094
epoch£º730	 i:5 	 global-step:14605	 l-p:0.11274154484272003
epoch£º730	 i:6 	 global-step:14606	 l-p:-0.11012164503335953
epoch£º730	 i:7 	 global-step:14607	 l-p:0.12489718943834305
epoch£º730	 i:8 	 global-step:14608	 l-p:0.14323516190052032
epoch£º730	 i:9 	 global-step:14609	 l-p:0.16190923750400543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6999, 3.2612, 2.6701],
        [3.6999, 3.3006, 3.3629],
        [3.6999, 3.0840, 2.8882],
        [3.6999, 3.6794, 3.6981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.10495731234550476 
model_pd.l_d.mean(): -23.298927307128906 
model_pd.lagr.mean(): -23.1939697265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2220], device='cuda:0')), ('power', tensor([-23.5209], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.10495731234550476
epoch£º731	 i:1 	 global-step:14621	 l-p:0.17211419343948364
epoch£º731	 i:2 	 global-step:14622	 l-p:0.132482647895813
epoch£º731	 i:3 	 global-step:14623	 l-p:0.18452177941799164
epoch£º731	 i:4 	 global-step:14624	 l-p:0.12405287474393845
epoch£º731	 i:5 	 global-step:14625	 l-p:0.1611892282962799
epoch£º731	 i:6 	 global-step:14626	 l-p:0.13867677748203278
epoch£º731	 i:7 	 global-step:14627	 l-p:0.11009540408849716
epoch£º731	 i:8 	 global-step:14628	 l-p:0.14791655540466309
epoch£º731	 i:9 	 global-step:14629	 l-p:0.1754782795906067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6695, 2.9865, 2.4071],
        [3.6695, 3.6651, 3.6694],
        [3.6695, 3.6696, 3.6696],
        [3.6695, 3.6626, 3.6692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.1262432485818863 
model_pd.l_d.mean(): -22.865402221679688 
model_pd.lagr.mean(): -22.739158630371094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2619], device='cuda:0')), ('power', tensor([-23.1273], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.1262432485818863
epoch£º732	 i:1 	 global-step:14641	 l-p:0.1386720985174179
epoch£º732	 i:2 	 global-step:14642	 l-p:0.11313571035861969
epoch£º732	 i:3 	 global-step:14643	 l-p:0.1456216424703598
epoch£º732	 i:4 	 global-step:14644	 l-p:0.12040510028600693
epoch£º732	 i:5 	 global-step:14645	 l-p:0.14093010127544403
epoch£º732	 i:6 	 global-step:14646	 l-p:0.345359742641449
epoch£º732	 i:7 	 global-step:14647	 l-p:0.33927732706069946
epoch£º732	 i:8 	 global-step:14648	 l-p:0.13023972511291504
epoch£º732	 i:9 	 global-step:14649	 l-p:0.11803790181875229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6405, 3.6405, 3.6405],
        [3.6405, 3.6178, 3.6383],
        [3.6405, 3.4844, 3.5817],
        [3.6405, 3.5772, 3.6284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.12717515230178833 
model_pd.l_d.mean(): -23.089319229125977 
model_pd.lagr.mean(): -22.96214485168457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2592], device='cuda:0')), ('power', tensor([-23.3485], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.12717515230178833
epoch£º733	 i:1 	 global-step:14661	 l-p:0.14638936519622803
epoch£º733	 i:2 	 global-step:14662	 l-p:0.15286047756671906
epoch£º733	 i:3 	 global-step:14663	 l-p:-0.24031274020671844
epoch£º733	 i:4 	 global-step:14664	 l-p:-0.4881245195865631
epoch£º733	 i:5 	 global-step:14665	 l-p:0.1434282809495926
epoch£º733	 i:6 	 global-step:14666	 l-p:0.23104672133922577
epoch£º733	 i:7 	 global-step:14667	 l-p:0.09277433156967163
epoch£º733	 i:8 	 global-step:14668	 l-p:0.15057091414928436
epoch£º733	 i:9 	 global-step:14669	 l-p:-0.015344209037721157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6157, 3.6134, 3.6157],
        [3.6157, 3.0099, 2.4036],
        [3.6157, 3.1540, 2.5664],
        [3.6157, 2.9224, 2.5606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.14562030136585236 
model_pd.l_d.mean(): -23.428936004638672 
model_pd.lagr.mean(): -23.283315658569336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2097], device='cuda:0')), ('power', tensor([-23.6386], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.14562030136585236
epoch£º734	 i:1 	 global-step:14681	 l-p:-4.005049228668213
epoch£º734	 i:2 	 global-step:14682	 l-p:0.16806304454803467
epoch£º734	 i:3 	 global-step:14683	 l-p:0.1457783281803131
epoch£º734	 i:4 	 global-step:14684	 l-p:1.9885972738265991
epoch£º734	 i:5 	 global-step:14685	 l-p:0.128223717212677
epoch£º734	 i:6 	 global-step:14686	 l-p:0.18564730882644653
epoch£º734	 i:7 	 global-step:14687	 l-p:0.07891714572906494
epoch£º734	 i:8 	 global-step:14688	 l-p:0.12904751300811768
epoch£º734	 i:9 	 global-step:14689	 l-p:0.1407172828912735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6634, 3.0896, 2.9755],
        [3.6634, 3.4964, 3.5970],
        [3.6634, 2.9652, 2.4055],
        [3.6634, 2.9572, 2.4248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.13298361003398895 
model_pd.l_d.mean(): -23.176162719726562 
model_pd.lagr.mean(): -23.04317855834961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2086], device='cuda:0')), ('power', tensor([-23.3847], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.13298361003398895
epoch£º735	 i:1 	 global-step:14701	 l-p:0.1408456712961197
epoch£º735	 i:2 	 global-step:14702	 l-p:0.2417311817407608
epoch£º735	 i:3 	 global-step:14703	 l-p:0.12452394515275955
epoch£º735	 i:4 	 global-step:14704	 l-p:0.14042414724826813
epoch£º735	 i:5 	 global-step:14705	 l-p:0.39400917291641235
epoch£º735	 i:6 	 global-step:14706	 l-p:0.1939660608768463
epoch£º735	 i:7 	 global-step:14707	 l-p:0.13437101244926453
epoch£º735	 i:8 	 global-step:14708	 l-p:-0.09261714667081833
epoch£º735	 i:9 	 global-step:14709	 l-p:0.15931543707847595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6098, 3.1524, 2.5661],
        [3.6098, 3.4777, 3.5663],
        [3.6098, 2.9733, 2.7599],
        [3.6098, 2.9821, 2.7861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.14465899765491486 
model_pd.l_d.mean(): -23.478565216064453 
model_pd.lagr.mean(): -23.333906173706055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2576], device='cuda:0')), ('power', tensor([-23.7362], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.14465899765491486
epoch£º736	 i:1 	 global-step:14721	 l-p:0.19305072724819183
epoch£º736	 i:2 	 global-step:14722	 l-p:0.15647448599338531
epoch£º736	 i:3 	 global-step:14723	 l-p:0.13584531843662262
epoch£º736	 i:4 	 global-step:14724	 l-p:0.21161584556102753
epoch£º736	 i:5 	 global-step:14725	 l-p:0.12768512964248657
epoch£º736	 i:6 	 global-step:14726	 l-p:0.1395639181137085
epoch£º736	 i:7 	 global-step:14727	 l-p:0.10919241607189178
epoch£º736	 i:8 	 global-step:14728	 l-p:0.07672719657421112
epoch£º736	 i:9 	 global-step:14729	 l-p:0.15046367049217224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5433, 3.4781, 3.5307],
        [3.5433, 3.1337, 2.5648],
        [3.5433, 2.8189, 2.2720],
        [3.5433, 3.0864, 2.5079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.1265646070241928 
model_pd.l_d.mean(): -23.599586486816406 
model_pd.lagr.mean(): -23.4730224609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1842], device='cuda:0')), ('power', tensor([-23.7838], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.1265646070241928
epoch£º737	 i:1 	 global-step:14741	 l-p:0.17783759534358978
epoch£º737	 i:2 	 global-step:14742	 l-p:-0.13093866407871246
epoch£º737	 i:3 	 global-step:14743	 l-p:0.09003248065710068
epoch£º737	 i:4 	 global-step:14744	 l-p:0.1430986374616623
epoch£º737	 i:5 	 global-step:14745	 l-p:0.1284245252609253
epoch£º737	 i:6 	 global-step:14746	 l-p:0.09582749754190445
epoch£º737	 i:7 	 global-step:14747	 l-p:0.12376078963279724
epoch£º737	 i:8 	 global-step:14748	 l-p:0.13025425374507904
epoch£º737	 i:9 	 global-step:14749	 l-p:0.41231855750083923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5869, 3.5635, 3.5847],
        [3.5869, 3.5822, 3.5868],
        [3.5869, 3.5856, 3.5869],
        [3.5869, 3.5869, 3.5870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.05123758316040039 
model_pd.l_d.mean(): -23.338037490844727 
model_pd.lagr.mean(): -23.286800384521484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2529], device='cuda:0')), ('power', tensor([-23.5910], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.05123758316040039
epoch£º738	 i:1 	 global-step:14761	 l-p:0.12667931616306305
epoch£º738	 i:2 	 global-step:14762	 l-p:0.14174003899097443
epoch£º738	 i:3 	 global-step:14763	 l-p:0.13490422070026398
epoch£º738	 i:4 	 global-step:14764	 l-p:0.17636041343212128
epoch£º738	 i:5 	 global-step:14765	 l-p:0.13766992092132568
epoch£º738	 i:6 	 global-step:14766	 l-p:0.16018004715442657
epoch£º738	 i:7 	 global-step:14767	 l-p:0.14744922518730164
epoch£º738	 i:8 	 global-step:14768	 l-p:0.1298704743385315
epoch£º738	 i:9 	 global-step:14769	 l-p:0.04002571478486061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6099, 3.6099, 3.6099],
        [3.6099, 3.6099, 3.6099],
        [3.6099, 2.9091, 2.3309],
        [3.6099, 3.3305, 3.4414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.13220885396003723 
model_pd.l_d.mean(): -23.023223876953125 
model_pd.lagr.mean(): -22.891014099121094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3001], device='cuda:0')), ('power', tensor([-23.3234], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.13220885396003723
epoch£º739	 i:1 	 global-step:14781	 l-p:0.12879541516304016
epoch£º739	 i:2 	 global-step:14782	 l-p:0.04779123142361641
epoch£º739	 i:3 	 global-step:14783	 l-p:0.07389761507511139
epoch£º739	 i:4 	 global-step:14784	 l-p:0.12613187730312347
epoch£º739	 i:5 	 global-step:14785	 l-p:0.14794550836086273
epoch£º739	 i:6 	 global-step:14786	 l-p:0.14711298048496246
epoch£º739	 i:7 	 global-step:14787	 l-p:0.133884996175766
epoch£º739	 i:8 	 global-step:14788	 l-p:0.1683536171913147
epoch£º739	 i:9 	 global-step:14789	 l-p:-1.2440325021743774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5801, 3.5801, 3.5801],
        [3.5801, 3.5586, 3.5781],
        [3.5801, 3.0651, 3.0398],
        [3.5801, 2.8793, 2.5185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.13113582134246826 
model_pd.l_d.mean(): -22.730323791503906 
model_pd.lagr.mean(): -22.59918785095215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3189], device='cuda:0')), ('power', tensor([-23.0492], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.13113582134246826
epoch£º740	 i:1 	 global-step:14801	 l-p:0.18350273370742798
epoch£º740	 i:2 	 global-step:14802	 l-p:0.09442232549190521
epoch£º740	 i:3 	 global-step:14803	 l-p:0.14401006698608398
epoch£º740	 i:4 	 global-step:14804	 l-p:0.1328364759683609
epoch£º740	 i:5 	 global-step:14805	 l-p:0.08395671844482422
epoch£º740	 i:6 	 global-step:14806	 l-p:0.14077778160572052
epoch£º740	 i:7 	 global-step:14807	 l-p:0.4862668812274933
epoch£º740	 i:8 	 global-step:14808	 l-p:0.08805685490369797
epoch£º740	 i:9 	 global-step:14809	 l-p:0.14978119730949402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6001, 2.8784, 2.4087],
        [3.6001, 3.1846, 2.6075],
        [3.6001, 2.8772, 2.3935],
        [3.6001, 3.6001, 3.6002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.10858741402626038 
model_pd.l_d.mean(): -23.391414642333984 
model_pd.lagr.mean(): -23.282827377319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2573], device='cuda:0')), ('power', tensor([-23.6488], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.10858741402626038
epoch£º741	 i:1 	 global-step:14821	 l-p:0.12048614025115967
epoch£º741	 i:2 	 global-step:14822	 l-p:0.006583189591765404
epoch£º741	 i:3 	 global-step:14823	 l-p:0.1413615643978119
epoch£º741	 i:4 	 global-step:14824	 l-p:0.1468777358531952
epoch£º741	 i:5 	 global-step:14825	 l-p:-0.06316737830638885
epoch£º741	 i:6 	 global-step:14826	 l-p:0.13386385142803192
epoch£º741	 i:7 	 global-step:14827	 l-p:0.13222432136535645
epoch£º741	 i:8 	 global-step:14828	 l-p:0.19299370050430298
epoch£º741	 i:9 	 global-step:14829	 l-p:0.14310050010681152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6328, 3.2291, 2.6509],
        [3.6328, 3.3693, 3.4816],
        [3.6328, 3.6251, 3.6324],
        [3.6328, 3.0654, 2.9696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.09862811118364334 
model_pd.l_d.mean(): -23.06740379333496 
model_pd.lagr.mean(): -22.968774795532227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3040], device='cuda:0')), ('power', tensor([-23.3715], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.09862811118364334
epoch£º742	 i:1 	 global-step:14841	 l-p:0.16097283363342285
epoch£º742	 i:2 	 global-step:14842	 l-p:1.3039555549621582
epoch£º742	 i:3 	 global-step:14843	 l-p:0.11384811252355576
epoch£º742	 i:4 	 global-step:14844	 l-p:0.16102777421474457
epoch£º742	 i:5 	 global-step:14845	 l-p:0.13316568732261658
epoch£º742	 i:6 	 global-step:14846	 l-p:0.13763505220413208
epoch£º742	 i:7 	 global-step:14847	 l-p:-0.0040586949326097965
epoch£º742	 i:8 	 global-step:14848	 l-p:0.1375790238380432
epoch£º742	 i:9 	 global-step:14849	 l-p:0.12618444859981537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5957, 3.5957, 3.5957],
        [3.5957, 3.4669, 3.5544],
        [3.5957, 2.8989, 2.3101],
        [3.5957, 2.8730, 2.4148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.05300083011388779 
model_pd.l_d.mean(): -23.099746704101562 
model_pd.lagr.mean(): -23.04674530029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2875], device='cuda:0')), ('power', tensor([-23.3873], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.05300083011388779
epoch£º743	 i:1 	 global-step:14861	 l-p:0.13117827475070953
epoch£º743	 i:2 	 global-step:14862	 l-p:0.0796734020113945
epoch£º743	 i:3 	 global-step:14863	 l-p:0.4218144416809082
epoch£º743	 i:4 	 global-step:14864	 l-p:0.12624505162239075
epoch£º743	 i:5 	 global-step:14865	 l-p:0.12770076096057892
epoch£º743	 i:6 	 global-step:14866	 l-p:0.15392322838306427
epoch£º743	 i:7 	 global-step:14867	 l-p:0.13617871701717377
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12465289980173111
epoch£º743	 i:9 	 global-step:14869	 l-p:0.14578332006931305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5731, 3.4012, 3.5042],
        [3.5731, 3.2872, 3.3988],
        [3.5731, 3.4043, 3.5064],
        [3.5731, 3.4982, 3.5571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.13344085216522217 
model_pd.l_d.mean(): -23.723834991455078 
model_pd.lagr.mean(): -23.590394973754883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1376], device='cuda:0')), ('power', tensor([-23.8614], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.13344085216522217
epoch£º744	 i:1 	 global-step:14881	 l-p:0.13357964158058167
epoch£º744	 i:2 	 global-step:14882	 l-p:0.07525332272052765
epoch£º744	 i:3 	 global-step:14883	 l-p:0.08544755727052689
epoch£º744	 i:4 	 global-step:14884	 l-p:0.17922578752040863
epoch£º744	 i:5 	 global-step:14885	 l-p:-0.0655389353632927
epoch£º744	 i:6 	 global-step:14886	 l-p:0.14960472285747528
epoch£º744	 i:7 	 global-step:14887	 l-p:0.13636019825935364
epoch£º744	 i:8 	 global-step:14888	 l-p:0.17069384455680847
epoch£º744	 i:9 	 global-step:14889	 l-p:0.18712753057479858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5086, 3.4439, 3.4962],
        [3.5086, 3.5085, 3.5086],
        [3.5086, 3.0826, 2.5133],
        [3.5086, 2.7679, 2.2545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.13075697422027588 
model_pd.l_d.mean(): -23.002517700195312 
model_pd.lagr.mean(): -22.871761322021484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3374], device='cuda:0')), ('power', tensor([-23.3399], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.13075697422027588
epoch£º745	 i:1 	 global-step:14901	 l-p:0.0828026533126831
epoch£º745	 i:2 	 global-step:14902	 l-p:0.1859111338853836
epoch£º745	 i:3 	 global-step:14903	 l-p:0.293675035238266
epoch£º745	 i:4 	 global-step:14904	 l-p:0.13459612429141998
epoch£º745	 i:5 	 global-step:14905	 l-p:0.11668646335601807
epoch£º745	 i:6 	 global-step:14906	 l-p:0.12079299986362457
epoch£º745	 i:7 	 global-step:14907	 l-p:0.16328951716423035
epoch£º745	 i:8 	 global-step:14908	 l-p:0.15457086265087128
epoch£º745	 i:9 	 global-step:14909	 l-p:0.3777599334716797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6475, 3.6350, 3.6467],
        [3.6475, 3.6292, 3.6460],
        [3.6475, 3.0056, 2.3951],
        [3.6475, 3.6475, 3.6475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.3622238039970398 
model_pd.l_d.mean(): -22.738521575927734 
model_pd.lagr.mean(): -22.376296997070312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3067], device='cuda:0')), ('power', tensor([-23.0452], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.3622238039970398
epoch£º746	 i:1 	 global-step:14921	 l-p:0.13942880928516388
epoch£º746	 i:2 	 global-step:14922	 l-p:0.12002316117286682
epoch£º746	 i:3 	 global-step:14923	 l-p:0.1501282900571823
epoch£º746	 i:4 	 global-step:14924	 l-p:0.11529236286878586
epoch£º746	 i:5 	 global-step:14925	 l-p:0.12445955723524094
epoch£º746	 i:6 	 global-step:14926	 l-p:0.14343813061714172
epoch£º746	 i:7 	 global-step:14927	 l-p:0.1498083919286728
epoch£º746	 i:8 	 global-step:14928	 l-p:0.15415096282958984
epoch£º746	 i:9 	 global-step:14929	 l-p:0.11521466821432114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7127, 3.6083, 3.6839],
        [3.7127, 3.6386, 3.6968],
        [3.7127, 3.0562, 2.4487],
        [3.7127, 3.0674, 2.4561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.14419354498386383 
model_pd.l_d.mean(): -23.141374588012695 
model_pd.lagr.mean(): -22.997180938720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2405], device='cuda:0')), ('power', tensor([-23.3819], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.14419354498386383
epoch£º747	 i:1 	 global-step:14941	 l-p:0.13576707243919373
epoch£º747	 i:2 	 global-step:14942	 l-p:0.0719272792339325
epoch£º747	 i:3 	 global-step:14943	 l-p:0.14646081626415253
epoch£º747	 i:4 	 global-step:14944	 l-p:0.1263345628976822
epoch£º747	 i:5 	 global-step:14945	 l-p:0.18259117007255554
epoch£º747	 i:6 	 global-step:14946	 l-p:0.13440151512622833
epoch£º747	 i:7 	 global-step:14947	 l-p:0.15972663462162018
epoch£º747	 i:8 	 global-step:14948	 l-p:0.12313652783632278
epoch£º747	 i:9 	 global-step:14949	 l-p:0.12969118356704712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6853, 3.2549, 3.3021],
        [3.6853, 3.2926, 2.7106],
        [3.6853, 3.4613, 3.5730],
        [3.6853, 3.2090, 3.2181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.18450114130973816 
model_pd.l_d.mean(): -23.59604835510254 
model_pd.lagr.mean(): -23.41154670715332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1536], device='cuda:0')), ('power', tensor([-23.7497], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.18450114130973816
epoch£º748	 i:1 	 global-step:14961	 l-p:0.1282399594783783
epoch£º748	 i:2 	 global-step:14962	 l-p:0.13898171484470367
epoch£º748	 i:3 	 global-step:14963	 l-p:0.1650717854499817
epoch£º748	 i:4 	 global-step:14964	 l-p:0.13744865357875824
epoch£º748	 i:5 	 global-step:14965	 l-p:0.32691287994384766
epoch£º748	 i:6 	 global-step:14966	 l-p:1.1381617784500122
epoch£º748	 i:7 	 global-step:14967	 l-p:0.12801653146743774
epoch£º748	 i:8 	 global-step:14968	 l-p:0.12040090560913086
epoch£º748	 i:9 	 global-step:14969	 l-p:0.18974092602729797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6245, 3.1840, 2.5975],
        [3.6245, 3.5803, 3.6180],
        [3.6245, 3.5722, 3.6159],
        [3.6245, 2.9377, 2.6200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 21.361629486083984 
model_pd.l_d.mean(): -23.21431541442871 
model_pd.lagr.mean(): -1.8526859283447266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2740], device='cuda:0')), ('power', tensor([-23.4883], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:21.361629486083984
epoch£º749	 i:1 	 global-step:14981	 l-p:0.1247907429933548
epoch£º749	 i:2 	 global-step:14982	 l-p:0.12547190487384796
epoch£º749	 i:3 	 global-step:14983	 l-p:0.1280067414045334
epoch£º749	 i:4 	 global-step:14984	 l-p:0.08887699246406555
epoch£º749	 i:5 	 global-step:14985	 l-p:0.262199729681015
epoch£º749	 i:6 	 global-step:14986	 l-p:0.14964812994003296
epoch£º749	 i:7 	 global-step:14987	 l-p:0.1376146376132965
epoch£º749	 i:8 	 global-step:14988	 l-p:0.139485165476799
epoch£º749	 i:9 	 global-step:14989	 l-p:0.08294439315795898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5711, 2.8472, 2.2792],
        [3.5711, 3.0852, 2.4961],
        [3.5711, 3.5628, 3.5707],
        [3.5711, 3.5608, 3.5705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.11874564737081528 
model_pd.l_d.mean(): -23.41661834716797 
model_pd.lagr.mean(): -23.29787254333496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2347], device='cuda:0')), ('power', tensor([-23.6514], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.11874564737081528
epoch£º750	 i:1 	 global-step:15001	 l-p:0.1647973656654358
epoch£º750	 i:2 	 global-step:15002	 l-p:0.17230582237243652
epoch£º750	 i:3 	 global-step:15003	 l-p:0.3508515954017639
epoch£º750	 i:4 	 global-step:15004	 l-p:-0.18858572840690613
epoch£º750	 i:5 	 global-step:15005	 l-p:0.13959074020385742
epoch£º750	 i:6 	 global-step:15006	 l-p:0.12969863414764404
epoch£º750	 i:7 	 global-step:15007	 l-p:0.12660901248455048
epoch£º750	 i:8 	 global-step:15008	 l-p:0.12366922199726105
epoch£º750	 i:9 	 global-step:15009	 l-p:0.13952414691448212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5112, 3.2757, 3.3908],
        [3.5112, 2.7835, 2.2074],
        [3.5112, 3.5113, 3.5112],
        [3.5112, 3.1306, 3.2169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.13420502841472626 
model_pd.l_d.mean(): -23.64104652404785 
model_pd.lagr.mean(): -23.5068416595459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2671], device='cuda:0')), ('power', tensor([-23.9081], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.13420502841472626
epoch£º751	 i:1 	 global-step:15021	 l-p:0.1339322030544281
epoch£º751	 i:2 	 global-step:15022	 l-p:0.16049782931804657
epoch£º751	 i:3 	 global-step:15023	 l-p:0.18395087122917175
epoch£º751	 i:4 	 global-step:15024	 l-p:0.05011441931128502
epoch£º751	 i:5 	 global-step:15025	 l-p:0.12146811932325363
epoch£º751	 i:6 	 global-step:15026	 l-p:0.15267862379550934
epoch£º751	 i:7 	 global-step:15027	 l-p:0.13515284657478333
epoch£º751	 i:8 	 global-step:15028	 l-p:0.14981284737586975
epoch£º751	 i:9 	 global-step:15029	 l-p:0.0776703804731369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5395, 3.5395, 3.5395],
        [3.5395, 3.4594, 3.5217],
        [3.5395, 3.2709, 3.3857],
        [3.5395, 3.5313, 3.5391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.5663350820541382 
model_pd.l_d.mean(): -22.859416961669922 
model_pd.lagr.mean(): -22.293081283569336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3340], device='cuda:0')), ('power', tensor([-23.1934], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.5663350820541382
epoch£º752	 i:1 	 global-step:15041	 l-p:-0.2479206770658493
epoch£º752	 i:2 	 global-step:15042	 l-p:0.048092152923345566
epoch£º752	 i:3 	 global-step:15043	 l-p:0.5771223306655884
epoch£º752	 i:4 	 global-step:15044	 l-p:0.14411026239395142
epoch£º752	 i:5 	 global-step:15045	 l-p:0.14697366952896118
epoch£º752	 i:6 	 global-step:15046	 l-p:0.10493072122335434
epoch£º752	 i:7 	 global-step:15047	 l-p:0.1300709992647171
epoch£º752	 i:8 	 global-step:15048	 l-p:0.12894871830940247
epoch£º752	 i:9 	 global-step:15049	 l-p:0.11308135092258453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6578, 3.6542, 3.6577],
        [3.6578, 3.6577, 3.6578],
        [3.6578, 3.3790, 3.4909],
        [3.6578, 3.1364, 3.1035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.13245229423046112 
model_pd.l_d.mean(): -23.551340103149414 
model_pd.lagr.mean(): -23.418888092041016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2179], device='cuda:0')), ('power', tensor([-23.7692], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.13245229423046112
epoch£º753	 i:1 	 global-step:15061	 l-p:0.13835562765598297
epoch£º753	 i:2 	 global-step:15062	 l-p:0.34709760546684265
epoch£º753	 i:3 	 global-step:15063	 l-p:-0.691502034664154
epoch£º753	 i:4 	 global-step:15064	 l-p:0.132644385099411
epoch£º753	 i:5 	 global-step:15065	 l-p:0.1840554028749466
epoch£º753	 i:6 	 global-step:15066	 l-p:0.134219229221344
epoch£º753	 i:7 	 global-step:15067	 l-p:-0.0021862697321921587
epoch£º753	 i:8 	 global-step:15068	 l-p:0.10768292844295502
epoch£º753	 i:9 	 global-step:15069	 l-p:0.12383485585451126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6110, 3.6007, 3.6104],
        [3.6110, 3.6033, 3.6107],
        [3.6110, 2.9865, 2.3751],
        [3.6110, 3.6107, 3.6110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.04313027113676071 
model_pd.l_d.mean(): -23.47322654724121 
model_pd.lagr.mean(): -23.430095672607422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1994], device='cuda:0')), ('power', tensor([-23.6726], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.04313027113676071
epoch£º754	 i:1 	 global-step:15081	 l-p:0.1532866507768631
epoch£º754	 i:2 	 global-step:15082	 l-p:0.03765812888741493
epoch£º754	 i:3 	 global-step:15083	 l-p:0.22502459585666656
epoch£º754	 i:4 	 global-step:15084	 l-p:0.15656648576259613
epoch£º754	 i:5 	 global-step:15085	 l-p:0.12036237120628357
epoch£º754	 i:6 	 global-step:15086	 l-p:0.12083157896995544
epoch£º754	 i:7 	 global-step:15087	 l-p:0.1338777393102646
epoch£º754	 i:8 	 global-step:15088	 l-p:0.1290823370218277
epoch£º754	 i:9 	 global-step:15089	 l-p:0.03631574660539627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5999, 3.2471, 3.3426],
        [3.5999, 3.5180, 3.5813],
        [3.5999, 2.8758, 2.3091],
        [3.5999, 3.0397, 2.9665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.055428799241781235 
model_pd.l_d.mean(): -23.219533920288086 
model_pd.lagr.mean(): -23.164104461669922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2447], device='cuda:0')), ('power', tensor([-23.4642], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.055428799241781235
epoch£º755	 i:1 	 global-step:15101	 l-p:0.3388899266719818
epoch£º755	 i:2 	 global-step:15102	 l-p:0.12843696773052216
epoch£º755	 i:3 	 global-step:15103	 l-p:0.04803391918540001
epoch£º755	 i:4 	 global-step:15104	 l-p:0.14009419083595276
epoch£º755	 i:5 	 global-step:15105	 l-p:0.17030660808086395
epoch£º755	 i:6 	 global-step:15106	 l-p:0.18615968525409698
epoch£º755	 i:7 	 global-step:15107	 l-p:0.12471902370452881
epoch£º755	 i:8 	 global-step:15108	 l-p:0.022013263776898384
epoch£º755	 i:9 	 global-step:15109	 l-p:0.14584237337112427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6237, 2.8966, 2.3524],
        [3.6237, 3.6237, 3.6237],
        [3.6237, 2.9503, 2.6755],
        [3.6237, 3.0500, 2.9559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.17147792875766754 
model_pd.l_d.mean(): -23.546607971191406 
model_pd.lagr.mean(): -23.37512969970703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1821], device='cuda:0')), ('power', tensor([-23.7287], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.17147792875766754
epoch£º756	 i:1 	 global-step:15121	 l-p:0.1399361491203308
epoch£º756	 i:2 	 global-step:15122	 l-p:0.1410321146249771
epoch£º756	 i:3 	 global-step:15123	 l-p:0.11692234128713608
epoch£º756	 i:4 	 global-step:15124	 l-p:0.1495029181241989
epoch£º756	 i:5 	 global-step:15125	 l-p:0.07977067679166794
epoch£º756	 i:6 	 global-step:15126	 l-p:0.3213581442832947
epoch£º756	 i:7 	 global-step:15127	 l-p:0.12750723958015442
epoch£º756	 i:8 	 global-step:15128	 l-p:0.0387767031788826
epoch£º756	 i:9 	 global-step:15129	 l-p:0.1421925574541092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5868, 2.8591, 2.4281],
        [3.5868, 2.8596, 2.2930],
        [3.5868, 3.3052, 3.4185],
        [3.5868, 3.5868, 3.5868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.13869613409042358 
model_pd.l_d.mean(): -22.3944149017334 
model_pd.lagr.mean(): -22.255718231201172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3208], device='cuda:0')), ('power', tensor([-22.7152], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.13869613409042358
epoch£º757	 i:1 	 global-step:15141	 l-p:0.14617642760276794
epoch£º757	 i:2 	 global-step:15142	 l-p:0.12445466965436935
epoch£º757	 i:3 	 global-step:15143	 l-p:0.12164147943258286
epoch£º757	 i:4 	 global-step:15144	 l-p:0.14458328485488892
epoch£º757	 i:5 	 global-step:15145	 l-p:0.13606685400009155
epoch£º757	 i:6 	 global-step:15146	 l-p:-0.02438386343419552
epoch£º757	 i:7 	 global-step:15147	 l-p:1.1708210706710815
epoch£º757	 i:8 	 global-step:15148	 l-p:0.07935337722301483
epoch£º757	 i:9 	 global-step:15149	 l-p:0.15141765773296356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5470, 3.1174, 3.1761],
        [3.5470, 3.1912, 3.2879],
        [3.5470, 3.4037, 3.4978],
        [3.5470, 2.9510, 2.8366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.3687790632247925 
model_pd.l_d.mean(): -23.662429809570312 
model_pd.lagr.mean(): -23.293651580810547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1943], device='cuda:0')), ('power', tensor([-23.8567], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.3687790632247925
epoch£º758	 i:1 	 global-step:15161	 l-p:0.1411098837852478
epoch£º758	 i:2 	 global-step:15162	 l-p:0.15675264596939087
epoch£º758	 i:3 	 global-step:15163	 l-p:-0.08063329756259918
epoch£º758	 i:4 	 global-step:15164	 l-p:0.17383085191249847
epoch£º758	 i:5 	 global-step:15165	 l-p:0.10930842906236649
epoch£º758	 i:6 	 global-step:15166	 l-p:0.11538130789995193
epoch£º758	 i:7 	 global-step:15167	 l-p:0.036575060337781906
epoch£º758	 i:8 	 global-step:15168	 l-p:0.2251506894826889
epoch£º758	 i:9 	 global-step:15169	 l-p:0.12844091653823853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5960, 3.5960, 3.5960],
        [3.5960, 3.5961, 3.5961],
        [3.5960, 2.8639, 2.3080],
        [3.5960, 3.5961, 3.5961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.19204513728618622 
model_pd.l_d.mean(): -23.0717830657959 
model_pd.lagr.mean(): -22.879737854003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2186], device='cuda:0')), ('power', tensor([-23.2904], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.19204513728618622
epoch£º759	 i:1 	 global-step:15181	 l-p:0.08526389300823212
epoch£º759	 i:2 	 global-step:15182	 l-p:0.010173005983233452
epoch£º759	 i:3 	 global-step:15183	 l-p:0.13957086205482483
epoch£º759	 i:4 	 global-step:15184	 l-p:0.11666753143072128
epoch£º759	 i:5 	 global-step:15185	 l-p:0.14920802414417267
epoch£º759	 i:6 	 global-step:15186	 l-p:0.17457175254821777
epoch£º759	 i:7 	 global-step:15187	 l-p:0.13070593774318695
epoch£º759	 i:8 	 global-step:15188	 l-p:0.14273010194301605
epoch£º759	 i:9 	 global-step:15189	 l-p:-0.11087820678949356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6409, 3.5776, 3.6290],
        [3.6409, 3.2164, 3.2742],
        [3.6409, 2.9712, 2.7073],
        [3.6409, 3.5024, 3.5943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 1.4964466094970703 
model_pd.l_d.mean(): -22.760610580444336 
model_pd.lagr.mean(): -21.264163970947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3299], device='cuda:0')), ('power', tensor([-23.0905], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:1.4964466094970703
epoch£º760	 i:1 	 global-step:15201	 l-p:0.24540187418460846
epoch£º760	 i:2 	 global-step:15202	 l-p:0.1369152069091797
epoch£º760	 i:3 	 global-step:15203	 l-p:0.1315218061208725
epoch£º760	 i:4 	 global-step:15204	 l-p:0.18720866739749908
epoch£º760	 i:5 	 global-step:15205	 l-p:0.1196426972746849
epoch£º760	 i:6 	 global-step:15206	 l-p:0.13820794224739075
epoch£º760	 i:7 	 global-step:15207	 l-p:0.1226729154586792
epoch£º760	 i:8 	 global-step:15208	 l-p:0.130510613322258
epoch£º760	 i:9 	 global-step:15209	 l-p:0.056233711540699005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7159, 3.6843, 3.7122],
        [3.7159, 3.0019, 2.5663],
        [3.7159, 3.2308, 3.2347],
        [3.7159, 3.0285, 2.4281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.16233038902282715 
model_pd.l_d.mean(): -22.254640579223633 
model_pd.lagr.mean(): -22.092309951782227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3604], device='cuda:0')), ('power', tensor([-22.6150], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.16233038902282715
epoch£º761	 i:1 	 global-step:15221	 l-p:0.16785532236099243
epoch£º761	 i:2 	 global-step:15222	 l-p:0.11312297731637955
epoch£º761	 i:3 	 global-step:15223	 l-p:0.1266058087348938
epoch£º761	 i:4 	 global-step:15224	 l-p:0.12013688683509827
epoch£º761	 i:5 	 global-step:15225	 l-p:0.12096526473760605
epoch£º761	 i:6 	 global-step:15226	 l-p:0.13381637632846832
epoch£º761	 i:7 	 global-step:15227	 l-p:0.19768355786800385
epoch£º761	 i:8 	 global-step:15228	 l-p:0.13201338052749634
epoch£º761	 i:9 	 global-step:15229	 l-p:0.11236198246479034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6682, 3.2221, 3.2634],
        [3.6682, 3.1850, 3.1947],
        [3.6682, 2.9663, 2.6085],
        [3.6682, 3.6682, 3.6682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.1251593828201294 
model_pd.l_d.mean(): -23.60008430480957 
model_pd.lagr.mean(): -23.474924087524414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1803], device='cuda:0')), ('power', tensor([-23.7804], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.1251593828201294
epoch£º762	 i:1 	 global-step:15241	 l-p:0.15357360243797302
epoch£º762	 i:2 	 global-step:15242	 l-p:0.15579086542129517
epoch£º762	 i:3 	 global-step:15243	 l-p:0.11071719974279404
epoch£º762	 i:4 	 global-step:15244	 l-p:0.12447723001241684
epoch£º762	 i:5 	 global-step:15245	 l-p:0.05954507738351822
epoch£º762	 i:6 	 global-step:15246	 l-p:0.41356775164604187
epoch£º762	 i:7 	 global-step:15247	 l-p:0.14279446005821228
epoch£º762	 i:8 	 global-step:15248	 l-p:0.15847642719745636
epoch£º762	 i:9 	 global-step:15249	 l-p:0.13331550359725952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5871, 2.9529, 2.7744],
        [3.5871, 3.5770, 3.5865],
        [3.5871, 3.5066, 3.5691],
        [3.5871, 3.0240, 2.4190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.08099618554115295 
model_pd.l_d.mean(): -22.83677864074707 
model_pd.lagr.mean(): -22.755783081054688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2823], device='cuda:0')), ('power', tensor([-23.1191], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.08099618554115295
epoch£º763	 i:1 	 global-step:15261	 l-p:0.14687636494636536
epoch£º763	 i:2 	 global-step:15262	 l-p:0.48033407330513
epoch£º763	 i:3 	 global-step:15263	 l-p:0.24576885998249054
epoch£º763	 i:4 	 global-step:15264	 l-p:0.13607528805732727
epoch£º763	 i:5 	 global-step:15265	 l-p:0.101035937666893
epoch£º763	 i:6 	 global-step:15266	 l-p:-0.010454540140926838
epoch£º763	 i:7 	 global-step:15267	 l-p:1.4640241861343384
epoch£º763	 i:8 	 global-step:15268	 l-p:0.13161522150039673
epoch£º763	 i:9 	 global-step:15269	 l-p:0.11864198744297028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6706, 3.5196, 3.6161],
        [3.6706, 3.6636, 3.6702],
        [3.6706, 2.9652, 2.5985],
        [3.6706, 3.1817, 3.1868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.2544041574001312 
model_pd.l_d.mean(): -23.647602081298828 
model_pd.lagr.mean(): -23.393198013305664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1676], device='cuda:0')), ('power', tensor([-23.8152], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.2544041574001312
epoch£º764	 i:1 	 global-step:15281	 l-p:0.14230577647686005
epoch£º764	 i:2 	 global-step:15282	 l-p:0.13191263377666473
epoch£º764	 i:3 	 global-step:15283	 l-p:0.16841331124305725
epoch£º764	 i:4 	 global-step:15284	 l-p:0.08021223545074463
epoch£º764	 i:5 	 global-step:15285	 l-p:0.15923918783664703
epoch£º764	 i:6 	 global-step:15286	 l-p:0.11138879507780075
epoch£º764	 i:7 	 global-step:15287	 l-p:0.13551561534404755
epoch£º764	 i:8 	 global-step:15288	 l-p:0.12169171869754791
epoch£º764	 i:9 	 global-step:15289	 l-p:0.12149029225111008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7490, 3.5374, 3.6483],
        [3.7490, 3.7379, 3.7484],
        [3.7490, 3.7488, 3.7490],
        [3.7490, 3.5067, 3.6199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.1444019228219986 
model_pd.l_d.mean(): -22.310832977294922 
model_pd.lagr.mean(): -22.166431427001953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3037], device='cuda:0')), ('power', tensor([-22.6145], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.1444019228219986
epoch£º765	 i:1 	 global-step:15301	 l-p:0.13150373101234436
epoch£º765	 i:2 	 global-step:15302	 l-p:0.137047678232193
epoch£º765	 i:3 	 global-step:15303	 l-p:0.11273343861103058
epoch£º765	 i:4 	 global-step:15304	 l-p:0.14217638969421387
epoch£º765	 i:5 	 global-step:15305	 l-p:0.1256127655506134
epoch£º765	 i:6 	 global-step:15306	 l-p:0.18906265497207642
epoch£º765	 i:7 	 global-step:15307	 l-p:0.12056495249271393
epoch£º765	 i:8 	 global-step:15308	 l-p:0.1645849645137787
epoch£º765	 i:9 	 global-step:15309	 l-p:0.5739012956619263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6430, 3.1815, 2.5864],
        [3.6430, 3.0239, 2.8642],
        [3.6430, 3.5498, 3.6198],
        [3.6430, 3.0199, 2.4026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.12465625256299973 
model_pd.l_d.mean(): -23.506072998046875 
model_pd.lagr.mean(): -23.38141632080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2031], device='cuda:0')), ('power', tensor([-23.7092], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.12465625256299973
epoch£º766	 i:1 	 global-step:15321	 l-p:0.12184958904981613
epoch£º766	 i:2 	 global-step:15322	 l-p:0.1445809006690979
epoch£º766	 i:3 	 global-step:15323	 l-p:0.22779345512390137
epoch£º766	 i:4 	 global-step:15324	 l-p:0.10684207081794739
epoch£º766	 i:5 	 global-step:15325	 l-p:0.12676285207271576
epoch£º766	 i:6 	 global-step:15326	 l-p:0.05941237136721611
epoch£º766	 i:7 	 global-step:15327	 l-p:0.12404797971248627
epoch£º766	 i:8 	 global-step:15328	 l-p:-0.004167546983808279
epoch£º766	 i:9 	 global-step:15329	 l-p:0.13847097754478455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5666, 3.4409, 3.5277],
        [3.5666, 3.5665, 3.5666],
        [3.5666, 3.5666, 3.5666],
        [3.5666, 3.0138, 2.4124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.1222943663597107 
model_pd.l_d.mean(): -23.68627166748047 
model_pd.lagr.mean(): -23.56397819519043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1801], device='cuda:0')), ('power', tensor([-23.8664], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.1222943663597107
epoch£º767	 i:1 	 global-step:15341	 l-p:0.05785346403717995
epoch£º767	 i:2 	 global-step:15342	 l-p:0.1222614198923111
epoch£º767	 i:3 	 global-step:15343	 l-p:0.06128142774105072
epoch£º767	 i:4 	 global-step:15344	 l-p:0.14254191517829895
epoch£º767	 i:5 	 global-step:15345	 l-p:0.1493629515171051
epoch£º767	 i:6 	 global-step:15346	 l-p:0.202511265873909
epoch£º767	 i:7 	 global-step:15347	 l-p:0.1795017272233963
epoch£º767	 i:8 	 global-step:15348	 l-p:0.13190646469593048
epoch£º767	 i:9 	 global-step:15349	 l-p:-0.0028919123578816652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5287, 2.8899, 2.7168],
        [3.5287, 3.1392, 2.5724],
        [3.5287, 3.4344, 3.5053],
        [3.5287, 2.7927, 2.2109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.1674201488494873 
model_pd.l_d.mean(): -22.929977416992188 
model_pd.lagr.mean(): -22.762557983398438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3146], device='cuda:0')), ('power', tensor([-23.2446], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.1674201488494873
epoch£º768	 i:1 	 global-step:15361	 l-p:0.1310102492570877
epoch£º768	 i:2 	 global-step:15362	 l-p:0.11428763717412949
epoch£º768	 i:3 	 global-step:15363	 l-p:0.08176366984844208
epoch£º768	 i:4 	 global-step:15364	 l-p:0.0977657362818718
epoch£º768	 i:5 	 global-step:15365	 l-p:0.21780219674110413
epoch£º768	 i:6 	 global-step:15366	 l-p:0.133392795920372
epoch£º768	 i:7 	 global-step:15367	 l-p:0.16106447577476501
epoch£º768	 i:8 	 global-step:15368	 l-p:0.13135699927806854
epoch£º768	 i:9 	 global-step:15369	 l-p:0.10887300968170166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6375, 2.9520, 2.6594],
        [3.6375, 2.9417, 2.3359],
        [3.6375, 2.9377, 2.3340],
        [3.6375, 3.6110, 3.6348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.12591953575611115 
model_pd.l_d.mean(): -23.33701515197754 
model_pd.lagr.mean(): -23.211095809936523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2336], device='cuda:0')), ('power', tensor([-23.5706], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.12591953575611115
epoch£º769	 i:1 	 global-step:15381	 l-p:0.14542153477668762
epoch£º769	 i:2 	 global-step:15382	 l-p:-0.06357204914093018
epoch£º769	 i:3 	 global-step:15383	 l-p:0.14767387509346008
epoch£º769	 i:4 	 global-step:15384	 l-p:0.11117976903915405
epoch£º769	 i:5 	 global-step:15385	 l-p:0.2755831182003021
epoch£º769	 i:6 	 global-step:15386	 l-p:0.05812462791800499
epoch£º769	 i:7 	 global-step:15387	 l-p:0.16662277281284332
epoch£º769	 i:8 	 global-step:15388	 l-p:0.14240366220474243
epoch£º769	 i:9 	 global-step:15389	 l-p:0.12367045879364014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6523, 3.6523, 3.6523],
        [3.6523, 2.9478, 2.3480],
        [3.6523, 2.9507, 2.6104],
        [3.6523, 3.4181, 3.5326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.7509968876838684 
model_pd.l_d.mean(): -22.172143936157227 
model_pd.lagr.mean(): -21.421146392822266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3691], device='cuda:0')), ('power', tensor([-22.5412], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:0.7509968876838684
epoch£º770	 i:1 	 global-step:15401	 l-p:0.1310492604970932
epoch£º770	 i:2 	 global-step:15402	 l-p:0.1560286581516266
epoch£º770	 i:3 	 global-step:15403	 l-p:0.12277276813983917
epoch£º770	 i:4 	 global-step:15404	 l-p:0.2467673271894455
epoch£º770	 i:5 	 global-step:15405	 l-p:0.09760728478431702
epoch£º770	 i:6 	 global-step:15406	 l-p:0.19040383398532867
epoch£º770	 i:7 	 global-step:15407	 l-p:0.13523130118846893
epoch£º770	 i:8 	 global-step:15408	 l-p:0.09875839203596115
epoch£º770	 i:9 	 global-step:15409	 l-p:0.16390003263950348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6986, 3.6984, 3.6986],
        [3.6986, 3.6166, 3.6800],
        [3.6986, 3.6900, 3.6981],
        [3.6986, 3.0324, 2.7714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.11749084293842316 
model_pd.l_d.mean(): -23.591022491455078 
model_pd.lagr.mean(): -23.47353172302246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1081], device='cuda:0')), ('power', tensor([-23.6991], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.11749084293842316
epoch£º771	 i:1 	 global-step:15421	 l-p:0.07318374514579773
epoch£º771	 i:2 	 global-step:15422	 l-p:0.18765081465244293
epoch£º771	 i:3 	 global-step:15423	 l-p:0.12243466824293137
epoch£º771	 i:4 	 global-step:15424	 l-p:0.18120503425598145
epoch£º771	 i:5 	 global-step:15425	 l-p:0.14691263437271118
epoch£º771	 i:6 	 global-step:15426	 l-p:0.1803206205368042
epoch£º771	 i:7 	 global-step:15427	 l-p:0.12621520459651947
epoch£º771	 i:8 	 global-step:15428	 l-p:0.1986488699913025
epoch£º771	 i:9 	 global-step:15429	 l-p:0.12575288116931915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6904, 3.6111, 3.6728],
        [3.6904, 3.5479, 3.6414],
        [3.6904, 3.2219, 2.6195],
        [3.6904, 3.6083, 3.6718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.14189936220645905 
model_pd.l_d.mean(): -23.62311553955078 
model_pd.lagr.mean(): -23.481216430664062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1406], device='cuda:0')), ('power', tensor([-23.7637], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.14189936220645905
epoch£º772	 i:1 	 global-step:15441	 l-p:0.10052981972694397
epoch£º772	 i:2 	 global-step:15442	 l-p:0.16075605154037476
epoch£º772	 i:3 	 global-step:15443	 l-p:0.13272956013679504
epoch£º772	 i:4 	 global-step:15444	 l-p:0.1397065967321396
epoch£º772	 i:5 	 global-step:15445	 l-p:0.1796969175338745
epoch£º772	 i:6 	 global-step:15446	 l-p:0.11055947095155716
epoch£º772	 i:7 	 global-step:15447	 l-p:0.1390446424484253
epoch£º772	 i:8 	 global-step:15448	 l-p:0.4886021316051483
epoch£º772	 i:9 	 global-step:15449	 l-p:0.1306208074092865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6493, 3.6493, 3.6493],
        [3.6493, 3.5771, 3.6344],
        [3.6493, 3.0855, 3.0110],
        [3.6493, 3.0991, 3.0419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.13489845395088196 
model_pd.l_d.mean(): -23.79220199584961 
model_pd.lagr.mean(): -23.657302856445312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1213], device='cuda:0')), ('power', tensor([-23.9135], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.13489845395088196
epoch£º773	 i:1 	 global-step:15461	 l-p:0.129627525806427
epoch£º773	 i:2 	 global-step:15462	 l-p:0.15309321880340576
epoch£º773	 i:3 	 global-step:15463	 l-p:0.13456527888774872
epoch£º773	 i:4 	 global-step:15464	 l-p:0.14557833969593048
epoch£º773	 i:5 	 global-step:15465	 l-p:0.1521972268819809
epoch£º773	 i:6 	 global-step:15466	 l-p:0.1504223644733429
epoch£º773	 i:7 	 global-step:15467	 l-p:0.13864390552043915
epoch£º773	 i:8 	 global-step:15468	 l-p:0.12324311584234238
epoch£º773	 i:9 	 global-step:15469	 l-p:0.10616129636764526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5724, 3.3707, 3.4822],
        [3.5724, 3.4941, 3.5554],
        [3.5724, 2.9436, 2.7840],
        [3.5724, 3.5354, 3.5676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.04182984307408333 
model_pd.l_d.mean(): -23.673175811767578 
model_pd.lagr.mean(): -23.631345748901367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2122], device='cuda:0')), ('power', tensor([-23.8854], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.04182984307408333
epoch£º774	 i:1 	 global-step:15481	 l-p:0.11976403743028641
epoch£º774	 i:2 	 global-step:15482	 l-p:0.1380699872970581
epoch£º774	 i:3 	 global-step:15483	 l-p:0.2013993263244629
epoch£º774	 i:4 	 global-step:15484	 l-p:0.14671607315540314
epoch£º774	 i:5 	 global-step:15485	 l-p:0.05492199957370758
epoch£º774	 i:6 	 global-step:15486	 l-p:0.1483989953994751
epoch£º774	 i:7 	 global-step:15487	 l-p:0.2684744894504547
epoch£º774	 i:8 	 global-step:15488	 l-p:0.1213013157248497
epoch£º774	 i:9 	 global-step:15489	 l-p:0.1253315657377243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6031, 3.0134, 2.9109],
        [3.6031, 3.0922, 3.0848],
        [3.6031, 3.6030, 3.6031],
        [3.6031, 2.9660, 2.3509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.1338670402765274 
model_pd.l_d.mean(): -22.467321395874023 
model_pd.lagr.mean(): -22.333454132080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4247], device='cuda:0')), ('power', tensor([-22.8920], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.1338670402765274
epoch£º775	 i:1 	 global-step:15501	 l-p:0.15107327699661255
epoch£º775	 i:2 	 global-step:15502	 l-p:0.05768122896552086
epoch£º775	 i:3 	 global-step:15503	 l-p:0.18055064976215363
epoch£º775	 i:4 	 global-step:15504	 l-p:-0.2660338282585144
epoch£º775	 i:5 	 global-step:15505	 l-p:0.10800811648368835
epoch£º775	 i:6 	 global-step:15506	 l-p:0.5209111571311951
epoch£º775	 i:7 	 global-step:15507	 l-p:0.13975365459918976
epoch£º775	 i:8 	 global-step:15508	 l-p:0.13832862675189972
epoch£º775	 i:9 	 global-step:15509	 l-p:0.14015428721904755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[3.6944, 3.3169, 3.4025],
        [3.6944, 3.1665, 3.1330],
        [3.6944, 3.1894, 3.1806],
        [3.6944, 3.2431, 2.6433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.1917419135570526 
model_pd.l_d.mean(): -23.57530975341797 
model_pd.lagr.mean(): -23.383567810058594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1926], device='cuda:0')), ('power', tensor([-23.7679], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.1917419135570526
epoch£º776	 i:1 	 global-step:15521	 l-p:0.12115687876939774
epoch£º776	 i:2 	 global-step:15522	 l-p:0.12638042867183685
epoch£º776	 i:3 	 global-step:15523	 l-p:0.12765716016292572
epoch£º776	 i:4 	 global-step:15524	 l-p:0.11977533251047134
epoch£º776	 i:5 	 global-step:15525	 l-p:0.17168888449668884
epoch£º776	 i:6 	 global-step:15526	 l-p:0.12670232355594635
epoch£º776	 i:7 	 global-step:15527	 l-p:0.1583702117204666
epoch£º776	 i:8 	 global-step:15528	 l-p:0.12922412157058716
epoch£º776	 i:9 	 global-step:15529	 l-p:0.18478743731975555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6802, 3.1977, 3.2122],
        [3.6802, 3.5133, 3.6155],
        [3.6802, 3.1583, 3.1332],
        [3.6802, 3.0342, 2.8265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.1138719990849495 
model_pd.l_d.mean(): -23.534433364868164 
model_pd.lagr.mean(): -23.420560836791992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1739], device='cuda:0')), ('power', tensor([-23.7083], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.1138719990849495
epoch£º777	 i:1 	 global-step:15541	 l-p:0.13553236424922943
epoch£º777	 i:2 	 global-step:15542	 l-p:0.1256483942270279
epoch£º777	 i:3 	 global-step:15543	 l-p:0.13691970705986023
epoch£º777	 i:4 	 global-step:15544	 l-p:0.1469617635011673
epoch£º777	 i:5 	 global-step:15545	 l-p:3.0462772846221924
epoch£º777	 i:6 	 global-step:15546	 l-p:-0.04095993563532829
epoch£º777	 i:7 	 global-step:15547	 l-p:0.14688575267791748
epoch£º777	 i:8 	 global-step:15548	 l-p:0.1128782257437706
epoch£º777	 i:9 	 global-step:15549	 l-p:0.13852304220199585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6224, 3.6206, 3.6223],
        [3.6224, 3.4673, 3.5660],
        [3.6224, 3.5902, 3.6186],
        [3.6224, 3.6096, 3.6215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.24217136204242706 
model_pd.l_d.mean(): -22.59419822692871 
model_pd.lagr.mean(): -22.352025985717773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3782], device='cuda:0')), ('power', tensor([-22.9724], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.24217136204242706
epoch£º778	 i:1 	 global-step:15561	 l-p:0.14101217687129974
epoch£º778	 i:2 	 global-step:15562	 l-p:-0.1611296385526657
epoch£º778	 i:3 	 global-step:15563	 l-p:0.1447317749261856
epoch£º778	 i:4 	 global-step:15564	 l-p:0.17794939875602722
epoch£º778	 i:5 	 global-step:15565	 l-p:0.14657281339168549
epoch£º778	 i:6 	 global-step:15566	 l-p:0.14555364847183228
epoch£º778	 i:7 	 global-step:15567	 l-p:0.13533712923526764
epoch£º778	 i:8 	 global-step:15568	 l-p:0.11893640458583832
epoch£º778	 i:9 	 global-step:15569	 l-p:0.14791087806224823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6006, 3.2635, 3.3685],
        [3.6006, 2.9133, 2.6391],
        [3.6006, 3.6003, 3.6006],
        [3.6006, 3.4450, 3.5440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.03129558637738228 
model_pd.l_d.mean(): -23.47301483154297 
model_pd.lagr.mean(): -23.44171905517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2120], device='cuda:0')), ('power', tensor([-23.6850], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.03129558637738228
epoch£º779	 i:1 	 global-step:15581	 l-p:0.14589384198188782
epoch£º779	 i:2 	 global-step:15582	 l-p:0.13412359356880188
epoch£º779	 i:3 	 global-step:15583	 l-p:0.12583667039871216
epoch£º779	 i:4 	 global-step:15584	 l-p:0.15383031964302063
epoch£º779	 i:5 	 global-step:15585	 l-p:0.35540375113487244
epoch£º779	 i:6 	 global-step:15586	 l-p:0.13549162447452545
epoch£º779	 i:7 	 global-step:15587	 l-p:0.12000089138746262
epoch£º779	 i:8 	 global-step:15588	 l-p:0.15668314695358276
epoch£º779	 i:9 	 global-step:15589	 l-p:0.127768412232399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5306, 3.5206, 3.5300],
        [3.5306, 3.4951, 3.5262],
        [3.5306, 2.9069, 2.3005],
        [3.5306, 2.9027, 2.2960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.1744866967201233 
model_pd.l_d.mean(): -22.401262283325195 
model_pd.lagr.mean(): -22.226776123046875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3881], device='cuda:0')), ('power', tensor([-22.7894], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.1744866967201233
epoch£º780	 i:1 	 global-step:15601	 l-p:-0.17765414714813232
epoch£º780	 i:2 	 global-step:15602	 l-p:0.14764904975891113
epoch£º780	 i:3 	 global-step:15603	 l-p:0.09844549000263214
epoch£º780	 i:4 	 global-step:15604	 l-p:0.15962769091129303
epoch£º780	 i:5 	 global-step:15605	 l-p:0.10292012989521027
epoch£º780	 i:6 	 global-step:15606	 l-p:0.1639862358570099
epoch£º780	 i:7 	 global-step:15607	 l-p:-0.581963062286377
epoch£º780	 i:8 	 global-step:15608	 l-p:0.1791778951883316
epoch£º780	 i:9 	 global-step:15609	 l-p:0.1332380622625351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6469, 3.0490, 2.4298],
        [3.6469, 3.6470, 3.6470],
        [3.6469, 3.6356, 3.6463],
        [3.6469, 3.6419, 3.6468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.15094852447509766 
model_pd.l_d.mean(): -23.50318145751953 
model_pd.lagr.mean(): -23.35223388671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2236], device='cuda:0')), ('power', tensor([-23.7268], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.15094852447509766
epoch£º781	 i:1 	 global-step:15621	 l-p:0.332157701253891
epoch£º781	 i:2 	 global-step:15622	 l-p:0.36562639474868774
epoch£º781	 i:3 	 global-step:15623	 l-p:0.15912318229675293
epoch£º781	 i:4 	 global-step:15624	 l-p:0.18250533938407898
epoch£º781	 i:5 	 global-step:15625	 l-p:0.07435455918312073
epoch£º781	 i:6 	 global-step:15626	 l-p:0.1040966585278511
epoch£º781	 i:7 	 global-step:15627	 l-p:0.14234058558940887
epoch£º781	 i:8 	 global-step:15628	 l-p:0.12019781023263931
epoch£º781	 i:9 	 global-step:15629	 l-p:0.11273524910211563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7763, 3.7421, 3.7721],
        [3.7763, 3.1437, 2.5139],
        [3.7763, 3.6518, 3.7377],
        [3.7763, 3.6440, 3.7333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.1462690234184265 
model_pd.l_d.mean(): -23.198259353637695 
model_pd.lagr.mean(): -23.051990509033203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1178], device='cuda:0')), ('power', tensor([-23.3161], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.1462690234184265
epoch£º782	 i:1 	 global-step:15641	 l-p:0.9890837669372559
epoch£º782	 i:2 	 global-step:15642	 l-p:0.11530844122171402
epoch£º782	 i:3 	 global-step:15643	 l-p:0.12086746096611023
epoch£º782	 i:4 	 global-step:15644	 l-p:0.1095060333609581
epoch£º782	 i:5 	 global-step:15645	 l-p:0.13479410111904144
epoch£º782	 i:6 	 global-step:15646	 l-p:0.13700857758522034
epoch£º782	 i:7 	 global-step:15647	 l-p:0.12400031834840775
epoch£º782	 i:8 	 global-step:15648	 l-p:0.21960541605949402
epoch£º782	 i:9 	 global-step:15649	 l-p:0.13542257249355316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6873, 2.9830, 2.3749],
        [3.6873, 3.1512, 3.1128],
        [3.6873, 2.9684, 2.3748],
        [3.6873, 3.6018, 3.6675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.13757093250751495 
model_pd.l_d.mean(): -23.562976837158203 
model_pd.lagr.mean(): -23.425405502319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1412], device='cuda:0')), ('power', tensor([-23.7042], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.13757093250751495
epoch£º783	 i:1 	 global-step:15661	 l-p:0.17568349838256836
epoch£º783	 i:2 	 global-step:15662	 l-p:0.25655922293663025
epoch£º783	 i:3 	 global-step:15663	 l-p:0.4506056010723114
epoch£º783	 i:4 	 global-step:15664	 l-p:0.12119802087545395
epoch£º783	 i:5 	 global-step:15665	 l-p:-15.837797164916992
epoch£º783	 i:6 	 global-step:15666	 l-p:0.16788683831691742
epoch£º783	 i:7 	 global-step:15667	 l-p:0.1328611671924591
epoch£º783	 i:8 	 global-step:15668	 l-p:0.14031416177749634
epoch£º783	 i:9 	 global-step:15669	 l-p:0.1457529217004776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6668, 2.9570, 2.3506],
        [3.6668, 3.6668, 3.6668],
        [3.6668, 3.4622, 3.5739],
        [3.6668, 3.4450, 3.5593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.1464284360408783 
model_pd.l_d.mean(): -23.298988342285156 
model_pd.lagr.mean(): -23.152559280395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2584], device='cuda:0')), ('power', tensor([-23.5573], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.1464284360408783
epoch£º784	 i:1 	 global-step:15681	 l-p:0.1293039470911026
epoch£º784	 i:2 	 global-step:15682	 l-p:0.1420830637216568
epoch£º784	 i:3 	 global-step:15683	 l-p:0.1473788619041443
epoch£º784	 i:4 	 global-step:15684	 l-p:0.1526218056678772
epoch£º784	 i:5 	 global-step:15685	 l-p:0.1344965398311615
epoch£º784	 i:6 	 global-step:15686	 l-p:0.36614805459976196
epoch£º784	 i:7 	 global-step:15687	 l-p:0.2678752541542053
epoch£º784	 i:8 	 global-step:15688	 l-p:0.18396270275115967
epoch£º784	 i:9 	 global-step:15689	 l-p:0.11016868054866791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6991, 3.6991, 3.6991],
        [3.6991, 3.6908, 3.6987],
        [3.6991, 2.9729, 2.5552],
        [3.6991, 3.3389, 3.4339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.11250825971364975 
model_pd.l_d.mean(): -23.12660026550293 
model_pd.lagr.mean(): -23.01409149169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1774], device='cuda:0')), ('power', tensor([-23.3040], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.11250825971364975
epoch£º785	 i:1 	 global-step:15701	 l-p:0.186579167842865
epoch£º785	 i:2 	 global-step:15702	 l-p:0.13074269890785217
epoch£º785	 i:3 	 global-step:15703	 l-p:0.09928400814533234
epoch£º785	 i:4 	 global-step:15704	 l-p:0.1485324203968048
epoch£º785	 i:5 	 global-step:15705	 l-p:0.136233389377594
epoch£º785	 i:6 	 global-step:15706	 l-p:0.13953633606433868
epoch£º785	 i:7 	 global-step:15707	 l-p:0.1289086639881134
epoch£º785	 i:8 	 global-step:15708	 l-p:0.14330080151557922
epoch£º785	 i:9 	 global-step:15709	 l-p:0.13227830827236176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7244, 3.7244, 3.7244],
        [3.7244, 3.3154, 2.7201],
        [3.7244, 3.0418, 2.4212],
        [3.7244, 3.5988, 3.6855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.16565285623073578 
model_pd.l_d.mean(): -23.56924057006836 
model_pd.lagr.mean(): -23.403587341308594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1270], device='cuda:0')), ('power', tensor([-23.6962], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.16565285623073578
epoch£º786	 i:1 	 global-step:15721	 l-p:0.13020122051239014
epoch£º786	 i:2 	 global-step:15722	 l-p:0.15067365765571594
epoch£º786	 i:3 	 global-step:15723	 l-p:0.12934774160385132
epoch£º786	 i:4 	 global-step:15724	 l-p:0.2662668526172638
epoch£º786	 i:5 	 global-step:15725	 l-p:0.17346763610839844
epoch£º786	 i:6 	 global-step:15726	 l-p:0.11982858180999756
epoch£º786	 i:7 	 global-step:15727	 l-p:0.12956593930721283
epoch£º786	 i:8 	 global-step:15728	 l-p:0.0984852984547615
epoch£º786	 i:9 	 global-step:15729	 l-p:0.13457243144512177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6250, 2.9009, 2.2978],
        [3.6250, 3.5387, 3.6050],
        [3.6250, 3.5901, 3.6207],
        [3.6250, 3.4667, 3.5669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.17227782309055328 
model_pd.l_d.mean(): -22.829845428466797 
model_pd.lagr.mean(): -22.657567977905273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3378], device='cuda:0')), ('power', tensor([-23.1676], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.17227782309055328
epoch£º787	 i:1 	 global-step:15741	 l-p:0.04415188729763031
epoch£º787	 i:2 	 global-step:15742	 l-p:0.13744889199733734
epoch£º787	 i:3 	 global-step:15743	 l-p:0.06859876215457916
epoch£º787	 i:4 	 global-step:15744	 l-p:0.0025329780764877796
epoch£º787	 i:5 	 global-step:15745	 l-p:0.14615631103515625
epoch£º787	 i:6 	 global-step:15746	 l-p:0.12026756256818771
epoch£º787	 i:7 	 global-step:15747	 l-p:0.11363101750612259
epoch£º787	 i:8 	 global-step:15748	 l-p:0.158293679356575
epoch£º787	 i:9 	 global-step:15749	 l-p:0.132259339094162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5657, 2.9708, 2.8757],
        [3.5657, 3.5657, 3.5657],
        [3.5657, 2.9307, 2.7750],
        [3.5657, 2.9053, 2.2907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.13810531795024872 
model_pd.l_d.mean(): -23.60535430908203 
model_pd.lagr.mean(): -23.467248916625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1809], device='cuda:0')), ('power', tensor([-23.7863], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.13810531795024872
epoch£º788	 i:1 	 global-step:15761	 l-p:0.13957570493221283
epoch£º788	 i:2 	 global-step:15762	 l-p:0.07835788279771805
epoch£º788	 i:3 	 global-step:15763	 l-p:0.10272899270057678
epoch£º788	 i:4 	 global-step:15764	 l-p:0.18037743866443634
epoch£º788	 i:5 	 global-step:15765	 l-p:0.011915847659111023
epoch£º788	 i:6 	 global-step:15766	 l-p:0.10454920679330826
epoch£º788	 i:7 	 global-step:15767	 l-p:0.12891001999378204
epoch£º788	 i:8 	 global-step:15768	 l-p:0.15094806253910065
epoch£º788	 i:9 	 global-step:15769	 l-p:0.14006349444389343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5612, 3.4716, 3.5400],
        [3.5612, 3.0018, 2.9536],
        [3.5612, 3.5612, 3.5612],
        [3.5612, 3.5612, 3.5612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.1006230041384697 
model_pd.l_d.mean(): -23.43198013305664 
model_pd.lagr.mean(): -23.331357955932617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2321], device='cuda:0')), ('power', tensor([-23.6640], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.1006230041384697
epoch£º789	 i:1 	 global-step:15781	 l-p:0.12175758183002472
epoch£º789	 i:2 	 global-step:15782	 l-p:0.08755029737949371
epoch£º789	 i:3 	 global-step:15783	 l-p:0.1081809401512146
epoch£º789	 i:4 	 global-step:15784	 l-p:0.14839763939380646
epoch£º789	 i:5 	 global-step:15785	 l-p:0.25496119260787964
epoch£º789	 i:6 	 global-step:15786	 l-p:0.11583777517080307
epoch£º789	 i:7 	 global-step:15787	 l-p:0.10753492265939713
epoch£º789	 i:8 	 global-step:15788	 l-p:0.13767610490322113
epoch£º789	 i:9 	 global-step:15789	 l-p:0.1862512230873108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6191, 3.4743, 3.5697],
        [3.6191, 3.6191, 3.6191],
        [3.6191, 2.8991, 2.2903],
        [3.6191, 3.0637, 3.0147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.3211575150489807 
model_pd.l_d.mean(): -23.75040054321289 
model_pd.lagr.mean(): -23.429243087768555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1562], device='cuda:0')), ('power', tensor([-23.9066], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.3211575150489807
epoch£º790	 i:1 	 global-step:15801	 l-p:0.16432784497737885
epoch£º790	 i:2 	 global-step:15802	 l-p:0.13298790156841278
epoch£º790	 i:3 	 global-step:15803	 l-p:-0.006364357192069292
epoch£º790	 i:4 	 global-step:15804	 l-p:0.09096245467662811
epoch£º790	 i:5 	 global-step:15805	 l-p:0.1280839443206787
epoch£º790	 i:6 	 global-step:15806	 l-p:0.14354266226291656
epoch£º790	 i:7 	 global-step:15807	 l-p:0.12568417191505432
epoch£º790	 i:8 	 global-step:15808	 l-p:0.11595502495765686
epoch£º790	 i:9 	 global-step:15809	 l-p:0.15394002199172974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6283, 3.1112, 3.1038],
        [3.6283, 3.2213, 2.6363],
        [3.6283, 3.4095, 3.5245],
        [3.6283, 3.2022, 3.2685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.14550341665744781 
model_pd.l_d.mean(): -23.544252395629883 
model_pd.lagr.mean(): -23.39874839782715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1466], device='cuda:0')), ('power', tensor([-23.6908], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.14550341665744781
epoch£º791	 i:1 	 global-step:15821	 l-p:0.1334790140390396
epoch£º791	 i:2 	 global-step:15822	 l-p:0.2573804557323456
epoch£º791	 i:3 	 global-step:15823	 l-p:0.1224704161286354
epoch£º791	 i:4 	 global-step:15824	 l-p:0.01177956536412239
epoch£º791	 i:5 	 global-step:15825	 l-p:0.10625258833169937
epoch£º791	 i:6 	 global-step:15826	 l-p:-2.551147937774658
epoch£º791	 i:7 	 global-step:15827	 l-p:0.13672885298728943
epoch£º791	 i:8 	 global-step:15828	 l-p:0.12730038166046143
epoch£º791	 i:9 	 global-step:15829	 l-p:0.14637470245361328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6052, 2.9009, 2.6033],
        [3.6052, 2.8511, 2.2836],
        [3.6052, 3.4694, 3.5612],
        [3.6052, 3.3158, 3.4324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.1365090012550354 
model_pd.l_d.mean(): -23.6630916595459 
model_pd.lagr.mean(): -23.526582717895508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1688], device='cuda:0')), ('power', tensor([-23.8319], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.1365090012550354
epoch£º792	 i:1 	 global-step:15841	 l-p:0.05534540116786957
epoch£º792	 i:2 	 global-step:15842	 l-p:0.1455521136522293
epoch£º792	 i:3 	 global-step:15843	 l-p:0.14244434237480164
epoch£º792	 i:4 	 global-step:15844	 l-p:0.12114547193050385
epoch£º792	 i:5 	 global-step:15845	 l-p:0.12776033580303192
epoch£º792	 i:6 	 global-step:15846	 l-p:0.03929389268159866
epoch£º792	 i:7 	 global-step:15847	 l-p:0.1729995459318161
epoch£º792	 i:8 	 global-step:15848	 l-p:0.1492617279291153
epoch£º792	 i:9 	 global-step:15849	 l-p:0.27108970284461975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5071, 3.5071, 3.5071],
        [3.5071, 3.5020, 3.5069],
        [3.5071, 3.5071, 3.5071],
        [3.5071, 3.4155, 3.4852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.13760703802108765 
model_pd.l_d.mean(): -23.65812110900879 
model_pd.lagr.mean(): -23.5205135345459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1787], device='cuda:0')), ('power', tensor([-23.8368], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.13760703802108765
epoch£º793	 i:1 	 global-step:15861	 l-p:0.1310841590166092
epoch£º793	 i:2 	 global-step:15862	 l-p:0.09169913828372955
epoch£º793	 i:3 	 global-step:15863	 l-p:0.10906510055065155
epoch£º793	 i:4 	 global-step:15864	 l-p:0.11048177629709244
epoch£º793	 i:5 	 global-step:15865	 l-p:0.17724227905273438
epoch£º793	 i:6 	 global-step:15866	 l-p:0.13600139319896698
epoch£º793	 i:7 	 global-step:15867	 l-p:0.13520431518554688
epoch£º793	 i:8 	 global-step:15868	 l-p:0.09738487750291824
epoch£º793	 i:9 	 global-step:15869	 l-p:0.15963329374790192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6093, 2.9487, 2.7452],
        [3.6093, 3.5683, 3.6037],
        [3.6093, 2.9741, 2.8155],
        [3.6093, 3.6093, 3.6093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.09277205169200897 
model_pd.l_d.mean(): -23.323423385620117 
model_pd.lagr.mean(): -23.23065185546875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2558], device='cuda:0')), ('power', tensor([-23.5793], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.09277205169200897
epoch£º794	 i:1 	 global-step:15881	 l-p:0.0998796597123146
epoch£º794	 i:2 	 global-step:15882	 l-p:0.147816002368927
epoch£º794	 i:3 	 global-step:15883	 l-p:0.12667909264564514
epoch£º794	 i:4 	 global-step:15884	 l-p:0.17118589580059052
epoch£º794	 i:5 	 global-step:15885	 l-p:3.0856640338897705
epoch£º794	 i:6 	 global-step:15886	 l-p:0.17277437448501587
epoch£º794	 i:7 	 global-step:15887	 l-p:0.14365185797214508
epoch£º794	 i:8 	 global-step:15888	 l-p:0.11087455600500107
epoch£º794	 i:9 	 global-step:15889	 l-p:0.2447388619184494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6872, 3.6872, 3.6872],
        [3.6872, 2.9439, 2.4795],
        [3.6872, 3.6063, 3.6693],
        [3.6872, 3.6101, 3.6708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.14766927063465118 
model_pd.l_d.mean(): -23.36316680908203 
model_pd.lagr.mean(): -23.215497970581055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1778], device='cuda:0')), ('power', tensor([-23.5410], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.14766927063465118
epoch£º795	 i:1 	 global-step:15901	 l-p:0.12398461997509003
epoch£º795	 i:2 	 global-step:15902	 l-p:0.13637834787368774
epoch£º795	 i:3 	 global-step:15903	 l-p:0.17285840213298798
epoch£º795	 i:4 	 global-step:15904	 l-p:0.13127487897872925
epoch£º795	 i:5 	 global-step:15905	 l-p:0.13486498594284058
epoch£º795	 i:6 	 global-step:15906	 l-p:0.1650877296924591
epoch£º795	 i:7 	 global-step:15907	 l-p:0.09049826115369797
epoch£º795	 i:8 	 global-step:15908	 l-p:0.18435446918010712
epoch£º795	 i:9 	 global-step:15909	 l-p:0.1706291139125824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7265, 3.6289, 3.7016],
        [3.7265, 3.0238, 2.4065],
        [3.7265, 3.4600, 3.5762],
        [3.7265, 3.7264, 3.7265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.16890043020248413 
model_pd.l_d.mean(): -23.13446617126465 
model_pd.lagr.mean(): -22.965566635131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2109], device='cuda:0')), ('power', tensor([-23.3454], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.16890043020248413
epoch£º796	 i:1 	 global-step:15921	 l-p:0.11868859082460403
epoch£º796	 i:2 	 global-step:15922	 l-p:0.13168106973171234
epoch£º796	 i:3 	 global-step:15923	 l-p:-0.1993170827627182
epoch£º796	 i:4 	 global-step:15924	 l-p:0.11643621325492859
epoch£º796	 i:5 	 global-step:15925	 l-p:0.13739463686943054
epoch£º796	 i:6 	 global-step:15926	 l-p:0.10286550223827362
epoch£º796	 i:7 	 global-step:15927	 l-p:0.13823747634887695
epoch£º796	 i:8 	 global-step:15928	 l-p:0.12472469359636307
epoch£º796	 i:9 	 global-step:15929	 l-p:0.12780684232711792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7733, 3.5985, 3.7033],
        [3.7733, 3.7428, 3.7699],
        [3.7733, 3.7722, 3.7733],
        [3.7733, 3.7733, 3.7733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.1439635008573532 
model_pd.l_d.mean(): -23.44287872314453 
model_pd.lagr.mean(): -23.29891586303711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1534], device='cuda:0')), ('power', tensor([-23.5963], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.1439635008573532
epoch£º797	 i:1 	 global-step:15941	 l-p:-0.010282563976943493
epoch£º797	 i:2 	 global-step:15942	 l-p:0.13133524358272552
epoch£º797	 i:3 	 global-step:15943	 l-p:0.10230498760938644
epoch£º797	 i:4 	 global-step:15944	 l-p:0.17156553268432617
epoch£º797	 i:5 	 global-step:15945	 l-p:0.11385404318571091
epoch£º797	 i:6 	 global-step:15946	 l-p:0.19457408785820007
epoch£º797	 i:7 	 global-step:15947	 l-p:0.24722516536712646
epoch£º797	 i:8 	 global-step:15948	 l-p:0.14946340024471283
epoch£º797	 i:9 	 global-step:15949	 l-p:0.16118644177913666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6658, 3.0002, 2.3736],
        [3.6658, 3.6573, 3.6654],
        [3.6658, 3.3938, 3.5110],
        [3.6658, 3.1599, 2.5496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.132097989320755 
model_pd.l_d.mean(): -23.113876342773438 
model_pd.lagr.mean(): -22.981779098510742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2057], device='cuda:0')), ('power', tensor([-23.3196], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.132097989320755
epoch£º798	 i:1 	 global-step:15961	 l-p:0.1086101308465004
epoch£º798	 i:2 	 global-step:15962	 l-p:0.1328047215938568
epoch£º798	 i:3 	 global-step:15963	 l-p:0.1318710744380951
epoch£º798	 i:4 	 global-step:15964	 l-p:0.0585138313472271
epoch£º798	 i:5 	 global-step:15965	 l-p:-0.4832157790660858
epoch£º798	 i:6 	 global-step:15966	 l-p:0.2732594609260559
epoch£º798	 i:7 	 global-step:15967	 l-p:0.16629645228385925
epoch£º798	 i:8 	 global-step:15968	 l-p:0.17615069448947906
epoch£º798	 i:9 	 global-step:15969	 l-p:0.14552725851535797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5912, 3.5094, 3.5731],
        [3.5912, 3.5887, 3.5911],
        [3.5912, 3.5912, 3.5912],
        [3.5912, 3.5862, 3.5910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.2073434740304947 
model_pd.l_d.mean(): -23.77585792541504 
model_pd.lagr.mean(): -23.568513870239258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1317], device='cuda:0')), ('power', tensor([-23.9075], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.2073434740304947
epoch£º799	 i:1 	 global-step:15981	 l-p:0.15065133571624756
epoch£º799	 i:2 	 global-step:15982	 l-p:0.13685931265354156
epoch£º799	 i:3 	 global-step:15983	 l-p:0.28622251749038696
epoch£º799	 i:4 	 global-step:15984	 l-p:0.08525114506483078
epoch£º799	 i:5 	 global-step:15985	 l-p:0.5728177428245544
epoch£º799	 i:6 	 global-step:15986	 l-p:0.013675520196557045
epoch£º799	 i:7 	 global-step:15987	 l-p:0.09794449806213379
epoch£º799	 i:8 	 global-step:15988	 l-p:0.17121975123882294
epoch£º799	 i:9 	 global-step:15989	 l-p:-1.0663502216339111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6710, 3.1788, 3.1942],
        [3.6710, 3.0943, 2.4718],
        [3.6710, 3.6566, 3.6700],
        [3.6710, 3.0114, 2.8006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.5019072890281677 
model_pd.l_d.mean(): -22.294397354125977 
model_pd.lagr.mean(): -21.792490005493164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3349], device='cuda:0')), ('power', tensor([-22.6293], device='cuda:0'))])
epoch£º800	 i:0 	 global-step:16000	 l-p:0.5019072890281677
epoch£º800	 i:1 	 global-step:16001	 l-p:0.15676747262477875
epoch£º800	 i:2 	 global-step:16002	 l-p:0.19978801906108856
epoch£º800	 i:3 	 global-step:16003	 l-p:0.21680720150470734
epoch£º800	 i:4 	 global-step:16004	 l-p:0.13170258700847626
epoch£º800	 i:5 	 global-step:16005	 l-p:0.11517427116632462
epoch£º800	 i:6 	 global-step:16006	 l-p:0.13502223789691925
epoch£º800	 i:7 	 global-step:16007	 l-p:0.12229273468255997
epoch£º800	 i:8 	 global-step:16008	 l-p:0.14520572125911713
epoch£º800	 i:9 	 global-step:16009	 l-p:0.11143764853477478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7501,  0.6816,  1.0000,  0.6193,
          1.0000,  0.9086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228]], device='cuda:0')
 pt:tensor([[3.7045, 3.1362, 2.5109],
        [3.7045, 3.1052, 2.4763],
        [3.7045, 2.9624, 2.5011],
        [3.7045, 3.2043, 3.2096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.14201153814792633 
model_pd.l_d.mean(): -22.969329833984375 
model_pd.lagr.mean(): -22.82731819152832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2044], device='cuda:0')), ('power', tensor([-23.1738], device='cuda:0'))])
epoch£º801	 i:0 	 global-step:16020	 l-p:0.14201153814792633
epoch£º801	 i:1 	 global-step:16021	 l-p:0.11383116245269775
epoch£º801	 i:2 	 global-step:16022	 l-p:0.24747249484062195
epoch£º801	 i:3 	 global-step:16023	 l-p:0.1693609207868576
epoch£º801	 i:4 	 global-step:16024	 l-p:0.13912086188793182
epoch£º801	 i:5 	 global-step:16025	 l-p:0.13621492683887482
epoch£º801	 i:6 	 global-step:16026	 l-p:0.09315406531095505
epoch£º801	 i:7 	 global-step:16027	 l-p:0.15629498660564423
epoch£º801	 i:8 	 global-step:16028	 l-p:0.13732409477233887
epoch£º801	 i:9 	 global-step:16029	 l-p:0.30725184082984924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6779, 3.6715, 3.6777],
        [3.6779, 3.4378, 3.5548],
        [3.6779, 3.6779, 3.6779],
        [3.6779, 3.0590, 2.9190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.27533385157585144 
model_pd.l_d.mean(): -22.62943458557129 
model_pd.lagr.mean(): -22.354101181030273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2576], device='cuda:0')), ('power', tensor([-22.8871], device='cuda:0'))])
epoch£º802	 i:0 	 global-step:16040	 l-p:0.27533385157585144
epoch£º802	 i:1 	 global-step:16041	 l-p:0.13574832677841187
epoch£º802	 i:2 	 global-step:16042	 l-p:0.23236726224422455
epoch£º802	 i:3 	 global-step:16043	 l-p:0.16169896721839905
epoch£º802	 i:4 	 global-step:16044	 l-p:0.27825459837913513
epoch£º802	 i:5 	 global-step:16045	 l-p:0.12694436311721802
epoch£º802	 i:6 	 global-step:16046	 l-p:0.11980239301919937
epoch£º802	 i:7 	 global-step:16047	 l-p:0.10656629502773285
epoch£º802	 i:8 	 global-step:16048	 l-p:0.1268143504858017
epoch£º802	 i:9 	 global-step:16049	 l-p:0.12610368430614471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4241,  0.3187,  1.0000,  0.2394,
          1.0000,  0.7513, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3293,  0.2274,  1.0000,  0.1570,
          1.0000,  0.6906, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2741,  0.1781,  1.0000,  0.1157,
          1.0000,  0.6496, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[3.6794, 2.9429, 2.5294],
        [3.6794, 3.0212, 2.8124],
        [3.6794, 3.1029, 3.0247],
        [3.6794, 3.0289, 2.8347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.3027813136577606 
model_pd.l_d.mean(): -23.615598678588867 
model_pd.lagr.mean(): -23.312816619873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1736], device='cuda:0')), ('power', tensor([-23.7892], device='cuda:0'))])
epoch£º803	 i:0 	 global-step:16060	 l-p:0.3027813136577606
epoch£º803	 i:1 	 global-step:16061	 l-p:0.11887288838624954
epoch£º803	 i:2 	 global-step:16062	 l-p:0.23913529515266418
epoch£º803	 i:3 	 global-step:16063	 l-p:0.13339222967624664
epoch£º803	 i:4 	 global-step:16064	 l-p:0.1583298295736313
epoch£º803	 i:5 	 global-step:16065	 l-p:0.1339828222990036
epoch£º803	 i:6 	 global-step:16066	 l-p:0.15245600044727325
epoch£º803	 i:7 	 global-step:16067	 l-p:0.13183580338954926
epoch£º803	 i:8 	 global-step:16068	 l-p:0.13837936520576477
epoch£º803	 i:9 	 global-step:16069	 l-p:-0.2910740077495575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6488, 3.6439, 3.6486],
        [3.6488, 3.5822, 3.6361],
        [3.6488, 3.0503, 2.9458],
        [3.6488, 2.8993, 2.3235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.1532435119152069 
model_pd.l_d.mean(): -23.064287185668945 
model_pd.lagr.mean(): -22.911043167114258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2979], device='cuda:0')), ('power', tensor([-23.3622], device='cuda:0'))])
epoch£º804	 i:0 	 global-step:16080	 l-p:0.1532435119152069
epoch£º804	 i:1 	 global-step:16081	 l-p:0.1360025405883789
epoch£º804	 i:2 	 global-step:16082	 l-p:0.1973506063222885
epoch£º804	 i:3 	 global-step:16083	 l-p:0.1327739953994751
epoch£º804	 i:4 	 global-step:16084	 l-p:0.03032851591706276
epoch£º804	 i:5 	 global-step:16085	 l-p:-0.06864143908023834
epoch£º804	 i:6 	 global-step:16086	 l-p:0.1405148208141327
epoch£º804	 i:7 	 global-step:16087	 l-p:0.020614847540855408
epoch£º804	 i:8 	 global-step:16088	 l-p:0.15378530323505402
epoch£º804	 i:9 	 global-step:16089	 l-p:0.13943125307559967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6437, 3.6438, 3.6438],
        [3.6437, 3.6437, 3.6437],
        [3.6437, 3.1444, 2.5378],
        [3.6437, 3.4100, 3.5270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.11708470433950424 
model_pd.l_d.mean(): -22.834598541259766 
model_pd.lagr.mean(): -22.717514038085938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2758], device='cuda:0')), ('power', tensor([-23.1104], device='cuda:0'))])
epoch£º805	 i:0 	 global-step:16100	 l-p:0.11708470433950424
epoch£º805	 i:1 	 global-step:16101	 l-p:0.12714648246765137
epoch£º805	 i:2 	 global-step:16102	 l-p:0.19395789504051208
epoch£º805	 i:3 	 global-step:16103	 l-p:-0.006694836542010307
epoch£º805	 i:4 	 global-step:16104	 l-p:-0.10769334435462952
epoch£º805	 i:5 	 global-step:16105	 l-p:0.2507421374320984
epoch£º805	 i:6 	 global-step:16106	 l-p:0.14206179976463318
epoch£º805	 i:7 	 global-step:16107	 l-p:0.13161206245422363
epoch£º805	 i:8 	 global-step:16108	 l-p:-0.04354744777083397
epoch£º805	 i:9 	 global-step:16109	 l-p:0.12409920245409012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6529, 3.6528, 3.6529],
        [3.6529, 3.6517, 3.6529],
        [3.6529, 3.6529, 3.6529],
        [3.6529, 3.6482, 3.6527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 1.4777271747589111 
model_pd.l_d.mean(): -23.390064239501953 
model_pd.lagr.mean(): -21.912336349487305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2145], device='cuda:0')), ('power', tensor([-23.6046], device='cuda:0'))])
epoch£º806	 i:0 	 global-step:16120	 l-p:1.4777271747589111
epoch£º806	 i:1 	 global-step:16121	 l-p:0.14507150650024414
epoch£º806	 i:2 	 global-step:16122	 l-p:0.21564683318138123
epoch£º806	 i:3 	 global-step:16123	 l-p:0.1426939070224762
epoch£º806	 i:4 	 global-step:16124	 l-p:0.1052764505147934
epoch£º806	 i:5 	 global-step:16125	 l-p:0.130942240357399
epoch£º806	 i:6 	 global-step:16126	 l-p:-0.3537699282169342
epoch£º806	 i:7 	 global-step:16127	 l-p:0.12284653633832932
epoch£º806	 i:8 	 global-step:16128	 l-p:0.14391040802001953
epoch£º806	 i:9 	 global-step:16129	 l-p:0.12463443726301193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6368, 3.6357, 3.6368],
        [3.6368, 3.6359, 3.6368],
        [3.6368, 3.6344, 3.6368],
        [3.6368, 3.6369, 3.6368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.04857243597507477 
model_pd.l_d.mean(): -23.72601318359375 
model_pd.lagr.mean(): -23.677440643310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1377], device='cuda:0')), ('power', tensor([-23.8638], device='cuda:0'))])
epoch£º807	 i:0 	 global-step:16140	 l-p:0.04857243597507477
epoch£º807	 i:1 	 global-step:16141	 l-p:0.13647910952568054
epoch£º807	 i:2 	 global-step:16142	 l-p:-0.005153784528374672
epoch£º807	 i:3 	 global-step:16143	 l-p:0.13953512907028198
epoch£º807	 i:4 	 global-step:16144	 l-p:0.11277372390031815
epoch£º807	 i:5 	 global-step:16145	 l-p:0.1610059291124344
epoch£º807	 i:6 	 global-step:16146	 l-p:0.20319749414920807
epoch£º807	 i:7 	 global-step:16147	 l-p:0.13939084112644196
epoch£º807	 i:8 	 global-step:16148	 l-p:0.12066171318292618
epoch£º807	 i:9 	 global-step:16149	 l-p:0.1620120257139206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6168, 3.6067, 3.6163],
        [3.6168, 3.5922, 3.6144],
        [3.6168, 3.6168, 3.6168],
        [3.6168, 3.6091, 3.6165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.14162598550319672 
model_pd.l_d.mean(): -23.192434310913086 
model_pd.lagr.mean(): -23.05080795288086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2020], device='cuda:0')), ('power', tensor([-23.3945], device='cuda:0'))])
epoch£º808	 i:0 	 global-step:16160	 l-p:0.14162598550319672
epoch£º808	 i:1 	 global-step:16161	 l-p:0.12435546517372131
epoch£º808	 i:2 	 global-step:16162	 l-p:0.1240997314453125
epoch£º808	 i:3 	 global-step:16163	 l-p:0.14699670672416687
epoch£º808	 i:4 	 global-step:16164	 l-p:1.0724924802780151
epoch£º808	 i:5 	 global-step:16165	 l-p:0.12962576746940613
epoch£º808	 i:6 	 global-step:16166	 l-p:0.1478404849767685
epoch£º808	 i:7 	 global-step:16167	 l-p:0.07579019665718079
epoch£º808	 i:8 	 global-step:16168	 l-p:0.13493525981903076
epoch£º808	 i:9 	 global-step:16169	 l-p:0.09881339967250824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5708, 3.5708, 3.5708],
        [3.5708, 3.5699, 3.5708],
        [3.5708, 3.4534, 3.5370],
        [3.5708, 3.5706, 3.5708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): -0.03053690865635872 
model_pd.l_d.mean(): -22.771408081054688 
model_pd.lagr.mean(): -22.801944732666016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3417], device='cuda:0')), ('power', tensor([-23.1131], device='cuda:0'))])
epoch£º809	 i:0 	 global-step:16180	 l-p:-0.03053690865635872
epoch£º809	 i:1 	 global-step:16181	 l-p:0.3128538727760315
epoch£º809	 i:2 	 global-step:16182	 l-p:0.12959299981594086
epoch£º809	 i:3 	 global-step:16183	 l-p:0.021391766145825386
epoch£º809	 i:4 	 global-step:16184	 l-p:0.07264106720685959
epoch£º809	 i:5 	 global-step:16185	 l-p:0.12595880031585693
epoch£º809	 i:6 	 global-step:16186	 l-p:0.14464397728443146
epoch£º809	 i:7 	 global-step:16187	 l-p:0.15576356649398804
epoch£º809	 i:8 	 global-step:16188	 l-p:0.12853485345840454
epoch£º809	 i:9 	 global-step:16189	 l-p:0.13701984286308289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5960, 3.1854, 3.2649],
        [3.5960, 2.8809, 2.5705],
        [3.5960, 3.2150, 2.6383],
        [3.5960, 3.5922, 3.5959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.15437059104442596 
model_pd.l_d.mean(): -23.113588333129883 
model_pd.lagr.mean(): -22.959217071533203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2292], device='cuda:0')), ('power', tensor([-23.3427], device='cuda:0'))])
epoch£º810	 i:0 	 global-step:16200	 l-p:0.15437059104442596
epoch£º810	 i:1 	 global-step:16201	 l-p:0.15322533249855042
epoch£º810	 i:2 	 global-step:16202	 l-p:-0.0511896088719368
epoch£º810	 i:3 	 global-step:16203	 l-p:0.0850900262594223
epoch£º810	 i:4 	 global-step:16204	 l-p:0.2943737506866455
epoch£º810	 i:5 	 global-step:16205	 l-p:0.1892213374376297
epoch£º810	 i:6 	 global-step:16206	 l-p:0.07788711041212082
epoch£º810	 i:7 	 global-step:16207	 l-p:0.1146012544631958
epoch£º810	 i:8 	 global-step:16208	 l-p:0.12540380656719208
epoch£º810	 i:9 	 global-step:16209	 l-p:0.05399966984987259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6373, 2.9807, 2.7863],
        [3.6373, 2.8790, 2.3221],
        [3.6373, 3.4606, 3.5671],
        [3.6373, 3.3508, 3.4681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.05167574808001518 
model_pd.l_d.mean(): -22.930727005004883 
model_pd.lagr.mean(): -22.879051208496094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3216], device='cuda:0')), ('power', tensor([-23.2523], device='cuda:0'))])
epoch£º811	 i:0 	 global-step:16220	 l-p:0.05167574808001518
epoch£º811	 i:1 	 global-step:16221	 l-p:0.14891113340854645
epoch£º811	 i:2 	 global-step:16222	 l-p:0.051693663001060486
epoch£º811	 i:3 	 global-step:16223	 l-p:0.124335378408432
epoch£º811	 i:4 	 global-step:16224	 l-p:0.12675808370113373
epoch£º811	 i:5 	 global-step:16225	 l-p:0.1407858282327652
epoch£º811	 i:6 	 global-step:16226	 l-p:0.12498027831315994
epoch£º811	 i:7 	 global-step:16227	 l-p:0.39658308029174805
epoch£º811	 i:8 	 global-step:16228	 l-p:0.16303321719169617
epoch£º811	 i:9 	 global-step:16229	 l-p:0.12147883325815201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6687, 3.6572, 3.6680],
        [3.6687, 3.6276, 3.6631],
        [3.6687, 3.0924, 2.4695],
        [3.6687, 3.6203, 3.6613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.1953665316104889 
model_pd.l_d.mean(): -23.617673873901367 
model_pd.lagr.mean(): -23.42230796813965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1900], device='cuda:0')), ('power', tensor([-23.8076], device='cuda:0'))])
epoch£º812	 i:0 	 global-step:16240	 l-p:0.1953665316104889
epoch£º812	 i:1 	 global-step:16241	 l-p:0.5668286681175232
epoch£º812	 i:2 	 global-step:16242	 l-p:0.22377613186836243
epoch£º812	 i:3 	 global-step:16243	 l-p:0.17373435199260712
epoch£º812	 i:4 	 global-step:16244	 l-p:0.14271831512451172
epoch£º812	 i:5 	 global-step:16245	 l-p:0.13072220981121063
epoch£º812	 i:6 	 global-step:16246	 l-p:0.10777287930250168
epoch£º812	 i:7 	 global-step:16247	 l-p:0.12650080025196075
epoch£º812	 i:8 	 global-step:16248	 l-p:0.1764543056488037
epoch£º812	 i:9 	 global-step:16249	 l-p:0.11828574538230896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6961, 3.4841, 3.5981],
        [3.6961, 3.1553, 3.1213],
        [3.6961, 3.1925, 3.1973],
        [3.6961, 3.6961, 3.6961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.1334282010793686 
model_pd.l_d.mean(): -23.068117141723633 
model_pd.lagr.mean(): -22.934688568115234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2110], device='cuda:0')), ('power', tensor([-23.2791], device='cuda:0'))])
epoch£º813	 i:0 	 global-step:16260	 l-p:0.1334282010793686
epoch£º813	 i:1 	 global-step:16261	 l-p:0.15410776436328888
epoch£º813	 i:2 	 global-step:16262	 l-p:0.12146355956792831
epoch£º813	 i:3 	 global-step:16263	 l-p:0.24946683645248413
epoch£º813	 i:4 	 global-step:16264	 l-p:0.1506054699420929
epoch£º813	 i:5 	 global-step:16265	 l-p:0.22251415252685547
epoch£º813	 i:6 	 global-step:16266	 l-p:0.16145728528499603
epoch£º813	 i:7 	 global-step:16267	 l-p:0.13243629038333893
epoch£º813	 i:8 	 global-step:16268	 l-p:-2.1599254608154297
epoch£º813	 i:9 	 global-step:16269	 l-p:0.1776638776063919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6669, 3.6669, 3.6669],
        [3.6669, 3.6669, 3.6669],
        [3.6669, 3.5824, 3.6477],
        [3.6669, 3.6669, 3.6669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.13098426163196564 
model_pd.l_d.mean(): -23.577306747436523 
model_pd.lagr.mean(): -23.44632339477539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1260], device='cuda:0')), ('power', tensor([-23.7033], device='cuda:0'))])
epoch£º814	 i:0 	 global-step:16280	 l-p:0.13098426163196564
epoch£º814	 i:1 	 global-step:16281	 l-p:0.32435882091522217
epoch£º814	 i:2 	 global-step:16282	 l-p:0.1645648181438446
epoch£º814	 i:3 	 global-step:16283	 l-p:0.656546950340271
epoch£º814	 i:4 	 global-step:16284	 l-p:0.16229397058486938
epoch£º814	 i:5 	 global-step:16285	 l-p:0.14514748752117157
epoch£º814	 i:6 	 global-step:16286	 l-p:0.13060525059700012
epoch£º814	 i:7 	 global-step:16287	 l-p:0.1738811433315277
epoch£º814	 i:8 	 global-step:16288	 l-p:0.11541254073381424
epoch£º814	 i:9 	 global-step:16289	 l-p:0.13899178802967072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6923, 3.6923, 3.6923],
        [3.6923, 3.1214, 2.4964],
        [3.6923, 3.5281, 3.6305],
        [3.6923, 3.4881, 3.6008]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.17769578099250793 
model_pd.l_d.mean(): -22.604658126831055 
model_pd.lagr.mean(): -22.42696189880371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3145], device='cuda:0')), ('power', tensor([-22.9192], device='cuda:0'))])
epoch£º815	 i:0 	 global-step:16300	 l-p:0.17769578099250793
epoch£º815	 i:1 	 global-step:16301	 l-p:0.13293346762657166
epoch£º815	 i:2 	 global-step:16302	 l-p:0.12772884964942932
epoch£º815	 i:3 	 global-step:16303	 l-p:0.14024142920970917
epoch£º815	 i:4 	 global-step:16304	 l-p:0.2332989126443863
epoch£º815	 i:5 	 global-step:16305	 l-p:0.12261289358139038
epoch£º815	 i:6 	 global-step:16306	 l-p:0.34200775623321533
epoch£º815	 i:7 	 global-step:16307	 l-p:0.11989043653011322
epoch£º815	 i:8 	 global-step:16308	 l-p:0.13692840933799744
epoch£º815	 i:9 	 global-step:16309	 l-p:0.2749236226081848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6665, 3.5069, 3.6079],
        [3.6665, 3.3740, 3.4903],
        [3.6665, 3.6515, 3.6655],
        [3.6665, 3.5821, 3.6474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.13144060969352722 
model_pd.l_d.mean(): -23.659700393676758 
model_pd.lagr.mean(): -23.52825927734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1041], device='cuda:0')), ('power', tensor([-23.7638], device='cuda:0'))])
epoch£º816	 i:0 	 global-step:16320	 l-p:0.13144060969352722
epoch£º816	 i:1 	 global-step:16321	 l-p:0.10574705898761749
epoch£º816	 i:2 	 global-step:16322	 l-p:0.12678302824497223
epoch£º816	 i:3 	 global-step:16323	 l-p:0.07409995794296265
epoch£º816	 i:4 	 global-step:16324	 l-p:0.13904738426208496
epoch£º816	 i:5 	 global-step:16325	 l-p:0.17051126062870026
epoch£º816	 i:6 	 global-step:16326	 l-p:-0.09306873381137848
epoch£º816	 i:7 	 global-step:16327	 l-p:0.13967204093933105
epoch£º816	 i:8 	 global-step:16328	 l-p:0.11768623441457748
epoch£º816	 i:9 	 global-step:16329	 l-p:-0.004214906599372625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6439, 3.0689, 2.4492],
        [3.6439, 3.6436, 3.6439],
        [3.6439, 3.6418, 3.6438],
        [3.6439, 2.9363, 2.6343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): -0.007580649573355913 
model_pd.l_d.mean(): -22.322811126708984 
model_pd.lagr.mean(): -22.33039093017578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3427], device='cuda:0')), ('power', tensor([-22.6655], device='cuda:0'))])
epoch£º817	 i:0 	 global-step:16340	 l-p:-0.007580649573355913
epoch£º817	 i:1 	 global-step:16341	 l-p:0.20124545693397522
epoch£º817	 i:2 	 global-step:16342	 l-p:0.1547238826751709
epoch£º817	 i:3 	 global-step:16343	 l-p:0.136834055185318
epoch£º817	 i:4 	 global-step:16344	 l-p:-0.7206506133079529
epoch£º817	 i:5 	 global-step:16345	 l-p:0.19329306483268738
epoch£º817	 i:6 	 global-step:16346	 l-p:0.1269509345293045
epoch£º817	 i:7 	 global-step:16347	 l-p:0.1475672870874405
epoch£º817	 i:8 	 global-step:16348	 l-p:0.11451216787099838
epoch£º817	 i:9 	 global-step:16349	 l-p:0.23044650256633759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6799, 3.1727, 2.5594],
        [3.6799, 3.0964, 2.4710],
        [3.6799, 3.6799, 3.6799],
        [3.6799, 3.5404, 3.6337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.17716550827026367 
model_pd.l_d.mean(): -22.72737693786621 
model_pd.lagr.mean(): -22.55021095275879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3324], device='cuda:0')), ('power', tensor([-23.0598], device='cuda:0'))])
epoch£º818	 i:0 	 global-step:16360	 l-p:0.17716550827026367
epoch£º818	 i:1 	 global-step:16361	 l-p:0.1367233395576477
epoch£º818	 i:2 	 global-step:16362	 l-p:0.15490207076072693
epoch£º818	 i:3 	 global-step:16363	 l-p:0.10714641958475113
epoch£º818	 i:4 	 global-step:16364	 l-p:0.11911842226982117
epoch£º818	 i:5 	 global-step:16365	 l-p:0.1869129091501236
epoch£º818	 i:6 	 global-step:16366	 l-p:0.3667195737361908
epoch£º818	 i:7 	 global-step:16367	 l-p:0.12159202992916107
epoch£º818	 i:8 	 global-step:16368	 l-p:0.12311223894357681
epoch£º818	 i:9 	 global-step:16369	 l-p:0.13879917562007904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6909, 3.4573, 3.5741],
        [3.6909, 2.9683, 2.6105],
        [3.6909, 3.6820, 3.6904],
        [3.6909, 3.0388, 2.4073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.17016932368278503 
model_pd.l_d.mean(): -23.708898544311523 
model_pd.lagr.mean(): -23.538728713989258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1594], device='cuda:0')), ('power', tensor([-23.8683], device='cuda:0'))])
epoch£º819	 i:0 	 global-step:16380	 l-p:0.17016932368278503
epoch£º819	 i:1 	 global-step:16381	 l-p:0.15218541026115417
epoch£º819	 i:2 	 global-step:16382	 l-p:0.15229067206382751
epoch£º819	 i:3 	 global-step:16383	 l-p:0.23437869548797607
epoch£º819	 i:4 	 global-step:16384	 l-p:0.298766553401947
epoch£º819	 i:5 	 global-step:16385	 l-p:0.11257005482912064
epoch£º819	 i:6 	 global-step:16386	 l-p:0.08626638352870941
epoch£º819	 i:7 	 global-step:16387	 l-p:0.13276484608650208
epoch£º819	 i:8 	 global-step:16388	 l-p:0.13336330652236938
epoch£º819	 i:9 	 global-step:16389	 l-p:0.28048500418663025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6700, 2.9260, 2.5022],
        [3.6700, 3.6676, 3.6699],
        [3.6700, 2.9795, 2.7122],
        [3.6700, 3.6700, 3.6700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.14885124564170837 
model_pd.l_d.mean(): -22.90479278564453 
model_pd.lagr.mean(): -22.75594139099121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2370], device='cuda:0')), ('power', tensor([-23.1418], device='cuda:0'))])
epoch£º820	 i:0 	 global-step:16400	 l-p:0.14885124564170837
epoch£º820	 i:1 	 global-step:16401	 l-p:0.331333726644516
epoch£º820	 i:2 	 global-step:16402	 l-p:0.15515576303005219
epoch£º820	 i:3 	 global-step:16403	 l-p:1.3741334676742554
epoch£º820	 i:4 	 global-step:16404	 l-p:0.12301097810268402
epoch£º820	 i:5 	 global-step:16405	 l-p:0.14948302507400513
epoch£º820	 i:6 	 global-step:16406	 l-p:0.13923431932926178
epoch£º820	 i:7 	 global-step:16407	 l-p:0.12547974288463593
epoch£º820	 i:8 	 global-step:16408	 l-p:0.1422402709722519
epoch£º820	 i:9 	 global-step:16409	 l-p:-0.059373628348112106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6482, 3.3331, 3.4465],
        [3.6482, 3.4830, 3.5861],
        [3.6482, 3.3056, 3.4122],
        [3.6482, 3.0086, 2.8450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.1673554629087448 
model_pd.l_d.mean(): -23.76450538635254 
model_pd.lagr.mean(): -23.597150802612305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1280], device='cuda:0')), ('power', tensor([-23.8925], device='cuda:0'))])
epoch£º821	 i:0 	 global-step:16420	 l-p:0.1673554629087448
epoch£º821	 i:1 	 global-step:16421	 l-p:-0.017550373449921608
epoch£º821	 i:2 	 global-step:16422	 l-p:-0.28120726346969604
epoch£º821	 i:3 	 global-step:16423	 l-p:0.13768532872200012
epoch£º821	 i:4 	 global-step:16424	 l-p:0.13639125227928162
epoch£º821	 i:5 	 global-step:16425	 l-p:-0.03040679357945919
epoch£º821	 i:6 	 global-step:16426	 l-p:0.15348491072654724
epoch£º821	 i:7 	 global-step:16427	 l-p:0.1848011165857315
epoch£º821	 i:8 	 global-step:16428	 l-p:0.10834486782550812
epoch£º821	 i:9 	 global-step:16429	 l-p:0.1904529333114624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6490, 3.6351, 3.6481],
        [3.6490, 2.9201, 2.3074],
        [3.6490, 2.8997, 2.3098],
        [3.6490, 3.6484, 3.6490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.11475280672311783 
model_pd.l_d.mean(): -23.20992660522461 
model_pd.lagr.mean(): -23.095172882080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2202], device='cuda:0')), ('power', tensor([-23.4302], device='cuda:0'))])
epoch£º822	 i:0 	 global-step:16440	 l-p:0.11475280672311783
epoch£º822	 i:1 	 global-step:16441	 l-p:0.16789962351322174
epoch£º822	 i:2 	 global-step:16442	 l-p:0.13642428815364838
epoch£º822	 i:3 	 global-step:16443	 l-p:0.16006429493427277
epoch£º822	 i:4 	 global-step:16444	 l-p:0.03876437991857529
epoch£º822	 i:5 	 global-step:16445	 l-p:0.1249360665678978
epoch£º822	 i:6 	 global-step:16446	 l-p:-0.028436584398150444
epoch£º822	 i:7 	 global-step:16447	 l-p:6.461221694946289
epoch£º822	 i:8 	 global-step:16448	 l-p:0.13463445007801056
epoch£º822	 i:9 	 global-step:16449	 l-p:0.14470385015010834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6607, 3.4926, 3.5966],
        [3.6607, 3.6607, 3.6607],
        [3.6607, 3.1538, 3.1597],
        [3.6607, 3.1689, 2.5607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.16319212317466736 
model_pd.l_d.mean(): -23.300172805786133 
model_pd.lagr.mean(): -23.136980056762695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3023], device='cuda:0')), ('power', tensor([-23.6024], device='cuda:0'))])
epoch£º823	 i:0 	 global-step:16460	 l-p:0.16319212317466736
epoch£º823	 i:1 	 global-step:16461	 l-p:0.14237689971923828
epoch£º823	 i:2 	 global-step:16462	 l-p:-150.70425415039062
epoch£º823	 i:3 	 global-step:16463	 l-p:0.14543984830379486
epoch£º823	 i:4 	 global-step:16464	 l-p:0.21077395975589752
epoch£º823	 i:5 	 global-step:16465	 l-p:-0.06446867436170578
epoch£º823	 i:6 	 global-step:16466	 l-p:0.011433381587266922
epoch£º823	 i:7 	 global-step:16467	 l-p:0.135464608669281
epoch£º823	 i:8 	 global-step:16468	 l-p:0.13809598982334137
epoch£º823	 i:9 	 global-step:16469	 l-p:0.11684007197618484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6558, 2.9932, 2.7879],
        [3.6558, 3.6072, 3.6484],
        [3.6558, 3.4482, 3.5621],
        [3.6558, 3.6558, 3.6558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.16899436712265015 
model_pd.l_d.mean(): -23.363521575927734 
model_pd.lagr.mean(): -23.19452667236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2300], device='cuda:0')), ('power', tensor([-23.5935], device='cuda:0'))])
epoch£º824	 i:0 	 global-step:16480	 l-p:0.16899436712265015
epoch£º824	 i:1 	 global-step:16481	 l-p:-0.12180221080780029
epoch£º824	 i:2 	 global-step:16482	 l-p:0.47352156043052673
epoch£º824	 i:3 	 global-step:16483	 l-p:0.11888440698385239
epoch£º824	 i:4 	 global-step:16484	 l-p:0.11353666335344315
epoch£º824	 i:5 	 global-step:16485	 l-p:0.3947289288043976
epoch£º824	 i:6 	 global-step:16486	 l-p:0.1280381679534912
epoch£º824	 i:7 	 global-step:16487	 l-p:0.13379691541194916
epoch£º824	 i:8 	 global-step:16488	 l-p:0.12970557808876038
epoch£º824	 i:9 	 global-step:16489	 l-p:0.17234954237937927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6751, 3.2398, 2.6414],
        [3.6751, 3.6743, 3.6751],
        [3.6751, 3.6751, 3.6751],
        [3.6751, 3.5976, 3.6586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.14397035539150238 
model_pd.l_d.mean(): -23.027544021606445 
model_pd.lagr.mean(): -22.883573532104492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2423], device='cuda:0')), ('power', tensor([-23.2699], device='cuda:0'))])
epoch£º825	 i:0 	 global-step:16500	 l-p:0.14397035539150238
epoch£º825	 i:1 	 global-step:16501	 l-p:0.11306967586278915
epoch£º825	 i:2 	 global-step:16502	 l-p:0.13046662509441376
epoch£º825	 i:3 	 global-step:16503	 l-p:0.8796983361244202
epoch£º825	 i:4 	 global-step:16504	 l-p:0.14866535365581512
epoch£º825	 i:5 	 global-step:16505	 l-p:0.1180240586400032
epoch£º825	 i:6 	 global-step:16506	 l-p:0.09007150679826736
epoch£º825	 i:7 	 global-step:16507	 l-p:0.37976789474487305
epoch£º825	 i:8 	 global-step:16508	 l-p:0.19879837334156036
epoch£º825	 i:9 	 global-step:16509	 l-p:0.16280677914619446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6728, 3.6661, 3.6725],
        [3.6728, 3.6544, 3.6714],
        [3.6728, 3.6679, 3.6727],
        [3.6728, 3.0780, 2.9808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.12346847355365753 
model_pd.l_d.mean(): -23.576528549194336 
model_pd.lagr.mean(): -23.453060150146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1516], device='cuda:0')), ('power', tensor([-23.7281], device='cuda:0'))])
epoch£º826	 i:0 	 global-step:16520	 l-p:0.12346847355365753
epoch£º826	 i:1 	 global-step:16521	 l-p:0.2794002592563629
epoch£º826	 i:2 	 global-step:16522	 l-p:0.12666068971157074
epoch£º826	 i:3 	 global-step:16523	 l-p:0.12323856353759766
epoch£º826	 i:4 	 global-step:16524	 l-p:0.32220664620399475
epoch£º826	 i:5 	 global-step:16525	 l-p:0.1211419552564621
epoch£º826	 i:6 	 global-step:16526	 l-p:0.1575944721698761
epoch£º826	 i:7 	 global-step:16527	 l-p:0.1309170126914978
epoch£º826	 i:8 	 global-step:16528	 l-p:0.13557066023349762
epoch£º826	 i:9 	 global-step:16529	 l-p:0.20556867122650146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6974, 3.6959, 3.6974],
        [3.6974, 3.6196, 3.6808],
        [3.6974, 3.6974, 3.6974],
        [3.6974, 3.3876, 3.5011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.11961116641759872 
model_pd.l_d.mean(): -23.36282730102539 
model_pd.lagr.mean(): -23.243215560913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1687], device='cuda:0')), ('power', tensor([-23.5315], device='cuda:0'))])
epoch£º827	 i:0 	 global-step:16540	 l-p:0.11961116641759872
epoch£º827	 i:1 	 global-step:16541	 l-p:0.20699338614940643
epoch£º827	 i:2 	 global-step:16542	 l-p:0.21671921014785767
epoch£º827	 i:3 	 global-step:16543	 l-p:0.14268432557582855
epoch£º827	 i:4 	 global-step:16544	 l-p:0.14073370397090912
epoch£º827	 i:5 	 global-step:16545	 l-p:0.22341571748256683
epoch£º827	 i:6 	 global-step:16546	 l-p:0.13924141228199005
epoch£º827	 i:7 	 global-step:16547	 l-p:0.12055033445358276
epoch£º827	 i:8 	 global-step:16548	 l-p:0.09794357419013977
epoch£º827	 i:9 	 global-step:16549	 l-p:0.11331921070814133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6962, 3.4706, 3.5868],
        [3.6962, 3.2801, 3.3535],
        [3.6962, 3.6099, 3.6763],
        [3.6962, 3.6218, 3.6808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.11406096816062927 
model_pd.l_d.mean(): -23.425029754638672 
model_pd.lagr.mean(): -23.31096839904785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1619], device='cuda:0')), ('power', tensor([-23.5869], device='cuda:0'))])
epoch£º828	 i:0 	 global-step:16560	 l-p:0.11406096816062927
epoch£º828	 i:1 	 global-step:16561	 l-p:0.14352241158485413
epoch£º828	 i:2 	 global-step:16562	 l-p:0.21854247152805328
epoch£º828	 i:3 	 global-step:16563	 l-p:0.1649833768606186
epoch£º828	 i:4 	 global-step:16564	 l-p:0.13918153941631317
epoch£º828	 i:5 	 global-step:16565	 l-p:0.14356116950511932
epoch£º828	 i:6 	 global-step:16566	 l-p:0.13065296411514282
epoch£º828	 i:7 	 global-step:16567	 l-p:0.13640308380126953
epoch£º828	 i:8 	 global-step:16568	 l-p:-0.18173572421073914
epoch£º828	 i:9 	 global-step:16569	 l-p:2.312272787094116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6543, 3.5069, 3.6036],
        [3.6543, 2.9140, 2.3094],
        [3.6543, 3.6539, 3.6543],
        [3.6543, 3.6531, 3.6542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.1494309902191162 
model_pd.l_d.mean(): -22.92731285095215 
model_pd.lagr.mean(): -22.777881622314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2428], device='cuda:0')), ('power', tensor([-23.1702], device='cuda:0'))])
epoch£º829	 i:0 	 global-step:16580	 l-p:0.1494309902191162
epoch£º829	 i:1 	 global-step:16581	 l-p:-0.048524994403123856
epoch£º829	 i:2 	 global-step:16582	 l-p:0.13514412939548492
epoch£º829	 i:3 	 global-step:16583	 l-p:3.620607614517212
epoch£º829	 i:4 	 global-step:16584	 l-p:0.12390315532684326
epoch£º829	 i:5 	 global-step:16585	 l-p:-0.9912214279174805
epoch£º829	 i:6 	 global-step:16586	 l-p:0.15330347418785095
epoch£º829	 i:7 	 global-step:16587	 l-p:0.14410893619060516
epoch£º829	 i:8 	 global-step:16588	 l-p:0.12883317470550537
epoch£º829	 i:9 	 global-step:16589	 l-p:0.1594400554895401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6695, 3.6684, 3.6695],
        [3.6695, 3.1679, 3.1789],
        [3.6695, 3.0228, 2.8456],
        [3.6695, 3.6695, 3.6695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.13315442204475403 
model_pd.l_d.mean(): -23.551799774169922 
model_pd.lagr.mean(): -23.41864585876465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1750], device='cuda:0')), ('power', tensor([-23.7268], device='cuda:0'))])
epoch£º830	 i:0 	 global-step:16600	 l-p:0.13315442204475403
epoch£º830	 i:1 	 global-step:16601	 l-p:0.16154485940933228
epoch£º830	 i:2 	 global-step:16602	 l-p:0.1308044195175171
epoch£º830	 i:3 	 global-step:16603	 l-p:-1.2629512548446655
epoch£º830	 i:4 	 global-step:16604	 l-p:0.33295372128486633
epoch£º830	 i:5 	 global-step:16605	 l-p:0.13135400414466858
epoch£º830	 i:6 	 global-step:16606	 l-p:0.13337162137031555
epoch£º830	 i:7 	 global-step:16607	 l-p:0.13409516215324402
epoch£º830	 i:8 	 global-step:16608	 l-p:0.7346417903900146
epoch£º830	 i:9 	 global-step:16609	 l-p:0.09538339078426361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6718, 3.6717, 3.6718],
        [3.6718, 3.6667, 3.6716],
        [3.6718, 3.6445, 3.6689],
        [3.6718, 3.6718, 3.6718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.13171081244945526 
model_pd.l_d.mean(): -23.194854736328125 
model_pd.lagr.mean(): -23.06314468383789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2077], device='cuda:0')), ('power', tensor([-23.4026], device='cuda:0'))])
epoch£º831	 i:0 	 global-step:16620	 l-p:0.13171081244945526
epoch£º831	 i:1 	 global-step:16621	 l-p:0.15777474641799927
epoch£º831	 i:2 	 global-step:16622	 l-p:0.13230566680431366
epoch£º831	 i:3 	 global-step:16623	 l-p:0.1654941737651825
epoch£º831	 i:4 	 global-step:16624	 l-p:0.13277198374271393
epoch£º831	 i:5 	 global-step:16625	 l-p:0.20708033442497253
epoch£º831	 i:6 	 global-step:16626	 l-p:0.14200091361999512
epoch£º831	 i:7 	 global-step:16627	 l-p:0.26602861285209656
epoch£º831	 i:8 	 global-step:16628	 l-p:0.2152319997549057
epoch£º831	 i:9 	 global-step:16629	 l-p:0.12242137640714645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7039, 3.6962, 3.7035],
        [3.7039, 3.1991, 3.2047],
        [3.7039, 2.9524, 2.3905],
        [3.7039, 3.1988, 3.2040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.14278994500637054 
model_pd.l_d.mean(): -23.298532485961914 
model_pd.lagr.mean(): -23.155742645263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1880], device='cuda:0')), ('power', tensor([-23.4865], device='cuda:0'))])
epoch£º832	 i:0 	 global-step:16640	 l-p:0.14278994500637054
epoch£º832	 i:1 	 global-step:16641	 l-p:0.18654166162014008
epoch£º832	 i:2 	 global-step:16642	 l-p:0.19409964978694916
epoch£º832	 i:3 	 global-step:16643	 l-p:0.07744818180799484
epoch£º832	 i:4 	 global-step:16644	 l-p:0.12258773297071457
epoch£º832	 i:5 	 global-step:16645	 l-p:0.12424463033676147
epoch£º832	 i:6 	 global-step:16646	 l-p:0.14791780710220337
epoch£º832	 i:7 	 global-step:16647	 l-p:0.12067175656557083
epoch£º832	 i:8 	 global-step:16648	 l-p:0.11357414722442627
epoch£º832	 i:9 	 global-step:16649	 l-p:0.16748295724391937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7412, 3.7413, 3.7413],
        [3.7412, 3.7283, 3.7404],
        [3.7412, 3.5220, 3.6370],
        [3.7412, 3.7343, 3.7409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.11865052580833435 
model_pd.l_d.mean(): -23.037187576293945 
model_pd.lagr.mean(): -22.918537139892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2537], device='cuda:0')), ('power', tensor([-23.2909], device='cuda:0'))])
epoch£º833	 i:0 	 global-step:16660	 l-p:0.11865052580833435
epoch£º833	 i:1 	 global-step:16661	 l-p:0.11748579889535904
epoch£º833	 i:2 	 global-step:16662	 l-p:0.07903381437063217
epoch£º833	 i:3 	 global-step:16663	 l-p:0.16617298126220703
epoch£º833	 i:4 	 global-step:16664	 l-p:0.14590564370155334
epoch£º833	 i:5 	 global-step:16665	 l-p:0.14272445440292358
epoch£º833	 i:6 	 global-step:16666	 l-p:0.14320844411849976
epoch£º833	 i:7 	 global-step:16667	 l-p:0.1460692137479782
epoch£º833	 i:8 	 global-step:16668	 l-p:0.14927934110164642
epoch£º833	 i:9 	 global-step:16669	 l-p:0.14025601744651794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7297, 3.7297, 3.7297],
        [3.7297, 3.3359, 2.7404],
        [3.7297, 3.7296, 3.7297],
        [3.7297, 3.4165, 3.5288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.10592784732580185 
model_pd.l_d.mean(): -22.987092971801758 
model_pd.lagr.mean(): -22.88116455078125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3040], device='cuda:0')), ('power', tensor([-23.2910], device='cuda:0'))])
epoch£º834	 i:0 	 global-step:16680	 l-p:0.10592784732580185
epoch£º834	 i:1 	 global-step:16681	 l-p:0.11469913274049759
epoch£º834	 i:2 	 global-step:16682	 l-p:0.10779135674238205
epoch£º834	 i:3 	 global-step:16683	 l-p:0.12345530837774277
epoch£º834	 i:4 	 global-step:16684	 l-p:0.1540055274963379
epoch£º834	 i:5 	 global-step:16685	 l-p:0.24603523313999176
epoch£º834	 i:6 	 global-step:16686	 l-p:0.15531601011753082
epoch£º834	 i:7 	 global-step:16687	 l-p:0.16468459367752075
epoch£º834	 i:8 	 global-step:16688	 l-p:0.33605971932411194
epoch£º834	 i:9 	 global-step:16689	 l-p:-0.5031598806381226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6653, 3.6650, 3.6653],
        [3.6653, 3.4233, 3.5415],
        [3.6653, 2.9643, 2.6796],
        [3.6653, 2.9041, 2.3864]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.19094346463680267 
model_pd.l_d.mean(): -23.08477020263672 
model_pd.lagr.mean(): -22.893827438354492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2305], device='cuda:0')), ('power', tensor([-23.3153], device='cuda:0'))])
epoch£º835	 i:0 	 global-step:16700	 l-p:0.19094346463680267
epoch£º835	 i:1 	 global-step:16701	 l-p:0.12400735914707184
epoch£º835	 i:2 	 global-step:16702	 l-p:0.3854881227016449
epoch£º835	 i:3 	 global-step:16703	 l-p:0.15530037879943848
epoch£º835	 i:4 	 global-step:16704	 l-p:0.1261955201625824
epoch£º835	 i:5 	 global-step:16705	 l-p:0.12191394716501236
epoch£º835	 i:6 	 global-step:16706	 l-p:0.14480894804000854
epoch£º835	 i:7 	 global-step:16707	 l-p:0.1389494091272354
epoch£º835	 i:8 	 global-step:16708	 l-p:0.13549228012561798
epoch£º835	 i:9 	 global-step:16709	 l-p:-0.34491005539894104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6546, 3.6306, 3.6523],
        [3.6546, 2.9685, 2.3405],
        [3.6546, 3.6528, 3.6546],
        [3.6546, 3.4464, 3.5607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.17990104854106903 
model_pd.l_d.mean(): -23.51785659790039 
model_pd.lagr.mean(): -23.337955474853516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2095], device='cuda:0')), ('power', tensor([-23.7274], device='cuda:0'))])
epoch£º836	 i:0 	 global-step:16720	 l-p:0.17990104854106903
epoch£º836	 i:1 	 global-step:16721	 l-p:0.102143794298172
epoch£º836	 i:2 	 global-step:16722	 l-p:0.11337941884994507
epoch£º836	 i:3 	 global-step:16723	 l-p:1.4129022359848022
epoch£º836	 i:4 	 global-step:16724	 l-p:-0.40314170718193054
epoch£º836	 i:5 	 global-step:16725	 l-p:0.12669669091701508
epoch£º836	 i:6 	 global-step:16726	 l-p:0.12236650288105011
epoch£º836	 i:7 	 global-step:16727	 l-p:0.13544292747974396
epoch£º836	 i:8 	 global-step:16728	 l-p:0.1674153357744217
epoch£º836	 i:9 	 global-step:16729	 l-p:0.19529353082180023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6647, 3.5278, 3.6203],
        [3.6647, 3.6641, 3.6647],
        [3.6647, 2.9242, 2.3190],
        [3.6647, 3.1838, 3.2142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.15520142018795013 
model_pd.l_d.mean(): -23.747882843017578 
model_pd.lagr.mean(): -23.592681884765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1732], device='cuda:0')), ('power', tensor([-23.9211], device='cuda:0'))])
epoch£º837	 i:0 	 global-step:16740	 l-p:0.15520142018795013
epoch£º837	 i:1 	 global-step:16741	 l-p:0.10844976454973221
epoch£º837	 i:2 	 global-step:16742	 l-p:-0.7658420205116272
epoch£º837	 i:3 	 global-step:16743	 l-p:0.12375400960445404
epoch£º837	 i:4 	 global-step:16744	 l-p:0.11792489886283875
epoch£º837	 i:5 	 global-step:16745	 l-p:0.1159197986125946
epoch£º837	 i:6 	 global-step:16746	 l-p:-0.3805066645145416
epoch£º837	 i:7 	 global-step:16747	 l-p:0.12429603934288025
epoch£º837	 i:8 	 global-step:16748	 l-p:0.12557077407836914
epoch£º837	 i:9 	 global-step:16749	 l-p:0.1246490329504013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6422, 3.6421, 3.6422],
        [3.6422, 3.1914, 2.5931],
        [3.6422, 3.3309, 3.4458],
        [3.6422, 3.3988, 3.5175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.15721428394317627 
model_pd.l_d.mean(): -23.05035972595215 
model_pd.lagr.mean(): -22.893144607543945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3677], device='cuda:0')), ('power', tensor([-23.4181], device='cuda:0'))])
epoch£º838	 i:0 	 global-step:16760	 l-p:0.15721428394317627
epoch£º838	 i:1 	 global-step:16761	 l-p:-0.008744620718061924
epoch£º838	 i:2 	 global-step:16762	 l-p:0.12145029008388519
epoch£º838	 i:3 	 global-step:16763	 l-p:0.12416652590036392
epoch£º838	 i:4 	 global-step:16764	 l-p:0.18681438267230988
epoch£º838	 i:5 	 global-step:16765	 l-p:0.06488562375307083
epoch£º838	 i:6 	 global-step:16766	 l-p:0.1099199429154396
epoch£º838	 i:7 	 global-step:16767	 l-p:0.16159524023532867
epoch£º838	 i:8 	 global-step:16768	 l-p:0.12992827594280243
epoch£º838	 i:9 	 global-step:16769	 l-p:0.11845538020133972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6218, 3.6214, 3.6218],
        [3.6218, 3.6214, 3.6218],
        [3.6218, 3.6217, 3.6218],
        [3.6218, 3.6168, 3.6216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.10860747843980789 
model_pd.l_d.mean(): -23.368003845214844 
model_pd.lagr.mean(): -23.259395599365234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2439], device='cuda:0')), ('power', tensor([-23.6119], device='cuda:0'))])
epoch£º839	 i:0 	 global-step:16780	 l-p:0.10860747843980789
epoch£º839	 i:1 	 global-step:16781	 l-p:0.15559376776218414
epoch£º839	 i:2 	 global-step:16782	 l-p:0.13703231513500214
epoch£º839	 i:3 	 global-step:16783	 l-p:0.16414165496826172
epoch£º839	 i:4 	 global-step:16784	 l-p:0.15192443132400513
epoch£º839	 i:5 	 global-step:16785	 l-p:0.03798743709921837
epoch£º839	 i:6 	 global-step:16786	 l-p:0.3392542600631714
epoch£º839	 i:7 	 global-step:16787	 l-p:0.12077441811561584
epoch£º839	 i:8 	 global-step:16788	 l-p:0.11197707802057266
epoch£º839	 i:9 	 global-step:16789	 l-p:0.04002032056450844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5661, 3.5661, 3.5661],
        [3.5661, 2.7965, 2.2145],
        [3.5661, 3.3140, 3.4343],
        [3.5661, 3.5661, 3.5661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.03220843896269798 
model_pd.l_d.mean(): -22.78154945373535 
model_pd.lagr.mean(): -22.74934196472168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3504], device='cuda:0')), ('power', tensor([-23.1319], device='cuda:0'))])
epoch£º840	 i:0 	 global-step:16800	 l-p:0.03220843896269798
epoch£º840	 i:1 	 global-step:16801	 l-p:0.08471711724996567
epoch£º840	 i:2 	 global-step:16802	 l-p:0.4622802138328552
epoch£º840	 i:3 	 global-step:16803	 l-p:0.11910966783761978
epoch£º840	 i:4 	 global-step:16804	 l-p:0.17370571196079254
epoch£º840	 i:5 	 global-step:16805	 l-p:0.15310968458652496
epoch£º840	 i:6 	 global-step:16806	 l-p:0.13225822150707245
epoch£º840	 i:7 	 global-step:16807	 l-p:0.14755435287952423
epoch£º840	 i:8 	 global-step:16808	 l-p:0.05008140578866005
epoch£º840	 i:9 	 global-step:16809	 l-p:0.12305782735347748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5865, 2.8432, 2.4730],
        [3.5865, 3.5757, 3.5858],
        [3.5865, 3.5826, 3.5863],
        [3.5865, 2.8209, 2.2355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): -0.5425774455070496 
model_pd.l_d.mean(): -23.527536392211914 
model_pd.lagr.mean(): -24.070114135742188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2570], device='cuda:0')), ('power', tensor([-23.7846], device='cuda:0'))])
epoch£º841	 i:0 	 global-step:16820	 l-p:-0.5425774455070496
epoch£º841	 i:1 	 global-step:16821	 l-p:0.23844243586063385
epoch£º841	 i:2 	 global-step:16822	 l-p:0.1252734512090683
epoch£º841	 i:3 	 global-step:16823	 l-p:0.17117950320243835
epoch£º841	 i:4 	 global-step:16824	 l-p:0.10487308353185654
epoch£º841	 i:5 	 global-step:16825	 l-p:0.11633141338825226
epoch£º841	 i:6 	 global-step:16826	 l-p:0.1463697850704193
epoch£º841	 i:7 	 global-step:16827	 l-p:-0.09020261466503143
epoch£º841	 i:8 	 global-step:16828	 l-p:0.13261646032333374
epoch£º841	 i:9 	 global-step:16829	 l-p:0.13852405548095703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6034, 3.4390, 3.5423],
        [3.6034, 2.8333, 2.3406],
        [3.6034, 3.0041, 2.3857],
        [3.6034, 3.6034, 3.6034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): -0.040702152997255325 
model_pd.l_d.mean(): -23.742755889892578 
model_pd.lagr.mean(): -23.783458709716797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1616], device='cuda:0')), ('power', tensor([-23.9044], device='cuda:0'))])
epoch£º842	 i:0 	 global-step:16840	 l-p:-0.040702152997255325
epoch£º842	 i:1 	 global-step:16841	 l-p:0.16766475141048431
epoch£º842	 i:2 	 global-step:16842	 l-p:0.15171286463737488
epoch£º842	 i:3 	 global-step:16843	 l-p:0.07992339879274368
epoch£º842	 i:4 	 global-step:16844	 l-p:0.13294021785259247
epoch£º842	 i:5 	 global-step:16845	 l-p:0.058948952704668045
epoch£º842	 i:6 	 global-step:16846	 l-p:0.0756862536072731
epoch£º842	 i:7 	 global-step:16847	 l-p:0.1953362673521042
epoch£º842	 i:8 	 global-step:16848	 l-p:0.1711374968290329
epoch£º842	 i:9 	 global-step:16849	 l-p:0.11583297699689865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6455, 2.8860, 2.3058],
        [3.6455, 3.6455, 3.6455],
        [3.6455, 3.6170, 3.6425],
        [3.6455, 3.0522, 2.4287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.17324455082416534 
model_pd.l_d.mean(): -23.155746459960938 
model_pd.lagr.mean(): -22.982501983642578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2423], device='cuda:0')), ('power', tensor([-23.3980], device='cuda:0'))])
epoch£º843	 i:0 	 global-step:16860	 l-p:0.17324455082416534
epoch£º843	 i:1 	 global-step:16861	 l-p:0.11285010725259781
epoch£º843	 i:2 	 global-step:16862	 l-p:-0.02408849261701107
epoch£º843	 i:3 	 global-step:16863	 l-p:0.13076815009117126
epoch£º843	 i:4 	 global-step:16864	 l-p:0.14620645344257355
epoch£º843	 i:5 	 global-step:16865	 l-p:-0.790821373462677
epoch£º843	 i:6 	 global-step:16866	 l-p:0.1333717256784439
epoch£º843	 i:7 	 global-step:16867	 l-p:0.19414222240447998
epoch£º843	 i:8 	 global-step:16868	 l-p:0.13276313245296478
epoch£º843	 i:9 	 global-step:16869	 l-p:0.1529683917760849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6519, 2.8888, 2.3260],
        [3.6519, 3.6518, 3.6519],
        [3.6519, 3.1113, 3.0868],
        [3.6519, 3.3760, 3.4951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 6.319046224234626e-05 
model_pd.l_d.mean(): -23.56598663330078 
model_pd.lagr.mean(): -23.5659236907959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1719], device='cuda:0')), ('power', tensor([-23.7379], device='cuda:0'))])
epoch£º844	 i:0 	 global-step:16880	 l-p:6.319046224234626e-05
epoch£º844	 i:1 	 global-step:16881	 l-p:0.12870153784751892
epoch£º844	 i:2 	 global-step:16882	 l-p:0.16314485669136047
epoch£º844	 i:3 	 global-step:16883	 l-p:0.06173958629369736
epoch£º844	 i:4 	 global-step:16884	 l-p:0.135120689868927
epoch£º844	 i:5 	 global-step:16885	 l-p:0.1857200264930725
epoch£º844	 i:6 	 global-step:16886	 l-p:0.03941740468144417
epoch£º844	 i:7 	 global-step:16887	 l-p:0.13751722872257233
epoch£º844	 i:8 	 global-step:16888	 l-p:0.11815505474805832
epoch£º844	 i:9 	 global-step:16889	 l-p:0.1243847906589508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6305, 3.6304, 3.6305],
        [3.6305, 3.5974, 3.6266],
        [3.6305, 3.2549, 3.3522],
        [3.6305, 3.0137, 2.8949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.12242734432220459 
model_pd.l_d.mean(): -23.447782516479492 
model_pd.lagr.mean(): -23.325355529785156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2213], device='cuda:0')), ('power', tensor([-23.6691], device='cuda:0'))])
epoch£º845	 i:0 	 global-step:16900	 l-p:0.12242734432220459
epoch£º845	 i:1 	 global-step:16901	 l-p:0.1436447948217392
epoch£º845	 i:2 	 global-step:16902	 l-p:0.09031926840543747
epoch£º845	 i:3 	 global-step:16903	 l-p:0.13659146428108215
epoch£º845	 i:4 	 global-step:16904	 l-p:0.23113176226615906
epoch£º845	 i:5 	 global-step:16905	 l-p:0.1220751702785492
epoch£º845	 i:6 	 global-step:16906	 l-p:-9.246402740478516
epoch£º845	 i:7 	 global-step:16907	 l-p:0.12809637188911438
epoch£º845	 i:8 	 global-step:16908	 l-p:0.07756837457418442
epoch£º845	 i:9 	 global-step:16909	 l-p:0.14081354439258575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6140, 3.4032, 3.5185],
        [3.6140, 3.5387, 3.5984],
        [3.6140, 3.4693, 3.5654],
        [3.6140, 3.6140, 3.6140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.0809050053358078 
model_pd.l_d.mean(): -22.904878616333008 
model_pd.lagr.mean(): -22.823972702026367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2879], device='cuda:0')), ('power', tensor([-23.1928], device='cuda:0'))])
epoch£º846	 i:0 	 global-step:16920	 l-p:0.0809050053358078
epoch£º846	 i:1 	 global-step:16921	 l-p:0.10566871613264084
epoch£º846	 i:2 	 global-step:16922	 l-p:0.1554723083972931
epoch£º846	 i:3 	 global-step:16923	 l-p:0.21658486127853394
epoch£º846	 i:4 	 global-step:16924	 l-p:0.12663625180721283
epoch£º846	 i:5 	 global-step:16925	 l-p:1.6287778615951538
epoch£º846	 i:6 	 global-step:16926	 l-p:0.11630590260028839
epoch£º846	 i:7 	 global-step:16927	 l-p:0.0888318419456482
epoch£º846	 i:8 	 global-step:16928	 l-p:0.14164887368679047
epoch£º846	 i:9 	 global-step:16929	 l-p:0.13168564438819885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6204, 3.5277, 3.5981],
        [3.6204, 3.6203, 3.6204],
        [3.6204, 3.5210, 3.5952],
        [3.6204, 3.4048, 3.5209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.1244368925690651 
model_pd.l_d.mean(): -23.625591278076172 
model_pd.lagr.mean(): -23.50115394592285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1629], device='cuda:0')), ('power', tensor([-23.7885], device='cuda:0'))])
epoch£º847	 i:0 	 global-step:16940	 l-p:0.1244368925690651
epoch£º847	 i:1 	 global-step:16941	 l-p:0.14610961079597473
epoch£º847	 i:2 	 global-step:16942	 l-p:0.14630989730358124
epoch£º847	 i:3 	 global-step:16943	 l-p:-0.09224224090576172
epoch£º847	 i:4 	 global-step:16944	 l-p:0.14036273956298828
epoch£º847	 i:5 	 global-step:16945	 l-p:0.13820789754390717
epoch£º847	 i:6 	 global-step:16946	 l-p:0.20661941170692444
epoch£º847	 i:7 	 global-step:16947	 l-p:0.1293969452381134
epoch£º847	 i:8 	 global-step:16948	 l-p:0.12274500727653503
epoch£º847	 i:9 	 global-step:16949	 l-p:1.9987984895706177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5982, 3.4564, 3.5513],
        [3.5982, 3.1544, 3.2177],
        [3.5982, 3.5974, 3.5982],
        [3.5982, 2.8294, 2.3620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.15278305113315582 
model_pd.l_d.mean(): -23.6734561920166 
model_pd.lagr.mean(): -23.520673751831055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1668], device='cuda:0')), ('power', tensor([-23.8403], device='cuda:0'))])
epoch£º848	 i:0 	 global-step:16960	 l-p:0.15278305113315582
epoch£º848	 i:1 	 global-step:16961	 l-p:0.032329823821783066
epoch£º848	 i:2 	 global-step:16962	 l-p:0.13258513808250427
epoch£º848	 i:3 	 global-step:16963	 l-p:0.1509961634874344
epoch£º848	 i:4 	 global-step:16964	 l-p:0.14693352580070496
epoch£º848	 i:5 	 global-step:16965	 l-p:0.1353679597377777
epoch£º848	 i:6 	 global-step:16966	 l-p:0.13569115102291107
epoch£º848	 i:7 	 global-step:16967	 l-p:0.15117205679416656
epoch£º848	 i:8 	 global-step:16968	 l-p:-0.6233357191085815
epoch£º848	 i:9 	 global-step:16969	 l-p:0.06361725926399231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5868, 3.0399, 3.0159],
        [3.5868, 2.8562, 2.2394],
        [3.5868, 3.0821, 3.0995],
        [3.5868, 3.4943, 3.5647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.12319127470254898 
model_pd.l_d.mean(): -22.5556583404541 
model_pd.lagr.mean(): -22.432466506958008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3419], device='cuda:0')), ('power', tensor([-22.8976], device='cuda:0'))])
epoch£º849	 i:0 	 global-step:16980	 l-p:0.12319127470254898
epoch£º849	 i:1 	 global-step:16981	 l-p:0.14296576380729675
epoch£º849	 i:2 	 global-step:16982	 l-p:0.13470657169818878
epoch£º849	 i:3 	 global-step:16983	 l-p:0.10741835087537766
epoch£º849	 i:4 	 global-step:16984	 l-p:0.03837170824408531
epoch£º849	 i:5 	 global-step:16985	 l-p:0.1404554694890976
epoch£º849	 i:6 	 global-step:16986	 l-p:0.14428791403770447
epoch£º849	 i:7 	 global-step:16987	 l-p:0.20931148529052734
epoch£º849	 i:8 	 global-step:16988	 l-p:-4.231618404388428
epoch£º849	 i:9 	 global-step:16989	 l-p:0.1558998078107834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8903,  0.8564,  1.0000,  0.8239,
          1.0000,  0.9620, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228]], device='cuda:0')
 pt:tensor([[3.5989, 3.1058, 2.5043],
        [3.5989, 2.8942, 2.6250],
        [3.5989, 3.2795, 3.3948],
        [3.5989, 3.1176, 2.5182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.08923851698637009 
model_pd.l_d.mean(): -23.403684616088867 
model_pd.lagr.mean(): -23.31444549560547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2610], device='cuda:0')), ('power', tensor([-23.6647], device='cuda:0'))])
epoch£º850	 i:0 	 global-step:17000	 l-p:0.08923851698637009
epoch£º850	 i:1 	 global-step:17001	 l-p:0.13775493204593658
epoch£º850	 i:2 	 global-step:17002	 l-p:0.116794154047966
epoch£º850	 i:3 	 global-step:17003	 l-p:0.14066939055919647
epoch£º850	 i:4 	 global-step:17004	 l-p:0.40998804569244385
epoch£º850	 i:5 	 global-step:17005	 l-p:0.1385023593902588
epoch£º850	 i:6 	 global-step:17006	 l-p:0.11442025750875473
epoch£º850	 i:7 	 global-step:17007	 l-p:0.16820725798606873
epoch£º850	 i:8 	 global-step:17008	 l-p:-0.08231256902217865
epoch£º850	 i:9 	 global-step:17009	 l-p:0.2062740921974182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6133, 3.6133, 3.6133],
        [3.6133, 3.6129, 3.6133],
        [3.6133, 3.5174, 3.5897],
        [3.6133, 2.8392, 2.3150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.13479208946228027 
model_pd.l_d.mean(): -22.995878219604492 
model_pd.lagr.mean(): -22.861085891723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3425], device='cuda:0')), ('power', tensor([-23.3384], device='cuda:0'))])
epoch£º851	 i:0 	 global-step:17020	 l-p:0.13479208946228027
epoch£º851	 i:1 	 global-step:17021	 l-p:0.06807399541139603
epoch£º851	 i:2 	 global-step:17022	 l-p:0.1604580283164978
epoch£º851	 i:3 	 global-step:17023	 l-p:0.7787745594978333
epoch£º851	 i:4 	 global-step:17024	 l-p:0.10947957634925842
epoch£º851	 i:5 	 global-step:17025	 l-p:0.14805708825588226
epoch£º851	 i:6 	 global-step:17026	 l-p:0.17093907296657562
epoch£º851	 i:7 	 global-step:17027	 l-p:0.09743558615446091
epoch£º851	 i:8 	 global-step:17028	 l-p:0.12907980382442474
epoch£º851	 i:9 	 global-step:17029	 l-p:0.1190100759267807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6350, 3.2514, 2.6678],
        [3.6350, 2.9794, 2.8003],
        [3.6350, 3.0303, 2.9307],
        [3.6350, 3.6349, 3.6350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.10916706174612045 
model_pd.l_d.mean(): -23.016244888305664 
model_pd.lagr.mean(): -22.90707778930664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2766], device='cuda:0')), ('power', tensor([-23.2929], device='cuda:0'))])
epoch£º852	 i:0 	 global-step:17040	 l-p:0.10916706174612045
epoch£º852	 i:1 	 global-step:17041	 l-p:0.0076591321267187595
epoch£º852	 i:2 	 global-step:17042	 l-p:0.12097892165184021
epoch£º852	 i:3 	 global-step:17043	 l-p:0.12344083189964294
epoch£º852	 i:4 	 global-step:17044	 l-p:0.26521822810173035
epoch£º852	 i:5 	 global-step:17045	 l-p:0.11496084183454514
epoch£º852	 i:6 	 global-step:17046	 l-p:0.14839038252830505
epoch£º852	 i:7 	 global-step:17047	 l-p:0.13682447373867035
epoch£º852	 i:8 	 global-step:17048	 l-p:0.14491935074329376
epoch£º852	 i:9 	 global-step:17049	 l-p:0.1371656060218811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6303, 2.9398, 2.3121],
        [3.6303, 3.6092, 3.6285],
        [3.6303, 3.2957, 3.4071],
        [3.6303, 3.3503, 3.4700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.13688243925571442 
model_pd.l_d.mean(): -22.899181365966797 
model_pd.lagr.mean(): -22.762298583984375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2754], device='cuda:0')), ('power', tensor([-23.1746], device='cuda:0'))])
epoch£º853	 i:0 	 global-step:17060	 l-p:0.13688243925571442
epoch£º853	 i:1 	 global-step:17061	 l-p:0.12470249086618423
epoch£º853	 i:2 	 global-step:17062	 l-p:0.07835748791694641
epoch£º853	 i:3 	 global-step:17063	 l-p:0.09386417269706726
epoch£º853	 i:4 	 global-step:17064	 l-p:0.1400831788778305
epoch£º853	 i:5 	 global-step:17065	 l-p:0.13609732687473297
epoch£º853	 i:6 	 global-step:17066	 l-p:0.12627004086971283
epoch£º853	 i:7 	 global-step:17067	 l-p:0.12928389012813568
epoch£º853	 i:8 	 global-step:17068	 l-p:0.1456373631954193
epoch£º853	 i:9 	 global-step:17069	 l-p:-0.6998511552810669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5903, 2.8136, 2.3152],
        [3.5903, 3.5901, 3.5903],
        [3.5903, 3.0912, 2.4895],
        [3.5903, 3.5903, 3.5903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.10679864883422852 
model_pd.l_d.mean(): -23.268835067749023 
model_pd.lagr.mean(): -23.162036895751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3271], device='cuda:0')), ('power', tensor([-23.5959], device='cuda:0'))])
epoch£º854	 i:0 	 global-step:17080	 l-p:0.10679864883422852
epoch£º854	 i:1 	 global-step:17081	 l-p:0.1838243007659912
epoch£º854	 i:2 	 global-step:17082	 l-p:0.09193792194128036
epoch£º854	 i:3 	 global-step:17083	 l-p:0.14937257766723633
epoch£º854	 i:4 	 global-step:17084	 l-p:0.12754002213478088
epoch£º854	 i:5 	 global-step:17085	 l-p:-0.04377486929297447
epoch£º854	 i:6 	 global-step:17086	 l-p:0.20789776742458344
epoch£º854	 i:7 	 global-step:17087	 l-p:0.12629972398281097
epoch£º854	 i:8 	 global-step:17088	 l-p:0.28047290444374084
epoch£º854	 i:9 	 global-step:17089	 l-p:0.09426382184028625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6205, 2.8660, 2.2616],
        [3.6205, 2.9894, 2.3643],
        [3.6205, 3.6100, 3.6199],
        [3.6205, 3.0752, 3.0512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.14210818707942963 
model_pd.l_d.mean(): -23.585140228271484 
model_pd.lagr.mean(): -23.443031311035156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2098], device='cuda:0')), ('power', tensor([-23.7949], device='cuda:0'))])
epoch£º855	 i:0 	 global-step:17100	 l-p:0.14210818707942963
epoch£º855	 i:1 	 global-step:17101	 l-p:0.12811383605003357
epoch£º855	 i:2 	 global-step:17102	 l-p:0.11933734267950058
epoch£º855	 i:3 	 global-step:17103	 l-p:0.06405066698789597
epoch£º855	 i:4 	 global-step:17104	 l-p:0.12796862423419952
epoch£º855	 i:5 	 global-step:17105	 l-p:0.13491131365299225
epoch£º855	 i:6 	 global-step:17106	 l-p:0.13370341062545776
epoch£º855	 i:7 	 global-step:17107	 l-p:0.1960972398519516
epoch£º855	 i:8 	 global-step:17108	 l-p:0.4617127776145935
epoch£º855	 i:9 	 global-step:17109	 l-p:0.07321588695049286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6373, 3.2046, 3.2740],
        [3.6373, 3.6256, 3.6366],
        [3.6373, 2.8659, 2.3562],
        [3.6373, 3.6373, 3.6373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.13309268653392792 
model_pd.l_d.mean(): -23.06805419921875 
model_pd.lagr.mean(): -22.934961318969727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2757], device='cuda:0')), ('power', tensor([-23.3438], device='cuda:0'))])
epoch£º856	 i:0 	 global-step:17120	 l-p:0.13309268653392792
epoch£º856	 i:1 	 global-step:17121	 l-p:0.13085371255874634
epoch£º856	 i:2 	 global-step:17122	 l-p:0.07471855729818344
epoch£º856	 i:3 	 global-step:17123	 l-p:0.02929898165166378
epoch£º856	 i:4 	 global-step:17124	 l-p:0.14599609375
epoch£º856	 i:5 	 global-step:17125	 l-p:0.14284634590148926
epoch£º856	 i:6 	 global-step:17126	 l-p:-0.052316464483737946
epoch£º856	 i:7 	 global-step:17127	 l-p:0.20578016340732574
epoch£º856	 i:8 	 global-step:17128	 l-p:0.10817252844572067
epoch£º856	 i:9 	 global-step:17129	 l-p:0.18528372049331665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6579, 2.9094, 2.3019],
        [3.6579, 3.2137, 3.2748],
        [3.6579, 3.6223, 3.6535],
        [3.6579, 2.9260, 2.3058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.10797145962715149 
model_pd.l_d.mean(): -22.752859115600586 
model_pd.lagr.mean(): -22.644887924194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3666], device='cuda:0')), ('power', tensor([-23.1195], device='cuda:0'))])
epoch£º857	 i:0 	 global-step:17140	 l-p:0.10797145962715149
epoch£º857	 i:1 	 global-step:17141	 l-p:0.10408605635166168
epoch£º857	 i:2 	 global-step:17142	 l-p:0.1464880406856537
epoch£º857	 i:3 	 global-step:17143	 l-p:0.16216877102851868
epoch£º857	 i:4 	 global-step:17144	 l-p:0.14806872606277466
epoch£º857	 i:5 	 global-step:17145	 l-p:-0.27789297699928284
epoch£º857	 i:6 	 global-step:17146	 l-p:1.4526333808898926
epoch£º857	 i:7 	 global-step:17147	 l-p:0.14019770920276642
epoch£º857	 i:8 	 global-step:17148	 l-p:0.1265685111284256
epoch£º857	 i:9 	 global-step:17149	 l-p:0.17261411249637604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6712, 3.6704, 3.6711],
        [3.6712, 3.3255, 3.4331],
        [3.6712, 3.6711, 3.6712],
        [3.6712, 3.6571, 3.6702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.14465755224227905 
model_pd.l_d.mean(): -23.554677963256836 
model_pd.lagr.mean(): -23.41002082824707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1527], device='cuda:0')), ('power', tensor([-23.7074], device='cuda:0'))])
epoch£º858	 i:0 	 global-step:17160	 l-p:0.14465755224227905
epoch£º858	 i:1 	 global-step:17161	 l-p:0.12355969101190567
epoch£º858	 i:2 	 global-step:17162	 l-p:0.1256011724472046
epoch£º858	 i:3 	 global-step:17163	 l-p:0.1378573328256607
epoch£º858	 i:4 	 global-step:17164	 l-p:-0.8797330856323242
epoch£º858	 i:5 	 global-step:17165	 l-p:0.19437308609485626
epoch£º858	 i:6 	 global-step:17166	 l-p:0.10846161842346191
epoch£º858	 i:7 	 global-step:17167	 l-p:0.012827219441533089
epoch£º858	 i:8 	 global-step:17168	 l-p:0.14098048210144043
epoch£º858	 i:9 	 global-step:17169	 l-p:0.20552918314933777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6494, 2.9456, 2.6711],
        [3.6494, 3.6493, 3.6494],
        [3.6494, 3.2198, 2.6240],
        [3.6494, 3.5523, 3.6253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.11748969554901123 
model_pd.l_d.mean(): -23.506376266479492 
model_pd.lagr.mean(): -23.388887405395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1779], device='cuda:0')), ('power', tensor([-23.6842], device='cuda:0'))])
epoch£º859	 i:0 	 global-step:17180	 l-p:0.11748969554901123
epoch£º859	 i:1 	 global-step:17181	 l-p:0.13587257266044617
epoch£º859	 i:2 	 global-step:17182	 l-p:-0.025851763784885406
epoch£º859	 i:3 	 global-step:17183	 l-p:0.1103142499923706
epoch£º859	 i:4 	 global-step:17184	 l-p:-0.014806975610554218
epoch£º859	 i:5 	 global-step:17185	 l-p:0.13925376534461975
epoch£º859	 i:6 	 global-step:17186	 l-p:0.18448686599731445
epoch£º859	 i:7 	 global-step:17187	 l-p:0.14660099148750305
epoch£º859	 i:8 	 global-step:17188	 l-p:0.13594211637973785
epoch£º859	 i:9 	 global-step:17189	 l-p:0.1721680611371994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6504, 3.5621, 3.6299],
        [3.6504, 3.6352, 3.6493],
        [3.6504, 3.3316, 3.4464],
        [3.6504, 3.6228, 3.6475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.019132642075419426 
model_pd.l_d.mean(): -23.46277618408203 
model_pd.lagr.mean(): -23.44364356994629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1946], device='cuda:0')), ('power', tensor([-23.6574], device='cuda:0'))])
epoch£º860	 i:0 	 global-step:17200	 l-p:0.019132642075419426
epoch£º860	 i:1 	 global-step:17201	 l-p:0.11532439291477203
epoch£º860	 i:2 	 global-step:17202	 l-p:0.12778161466121674
epoch£º860	 i:3 	 global-step:17203	 l-p:0.14591611921787262
epoch£º860	 i:4 	 global-step:17204	 l-p:0.1968543380498886
epoch£º860	 i:5 	 global-step:17205	 l-p:-0.11412353068590164
epoch£º860	 i:6 	 global-step:17206	 l-p:0.12878182530403137
epoch£º860	 i:7 	 global-step:17207	 l-p:0.27588537335395813
epoch£º860	 i:8 	 global-step:17208	 l-p:0.14366038143634796
epoch£º860	 i:9 	 global-step:17209	 l-p:0.1397417187690735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6417, 3.0220, 2.9025],
        [3.6417, 3.0109, 2.3826],
        [3.6417, 3.4680, 3.5745],
        [3.6417, 3.6189, 3.6396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.313166081905365 
model_pd.l_d.mean(): -23.542722702026367 
model_pd.lagr.mean(): -23.229557037353516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1858], device='cuda:0')), ('power', tensor([-23.7285], device='cuda:0'))])
epoch£º861	 i:0 	 global-step:17220	 l-p:0.313166081905365
epoch£º861	 i:1 	 global-step:17221	 l-p:0.001278943964280188
epoch£º861	 i:2 	 global-step:17222	 l-p:0.12080366909503937
epoch£º861	 i:3 	 global-step:17223	 l-p:0.1394558548927307
epoch£º861	 i:4 	 global-step:17224	 l-p:0.13319963216781616
epoch£º861	 i:5 	 global-step:17225	 l-p:0.06416837871074677
epoch£º861	 i:6 	 global-step:17226	 l-p:0.12092958390712738
epoch£º861	 i:7 	 global-step:17227	 l-p:0.11720985174179077
epoch£º861	 i:8 	 global-step:17228	 l-p:0.12438758462667465
epoch£º861	 i:9 	 global-step:17229	 l-p:0.1262558102607727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6316, 2.8765, 2.4674],
        [3.6316, 3.2096, 3.2863],
        [3.6316, 3.6169, 3.6306],
        [3.6316, 3.6316, 3.6317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.13118857145309448 
model_pd.l_d.mean(): -23.431983947753906 
model_pd.lagr.mean(): -23.30079460144043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2051], device='cuda:0')), ('power', tensor([-23.6371], device='cuda:0'))])
epoch£º862	 i:0 	 global-step:17240	 l-p:0.13118857145309448
epoch£º862	 i:1 	 global-step:17241	 l-p:0.12958669662475586
epoch£º862	 i:2 	 global-step:17242	 l-p:0.07265961170196533
epoch£º862	 i:3 	 global-step:17243	 l-p:-0.19431939721107483
epoch£º862	 i:4 	 global-step:17244	 l-p:0.15626399219036102
epoch£º862	 i:5 	 global-step:17245	 l-p:0.3500789701938629
epoch£º862	 i:6 	 global-step:17246	 l-p:0.06678871810436249
epoch£º862	 i:7 	 global-step:17247	 l-p:0.21093715727329254
epoch£º862	 i:8 	 global-step:17248	 l-p:0.12604676187038422
epoch£º862	 i:9 	 global-step:17249	 l-p:0.12660819292068481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6098, 3.5340, 3.5941],
        [3.6098, 3.5709, 3.6047],
        [3.6098, 3.4496, 3.5518],
        [3.6098, 2.8462, 2.4170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.1426854431629181 
model_pd.l_d.mean(): -22.9248104095459 
model_pd.lagr.mean(): -22.78212547302246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2625], device='cuda:0')), ('power', tensor([-23.1873], device='cuda:0'))])
epoch£º863	 i:0 	 global-step:17260	 l-p:0.1426854431629181
epoch£º863	 i:1 	 global-step:17261	 l-p:0.15446458756923676
epoch£º863	 i:2 	 global-step:17262	 l-p:0.13244660198688507
epoch£º863	 i:3 	 global-step:17263	 l-p:0.09950453788042068
epoch£º863	 i:4 	 global-step:17264	 l-p:0.24539071321487427
epoch£º863	 i:5 	 global-step:17265	 l-p:0.12498338520526886
epoch£º863	 i:6 	 global-step:17266	 l-p:0.09555181115865707
epoch£º863	 i:7 	 global-step:17267	 l-p:-0.0595010444521904
epoch£º863	 i:8 	 global-step:17268	 l-p:0.14749199151992798
epoch£º863	 i:9 	 global-step:17269	 l-p:0.032657358795404434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5950, 2.8333, 2.2293],
        [3.5950, 3.5949, 3.5950],
        [3.5950, 3.5950, 3.5950],
        [3.5950, 3.1430, 2.5491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.15596508979797363 
model_pd.l_d.mean(): -22.88722801208496 
model_pd.lagr.mean(): -22.73126220703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2559], device='cuda:0')), ('power', tensor([-23.1431], device='cuda:0'))])
epoch£º864	 i:0 	 global-step:17280	 l-p:0.15596508979797363
epoch£º864	 i:1 	 global-step:17281	 l-p:0.12512896955013275
epoch£º864	 i:2 	 global-step:17282	 l-p:-0.01456841453909874
epoch£º864	 i:3 	 global-step:17283	 l-p:0.11192424595355988
epoch£º864	 i:4 	 global-step:17284	 l-p:0.13084955513477325
epoch£º864	 i:5 	 global-step:17285	 l-p:0.11513572186231613
epoch£º864	 i:6 	 global-step:17286	 l-p:0.1198260486125946
epoch£º864	 i:7 	 global-step:17287	 l-p:0.07495889812707901
epoch£º864	 i:8 	 global-step:17288	 l-p:0.140296071767807
epoch£º864	 i:9 	 global-step:17289	 l-p:0.20609569549560547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6431, 3.6430, 3.6431],
        [3.6431, 3.6412, 3.6430],
        [3.6431, 3.6431, 3.6431],
        [3.6431, 3.6431, 3.6431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.12349290400743484 
model_pd.l_d.mean(): -23.380502700805664 
model_pd.lagr.mean(): -23.257009506225586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1738], device='cuda:0')), ('power', tensor([-23.5543], device='cuda:0'))])
epoch£º865	 i:0 	 global-step:17300	 l-p:0.12349290400743484
epoch£º865	 i:1 	 global-step:17301	 l-p:0.1542825698852539
epoch£º865	 i:2 	 global-step:17302	 l-p:0.13505318760871887
epoch£º865	 i:3 	 global-step:17303	 l-p:0.15346500277519226
epoch£º865	 i:4 	 global-step:17304	 l-p:0.14223752915859222
epoch£º865	 i:5 	 global-step:17305	 l-p:0.19125716388225555
epoch£º865	 i:6 	 global-step:17306	 l-p:-0.04631936922669411
epoch£º865	 i:7 	 global-step:17307	 l-p:-0.047196488827466965
epoch£º865	 i:8 	 global-step:17308	 l-p:0.38593530654907227
epoch£º865	 i:9 	 global-step:17309	 l-p:0.13704830408096313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6909, 3.0961, 2.4649],
        [3.6909, 3.3204, 2.7329],
        [3.6909, 3.5728, 3.6569],
        [3.6909, 3.6858, 3.6907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.13882769644260406 
model_pd.l_d.mean(): -22.946788787841797 
model_pd.lagr.mean(): -22.807960510253906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2360], device='cuda:0')), ('power', tensor([-23.1828], device='cuda:0'))])
epoch£º866	 i:0 	 global-step:17320	 l-p:0.13882769644260406
epoch£º866	 i:1 	 global-step:17321	 l-p:0.13676723837852478
epoch£º866	 i:2 	 global-step:17322	 l-p:0.1917760968208313
epoch£º866	 i:3 	 global-step:17323	 l-p:0.13387881219387054
epoch£º866	 i:4 	 global-step:17324	 l-p:0.1493421494960785
epoch£º866	 i:5 	 global-step:17325	 l-p:0.141322061419487
epoch£º866	 i:6 	 global-step:17326	 l-p:0.13382481038570404
epoch£º866	 i:7 	 global-step:17327	 l-p:0.18389244377613068
epoch£º866	 i:8 	 global-step:17328	 l-p:0.14879021048545837
epoch£º866	 i:9 	 global-step:17329	 l-p:0.10921695828437805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7373, 3.7373, 3.7373],
        [3.7373, 3.3769, 3.4786],
        [3.7373, 3.1167, 2.4772],
        [3.7373, 3.7310, 3.7370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.11618488281965256 
model_pd.l_d.mean(): -22.844345092773438 
model_pd.lagr.mean(): -22.728160858154297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2599], device='cuda:0')), ('power', tensor([-23.1043], device='cuda:0'))])
epoch£º867	 i:0 	 global-step:17340	 l-p:0.11618488281965256
epoch£º867	 i:1 	 global-step:17341	 l-p:0.1469542682170868
epoch£º867	 i:2 	 global-step:17342	 l-p:0.1942724883556366
epoch£º867	 i:3 	 global-step:17343	 l-p:0.11876382678747177
epoch£º867	 i:4 	 global-step:17344	 l-p:0.06659109145402908
epoch£º867	 i:5 	 global-step:17345	 l-p:0.2061649113893509
epoch£º867	 i:6 	 global-step:17346	 l-p:0.11770633608102798
epoch£º867	 i:7 	 global-step:17347	 l-p:0.13386772572994232
epoch£º867	 i:8 	 global-step:17348	 l-p:0.13472364842891693
epoch£º867	 i:9 	 global-step:17349	 l-p:0.1358136236667633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7108,  0.6343,  1.0000,  0.5661,
          1.0000,  0.8924, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2047,  0.1207,  1.0000,  0.0711,
          1.0000,  0.5894, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228]], device='cuda:0')
 pt:tensor([[3.7302, 3.0600, 2.8431],
        [3.7302, 3.0807, 2.4403],
        [3.7302, 3.2878, 3.3480],
        [3.7302, 3.0196, 2.7134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.1686016470193863 
model_pd.l_d.mean(): -23.52766227722168 
model_pd.lagr.mean(): -23.359060287475586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1836], device='cuda:0')), ('power', tensor([-23.7113], device='cuda:0'))])
epoch£º868	 i:0 	 global-step:17360	 l-p:0.1686016470193863
epoch£º868	 i:1 	 global-step:17361	 l-p:0.1109999343752861
epoch£º868	 i:2 	 global-step:17362	 l-p:0.128011554479599
epoch£º868	 i:3 	 global-step:17363	 l-p:0.14219805598258972
epoch£º868	 i:4 	 global-step:17364	 l-p:0.178965762257576
epoch£º868	 i:5 	 global-step:17365	 l-p:0.22587783634662628
epoch£º868	 i:6 	 global-step:17366	 l-p:0.21056640148162842
epoch£º868	 i:7 	 global-step:17367	 l-p:0.1307273656129837
epoch£º868	 i:8 	 global-step:17368	 l-p:0.09819409251213074
epoch£º868	 i:9 	 global-step:17369	 l-p:0.13214492797851562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7024, 3.2853, 2.6853],
        [3.7024, 3.6956, 3.7021],
        [3.7024, 2.9357, 2.3937],
        [3.7024, 3.3883, 3.5034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.1190679743885994 
model_pd.l_d.mean(): -22.919265747070312 
model_pd.lagr.mean(): -22.80019760131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3234], device='cuda:0')), ('power', tensor([-23.2427], device='cuda:0'))])
epoch£º869	 i:0 	 global-step:17380	 l-p:0.1190679743885994
epoch£º869	 i:1 	 global-step:17381	 l-p:0.21314769983291626
epoch£º869	 i:2 	 global-step:17382	 l-p:0.252093106508255
epoch£º869	 i:3 	 global-step:17383	 l-p:0.13858339190483093
epoch£º869	 i:4 	 global-step:17384	 l-p:0.11966188251972198
epoch£º869	 i:5 	 global-step:17385	 l-p:0.1475869119167328
epoch£º869	 i:6 	 global-step:17386	 l-p:0.12949633598327637
epoch£º869	 i:7 	 global-step:17387	 l-p:0.250160813331604
epoch£º869	 i:8 	 global-step:17388	 l-p:0.11903882771730423
epoch£º869	 i:9 	 global-step:17389	 l-p:0.18579337000846863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2005,  0.1173,  1.0000,  0.0687,
          1.0000,  0.5853, 31.6228]], device='cuda:0')
 pt:tensor([[3.6814, 3.2349, 3.2952],
        [3.6814, 3.0221, 2.8359],
        [3.6814, 3.0915, 3.0120],
        [3.6814, 3.2461, 3.3138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.12325052171945572 
model_pd.l_d.mean(): -23.76307487487793 
model_pd.lagr.mean(): -23.63982391357422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1163], device='cuda:0')), ('power', tensor([-23.8793], device='cuda:0'))])
epoch£º870	 i:0 	 global-step:17400	 l-p:0.12325052171945572
epoch£º870	 i:1 	 global-step:17401	 l-p:0.4349150061607361
epoch£º870	 i:2 	 global-step:17402	 l-p:-0.708975613117218
epoch£º870	 i:3 	 global-step:17403	 l-p:0.10978931933641434
epoch£º870	 i:4 	 global-step:17404	 l-p:0.15891727805137634
epoch£º870	 i:5 	 global-step:17405	 l-p:0.1290954351425171
epoch£º870	 i:6 	 global-step:17406	 l-p:0.12447936832904816
epoch£º870	 i:7 	 global-step:17407	 l-p:0.1347162425518036
epoch£º870	 i:8 	 global-step:17408	 l-p:0.11811234802007675
epoch£º870	 i:9 	 global-step:17409	 l-p:0.07059400528669357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6372, 3.0175, 2.9024],
        [3.6372, 3.6044, 3.6334],
        [3.6372, 2.8688, 2.2777],
        [3.6372, 3.1206, 3.1278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.04786863178014755 
model_pd.l_d.mean(): -23.361085891723633 
model_pd.lagr.mean(): -23.313217163085938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2798], device='cuda:0')), ('power', tensor([-23.6409], device='cuda:0'))])
epoch£º871	 i:0 	 global-step:17420	 l-p:0.04786863178014755
epoch£º871	 i:1 	 global-step:17421	 l-p:0.14188960194587708
epoch£º871	 i:2 	 global-step:17422	 l-p:0.1345381736755371
epoch£º871	 i:3 	 global-step:17423	 l-p:0.1256532520055771
epoch£º871	 i:4 	 global-step:17424	 l-p:0.09951669722795486
epoch£º871	 i:5 	 global-step:17425	 l-p:0.11715446412563324
epoch£º871	 i:6 	 global-step:17426	 l-p:0.12543681263923645
epoch£º871	 i:7 	 global-step:17427	 l-p:0.13801917433738708
epoch£º871	 i:8 	 global-step:17428	 l-p:0.29152125120162964
epoch£º871	 i:9 	 global-step:17429	 l-p:-0.11230701208114624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5857, 3.5060, 3.5687],
        [3.5857, 2.8347, 2.2200],
        [3.5857, 3.5857, 3.5857],
        [3.5857, 3.5247, 3.5750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): -0.02186659723520279 
model_pd.l_d.mean(): -23.47924041748047 
model_pd.lagr.mean(): -23.50110626220703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2640], device='cuda:0')), ('power', tensor([-23.7432], device='cuda:0'))])
epoch£º872	 i:0 	 global-step:17440	 l-p:-0.02186659723520279
epoch£º872	 i:1 	 global-step:17441	 l-p:0.13511352241039276
epoch£º872	 i:2 	 global-step:17442	 l-p:0.2865666151046753
epoch£º872	 i:3 	 global-step:17443	 l-p:0.09210048615932465
epoch£º872	 i:4 	 global-step:17444	 l-p:0.13972598314285278
epoch£º872	 i:5 	 global-step:17445	 l-p:0.1357189118862152
epoch£º872	 i:6 	 global-step:17446	 l-p:0.15897484123706818
epoch£º872	 i:7 	 global-step:17447	 l-p:0.02947305701673031
epoch£º872	 i:8 	 global-step:17448	 l-p:0.16225256025791168
epoch£º872	 i:9 	 global-step:17449	 l-p:0.107087641954422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6012, 3.3428, 3.4642],
        [3.6012, 3.5870, 3.6003],
        [3.6012, 2.9886, 2.8888],
        [3.6012, 3.0812, 3.0883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.10866790264844894 
model_pd.l_d.mean(): -22.832441329956055 
model_pd.lagr.mean(): -22.723773956298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3082], device='cuda:0')), ('power', tensor([-23.1407], device='cuda:0'))])
epoch£º873	 i:0 	 global-step:17460	 l-p:0.10866790264844894
epoch£º873	 i:1 	 global-step:17461	 l-p:0.12751318514347076
epoch£º873	 i:2 	 global-step:17462	 l-p:0.15608647465705872
epoch£º873	 i:3 	 global-step:17463	 l-p:0.21810245513916016
epoch£º873	 i:4 	 global-step:17464	 l-p:0.13586914539337158
epoch£º873	 i:5 	 global-step:17465	 l-p:0.1469309777021408
epoch£º873	 i:6 	 global-step:17466	 l-p:0.14620637893676758
epoch£º873	 i:7 	 global-step:17467	 l-p:-0.30653029680252075
epoch£º873	 i:8 	 global-step:17468	 l-p:0.09860152751207352
epoch£º873	 i:9 	 global-step:17469	 l-p:0.11814601719379425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5867, 3.5867, 3.5867],
        [3.5867, 3.5861, 3.5867],
        [3.5867, 3.5867, 3.5867],
        [3.5867, 2.9786, 2.3597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.09694375842809677 
model_pd.l_d.mean(): -23.576265335083008 
model_pd.lagr.mean(): -23.47932243347168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1716], device='cuda:0')), ('power', tensor([-23.7479], device='cuda:0'))])
epoch£º874	 i:0 	 global-step:17480	 l-p:0.09694375842809677
epoch£º874	 i:1 	 global-step:17481	 l-p:0.13701190054416656
epoch£º874	 i:2 	 global-step:17482	 l-p:0.12288115918636322
epoch£º874	 i:3 	 global-step:17483	 l-p:0.13622188568115234
epoch£º874	 i:4 	 global-step:17484	 l-p:-0.19054986536502838
epoch£º874	 i:5 	 global-step:17485	 l-p:0.12515607476234436
epoch£º874	 i:6 	 global-step:17486	 l-p:0.17713308334350586
epoch£º874	 i:7 	 global-step:17487	 l-p:0.11688242852687836
epoch£º874	 i:8 	 global-step:17488	 l-p:0.09584236145019531
epoch£º874	 i:9 	 global-step:17489	 l-p:0.04097872972488403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6051, 3.5215, 3.5866],
        [3.6051, 3.2709, 3.3848],
        [3.6051, 3.6051, 3.6051],
        [3.6051, 2.8214, 2.2837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): -0.004384040832519531 
model_pd.l_d.mean(): -23.578975677490234 
model_pd.lagr.mean(): -23.583358764648438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1568], device='cuda:0')), ('power', tensor([-23.7358], device='cuda:0'))])
epoch£º875	 i:0 	 global-step:17500	 l-p:-0.004384040832519531
epoch£º875	 i:1 	 global-step:17501	 l-p:0.1462140530347824
epoch£º875	 i:2 	 global-step:17502	 l-p:0.3185533881187439
epoch£º875	 i:3 	 global-step:17503	 l-p:0.1583310216665268
epoch£º875	 i:4 	 global-step:17504	 l-p:0.07490821927785873
epoch£º875	 i:5 	 global-step:17505	 l-p:0.02012878842651844
epoch£º875	 i:6 	 global-step:17506	 l-p:0.12984317541122437
epoch£º875	 i:7 	 global-step:17507	 l-p:0.09740561991930008
epoch£º875	 i:8 	 global-step:17508	 l-p:0.15301737189292908
epoch£º875	 i:9 	 global-step:17509	 l-p:0.16781768202781677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6527, 2.8812, 2.3005],
        [3.6527, 3.2625, 2.6739],
        [3.6527, 3.2281, 2.6320],
        [3.6527, 3.6526, 3.6527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.1476583033800125 
model_pd.l_d.mean(): -22.857833862304688 
model_pd.lagr.mean(): -22.710176467895508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2848], device='cuda:0')), ('power', tensor([-23.1426], device='cuda:0'))])
epoch£º876	 i:0 	 global-step:17520	 l-p:0.1476583033800125
epoch£º876	 i:1 	 global-step:17521	 l-p:0.08481983095407486
epoch£º876	 i:2 	 global-step:17522	 l-p:-0.0017145394813269377
epoch£º876	 i:3 	 global-step:17523	 l-p:0.13096141815185547
epoch£º876	 i:4 	 global-step:17524	 l-p:-0.12431776523590088
epoch£º876	 i:5 	 global-step:17525	 l-p:0.1653173267841339
epoch£º876	 i:6 	 global-step:17526	 l-p:0.12764713168144226
epoch£º876	 i:7 	 global-step:17527	 l-p:0.1334533542394638
epoch£º876	 i:8 	 global-step:17528	 l-p:0.2786455750465393
epoch£º876	 i:9 	 global-step:17529	 l-p:0.14008170366287231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6877, 3.6850, 3.6877],
        [3.6877, 3.6877, 3.6877],
        [3.6877, 2.9368, 2.5328],
        [3.6877, 3.3081, 2.7180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.3235228657722473 
model_pd.l_d.mean(): -22.6739559173584 
model_pd.lagr.mean(): -22.350433349609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2416], device='cuda:0')), ('power', tensor([-22.9155], device='cuda:0'))])
epoch£º877	 i:0 	 global-step:17540	 l-p:0.3235228657722473
epoch£º877	 i:1 	 global-step:17541	 l-p:0.13178078830242157
epoch£º877	 i:2 	 global-step:17542	 l-p:0.160862535238266
epoch£º877	 i:3 	 global-step:17543	 l-p:0.3149510622024536
epoch£º877	 i:4 	 global-step:17544	 l-p:0.15765172243118286
epoch£º877	 i:5 	 global-step:17545	 l-p:0.13599207997322083
epoch£º877	 i:6 	 global-step:17546	 l-p:0.1280856430530548
epoch£º877	 i:7 	 global-step:17547	 l-p:0.1264534294605255
epoch£º877	 i:8 	 global-step:17548	 l-p:0.12231183797121048
epoch£º877	 i:9 	 global-step:17549	 l-p:0.21398669481277466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7002, 3.7001, 3.7002],
        [3.7002, 3.2252, 2.6125],
        [3.7002, 3.2308, 2.6192],
        [3.7002, 3.7001, 3.7002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.1343832165002823 
model_pd.l_d.mean(): -23.535926818847656 
model_pd.lagr.mean(): -23.40154266357422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1405], device='cuda:0')), ('power', tensor([-23.6765], device='cuda:0'))])
epoch£º878	 i:0 	 global-step:17560	 l-p:0.1343832165002823
epoch£º878	 i:1 	 global-step:17561	 l-p:0.17906543612480164
epoch£º878	 i:2 	 global-step:17562	 l-p:0.14735956490039825
epoch£º878	 i:3 	 global-step:17563	 l-p:0.20464545488357544
epoch£º878	 i:4 	 global-step:17564	 l-p:0.1109883114695549
epoch£º878	 i:5 	 global-step:17565	 l-p:0.1256917417049408
epoch£º878	 i:6 	 global-step:17566	 l-p:0.16549049317836761
epoch£º878	 i:7 	 global-step:17567	 l-p:0.23690736293792725
epoch£º878	 i:8 	 global-step:17568	 l-p:0.1310400813817978
epoch£º878	 i:9 	 global-step:17569	 l-p:0.2164011299610138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6979, 3.6976, 3.6979],
        [3.6979, 3.5328, 3.6365],
        [3.6979, 2.9288, 2.4263],
        [3.6979, 3.6364, 3.6869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.2862240970134735 
model_pd.l_d.mean(): -23.520410537719727 
model_pd.lagr.mean(): -23.23418617248535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1672], device='cuda:0')), ('power', tensor([-23.6876], device='cuda:0'))])
epoch£º879	 i:0 	 global-step:17580	 l-p:0.2862240970134735
epoch£º879	 i:1 	 global-step:17581	 l-p:0.22311121225357056
epoch£º879	 i:2 	 global-step:17582	 l-p:0.22441241145133972
epoch£º879	 i:3 	 global-step:17583	 l-p:0.14415468275547028
epoch£º879	 i:4 	 global-step:17584	 l-p:0.1619744896888733
epoch£º879	 i:5 	 global-step:17585	 l-p:0.13545894622802734
epoch£º879	 i:6 	 global-step:17586	 l-p:0.10288454592227936
epoch£º879	 i:7 	 global-step:17587	 l-p:0.09314244985580444
epoch£º879	 i:8 	 global-step:17588	 l-p:0.13230128586292267
epoch£º879	 i:9 	 global-step:17589	 l-p:0.12181932479143143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7182, 3.0127, 2.7290],
        [3.7182, 3.5402, 3.6479],
        [3.7182, 3.1528, 3.1022],
        [3.7182, 3.7179, 3.7182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.20462052524089813 
model_pd.l_d.mean(): -23.30233383178711 
model_pd.lagr.mean(): -23.097713470458984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2745], device='cuda:0')), ('power', tensor([-23.5769], device='cuda:0'))])
epoch£º880	 i:0 	 global-step:17600	 l-p:0.20462052524089813
epoch£º880	 i:1 	 global-step:17601	 l-p:0.12051881849765778
epoch£º880	 i:2 	 global-step:17602	 l-p:0.12989483773708344
epoch£º880	 i:3 	 global-step:17603	 l-p:0.15089064836502075
epoch£º880	 i:4 	 global-step:17604	 l-p:0.13345275819301605
epoch£º880	 i:5 	 global-step:17605	 l-p:0.09250617772340775
epoch£º880	 i:6 	 global-step:17606	 l-p:0.24744963645935059
epoch£º880	 i:7 	 global-step:17607	 l-p:0.12551961839199066
epoch£º880	 i:8 	 global-step:17608	 l-p:0.25910308957099915
epoch£º880	 i:9 	 global-step:17609	 l-p:0.17472322285175323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6903, 3.6903, 3.6903],
        [3.6903, 3.4795, 3.5951],
        [3.6903, 3.1225, 3.0724],
        [3.6903, 3.1980, 2.5830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.12376775592565536 
model_pd.l_d.mean(): -22.749006271362305 
model_pd.lagr.mean(): -22.6252384185791 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2241], device='cuda:0')), ('power', tensor([-22.9731], device='cuda:0'))])
epoch£º881	 i:0 	 global-step:17620	 l-p:0.12376775592565536
epoch£º881	 i:1 	 global-step:17621	 l-p:0.16695544123649597
epoch£º881	 i:2 	 global-step:17622	 l-p:0.12380029261112213
epoch£º881	 i:3 	 global-step:17623	 l-p:288.35211181640625
epoch£º881	 i:4 	 global-step:17624	 l-p:0.9556670784950256
epoch£º881	 i:5 	 global-step:17625	 l-p:0.1370941698551178
epoch£º881	 i:6 	 global-step:17626	 l-p:0.11728492379188538
epoch£º881	 i:7 	 global-step:17627	 l-p:0.20526769757270813
epoch£º881	 i:8 	 global-step:17628	 l-p:0.12763561308383942
epoch£º881	 i:9 	 global-step:17629	 l-p:0.14228776097297668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6679, 3.1204, 3.0957],
        [3.6679, 2.9288, 2.5739],
        [3.6679, 3.6679, 3.6679],
        [3.6679, 3.6606, 3.6676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): -0.04298535734415054 
model_pd.l_d.mean(): -23.433176040649414 
model_pd.lagr.mean(): -23.47616195678711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2267], device='cuda:0')), ('power', tensor([-23.6599], device='cuda:0'))])
epoch£º882	 i:0 	 global-step:17640	 l-p:-0.04298535734415054
epoch£º882	 i:1 	 global-step:17641	 l-p:0.1387181580066681
epoch£º882	 i:2 	 global-step:17642	 l-p:0.14795126020908356
epoch£º882	 i:3 	 global-step:17643	 l-p:-0.04876764118671417
epoch£º882	 i:4 	 global-step:17644	 l-p:0.15448114275932312
epoch£º882	 i:5 	 global-step:17645	 l-p:0.13032887876033783
epoch£º882	 i:6 	 global-step:17646	 l-p:-0.24486903846263885
epoch£º882	 i:7 	 global-step:17647	 l-p:0.13051337003707886
epoch£º882	 i:8 	 global-step:17648	 l-p:0.11567184329032898
epoch£º882	 i:9 	 global-step:17649	 l-p:0.16089792549610138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6522, 3.6471, 3.6520],
        [3.6522, 3.6513, 3.6522],
        [3.6522, 3.6286, 3.6500],
        [3.6522, 3.3189, 3.4322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.12794731557369232 
model_pd.l_d.mean(): -23.58892250061035 
model_pd.lagr.mean(): -23.460975646972656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1903], device='cuda:0')), ('power', tensor([-23.7792], device='cuda:0'))])
epoch£º883	 i:0 	 global-step:17660	 l-p:0.12794731557369232
epoch£º883	 i:1 	 global-step:17661	 l-p:0.08814404904842377
epoch£º883	 i:2 	 global-step:17662	 l-p:0.12536875903606415
epoch£º883	 i:3 	 global-step:17663	 l-p:0.22233949601650238
epoch£º883	 i:4 	 global-step:17664	 l-p:0.1383729726076126
epoch£º883	 i:5 	 global-step:17665	 l-p:0.13624675571918488
epoch£º883	 i:6 	 global-step:17666	 l-p:0.09820574522018433
epoch£º883	 i:7 	 global-step:17667	 l-p:0.37652167677879333
epoch£º883	 i:8 	 global-step:17668	 l-p:0.10680929571390152
epoch£º883	 i:9 	 global-step:17669	 l-p:0.11169791966676712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6325, 3.6325, 3.6325],
        [3.6325, 2.9075, 2.6021],
        [3.6325, 3.6325, 3.6325],
        [3.6325, 2.8506, 2.3253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.04151338338851929 
model_pd.l_d.mean(): -23.16047477722168 
model_pd.lagr.mean(): -23.118961334228516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2387], device='cuda:0')), ('power', tensor([-23.3992], device='cuda:0'))])
epoch£º884	 i:0 	 global-step:17680	 l-p:0.04151338338851929
epoch£º884	 i:1 	 global-step:17681	 l-p:0.16762354969978333
epoch£º884	 i:2 	 global-step:17682	 l-p:0.13540580868721008
epoch£º884	 i:3 	 global-step:17683	 l-p:0.12802168726921082
epoch£º884	 i:4 	 global-step:17684	 l-p:0.07810202240943909
epoch£º884	 i:5 	 global-step:17685	 l-p:0.4404178857803345
epoch£º884	 i:6 	 global-step:17686	 l-p:0.1383480280637741
epoch£º884	 i:7 	 global-step:17687	 l-p:0.1470870077610016
epoch£º884	 i:8 	 global-step:17688	 l-p:0.09325116127729416
epoch£º884	 i:9 	 global-step:17689	 l-p:0.13102905452251434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6388, 2.8596, 2.2898],
        [3.6388, 3.6388, 3.6388],
        [3.6388, 3.0446, 2.9683],
        [3.6388, 2.9416, 2.3093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.13459210097789764 
model_pd.l_d.mean(): -23.428050994873047 
model_pd.lagr.mean(): -23.293458938598633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2467], device='cuda:0')), ('power', tensor([-23.6748], device='cuda:0'))])
epoch£º885	 i:0 	 global-step:17700	 l-p:0.13459210097789764
epoch£º885	 i:1 	 global-step:17701	 l-p:0.12110929191112518
epoch£º885	 i:2 	 global-step:17702	 l-p:-0.02507474645972252
epoch£º885	 i:3 	 global-step:17703	 l-p:0.1377873718738556
epoch£º885	 i:4 	 global-step:17704	 l-p:0.12816284596920013
epoch£º885	 i:5 	 global-step:17705	 l-p:0.7211677432060242
epoch£º885	 i:6 	 global-step:17706	 l-p:0.11695761233568192
epoch£º885	 i:7 	 global-step:17707	 l-p:0.29013752937316895
epoch£º885	 i:8 	 global-step:17708	 l-p:0.1154976561665535
epoch£º885	 i:9 	 global-step:17709	 l-p:0.13810133934020996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6223, 3.6192, 3.6222],
        [3.6223, 3.3037, 3.4209],
        [3.6223, 3.6081, 3.6214],
        [3.6223, 3.4391, 3.5490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.14110033214092255 
model_pd.l_d.mean(): -22.832382202148438 
model_pd.lagr.mean(): -22.691282272338867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3609], device='cuda:0')), ('power', tensor([-23.1932], device='cuda:0'))])
epoch£º886	 i:0 	 global-step:17720	 l-p:0.14110033214092255
epoch£º886	 i:1 	 global-step:17721	 l-p:-0.10498868674039841
epoch£º886	 i:2 	 global-step:17722	 l-p:0.14177177846431732
epoch£º886	 i:3 	 global-step:17723	 l-p:0.1346309930086136
epoch£º886	 i:4 	 global-step:17724	 l-p:0.11373765766620636
epoch£º886	 i:5 	 global-step:17725	 l-p:0.19563810527324677
epoch£º886	 i:6 	 global-step:17726	 l-p:0.11625948548316956
epoch£º886	 i:7 	 global-step:17727	 l-p:0.056074656546115875
epoch£º886	 i:8 	 global-step:17728	 l-p:0.148206427693367
epoch£º886	 i:9 	 global-step:17729	 l-p:-1.6631520986557007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6027, 3.5604, 3.5969],
        [3.6027, 2.9471, 2.7869],
        [3.6027, 3.3703, 3.4903],
        [3.6027, 3.4197, 3.5297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.10139492899179459 
model_pd.l_d.mean(): -23.77596664428711 
model_pd.lagr.mean(): -23.674571990966797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1353], device='cuda:0')), ('power', tensor([-23.9113], device='cuda:0'))])
epoch£º887	 i:0 	 global-step:17740	 l-p:0.10139492899179459
epoch£º887	 i:1 	 global-step:17741	 l-p:0.1168942078948021
epoch£º887	 i:2 	 global-step:17742	 l-p:0.10826427489519119
epoch£º887	 i:3 	 global-step:17743	 l-p:0.21607109904289246
epoch£º887	 i:4 	 global-step:17744	 l-p:0.1539584845304489
epoch£º887	 i:5 	 global-step:17745	 l-p:0.11515886336565018
epoch£º887	 i:6 	 global-step:17746	 l-p:0.1422824114561081
epoch£º887	 i:7 	 global-step:17747	 l-p:-21.11862564086914
epoch£º887	 i:8 	 global-step:17748	 l-p:0.16498184204101562
epoch£º887	 i:9 	 global-step:17749	 l-p:0.14423225820064545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[3.6354, 3.1261, 2.5147],
        [3.6354, 3.0764, 3.0436],
        [3.6354, 3.1098, 2.4955],
        [3.6354, 3.0837, 3.0589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.16666392982006073 
model_pd.l_d.mean(): -22.83368682861328 
model_pd.lagr.mean(): -22.667022705078125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3487], device='cuda:0')), ('power', tensor([-23.1824], device='cuda:0'))])
epoch£º888	 i:0 	 global-step:17760	 l-p:0.16666392982006073
epoch£º888	 i:1 	 global-step:17761	 l-p:0.16230933368206024
epoch£º888	 i:2 	 global-step:17762	 l-p:0.1264430433511734
epoch£º888	 i:3 	 global-step:17763	 l-p:0.3115355372428894
epoch£º888	 i:4 	 global-step:17764	 l-p:0.14623676240444183
epoch£º888	 i:5 	 global-step:17765	 l-p:0.0659383237361908
epoch£º888	 i:6 	 global-step:17766	 l-p:0.010774126276373863
epoch£º888	 i:7 	 global-step:17767	 l-p:0.08106372505426407
epoch£º888	 i:8 	 global-step:17768	 l-p:0.12623971700668335
epoch£º888	 i:9 	 global-step:17769	 l-p:0.15601682662963867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6585, 3.6585, 3.6585],
        [3.6585, 3.6585, 3.6585],
        [3.6585, 3.3095, 3.4190],
        [3.6585, 3.5391, 3.6241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.15158796310424805 
model_pd.l_d.mean(): -23.741186141967773 
model_pd.lagr.mean(): -23.589597702026367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1699], device='cuda:0')), ('power', tensor([-23.9111], device='cuda:0'))])
epoch£º889	 i:0 	 global-step:17780	 l-p:0.15158796310424805
epoch£º889	 i:1 	 global-step:17781	 l-p:0.11195394396781921
epoch£º889	 i:2 	 global-step:17782	 l-p:0.12859773635864258
epoch£º889	 i:3 	 global-step:17783	 l-p:0.13432112336158752
epoch£º889	 i:4 	 global-step:17784	 l-p:0.16836324334144592
epoch£º889	 i:5 	 global-step:17785	 l-p:0.13121666014194489
epoch£º889	 i:6 	 global-step:17786	 l-p:-0.023329157382249832
epoch£º889	 i:7 	 global-step:17787	 l-p:0.13346973061561584
epoch£º889	 i:8 	 global-step:17788	 l-p:0.16905078291893005
epoch£º889	 i:9 	 global-step:17789	 l-p:-0.4061877429485321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6691, 3.6690, 3.6691],
        [3.6691, 3.6691, 3.6691],
        [3.6691, 3.2188, 3.2799],
        [3.6691, 3.1961, 3.2410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.09859231859445572 
model_pd.l_d.mean(): -23.44821548461914 
model_pd.lagr.mean(): -23.34962272644043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2108], device='cuda:0')), ('power', tensor([-23.6590], device='cuda:0'))])
epoch£º890	 i:0 	 global-step:17800	 l-p:0.09859231859445572
epoch£º890	 i:1 	 global-step:17801	 l-p:-0.21250925958156586
epoch£º890	 i:2 	 global-step:17802	 l-p:0.15690095722675323
epoch£º890	 i:3 	 global-step:17803	 l-p:-0.47580161690711975
epoch£º890	 i:4 	 global-step:17804	 l-p:0.1937263309955597
epoch£º890	 i:5 	 global-step:17805	 l-p:0.23864255845546722
epoch£º890	 i:6 	 global-step:17806	 l-p:0.13235315680503845
epoch£º890	 i:7 	 global-step:17807	 l-p:0.13075008988380432
epoch£º890	 i:8 	 global-step:17808	 l-p:0.10284411162137985
epoch£º890	 i:9 	 global-step:17809	 l-p:0.15925605595111847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7182, 3.6794, 3.7132],
        [3.7182, 3.0858, 2.4454],
        [3.7182, 3.6060, 3.6872],
        [3.7182, 3.7138, 3.7181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.15771892666816711 
model_pd.l_d.mean(): -22.651723861694336 
model_pd.lagr.mean(): -22.49400520324707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3008], device='cuda:0')), ('power', tensor([-22.9525], device='cuda:0'))])
epoch£º891	 i:0 	 global-step:17820	 l-p:0.15771892666816711
epoch£º891	 i:1 	 global-step:17821	 l-p:0.1715928167104721
epoch£º891	 i:2 	 global-step:17822	 l-p:0.12128785997629166
epoch£º891	 i:3 	 global-step:17823	 l-p:0.1386534720659256
epoch£º891	 i:4 	 global-step:17824	 l-p:0.15588389337062836
epoch£º891	 i:5 	 global-step:17825	 l-p:0.15438832342624664
epoch£º891	 i:6 	 global-step:17826	 l-p:0.15315315127372742
epoch£º891	 i:7 	 global-step:17827	 l-p:0.08594430983066559
epoch£º891	 i:8 	 global-step:17828	 l-p:0.13096493482589722
epoch£º891	 i:9 	 global-step:17829	 l-p:0.15840578079223633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7338, 3.2040, 3.1940],
        [3.7338, 3.7338, 3.7338],
        [3.7338, 2.9780, 2.3757],
        [3.7338, 3.7257, 3.7335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.11639732867479324 
model_pd.l_d.mean(): -23.18301773071289 
model_pd.lagr.mean(): -23.066619873046875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2754], device='cuda:0')), ('power', tensor([-23.4585], device='cuda:0'))])
epoch£º892	 i:0 	 global-step:17840	 l-p:0.11639732867479324
epoch£º892	 i:1 	 global-step:17841	 l-p:0.11180911958217621
epoch£º892	 i:2 	 global-step:17842	 l-p:0.13234998285770416
epoch£º892	 i:3 	 global-step:17843	 l-p:0.13071563839912415
epoch£º892	 i:4 	 global-step:17844	 l-p:0.1431078165769577
epoch£º892	 i:5 	 global-step:17845	 l-p:0.23257988691329956
epoch£º892	 i:6 	 global-step:17846	 l-p:0.14694955945014954
epoch£º892	 i:7 	 global-step:17847	 l-p:0.20224207639694214
epoch£º892	 i:8 	 global-step:17848	 l-p:0.12532059848308563
epoch£º892	 i:9 	 global-step:17849	 l-p:0.12935197353363037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7139, 3.0591, 2.4179],
        [3.7139, 3.2895, 2.6853],
        [3.7139, 3.6892, 3.7115],
        [3.7139, 3.2794, 3.3486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.18766173720359802 
model_pd.l_d.mean(): -22.809791564941406 
model_pd.lagr.mean(): -22.622129440307617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3434], device='cuda:0')), ('power', tensor([-23.1532], device='cuda:0'))])
epoch£º893	 i:0 	 global-step:17860	 l-p:0.18766173720359802
epoch£º893	 i:1 	 global-step:17861	 l-p:0.23225921392440796
epoch£º893	 i:2 	 global-step:17862	 l-p:0.1252075880765915
epoch£º893	 i:3 	 global-step:17863	 l-p:0.11963498592376709
epoch£º893	 i:4 	 global-step:17864	 l-p:0.1503920704126358
epoch£º893	 i:5 	 global-step:17865	 l-p:0.11556203663349152
epoch£º893	 i:6 	 global-step:17866	 l-p:0.11155322939157486
epoch£º893	 i:7 	 global-step:17867	 l-p:0.13870128989219666
epoch£º893	 i:8 	 global-step:17868	 l-p:0.7691051959991455
epoch£º893	 i:9 	 global-step:17869	 l-p:0.1485619693994522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6736, 3.6726, 3.6736],
        [3.6736, 3.6627, 3.6729],
        [3.6736, 2.8970, 2.3924],
        [3.6736, 3.0082, 2.8210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.12234645336866379 
model_pd.l_d.mean(): -23.21378517150879 
model_pd.lagr.mean(): -23.09143829345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2246], device='cuda:0')), ('power', tensor([-23.4384], device='cuda:0'))])
epoch£º894	 i:0 	 global-step:17880	 l-p:0.12234645336866379
epoch£º894	 i:1 	 global-step:17881	 l-p:-0.02005355805158615
epoch£º894	 i:2 	 global-step:17882	 l-p:0.12793438136577606
epoch£º894	 i:3 	 global-step:17883	 l-p:0.11949267238378525
epoch£º894	 i:4 	 global-step:17884	 l-p:0.1612095981836319
epoch£º894	 i:5 	 global-step:17885	 l-p:0.22419598698616028
epoch£º894	 i:6 	 global-step:17886	 l-p:-0.017003383487462997
epoch£º894	 i:7 	 global-step:17887	 l-p:0.16907361149787903
epoch£º894	 i:8 	 global-step:17888	 l-p:0.05128876492381096
epoch£º894	 i:9 	 global-step:17889	 l-p:0.16353252530097961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6525, 3.6525, 3.6525],
        [3.6525, 3.6288, 3.6503],
        [3.6525, 3.1468, 2.5336],
        [3.6525, 2.9850, 2.7979]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.1707136482000351 
model_pd.l_d.mean(): -22.533588409423828 
model_pd.lagr.mean(): -22.36287498474121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3553], device='cuda:0')), ('power', tensor([-22.8889], device='cuda:0'))])
epoch£º895	 i:0 	 global-step:17900	 l-p:0.1707136482000351
epoch£º895	 i:1 	 global-step:17901	 l-p:0.037697408348321915
epoch£º895	 i:2 	 global-step:17902	 l-p:0.2322656363248825
epoch£º895	 i:3 	 global-step:17903	 l-p:0.13104601204395294
epoch£º895	 i:4 	 global-step:17904	 l-p:0.1349591463804245
epoch£º895	 i:5 	 global-step:17905	 l-p:0.11460921913385391
epoch£º895	 i:6 	 global-step:17906	 l-p:0.13158155977725983
epoch£º895	 i:7 	 global-step:17907	 l-p:-0.06875143200159073
epoch£º895	 i:8 	 global-step:17908	 l-p:0.15004923939704895
epoch£º895	 i:9 	 global-step:17909	 l-p:0.05748004838824272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6528, 3.2137, 2.6136],
        [3.6528, 3.5589, 3.6303],
        [3.6528, 3.6528, 3.6528],
        [3.6528, 2.9994, 2.8365]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.12948021292686462 
model_pd.l_d.mean(): -23.349987030029297 
model_pd.lagr.mean(): -23.22050666809082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2390], device='cuda:0')), ('power', tensor([-23.5890], device='cuda:0'))])
epoch£º896	 i:0 	 global-step:17920	 l-p:0.12948021292686462
epoch£º896	 i:1 	 global-step:17921	 l-p:0.05737441033124924
epoch£º896	 i:2 	 global-step:17922	 l-p:-0.0007055377936922014
epoch£º896	 i:3 	 global-step:17923	 l-p:0.17599910497665405
epoch£º896	 i:4 	 global-step:17924	 l-p:0.14333787560462952
epoch£º896	 i:5 	 global-step:17925	 l-p:0.1803411841392517
epoch£º896	 i:6 	 global-step:17926	 l-p:0.20757286250591278
epoch£º896	 i:7 	 global-step:17927	 l-p:0.1396254003047943
epoch£º896	 i:8 	 global-step:17928	 l-p:0.09905201941728592
epoch£º896	 i:9 	 global-step:17929	 l-p:0.004003543872386217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6678, 3.5842, 3.6493],
        [3.6678, 3.6596, 3.6674],
        [3.6678, 3.5812, 3.6482],
        [3.6678, 3.1099, 3.0762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.13663572072982788 
model_pd.l_d.mean(): -23.515697479248047 
model_pd.lagr.mean(): -23.37906265258789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1669], device='cuda:0')), ('power', tensor([-23.6826], device='cuda:0'))])
epoch£º897	 i:0 	 global-step:17940	 l-p:0.13663572072982788
epoch£º897	 i:1 	 global-step:17941	 l-p:-0.007703933399170637
epoch£º897	 i:2 	 global-step:17942	 l-p:0.1167510598897934
epoch£º897	 i:3 	 global-step:17943	 l-p:-0.12450870126485825
epoch£º897	 i:4 	 global-step:17944	 l-p:0.1325145661830902
epoch£º897	 i:5 	 global-step:17945	 l-p:0.13055925071239471
epoch£º897	 i:6 	 global-step:17946	 l-p:0.14625680446624756
epoch£º897	 i:7 	 global-step:17947	 l-p:0.20422793924808502
epoch£º897	 i:8 	 global-step:17948	 l-p:0.15931908786296844
epoch£º897	 i:9 	 global-step:17949	 l-p:-0.06592729687690735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6570, 3.1058, 3.0807],
        [3.6570, 3.6558, 3.6570],
        [3.6570, 3.5732, 3.6385],
        [3.6570, 3.2676, 2.6779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.13576693832874298 
model_pd.l_d.mean(): -22.96395492553711 
model_pd.lagr.mean(): -22.828187942504883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2655], device='cuda:0')), ('power', tensor([-23.2295], device='cuda:0'))])
epoch£º898	 i:0 	 global-step:17960	 l-p:0.13576693832874298
epoch£º898	 i:1 	 global-step:17961	 l-p:0.14049021899700165
epoch£º898	 i:2 	 global-step:17962	 l-p:0.03189516067504883
epoch£º898	 i:3 	 global-step:17963	 l-p:0.14328551292419434
epoch£º898	 i:4 	 global-step:17964	 l-p:0.30051565170288086
epoch£º898	 i:5 	 global-step:17965	 l-p:0.17753058671951294
epoch£º898	 i:6 	 global-step:17966	 l-p:0.10597220808267593
epoch£º898	 i:7 	 global-step:17967	 l-p:0.14533881843090057
epoch£º898	 i:8 	 global-step:17968	 l-p:0.09698005765676498
epoch£º898	 i:9 	 global-step:17969	 l-p:0.10809990018606186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6326, 2.8679, 2.2569],
        [3.6326, 2.8826, 2.5168],
        [3.6326, 3.0800, 3.0557],
        [3.6326, 3.6307, 3.6326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.14703240990638733 
model_pd.l_d.mean(): -23.244070053100586 
model_pd.lagr.mean(): -23.09703826904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2121], device='cuda:0')), ('power', tensor([-23.4562], device='cuda:0'))])
epoch£º899	 i:0 	 global-step:17980	 l-p:0.14703240990638733
epoch£º899	 i:1 	 global-step:17981	 l-p:0.17436033487319946
epoch£º899	 i:2 	 global-step:17982	 l-p:-0.575640082359314
epoch£º899	 i:3 	 global-step:17983	 l-p:0.12343015521764755
epoch£º899	 i:4 	 global-step:17984	 l-p:0.1294669359922409
epoch£º899	 i:5 	 global-step:17985	 l-p:0.17167268693447113
epoch£º899	 i:6 	 global-step:17986	 l-p:0.14935894310474396
epoch£º899	 i:7 	 global-step:17987	 l-p:0.15833227336406708
epoch£º899	 i:8 	 global-step:17988	 l-p:0.12897300720214844
epoch£º899	 i:9 	 global-step:17989	 l-p:0.1284654438495636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6073, 2.9469, 2.7806],
        [3.6073, 3.3926, 3.5101],
        [3.6073, 3.6072, 3.6073],
        [3.6073, 3.4071, 3.5217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.14935332536697388 
model_pd.l_d.mean(): -23.61961555480957 
model_pd.lagr.mean(): -23.47026252746582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2048], device='cuda:0')), ('power', tensor([-23.8245], device='cuda:0'))])
epoch£º900	 i:0 	 global-step:18000	 l-p:0.14935332536697388
epoch£º900	 i:1 	 global-step:18001	 l-p:0.17302657663822174
epoch£º900	 i:2 	 global-step:18002	 l-p:0.1519252508878708
epoch£º900	 i:3 	 global-step:18003	 l-p:0.15728211402893066
epoch£º900	 i:4 	 global-step:18004	 l-p:0.10818807035684586
epoch£º900	 i:5 	 global-step:18005	 l-p:0.12504842877388
epoch£º900	 i:6 	 global-step:18006	 l-p:0.09358768165111542
epoch£º900	 i:7 	 global-step:18007	 l-p:0.12405885756015778
epoch£º900	 i:8 	 global-step:18008	 l-p:-0.003280749311670661
epoch£º900	 i:9 	 global-step:18009	 l-p:0.11890710890293121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5898, 3.5095, 3.5727],
        [3.5898, 3.5818, 3.5894],
        [3.5898, 2.9380, 2.3128],
        [3.5898, 3.0764, 3.0944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.17616449296474457 
model_pd.l_d.mean(): -23.490686416625977 
model_pd.lagr.mean(): -23.31452178955078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2666], device='cuda:0')), ('power', tensor([-23.7573], device='cuda:0'))])
epoch£º901	 i:0 	 global-step:18020	 l-p:0.17616449296474457
epoch£º901	 i:1 	 global-step:18021	 l-p:0.13883228600025177
epoch£º901	 i:2 	 global-step:18022	 l-p:0.08036389946937561
epoch£º901	 i:3 	 global-step:18023	 l-p:0.0640333890914917
epoch£º901	 i:4 	 global-step:18024	 l-p:0.12940317392349243
epoch£º901	 i:5 	 global-step:18025	 l-p:0.10594740509986877
epoch£º901	 i:6 	 global-step:18026	 l-p:0.15212777256965637
epoch£º901	 i:7 	 global-step:18027	 l-p:-0.8548198342323303
epoch£º901	 i:8 	 global-step:18028	 l-p:0.15016354620456696
epoch£º901	 i:9 	 global-step:18029	 l-p:0.23708555102348328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6072, 3.6015, 3.6070],
        [3.6072, 3.0840, 3.0919],
        [3.6072, 2.8279, 2.2313],
        [3.6072, 3.4668, 3.5618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.13064169883728027 
model_pd.l_d.mean(): -22.938650131225586 
model_pd.lagr.mean(): -22.808008193969727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2535], device='cuda:0')), ('power', tensor([-23.1921], device='cuda:0'))])
epoch£º902	 i:0 	 global-step:18040	 l-p:0.13064169883728027
epoch£º902	 i:1 	 global-step:18041	 l-p:0.14019273221492767
epoch£º902	 i:2 	 global-step:18042	 l-p:0.04073711484670639
epoch£º902	 i:3 	 global-step:18043	 l-p:0.1478644460439682
epoch£º902	 i:4 	 global-step:18044	 l-p:0.4326278269290924
epoch£º902	 i:5 	 global-step:18045	 l-p:0.1437341719865799
epoch£º902	 i:6 	 global-step:18046	 l-p:0.04531680792570114
epoch£º902	 i:7 	 global-step:18047	 l-p:0.1850002557039261
epoch£º902	 i:8 	 global-step:18048	 l-p:0.08340459316968918
epoch£º902	 i:9 	 global-step:18049	 l-p:0.10713381320238113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6550, 3.5655, 3.6343],
        [3.6550, 3.1484, 2.5345],
        [3.6550, 3.2730, 3.3720],
        [3.6550, 3.3609, 3.4814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.03157086670398712 
model_pd.l_d.mean(): -23.360822677612305 
model_pd.lagr.mean(): -23.329252243041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2821], device='cuda:0')), ('power', tensor([-23.6430], device='cuda:0'))])
epoch£º903	 i:0 	 global-step:18060	 l-p:0.03157086670398712
epoch£º903	 i:1 	 global-step:18061	 l-p:0.12009113281965256
epoch£º903	 i:2 	 global-step:18062	 l-p:0.17489555478096008
epoch£º903	 i:3 	 global-step:18063	 l-p:0.16966812312602997
epoch£º903	 i:4 	 global-step:18064	 l-p:0.31478914618492126
epoch£º903	 i:5 	 global-step:18065	 l-p:0.14223290979862213
epoch£º903	 i:6 	 global-step:18066	 l-p:0.1297992318868637
epoch£º903	 i:7 	 global-step:18067	 l-p:0.2821633815765381
epoch£º903	 i:8 	 global-step:18068	 l-p:0.12340647727251053
epoch£º903	 i:9 	 global-step:18069	 l-p:0.1559290885925293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7155, 3.6685, 3.7086],
        [3.7155, 3.3432, 3.4439],
        [3.7155, 3.7155, 3.7155],
        [3.7155, 3.7077, 3.7151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.11187022179365158 
model_pd.l_d.mean(): -22.767248153686523 
model_pd.lagr.mean(): -22.655378341674805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2642], device='cuda:0')), ('power', tensor([-23.0314], device='cuda:0'))])
epoch£º904	 i:0 	 global-step:18080	 l-p:0.11187022179365158
epoch£º904	 i:1 	 global-step:18081	 l-p:0.13356488943099976
epoch£º904	 i:2 	 global-step:18082	 l-p:0.1344599574804306
epoch£º904	 i:3 	 global-step:18083	 l-p:0.14064191281795502
epoch£º904	 i:4 	 global-step:18084	 l-p:0.12389381229877472
epoch£º904	 i:5 	 global-step:18085	 l-p:0.2297964245080948
epoch£º904	 i:6 	 global-step:18086	 l-p:0.19706453382968903
epoch£º904	 i:7 	 global-step:18087	 l-p:0.11245676130056381
epoch£º904	 i:8 	 global-step:18088	 l-p:0.2191661149263382
epoch£º904	 i:9 	 global-step:18089	 l-p:0.12123899161815643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7294, 3.7280, 3.7294],
        [3.7294, 3.5169, 3.6330],
        [3.7294, 3.0111, 2.7044],
        [3.7294, 3.1603, 3.1089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.12957020103931427 
model_pd.l_d.mean(): -23.48102569580078 
model_pd.lagr.mean(): -23.351455688476562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1197], device='cuda:0')), ('power', tensor([-23.6007], device='cuda:0'))])
epoch£º905	 i:0 	 global-step:18100	 l-p:0.12957020103931427
epoch£º905	 i:1 	 global-step:18101	 l-p:0.11824075132608414
epoch£º905	 i:2 	 global-step:18102	 l-p:0.08389142155647278
epoch£º905	 i:3 	 global-step:18103	 l-p:0.24751441180706024
epoch£º905	 i:4 	 global-step:18104	 l-p:0.1573597937822342
epoch£º905	 i:5 	 global-step:18105	 l-p:0.13256341218948364
epoch£º905	 i:6 	 global-step:18106	 l-p:0.17585299909114838
epoch£º905	 i:7 	 global-step:18107	 l-p:0.12120404094457626
epoch£º905	 i:8 	 global-step:18108	 l-p:0.11933103948831558
epoch£º905	 i:9 	 global-step:18109	 l-p:0.13332080841064453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7417, 2.9765, 2.5023],
        [3.7417, 2.9730, 2.4716],
        [3.7417, 3.1117, 2.4680],
        [3.7417, 3.7410, 3.7417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.16286513209342957 
model_pd.l_d.mean(): -23.273500442504883 
model_pd.lagr.mean(): -23.11063575744629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1779], device='cuda:0')), ('power', tensor([-23.4514], device='cuda:0'))])
epoch£º906	 i:0 	 global-step:18120	 l-p:0.16286513209342957
epoch£º906	 i:1 	 global-step:18121	 l-p:0.15273134410381317
epoch£º906	 i:2 	 global-step:18122	 l-p:0.12480440735816956
epoch£º906	 i:3 	 global-step:18123	 l-p:0.20801407098770142
epoch£º906	 i:4 	 global-step:18124	 l-p:0.08258785307407379
epoch£º906	 i:5 	 global-step:18125	 l-p:0.1345789134502411
epoch£º906	 i:6 	 global-step:18126	 l-p:0.12750688195228577
epoch£º906	 i:7 	 global-step:18127	 l-p:0.16993850469589233
epoch£º906	 i:8 	 global-step:18128	 l-p:0.10771415382623672
epoch£º906	 i:9 	 global-step:18129	 l-p:0.13980241119861603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7202, 3.6695, 3.7123],
        [3.7202, 3.4738, 3.5941],
        [3.7202, 3.5701, 3.6686],
        [3.7202, 2.9544, 2.4951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.2161552757024765 
model_pd.l_d.mean(): -22.926307678222656 
model_pd.lagr.mean(): -22.71015167236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1953], device='cuda:0')), ('power', tensor([-23.1216], device='cuda:0'))])
epoch£º907	 i:0 	 global-step:18140	 l-p:0.2161552757024765
epoch£º907	 i:1 	 global-step:18141	 l-p:0.12353719770908356
epoch£º907	 i:2 	 global-step:18142	 l-p:0.25845465064048767
epoch£º907	 i:3 	 global-step:18143	 l-p:0.11668836325407028
epoch£º907	 i:4 	 global-step:18144	 l-p:0.1562880128622055
epoch£º907	 i:5 	 global-step:18145	 l-p:0.19442902505397797
epoch£º907	 i:6 	 global-step:18146	 l-p:0.12890875339508057
epoch£º907	 i:7 	 global-step:18147	 l-p:0.09028033912181854
epoch£º907	 i:8 	 global-step:18148	 l-p:0.13724969327449799
epoch£º907	 i:9 	 global-step:18149	 l-p:0.1218726858496666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1810,  0.1024,  1.0000,  0.0579,
          1.0000,  0.5657, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4788,  0.3746,  1.0000,  0.2931,
          1.0000,  0.7823, 31.6228]], device='cuda:0')
 pt:tensor([[3.7054, 3.1952, 2.5737],
        [3.7054, 3.3149, 3.4088],
        [3.7054, 2.9289, 2.3964],
        [3.7054, 2.9296, 2.4128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.20900149643421173 
model_pd.l_d.mean(): -22.76013946533203 
model_pd.lagr.mean(): -22.551137924194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3341], device='cuda:0')), ('power', tensor([-23.0943], device='cuda:0'))])
epoch£º908	 i:0 	 global-step:18160	 l-p:0.20900149643421173
epoch£º908	 i:1 	 global-step:18161	 l-p:0.16768258810043335
epoch£º908	 i:2 	 global-step:18162	 l-p:0.25082719326019287
epoch£º908	 i:3 	 global-step:18163	 l-p:0.12506328523159027
epoch£º908	 i:4 	 global-step:18164	 l-p:0.1437525749206543
epoch£º908	 i:5 	 global-step:18165	 l-p:0.1691301465034485
epoch£º908	 i:6 	 global-step:18166	 l-p:0.14769122004508972
epoch£º908	 i:7 	 global-step:18167	 l-p:0.1236252561211586
epoch£º908	 i:8 	 global-step:18168	 l-p:0.1343124508857727
epoch£º908	 i:9 	 global-step:18169	 l-p:0.15465065836906433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6997, 3.6740, 3.6972],
        [3.6997, 3.6997, 3.6997],
        [3.6997, 2.9437, 2.3271],
        [3.6997, 3.6997, 3.6997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.12852206826210022 
model_pd.l_d.mean(): -23.41319465637207 
model_pd.lagr.mean(): -23.284671783447266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1814], device='cuda:0')), ('power', tensor([-23.5946], device='cuda:0'))])
epoch£º909	 i:0 	 global-step:18180	 l-p:0.12852206826210022
epoch£º909	 i:1 	 global-step:18181	 l-p:0.16709083318710327
epoch£º909	 i:2 	 global-step:18182	 l-p:0.11597294360399246
epoch£º909	 i:3 	 global-step:18183	 l-p:0.157998189330101
epoch£º909	 i:4 	 global-step:18184	 l-p:0.12447360903024673
epoch£º909	 i:5 	 global-step:18185	 l-p:0.32214394211769104
epoch£º909	 i:6 	 global-step:18186	 l-p:0.15550749003887177
epoch£º909	 i:7 	 global-step:18187	 l-p:-0.5230234265327454
epoch£º909	 i:8 	 global-step:18188	 l-p:1.1268454790115356
epoch£º909	 i:9 	 global-step:18189	 l-p:0.11579867452383041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6871, 3.6871, 3.6871],
        [3.6871, 3.6865, 3.6871],
        [3.6871, 3.6871, 3.6871],
        [3.6871, 2.9126, 2.4337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.1392350196838379 
model_pd.l_d.mean(): -22.78763771057129 
model_pd.lagr.mean(): -22.64840316772461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2942], device='cuda:0')), ('power', tensor([-23.0818], device='cuda:0'))])
epoch£º910	 i:0 	 global-step:18200	 l-p:0.1392350196838379
epoch£º910	 i:1 	 global-step:18201	 l-p:0.7395573258399963
epoch£º910	 i:2 	 global-step:18202	 l-p:0.16587421298027039
epoch£º910	 i:3 	 global-step:18203	 l-p:0.1317884773015976
epoch£º910	 i:4 	 global-step:18204	 l-p:0.13906562328338623
epoch£º910	 i:5 	 global-step:18205	 l-p:0.3043268024921417
epoch£º910	 i:6 	 global-step:18206	 l-p:0.10551546514034271
epoch£º910	 i:7 	 global-step:18207	 l-p:0.11815782636404037
epoch£º910	 i:8 	 global-step:18208	 l-p:0.16621115803718567
epoch£º910	 i:9 	 global-step:18209	 l-p:-0.3935120701789856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6833, 3.2638, 3.3446],
        [3.6833, 3.6833, 3.6833],
        [3.6833, 2.9140, 2.3121],
        [3.6833, 3.1913, 3.2226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.18913520872592926 
model_pd.l_d.mean(): -23.61519432067871 
model_pd.lagr.mean(): -23.42605972290039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1680], device='cuda:0')), ('power', tensor([-23.7832], device='cuda:0'))])
epoch£º911	 i:0 	 global-step:18220	 l-p:0.18913520872592926
epoch£º911	 i:1 	 global-step:18221	 l-p:0.15728625655174255
epoch£º911	 i:2 	 global-step:18222	 l-p:0.12128295004367828
epoch£º911	 i:3 	 global-step:18223	 l-p:0.12973977625370026
epoch£º911	 i:4 	 global-step:18224	 l-p:0.12518756091594696
epoch£º911	 i:5 	 global-step:18225	 l-p:0.14011035859584808
epoch£º911	 i:6 	 global-step:18226	 l-p:-1.1834043264389038
epoch£º911	 i:7 	 global-step:18227	 l-p:0.1261340230703354
epoch£º911	 i:8 	 global-step:18228	 l-p:0.17410694062709808
epoch£º911	 i:9 	 global-step:18229	 l-p:-0.018776265904307365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6589, 3.0217, 2.3873],
        [3.6589, 3.5150, 3.6113],
        [3.6589, 2.9052, 2.5278],
        [3.6589, 3.6494, 3.6584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.22962789237499237 
model_pd.l_d.mean(): -22.847293853759766 
model_pd.lagr.mean(): -22.617666244506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3580], device='cuda:0')), ('power', tensor([-23.2053], device='cuda:0'))])
epoch£º912	 i:0 	 global-step:18240	 l-p:0.22962789237499237
epoch£º912	 i:1 	 global-step:18241	 l-p:0.06324635446071625
epoch£º912	 i:2 	 global-step:18242	 l-p:0.027051333338022232
epoch£º912	 i:3 	 global-step:18243	 l-p:0.15032166242599487
epoch£º912	 i:4 	 global-step:18244	 l-p:0.14237132668495178
epoch£º912	 i:5 	 global-step:18245	 l-p:0.09656838327646255
epoch£º912	 i:6 	 global-step:18246	 l-p:0.13445059955120087
epoch£º912	 i:7 	 global-step:18247	 l-p:0.41696494817733765
epoch£º912	 i:8 	 global-step:18248	 l-p:0.13748739659786224
epoch£º912	 i:9 	 global-step:18249	 l-p:0.12577207386493683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6861, 3.6860, 3.6861],
        [3.6861, 2.9355, 2.5580],
        [3.6861, 3.6853, 3.6861],
        [3.6861, 2.9057, 2.3882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.10200994461774826 
model_pd.l_d.mean(): -23.24557113647461 
model_pd.lagr.mean(): -23.1435604095459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2492], device='cuda:0')), ('power', tensor([-23.4948], device='cuda:0'))])
epoch£º913	 i:0 	 global-step:18260	 l-p:0.10200994461774826
epoch£º913	 i:1 	 global-step:18261	 l-p:0.13486766815185547
epoch£º913	 i:2 	 global-step:18262	 l-p:0.12277588248252869
epoch£º913	 i:3 	 global-step:18263	 l-p:0.1298763006925583
epoch£º913	 i:4 	 global-step:18264	 l-p:-0.19859731197357178
epoch£º913	 i:5 	 global-step:18265	 l-p:0.1381722092628479
epoch£º913	 i:6 	 global-step:18266	 l-p:0.1447584629058838
epoch£º913	 i:7 	 global-step:18267	 l-p:0.22069498896598816
epoch£º913	 i:8 	 global-step:18268	 l-p:0.13612769544124603
epoch£º913	 i:9 	 global-step:18269	 l-p:-0.09526805579662323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6550, 3.0389, 2.9378],
        [3.6550, 3.6548, 3.6550],
        [3.6550, 3.5606, 3.6323],
        [3.6550, 2.9669, 2.7479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.126333087682724 
model_pd.l_d.mean(): -23.174823760986328 
model_pd.lagr.mean(): -23.048490524291992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2558], device='cuda:0')), ('power', tensor([-23.4306], device='cuda:0'))])
epoch£º914	 i:0 	 global-step:18280	 l-p:0.126333087682724
epoch£º914	 i:1 	 global-step:18281	 l-p:0.16132239997386932
epoch£º914	 i:2 	 global-step:18282	 l-p:0.15867240726947784
epoch£º914	 i:3 	 global-step:18283	 l-p:0.13164205849170685
epoch£º914	 i:4 	 global-step:18284	 l-p:0.06812970340251923
epoch£º914	 i:5 	 global-step:18285	 l-p:0.14701129496097565
epoch£º914	 i:6 	 global-step:18286	 l-p:0.1504427194595337
epoch£º914	 i:7 	 global-step:18287	 l-p:0.12422247976064682
epoch£º914	 i:8 	 global-step:18288	 l-p:0.1378580629825592
epoch£º914	 i:9 	 global-step:18289	 l-p:0.14756862819194794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6478, 3.5047, 3.6008],
        [3.6478, 3.6439, 3.6477],
        [3.6478, 3.6478, 3.6479],
        [3.6478, 3.6396, 3.6475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.2984916865825653 
model_pd.l_d.mean(): -23.6095027923584 
model_pd.lagr.mean(): -23.311010360717773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2170], device='cuda:0')), ('power', tensor([-23.8265], device='cuda:0'))])
epoch£º915	 i:0 	 global-step:18300	 l-p:0.2984916865825653
epoch£º915	 i:1 	 global-step:18301	 l-p:0.12408026307821274
epoch£º915	 i:2 	 global-step:18302	 l-p:0.1813793182373047
epoch£º915	 i:3 	 global-step:18303	 l-p:0.1331346035003662
epoch£º915	 i:4 	 global-step:18304	 l-p:0.10046511143445969
epoch£º915	 i:5 	 global-step:18305	 l-p:0.11887111514806747
epoch£º915	 i:6 	 global-step:18306	 l-p:0.15043145418167114
epoch£º915	 i:7 	 global-step:18307	 l-p:0.004331240430474281
epoch£º915	 i:8 	 global-step:18308	 l-p:0.1069423258304596
epoch£º915	 i:9 	 global-step:18309	 l-p:0.12196674197912216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6541, 3.2642, 2.6738],
        [3.6541, 3.6541, 3.6541],
        [3.6541, 3.6067, 3.6471],
        [3.6541, 2.8750, 2.4063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.13192379474639893 
model_pd.l_d.mean(): -23.150836944580078 
model_pd.lagr.mean(): -23.01891326904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1981], device='cuda:0')), ('power', tensor([-23.3489], device='cuda:0'))])
epoch£º916	 i:0 	 global-step:18320	 l-p:0.13192379474639893
epoch£º916	 i:1 	 global-step:18321	 l-p:0.038607124239206314
epoch£º916	 i:2 	 global-step:18322	 l-p:0.0914042666554451
epoch£º916	 i:3 	 global-step:18323	 l-p:0.19227488338947296
epoch£º916	 i:4 	 global-step:18324	 l-p:0.12152490019798279
epoch£º916	 i:5 	 global-step:18325	 l-p:0.24585214257240295
epoch£º916	 i:6 	 global-step:18326	 l-p:0.17857219278812408
epoch£º916	 i:7 	 global-step:18327	 l-p:0.13306914269924164
epoch£º916	 i:8 	 global-step:18328	 l-p:0.12287471443414688
epoch£º916	 i:9 	 global-step:18329	 l-p:-0.08293300122022629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6648, 3.5846, 3.6476],
        [3.6648, 3.6648, 3.6648],
        [3.6648, 3.1379, 2.5183],
        [3.6648, 3.5326, 3.6239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.12720634043216705 
model_pd.l_d.mean(): -23.712909698486328 
model_pd.lagr.mean(): -23.585702896118164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1160], device='cuda:0')), ('power', tensor([-23.8289], device='cuda:0'))])
epoch£º917	 i:0 	 global-step:18340	 l-p:0.12720634043216705
epoch£º917	 i:1 	 global-step:18341	 l-p:-0.10707405209541321
epoch£º917	 i:2 	 global-step:18342	 l-p:0.12034959346055984
epoch£º917	 i:3 	 global-step:18343	 l-p:0.039373721927404404
epoch£º917	 i:4 	 global-step:18344	 l-p:0.1326197236776352
epoch£º917	 i:5 	 global-step:18345	 l-p:0.11951307207345963
epoch£º917	 i:6 	 global-step:18346	 l-p:0.23489375412464142
epoch£º917	 i:7 	 global-step:18347	 l-p:0.14572526514530182
epoch£º917	 i:8 	 global-step:18348	 l-p:0.11503255367279053
epoch£º917	 i:9 	 global-step:18349	 l-p:0.06820561736822128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6556, 3.6556, 3.6556],
        [3.6556, 3.6303, 3.6531],
        [3.6556, 3.6556, 3.6556],
        [3.6556, 2.8777, 2.2802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): -0.0037777042016386986 
model_pd.l_d.mean(): -22.20391273498535 
model_pd.lagr.mean(): -22.207691192626953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3801], device='cuda:0')), ('power', tensor([-22.5840], device='cuda:0'))])
epoch£º918	 i:0 	 global-step:18360	 l-p:-0.0037777042016386986
epoch£º918	 i:1 	 global-step:18361	 l-p:0.056585609912872314
epoch£º918	 i:2 	 global-step:18362	 l-p:0.08904869854450226
epoch£º918	 i:3 	 global-step:18363	 l-p:0.1878374069929123
epoch£º918	 i:4 	 global-step:18364	 l-p:0.15011180937290192
epoch£º918	 i:5 	 global-step:18365	 l-p:0.13178834319114685
epoch£º918	 i:6 	 global-step:18366	 l-p:0.13082453608512878
epoch£º918	 i:7 	 global-step:18367	 l-p:0.14681307971477509
epoch£º918	 i:8 	 global-step:18368	 l-p:0.16148991882801056
epoch£º918	 i:9 	 global-step:18369	 l-p:0.11789350211620331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228]], device='cuda:0')
 pt:tensor([[3.6658, 3.2208, 2.6169],
        [3.6658, 2.9296, 2.2972],
        [3.6658, 2.9937, 2.3556],
        [3.6658, 3.2378, 3.3156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.13482865691184998 
model_pd.l_d.mean(): -22.85942840576172 
model_pd.lagr.mean(): -22.724599838256836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2639], device='cuda:0')), ('power', tensor([-23.1233], device='cuda:0'))])
epoch£º919	 i:0 	 global-step:18380	 l-p:0.13482865691184998
epoch£º919	 i:1 	 global-step:18381	 l-p:0.15902668237686157
epoch£º919	 i:2 	 global-step:18382	 l-p:0.14281803369522095
epoch£º919	 i:3 	 global-step:18383	 l-p:0.11232315748929977
epoch£º919	 i:4 	 global-step:18384	 l-p:0.14376181364059448
epoch£º919	 i:5 	 global-step:18385	 l-p:0.11802516877651215
epoch£º919	 i:6 	 global-step:18386	 l-p:0.08515306562185287
epoch£º919	 i:7 	 global-step:18387	 l-p:0.09679268300533295
epoch£º919	 i:8 	 global-step:18388	 l-p:-0.20847190916538239
epoch£º919	 i:9 	 global-step:18389	 l-p:0.23416712880134583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6276, 2.8370, 2.3189],
        [3.6276, 3.6276, 3.6276],
        [3.6276, 3.1856, 3.2569],
        [3.6276, 3.0706, 3.0460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.12464743107557297 
model_pd.l_d.mean(): -23.11614227294922 
model_pd.lagr.mean(): -22.99149513244629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2753], device='cuda:0')), ('power', tensor([-23.3915], device='cuda:0'))])
epoch£º920	 i:0 	 global-step:18400	 l-p:0.12464743107557297
epoch£º920	 i:1 	 global-step:18401	 l-p:0.04262392967939377
epoch£º920	 i:2 	 global-step:18402	 l-p:0.14202472567558289
epoch£º920	 i:3 	 global-step:18403	 l-p:0.25626182556152344
epoch£º920	 i:4 	 global-step:18404	 l-p:0.11867202818393707
epoch£º920	 i:5 	 global-step:18405	 l-p:0.20374107360839844
epoch£º920	 i:6 	 global-step:18406	 l-p:0.133196622133255
epoch£º920	 i:7 	 global-step:18407	 l-p:0.10083192586898804
epoch£º920	 i:8 	 global-step:18408	 l-p:0.15979059040546417
epoch£º920	 i:9 	 global-step:18409	 l-p:-0.140766441822052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6244, 3.6244, 3.6244],
        [3.6244, 2.8501, 2.2398],
        [3.6244, 3.4790, 3.5762],
        [3.6244, 3.6244, 3.6244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.12592314183712006 
model_pd.l_d.mean(): -22.74858856201172 
model_pd.lagr.mean(): -22.622665405273438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3444], device='cuda:0')), ('power', tensor([-23.0930], device='cuda:0'))])
epoch£º921	 i:0 	 global-step:18420	 l-p:0.12592314183712006
epoch£º921	 i:1 	 global-step:18421	 l-p:0.08930502086877823
epoch£º921	 i:2 	 global-step:18422	 l-p:0.15028494596481323
epoch£º921	 i:3 	 global-step:18423	 l-p:-1.0006258487701416
epoch£º921	 i:4 	 global-step:18424	 l-p:0.13797397911548615
epoch£º921	 i:5 	 global-step:18425	 l-p:0.20762774348258972
epoch£º921	 i:6 	 global-step:18426	 l-p:0.12542138993740082
epoch£º921	 i:7 	 global-step:18427	 l-p:0.12222043424844742
epoch£º921	 i:8 	 global-step:18428	 l-p:0.02637677639722824
epoch£º921	 i:9 	 global-step:18429	 l-p:0.13922980427742004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6571, 3.6571, 3.6571],
        [3.6571, 3.2628, 3.3581],
        [3.6571, 3.1462, 2.5304],
        [3.6571, 3.0125, 2.3772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.14109660685062408 
model_pd.l_d.mean(): -23.488786697387695 
model_pd.lagr.mean(): -23.34769058227539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2095], device='cuda:0')), ('power', tensor([-23.6983], device='cuda:0'))])
epoch£º922	 i:0 	 global-step:18440	 l-p:0.14109660685062408
epoch£º922	 i:1 	 global-step:18441	 l-p:-0.002040915424004197
epoch£º922	 i:2 	 global-step:18442	 l-p:0.12988319993019104
epoch£º922	 i:3 	 global-step:18443	 l-p:0.10648935288190842
epoch£º922	 i:4 	 global-step:18444	 l-p:0.041051141917705536
epoch£º922	 i:5 	 global-step:18445	 l-p:0.17742568254470825
epoch£º922	 i:6 	 global-step:18446	 l-p:0.1641063392162323
epoch£º922	 i:7 	 global-step:18447	 l-p:0.1462397426366806
epoch£º922	 i:8 	 global-step:18448	 l-p:0.030148109421133995
epoch£º922	 i:9 	 global-step:18449	 l-p:0.18491889536380768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6795, 2.8989, 2.4083],
        [3.6795, 3.1679, 3.1843],
        [3.6795, 3.3796, 3.5000],
        [3.6795, 3.6750, 3.6793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): -0.42370131611824036 
model_pd.l_d.mean(): -23.453758239746094 
model_pd.lagr.mean(): -23.877460479736328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1790], device='cuda:0')), ('power', tensor([-23.6328], device='cuda:0'))])
epoch£º923	 i:0 	 global-step:18460	 l-p:-0.42370131611824036
epoch£º923	 i:1 	 global-step:18461	 l-p:0.14811749756336212
epoch£º923	 i:2 	 global-step:18462	 l-p:0.16201792657375336
epoch£º923	 i:3 	 global-step:18463	 l-p:0.15047809481620789
epoch£º923	 i:4 	 global-step:18464	 l-p:0.21197614073753357
epoch£º923	 i:5 	 global-step:18465	 l-p:0.1242656335234642
epoch£º923	 i:6 	 global-step:18466	 l-p:0.09940872341394424
epoch£º923	 i:7 	 global-step:18467	 l-p:0.13510355353355408
epoch£º923	 i:8 	 global-step:18468	 l-p:0.1908133625984192
epoch£º923	 i:9 	 global-step:18469	 l-p:0.14243248105049133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7295, 3.3021, 3.3776],
        [3.7295, 2.9539, 2.4463],
        [3.7295, 3.6169, 3.6985],
        [3.7295, 3.4290, 3.5484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.2540183663368225 
model_pd.l_d.mean(): -23.305824279785156 
model_pd.lagr.mean(): -23.05180549621582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2222], device='cuda:0')), ('power', tensor([-23.5280], device='cuda:0'))])
epoch£º924	 i:0 	 global-step:18480	 l-p:0.2540183663368225
epoch£º924	 i:1 	 global-step:18481	 l-p:0.11065983772277832
epoch£º924	 i:2 	 global-step:18482	 l-p:0.1292494386434555
epoch£º924	 i:3 	 global-step:18483	 l-p:0.12366193532943726
epoch£º924	 i:4 	 global-step:18484	 l-p:0.12019208818674088
epoch£º924	 i:5 	 global-step:18485	 l-p:0.12202123552560806
epoch£º924	 i:6 	 global-step:18486	 l-p:0.14637698233127594
epoch£º924	 i:7 	 global-step:18487	 l-p:0.14891916513442993
epoch£º924	 i:8 	 global-step:18488	 l-p:0.1293392777442932
epoch£º924	 i:9 	 global-step:18489	 l-p:0.18137863278388977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7165, 3.7163, 3.7165],
        [3.7165, 3.0019, 2.3606],
        [3.7165, 2.9427, 2.3541],
        [3.7165, 2.9406, 2.4499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.14242927730083466 
model_pd.l_d.mean(): -23.449771881103516 
model_pd.lagr.mean(): -23.307342529296875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1864], device='cuda:0')), ('power', tensor([-23.6362], device='cuda:0'))])
epoch£º925	 i:0 	 global-step:18500	 l-p:0.14242927730083466
epoch£º925	 i:1 	 global-step:18501	 l-p:0.20852228999137878
epoch£º925	 i:2 	 global-step:18502	 l-p:0.12095487117767334
epoch£º925	 i:3 	 global-step:18503	 l-p:0.140075221657753
epoch£º925	 i:4 	 global-step:18504	 l-p:0.10753372311592102
epoch£º925	 i:5 	 global-step:18505	 l-p:0.835821270942688
epoch£º925	 i:6 	 global-step:18506	 l-p:0.12484670430421829
epoch£º925	 i:7 	 global-step:18507	 l-p:0.11716810613870621
epoch£º925	 i:8 	 global-step:18508	 l-p:0.1273292452096939
epoch£º925	 i:9 	 global-step:18509	 l-p:0.21391522884368896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6651, 3.6650, 3.6651],
        [3.6651, 3.0067, 2.8426],
        [3.6651, 2.9915, 2.8014],
        [3.6651, 2.9081, 2.2836]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.12782178819179535 
model_pd.l_d.mean(): -23.13230323791504 
model_pd.lagr.mean(): -23.00448226928711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1912], device='cuda:0')), ('power', tensor([-23.3235], device='cuda:0'))])
epoch£º926	 i:0 	 global-step:18520	 l-p:0.12782178819179535
epoch£º926	 i:1 	 global-step:18521	 l-p:0.1263805329799652
epoch£º926	 i:2 	 global-step:18522	 l-p:0.013248752802610397
epoch£º926	 i:3 	 global-step:18523	 l-p:0.27886804938316345
epoch£º926	 i:4 	 global-step:18524	 l-p:0.16887106001377106
epoch£º926	 i:5 	 global-step:18525	 l-p:0.04509272798895836
epoch£º926	 i:6 	 global-step:18526	 l-p:0.06694386899471283
epoch£º926	 i:7 	 global-step:18527	 l-p:0.10961493849754333
epoch£º926	 i:8 	 global-step:18528	 l-p:0.15995989739894867
epoch£º926	 i:9 	 global-step:18529	 l-p:0.1738089770078659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6729, 3.6717, 3.6729],
        [3.6729, 3.5295, 3.6258],
        [3.6729, 3.6654, 3.6725],
        [3.6729, 3.5051, 3.6105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.1249062791466713 
model_pd.l_d.mean(): -22.34442901611328 
model_pd.lagr.mean(): -22.21952247619629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2868], device='cuda:0')), ('power', tensor([-22.6312], device='cuda:0'))])
epoch£º927	 i:0 	 global-step:18540	 l-p:0.1249062791466713
epoch£º927	 i:1 	 global-step:18541	 l-p:-0.6903403401374817
epoch£º927	 i:2 	 global-step:18542	 l-p:0.12669086456298828
epoch£º927	 i:3 	 global-step:18543	 l-p:0.12723521888256073
epoch£º927	 i:4 	 global-step:18544	 l-p:-0.027139395475387573
epoch£º927	 i:5 	 global-step:18545	 l-p:0.19164863228797913
epoch£º927	 i:6 	 global-step:18546	 l-p:0.15685400366783142
epoch£º927	 i:7 	 global-step:18547	 l-p:0.15259912610054016
epoch£º927	 i:8 	 global-step:18548	 l-p:-2.2323312759399414
epoch£º927	 i:9 	 global-step:18549	 l-p:0.11829306930303574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6892, 3.6892, 3.6892],
        [3.6892, 3.4777, 3.5946],
        [3.6892, 3.6892, 3.6892],
        [3.6892, 3.6860, 3.6891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.3211829960346222 
model_pd.l_d.mean(): -23.40599822998047 
model_pd.lagr.mean(): -23.084815979003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1673], device='cuda:0')), ('power', tensor([-23.5733], device='cuda:0'))])
epoch£º928	 i:0 	 global-step:18560	 l-p:0.3211829960346222
epoch£º928	 i:1 	 global-step:18561	 l-p:0.15301483869552612
epoch£º928	 i:2 	 global-step:18562	 l-p:0.14198970794677734
epoch£º928	 i:3 	 global-step:18563	 l-p:0.11663713306188583
epoch£º928	 i:4 	 global-step:18564	 l-p:0.15185928344726562
epoch£º928	 i:5 	 global-step:18565	 l-p:0.10596248507499695
epoch£º928	 i:6 	 global-step:18566	 l-p:0.37323132157325745
epoch£º928	 i:7 	 global-step:18567	 l-p:0.25672438740730286
epoch£º928	 i:8 	 global-step:18568	 l-p:0.13112646341323853
epoch£º928	 i:9 	 global-step:18569	 l-p:0.1519308090209961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7088, 3.7088, 3.7088],
        [3.7088, 3.6174, 3.6873],
        [3.7088, 3.0117, 2.7689],
        [3.7088, 3.7088, 3.7088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.1314406841993332 
model_pd.l_d.mean(): -23.529632568359375 
model_pd.lagr.mean(): -23.398191452026367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1528], device='cuda:0')), ('power', tensor([-23.6824], device='cuda:0'))])
epoch£º929	 i:0 	 global-step:18580	 l-p:0.1314406841993332
epoch£º929	 i:1 	 global-step:18581	 l-p:0.30168166756629944
epoch£º929	 i:2 	 global-step:18582	 l-p:0.14816035330295563
epoch£º929	 i:3 	 global-step:18583	 l-p:0.15159723162651062
epoch£º929	 i:4 	 global-step:18584	 l-p:0.3367736339569092
epoch£º929	 i:5 	 global-step:18585	 l-p:0.13181719183921814
epoch£º929	 i:6 	 global-step:18586	 l-p:0.1379796862602234
epoch£º929	 i:7 	 global-step:18587	 l-p:0.10307609289884567
epoch£º929	 i:8 	 global-step:18588	 l-p:0.35679492354393005
epoch£º929	 i:9 	 global-step:18589	 l-p:0.12625566124916077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6853, 3.6703, 3.6843],
        [3.6853, 2.9110, 2.3067],
        [3.6853, 3.6485, 3.6807],
        [3.6853, 3.6832, 3.6853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.16584041714668274 
model_pd.l_d.mean(): -23.48748207092285 
model_pd.lagr.mean(): -23.32164192199707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1856], device='cuda:0')), ('power', tensor([-23.6731], device='cuda:0'))])
epoch£º930	 i:0 	 global-step:18600	 l-p:0.16584041714668274
epoch£º930	 i:1 	 global-step:18601	 l-p:0.1095619946718216
epoch£º930	 i:2 	 global-step:18602	 l-p:0.12916354835033417
epoch£º930	 i:3 	 global-step:18603	 l-p:0.1369156390428543
epoch£º930	 i:4 	 global-step:18604	 l-p:0.14345669746398926
epoch£º930	 i:5 	 global-step:18605	 l-p:0.1303452104330063
epoch£º930	 i:6 	 global-step:18606	 l-p:0.13206779956817627
epoch£º930	 i:7 	 global-step:18607	 l-p:0.15397611260414124
epoch£º930	 i:8 	 global-step:18608	 l-p:0.010492882691323757
epoch£º930	 i:9 	 global-step:18609	 l-p:0.02963111735880375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6514, 3.6474, 3.6512],
        [3.6514, 2.9803, 2.7987],
        [3.6514, 3.6513, 3.6514],
        [3.6514, 3.4355, 3.5536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.2040235847234726 
model_pd.l_d.mean(): -23.329242706298828 
model_pd.lagr.mean(): -23.125219345092773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2076], device='cuda:0')), ('power', tensor([-23.5369], device='cuda:0'))])
epoch£º931	 i:0 	 global-step:18620	 l-p:0.2040235847234726
epoch£º931	 i:1 	 global-step:18621	 l-p:0.34899088740348816
epoch£º931	 i:2 	 global-step:18622	 l-p:0.06251939386129379
epoch£º931	 i:3 	 global-step:18623	 l-p:0.0851442962884903
epoch£º931	 i:4 	 global-step:18624	 l-p:0.1179511547088623
epoch£º931	 i:5 	 global-step:18625	 l-p:0.1255679726600647
epoch£º931	 i:6 	 global-step:18626	 l-p:-0.06606052070856094
epoch£º931	 i:7 	 global-step:18627	 l-p:0.13153129816055298
epoch£º931	 i:8 	 global-step:18628	 l-p:0.14232119917869568
epoch£º931	 i:9 	 global-step:18629	 l-p:0.10672777146100998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6605, 2.8776, 2.2829],
        [3.6605, 3.4978, 3.6016],
        [3.6605, 3.1768, 3.2192],
        [3.6605, 3.6351, 3.6581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.13395556807518005 
model_pd.l_d.mean(): -22.623855590820312 
model_pd.lagr.mean(): -22.489900588989258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3380], device='cuda:0')), ('power', tensor([-22.9619], device='cuda:0'))])
epoch£º932	 i:0 	 global-step:18640	 l-p:0.13395556807518005
epoch£º932	 i:1 	 global-step:18641	 l-p:0.06848286837339401
epoch£º932	 i:2 	 global-step:18642	 l-p:0.034533899277448654
epoch£º932	 i:3 	 global-step:18643	 l-p:0.15820857882499695
epoch£º932	 i:4 	 global-step:18644	 l-p:0.09935496747493744
epoch£º932	 i:5 	 global-step:18645	 l-p:0.3143942654132843
epoch£º932	 i:6 	 global-step:18646	 l-p:0.12364057451486588
epoch£º932	 i:7 	 global-step:18647	 l-p:0.1571216583251953
epoch£º932	 i:8 	 global-step:18648	 l-p:0.1313966065645218
epoch£º932	 i:9 	 global-step:18649	 l-p:0.1582128256559372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6549, 2.8744, 2.2711],
        [3.6549, 3.0962, 3.0700],
        [3.6549, 2.9388, 2.6703],
        [3.6549, 3.6549, 3.6549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.05431336164474487 
model_pd.l_d.mean(): -23.554466247558594 
model_pd.lagr.mean(): -23.500152587890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2083], device='cuda:0')), ('power', tensor([-23.7627], device='cuda:0'))])
epoch£º933	 i:0 	 global-step:18660	 l-p:0.05431336164474487
epoch£º933	 i:1 	 global-step:18661	 l-p:0.2248377799987793
epoch£º933	 i:2 	 global-step:18662	 l-p:0.12824296951293945
epoch£º933	 i:3 	 global-step:18663	 l-p:0.12355531752109528
epoch£º933	 i:4 	 global-step:18664	 l-p:0.2330222874879837
epoch£º933	 i:5 	 global-step:18665	 l-p:-0.1705014705657959
epoch£º933	 i:6 	 global-step:18666	 l-p:0.12041743099689484
epoch£º933	 i:7 	 global-step:18667	 l-p:0.1304127424955368
epoch£º933	 i:8 	 global-step:18668	 l-p:-0.042288750410079956
epoch£º933	 i:9 	 global-step:18669	 l-p:0.13439133763313293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6774, 3.6774, 3.6774],
        [3.6774, 3.6774, 3.6774],
        [3.6774, 3.3656, 3.4852],
        [3.6774, 3.4495, 3.5693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.1385933756828308 
model_pd.l_d.mean(): -23.19270133972168 
model_pd.lagr.mean(): -23.054107666015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2328], device='cuda:0')), ('power', tensor([-23.4255], device='cuda:0'))])
epoch£º934	 i:0 	 global-step:18680	 l-p:0.1385933756828308
epoch£º934	 i:1 	 global-step:18681	 l-p:0.12831424176692963
epoch£º934	 i:2 	 global-step:18682	 l-p:0.13995490968227386
epoch£º934	 i:3 	 global-step:18683	 l-p:0.11698158085346222
epoch£º934	 i:4 	 global-step:18684	 l-p:0.179198756814003
epoch£º934	 i:5 	 global-step:18685	 l-p:0.05093305557966232
epoch£º934	 i:6 	 global-step:18686	 l-p:-0.04531646519899368
epoch£º934	 i:7 	 global-step:18687	 l-p:0.11295400559902191
epoch£º934	 i:8 	 global-step:18688	 l-p:0.21435073018074036
epoch£º934	 i:9 	 global-step:18689	 l-p:-0.02641308680176735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6761, 2.8891, 2.3797],
        [3.6761, 3.6493, 3.6734],
        [3.6761, 3.6617, 3.6751],
        [3.6761, 3.3727, 3.4935]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): -0.026856640353798866 
model_pd.l_d.mean(): -23.468719482421875 
model_pd.lagr.mean(): -23.495576858520508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2189], device='cuda:0')), ('power', tensor([-23.6876], device='cuda:0'))])
epoch£º935	 i:0 	 global-step:18700	 l-p:-0.026856640353798866
epoch£º935	 i:1 	 global-step:18701	 l-p:0.11958091706037521
epoch£º935	 i:2 	 global-step:18702	 l-p:0.11984525620937347
epoch£º935	 i:3 	 global-step:18703	 l-p:0.1263057440519333
epoch£º935	 i:4 	 global-step:18704	 l-p:0.17850171029567719
epoch£º935	 i:5 	 global-step:18705	 l-p:0.12840689718723297
epoch£º935	 i:6 	 global-step:18706	 l-p:0.49183881282806396
epoch£º935	 i:7 	 global-step:18707	 l-p:0.17488908767700195
epoch£º935	 i:8 	 global-step:18708	 l-p:1.1768100261688232
epoch£º935	 i:9 	 global-step:18709	 l-p:0.13246913254261017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7007, 3.4856, 3.6032],
        [3.7007, 3.6391, 3.6898],
        [3.7007, 3.0737, 2.9565],
        [3.7007, 3.0889, 2.4502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.16373872756958008 
model_pd.l_d.mean(): -23.018104553222656 
model_pd.lagr.mean(): -22.854366302490234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2143], device='cuda:0')), ('power', tensor([-23.2324], device='cuda:0'))])
epoch£º936	 i:0 	 global-step:18720	 l-p:0.16373872756958008
epoch£º936	 i:1 	 global-step:18721	 l-p:0.16025184094905853
epoch£º936	 i:2 	 global-step:18722	 l-p:0.13556720316410065
epoch£º936	 i:3 	 global-step:18723	 l-p:0.12601284682750702
epoch£º936	 i:4 	 global-step:18724	 l-p:0.12266576290130615
epoch£º936	 i:5 	 global-step:18725	 l-p:0.2348521202802658
epoch£º936	 i:6 	 global-step:18726	 l-p:0.14978180825710297
epoch£º936	 i:7 	 global-step:18727	 l-p:0.11705638468265533
epoch£º936	 i:8 	 global-step:18728	 l-p:0.11662586033344269
epoch£º936	 i:9 	 global-step:18729	 l-p:0.15493559837341309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7334, 3.1650, 3.1208],
        [3.7334, 3.0476, 2.4003],
        [3.7334, 3.2123, 3.2181],
        [3.7334, 3.7328, 3.7334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.17081120610237122 
model_pd.l_d.mean(): -23.60004425048828 
model_pd.lagr.mean(): -23.42923355102539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1434], device='cuda:0')), ('power', tensor([-23.7435], device='cuda:0'))])
epoch£º937	 i:0 	 global-step:18740	 l-p:0.17081120610237122
epoch£º937	 i:1 	 global-step:18741	 l-p:0.16180011630058289
epoch£º937	 i:2 	 global-step:18742	 l-p:0.12624968588352203
epoch£º937	 i:3 	 global-step:18743	 l-p:0.11085794121026993
epoch£º937	 i:4 	 global-step:18744	 l-p:0.1939561367034912
epoch£º937	 i:5 	 global-step:18745	 l-p:0.12098199874162674
epoch£º937	 i:6 	 global-step:18746	 l-p:0.1817581206560135
epoch£º937	 i:7 	 global-step:18747	 l-p:0.1513444185256958
epoch£º937	 i:8 	 global-step:18748	 l-p:0.11478541791439056
epoch£º937	 i:9 	 global-step:18749	 l-p:0.08291585743427277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7413, 3.7412, 3.7413],
        [3.7413, 3.6852, 3.7320],
        [3.7413, 3.0592, 2.8430],
        [3.7413, 3.4857, 3.6073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.11970885097980499 
model_pd.l_d.mean(): -22.813879013061523 
model_pd.lagr.mean(): -22.694169998168945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1930], device='cuda:0')), ('power', tensor([-23.0069], device='cuda:0'))])
epoch£º938	 i:0 	 global-step:18760	 l-p:0.11970885097980499
epoch£º938	 i:1 	 global-step:18761	 l-p:0.12578065693378448
epoch£º938	 i:2 	 global-step:18762	 l-p:0.12004271149635315
epoch£º938	 i:3 	 global-step:18763	 l-p:0.21278390288352966
epoch£º938	 i:4 	 global-step:18764	 l-p:0.1315523386001587
epoch£º938	 i:5 	 global-step:18765	 l-p:0.11779989302158356
epoch£º938	 i:6 	 global-step:18766	 l-p:0.1890297681093216
epoch£º938	 i:7 	 global-step:18767	 l-p:0.14969158172607422
epoch£º938	 i:8 	 global-step:18768	 l-p:0.2083609402179718
epoch£º938	 i:9 	 global-step:18769	 l-p:0.14129631221294403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6033,  0.5098,  1.0000,  0.4308,
          1.0000,  0.8450, 31.6228]], device='cuda:0')
 pt:tensor([[3.7266, 3.1988, 2.5698],
        [3.7266, 2.9447, 2.4050],
        [3.7266, 3.0690, 2.9003],
        [3.7266, 2.9775, 2.3463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.1352185308933258 
model_pd.l_d.mean(): -23.23554039001465 
model_pd.lagr.mean(): -23.100322723388672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2225], device='cuda:0')), ('power', tensor([-23.4581], device='cuda:0'))])
epoch£º939	 i:0 	 global-step:18780	 l-p:0.1352185308933258
epoch£º939	 i:1 	 global-step:18781	 l-p:0.1521153748035431
epoch£º939	 i:2 	 global-step:18782	 l-p:0.1173848956823349
epoch£º939	 i:3 	 global-step:18783	 l-p:0.13522355258464813
epoch£º939	 i:4 	 global-step:18784	 l-p:0.21660472452640533
epoch£º939	 i:5 	 global-step:18785	 l-p:0.141279935836792
epoch£º939	 i:6 	 global-step:18786	 l-p:0.19913291931152344
epoch£º939	 i:7 	 global-step:18787	 l-p:0.13773049414157867
epoch£º939	 i:8 	 global-step:18788	 l-p:0.12853212654590607
epoch£º939	 i:9 	 global-step:18789	 l-p:0.17184986174106598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7136, 3.7116, 3.7135],
        [3.7136, 2.9295, 2.3897],
        [3.7136, 3.6869, 3.7109],
        [3.7136, 3.5692, 3.6658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.12102130800485611 
model_pd.l_d.mean(): -22.94464683532715 
model_pd.lagr.mean(): -22.823625564575195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2770], device='cuda:0')), ('power', tensor([-23.2216], device='cuda:0'))])
epoch£º940	 i:0 	 global-step:18800	 l-p:0.12102130800485611
epoch£º940	 i:1 	 global-step:18801	 l-p:0.1392105668783188
epoch£º940	 i:2 	 global-step:18802	 l-p:0.12860795855522156
epoch£º940	 i:3 	 global-step:18803	 l-p:0.3550321161746979
epoch£º940	 i:4 	 global-step:18804	 l-p:0.13448458909988403
epoch£º940	 i:5 	 global-step:18805	 l-p:0.09147482365369797
epoch£º940	 i:6 	 global-step:18806	 l-p:0.13784611225128174
epoch£º940	 i:7 	 global-step:18807	 l-p:0.015215725637972355
epoch£º940	 i:8 	 global-step:18808	 l-p:-0.024881968274712563
epoch£º940	 i:9 	 global-step:18809	 l-p:0.21787360310554504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6648, 2.8840, 2.2793],
        [3.6648, 3.6645, 3.6648],
        [3.6648, 3.0627, 2.9854],
        [3.6648, 2.8881, 2.4504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): -0.028716010972857475 
model_pd.l_d.mean(): -22.94156837463379 
model_pd.lagr.mean(): -22.97028350830078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2670], device='cuda:0')), ('power', tensor([-23.2085], device='cuda:0'))])
epoch£º941	 i:0 	 global-step:18820	 l-p:-0.028716010972857475
epoch£º941	 i:1 	 global-step:18821	 l-p:0.1287815123796463
epoch£º941	 i:2 	 global-step:18822	 l-p:0.13295738399028778
epoch£º941	 i:3 	 global-step:18823	 l-p:0.17420020699501038
epoch£º941	 i:4 	 global-step:18824	 l-p:0.13774165511131287
epoch£º941	 i:5 	 global-step:18825	 l-p:0.15345773100852966
epoch£º941	 i:6 	 global-step:18826	 l-p:0.05350210890173912
epoch£º941	 i:7 	 global-step:18827	 l-p:0.11360820382833481
epoch£º941	 i:8 	 global-step:18828	 l-p:0.24607571959495544
epoch£º941	 i:9 	 global-step:18829	 l-p:0.18359656631946564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6511, 3.2432, 3.3343],
        [3.6511, 3.2103, 2.6080],
        [3.6511, 2.8590, 2.3486],
        [3.6511, 3.6415, 3.6506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.12077043205499649 
model_pd.l_d.mean(): -23.353721618652344 
model_pd.lagr.mean(): -23.232952117919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1815], device='cuda:0')), ('power', tensor([-23.5352], device='cuda:0'))])
epoch£º942	 i:0 	 global-step:18840	 l-p:0.12077043205499649
epoch£º942	 i:1 	 global-step:18841	 l-p:0.14482860267162323
epoch£º942	 i:2 	 global-step:18842	 l-p:0.21583400666713715
epoch£º942	 i:3 	 global-step:18843	 l-p:0.1482102870941162
epoch£º942	 i:4 	 global-step:18844	 l-p:0.1327056735754013
epoch£º942	 i:5 	 global-step:18845	 l-p:0.05924360454082489
epoch£º942	 i:6 	 global-step:18846	 l-p:-2.08016037940979
epoch£º942	 i:7 	 global-step:18847	 l-p:0.12322346121072769
epoch£º942	 i:8 	 global-step:18848	 l-p:0.12088453024625778
epoch£º942	 i:9 	 global-step:18849	 l-p:0.11098247021436691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6390, 3.6390, 3.6390],
        [3.6390, 3.0344, 2.9572],
        [3.6390, 3.6390, 3.6390],
        [3.6390, 3.6390, 3.6390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.12313112616539001 
model_pd.l_d.mean(): -23.394107818603516 
model_pd.lagr.mean(): -23.270977020263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1598], device='cuda:0')), ('power', tensor([-23.5539], device='cuda:0'))])
epoch£º943	 i:0 	 global-step:18860	 l-p:0.12313112616539001
epoch£º943	 i:1 	 global-step:18861	 l-p:0.13572555780410767
epoch£º943	 i:2 	 global-step:18862	 l-p:0.13605064153671265
epoch£º943	 i:3 	 global-step:18863	 l-p:0.20033027231693268
epoch£º943	 i:4 	 global-step:18864	 l-p:0.08253791928291321
epoch£º943	 i:5 	 global-step:18865	 l-p:0.18731604516506195
epoch£º943	 i:6 	 global-step:18866	 l-p:0.12842658162117004
epoch£º943	 i:7 	 global-step:18867	 l-p:0.13113592565059662
epoch£º943	 i:8 	 global-step:18868	 l-p:0.7254254221916199
epoch£º943	 i:9 	 global-step:18869	 l-p:0.10673277825117111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6128,  0.5205,  1.0000,  0.4421,
          1.0000,  0.8494, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1755,  0.0983,  1.0000,  0.0550,
          1.0000,  0.5599, 31.6228]], device='cuda:0')
 pt:tensor([[3.6526, 2.8812, 2.4704],
        [3.6526, 2.8939, 2.2659],
        [3.6526, 3.1091, 3.1006],
        [3.6526, 3.2699, 3.3718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.06942576915025711 
model_pd.l_d.mean(): -23.78775978088379 
model_pd.lagr.mean(): -23.718334197998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1272], device='cuda:0')), ('power', tensor([-23.9150], device='cuda:0'))])
epoch£º944	 i:0 	 global-step:18880	 l-p:0.06942576915025711
epoch£º944	 i:1 	 global-step:18881	 l-p:0.03256823867559433
epoch£º944	 i:2 	 global-step:18882	 l-p:0.2227175533771515
epoch£º944	 i:3 	 global-step:18883	 l-p:-1.307502269744873
epoch£º944	 i:4 	 global-step:18884	 l-p:0.11367383599281311
epoch£º944	 i:5 	 global-step:18885	 l-p:0.15432733297348022
epoch£º944	 i:6 	 global-step:18886	 l-p:0.1317671686410904
epoch£º944	 i:7 	 global-step:18887	 l-p:0.13103732466697693
epoch£º944	 i:8 	 global-step:18888	 l-p:0.08877665549516678
epoch£º944	 i:9 	 global-step:18889	 l-p:0.12346801161766052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7229, 2.9537, 2.3419],
        [3.7229, 3.7153, 3.7225],
        [3.7229, 3.5385, 3.6490],
        [3.7229, 3.7228, 3.7229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.14902690052986145 
model_pd.l_d.mean(): -22.934560775756836 
model_pd.lagr.mean(): -22.785533905029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1696], device='cuda:0')), ('power', tensor([-23.1042], device='cuda:0'))])
epoch£º945	 i:0 	 global-step:18900	 l-p:0.14902690052986145
epoch£º945	 i:1 	 global-step:18901	 l-p:0.11844203621149063
epoch£º945	 i:2 	 global-step:18902	 l-p:0.11546842008829117
epoch£º945	 i:3 	 global-step:18903	 l-p:0.09782788902521133
epoch£º945	 i:4 	 global-step:18904	 l-p:0.17204789817333221
epoch£º945	 i:5 	 global-step:18905	 l-p:0.14120300114154816
epoch£º945	 i:6 	 global-step:18906	 l-p:0.13914738595485687
epoch£º945	 i:7 	 global-step:18907	 l-p:0.2467765063047409
epoch£º945	 i:8 	 global-step:18908	 l-p:0.1244666576385498
epoch£º945	 i:9 	 global-step:18909	 l-p:0.156038299202919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7319, 2.9641, 2.3518],
        [3.7319, 3.0268, 2.3803],
        [3.7319, 3.7318, 3.7319],
        [3.7319, 3.3501, 2.7514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.1353028118610382 
model_pd.l_d.mean(): -22.966720581054688 
model_pd.lagr.mean(): -22.831417083740234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2587], device='cuda:0')), ('power', tensor([-23.2254], device='cuda:0'))])
epoch£º946	 i:0 	 global-step:18920	 l-p:0.1353028118610382
epoch£º946	 i:1 	 global-step:18921	 l-p:0.11309131234884262
epoch£º946	 i:2 	 global-step:18922	 l-p:0.15384626388549805
epoch£º946	 i:3 	 global-step:18923	 l-p:0.255477637052536
epoch£º946	 i:4 	 global-step:18924	 l-p:0.22141432762145996
epoch£º946	 i:5 	 global-step:18925	 l-p:0.12803101539611816
epoch£º946	 i:6 	 global-step:18926	 l-p:0.12170272320508957
epoch£º946	 i:7 	 global-step:18927	 l-p:0.5512769818305969
epoch£º946	 i:8 	 global-step:18928	 l-p:0.12758101522922516
epoch£º946	 i:9 	 global-step:18929	 l-p:0.11182603240013123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6960, 3.6712, 3.6936],
        [3.6960, 3.6922, 3.6958],
        [3.6960, 3.6960, 3.6960],
        [3.6960, 3.6960, 3.6960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.12652671337127686 
model_pd.l_d.mean(): -23.688329696655273 
model_pd.lagr.mean(): -23.561803817749023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1540], device='cuda:0')), ('power', tensor([-23.8423], device='cuda:0'))])
epoch£º947	 i:0 	 global-step:18940	 l-p:0.12652671337127686
epoch£º947	 i:1 	 global-step:18941	 l-p:0.9842464923858643
epoch£º947	 i:2 	 global-step:18942	 l-p:0.12441136687994003
epoch£º947	 i:3 	 global-step:18943	 l-p:0.11218677461147308
epoch£º947	 i:4 	 global-step:18944	 l-p:0.22670897841453552
epoch£º947	 i:5 	 global-step:18945	 l-p:0.1332017481327057
epoch£º947	 i:6 	 global-step:18946	 l-p:0.010838866233825684
epoch£º947	 i:7 	 global-step:18947	 l-p:0.14819085597991943
epoch£º947	 i:8 	 global-step:18948	 l-p:-0.15577878057956696
epoch£º947	 i:9 	 global-step:18949	 l-p:0.1342950463294983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6737, 2.9818, 2.3403],
        [3.6737, 3.5322, 3.6278],
        [3.6737, 3.6735, 3.6737],
        [3.6737, 3.4080, 3.5311]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.10936841368675232 
model_pd.l_d.mean(): -22.7569580078125 
model_pd.lagr.mean(): -22.6475887298584 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3299], device='cuda:0')), ('power', tensor([-23.0868], device='cuda:0'))])
epoch£º948	 i:0 	 global-step:18960	 l-p:0.10936841368675232
epoch£º948	 i:1 	 global-step:18961	 l-p:0.16003069281578064
epoch£º948	 i:2 	 global-step:18962	 l-p:0.11475503444671631
epoch£º948	 i:3 	 global-step:18963	 l-p:0.007512865122407675
epoch£º948	 i:4 	 global-step:18964	 l-p:0.13807694613933563
epoch£º948	 i:5 	 global-step:18965	 l-p:0.14650796353816986
epoch£º948	 i:6 	 global-step:18966	 l-p:0.14626933634281158
epoch£º948	 i:7 	 global-step:18967	 l-p:0.026644285768270493
epoch£º948	 i:8 	 global-step:18968	 l-p:0.1364840269088745
epoch£º948	 i:9 	 global-step:18969	 l-p:0.19981715083122253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6507, 3.6486, 3.6506],
        [3.6507, 3.4195, 3.5404],
        [3.6507, 3.5046, 3.6023],
        [3.6507, 3.5485, 3.6248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.04884357377886772 
model_pd.l_d.mean(): -23.619644165039062 
model_pd.lagr.mean(): -23.57080078125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1347], device='cuda:0')), ('power', tensor([-23.7543], device='cuda:0'))])
epoch£º949	 i:0 	 global-step:18980	 l-p:0.04884357377886772
epoch£º949	 i:1 	 global-step:18981	 l-p:0.18419669568538666
epoch£º949	 i:2 	 global-step:18982	 l-p:0.3180692493915558
epoch£º949	 i:3 	 global-step:18983	 l-p:0.13463301956653595
epoch£º949	 i:4 	 global-step:18984	 l-p:0.12281376868486404
epoch£º949	 i:5 	 global-step:18985	 l-p:0.1472543627023697
epoch£º949	 i:6 	 global-step:18986	 l-p:0.04239105060696602
epoch£º949	 i:7 	 global-step:18987	 l-p:0.14977361261844635
epoch£º949	 i:8 	 global-step:18988	 l-p:0.14191845059394836
epoch£º949	 i:9 	 global-step:18989	 l-p:-0.04285341128706932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6790, 3.6646, 3.6780],
        [3.6790, 2.9888, 2.7725],
        [3.6790, 2.9228, 2.5526],
        [3.6790, 3.6191, 3.6687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): -0.040978483855724335 
model_pd.l_d.mean(): -23.026519775390625 
model_pd.lagr.mean(): -23.0674991607666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2507], device='cuda:0')), ('power', tensor([-23.2772], device='cuda:0'))])
epoch£º950	 i:0 	 global-step:19000	 l-p:-0.040978483855724335
epoch£º950	 i:1 	 global-step:19001	 l-p:0.1257087141275406
epoch£º950	 i:2 	 global-step:19002	 l-p:0.14008355140686035
epoch£º950	 i:3 	 global-step:19003	 l-p:0.17555151879787445
epoch£º950	 i:4 	 global-step:19004	 l-p:0.10935601592063904
epoch£º950	 i:5 	 global-step:19005	 l-p:0.13226042687892914
epoch£º950	 i:6 	 global-step:19006	 l-p:0.14139814674854279
epoch£º950	 i:7 	 global-step:19007	 l-p:1.2857465744018555
epoch£º950	 i:8 	 global-step:19008	 l-p:0.23953919112682343
epoch£º950	 i:9 	 global-step:19009	 l-p:-0.17053040862083435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6813, 3.3263, 3.4370],
        [3.6813, 3.6813, 3.6813],
        [3.6813, 3.6813, 3.6813],
        [3.6813, 2.9912, 2.7749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.14472374320030212 
model_pd.l_d.mean(): -22.60828399658203 
model_pd.lagr.mean(): -22.463560104370117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2752], device='cuda:0')), ('power', tensor([-22.8835], device='cuda:0'))])
epoch£º951	 i:0 	 global-step:19020	 l-p:0.14472374320030212
epoch£º951	 i:1 	 global-step:19021	 l-p:0.12008947879076004
epoch£º951	 i:2 	 global-step:19022	 l-p:0.17514576017856598
epoch£º951	 i:3 	 global-step:19023	 l-p:0.21972385048866272
epoch£º951	 i:4 	 global-step:19024	 l-p:0.12128766626119614
epoch£º951	 i:5 	 global-step:19025	 l-p:0.12536992132663727
epoch£º951	 i:6 	 global-step:19026	 l-p:0.13109582662582397
epoch£º951	 i:7 	 global-step:19027	 l-p:0.13509221374988556
epoch£º951	 i:8 	 global-step:19028	 l-p:-0.03149028867483139
epoch£º951	 i:9 	 global-step:19029	 l-p:-0.1862526684999466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6688, 3.6578, 3.6682],
        [3.6688, 2.8866, 2.2812],
        [3.6688, 3.5915, 3.6529],
        [3.6688, 3.6261, 3.6630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.030247854068875313 
model_pd.l_d.mean(): -23.386367797851562 
model_pd.lagr.mean(): -23.35611915588379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2066], device='cuda:0')), ('power', tensor([-23.5930], device='cuda:0'))])
epoch£º952	 i:0 	 global-step:19040	 l-p:0.030247854068875313
epoch£º952	 i:1 	 global-step:19041	 l-p:-0.15964184701442719
epoch£º952	 i:2 	 global-step:19042	 l-p:0.1678709238767624
epoch£º952	 i:3 	 global-step:19043	 l-p:0.13373830914497375
epoch£º952	 i:4 	 global-step:19044	 l-p:0.17913953959941864
epoch£º952	 i:5 	 global-step:19045	 l-p:0.11302005499601364
epoch£º952	 i:6 	 global-step:19046	 l-p:0.10997214913368225
epoch£º952	 i:7 	 global-step:19047	 l-p:0.12554174661636353
epoch£º952	 i:8 	 global-step:19048	 l-p:0.11818807572126389
epoch£º952	 i:9 	 global-step:19049	 l-p:0.14912238717079163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6763, 2.9560, 2.6797],
        [3.6763, 3.6761, 3.6763],
        [3.6763, 3.6762, 3.6763],
        [3.6763, 3.6694, 3.6760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.0030712794978171587 
model_pd.l_d.mean(): -23.457067489624023 
model_pd.lagr.mean(): -23.453996658325195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2267], device='cuda:0')), ('power', tensor([-23.6837], device='cuda:0'))])
epoch£º953	 i:0 	 global-step:19060	 l-p:0.0030712794978171587
epoch£º953	 i:1 	 global-step:19061	 l-p:0.12991532683372498
epoch£º953	 i:2 	 global-step:19062	 l-p:0.04705211892724037
epoch£º953	 i:3 	 global-step:19063	 l-p:0.13744953274726868
epoch£º953	 i:4 	 global-step:19064	 l-p:0.1414920538663864
epoch£º953	 i:5 	 global-step:19065	 l-p:0.1294061243534088
epoch£º953	 i:6 	 global-step:19066	 l-p:0.030325278639793396
epoch£º953	 i:7 	 global-step:19067	 l-p:0.20897828042507172
epoch£º953	 i:8 	 global-step:19068	 l-p:0.18577693402767181
epoch£º953	 i:9 	 global-step:19069	 l-p:0.14338338375091553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6411, 3.5134, 3.6030],
        [3.6411, 3.1884, 2.5847],
        [3.6411, 3.6357, 3.6409],
        [3.6411, 3.2571, 3.3595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 2.487888813018799 
model_pd.l_d.mean(): -23.09674835205078 
model_pd.lagr.mean(): -20.60886001586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3322], device='cuda:0')), ('power', tensor([-23.4290], device='cuda:0'))])
epoch£º954	 i:0 	 global-step:19080	 l-p:2.487888813018799
epoch£º954	 i:1 	 global-step:19081	 l-p:0.07317082583904266
epoch£º954	 i:2 	 global-step:19082	 l-p:0.13758476078510284
epoch£º954	 i:3 	 global-step:19083	 l-p:0.15407265722751617
epoch£º954	 i:4 	 global-step:19084	 l-p:0.12306564301252365
epoch£º954	 i:5 	 global-step:19085	 l-p:0.11895130574703217
epoch£º954	 i:6 	 global-step:19086	 l-p:0.15510153770446777
epoch£º954	 i:7 	 global-step:19087	 l-p:0.0828469842672348
epoch£º954	 i:8 	 global-step:19088	 l-p:0.14888696372509003
epoch£º954	 i:9 	 global-step:19089	 l-p:0.12762199342250824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6515, 3.6515, 3.6515],
        [3.6515, 3.2016, 2.5970],
        [3.6515, 3.5025, 3.6014],
        [3.6515, 3.5744, 3.6356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.15694278478622437 
model_pd.l_d.mean(): -23.30743980407715 
model_pd.lagr.mean(): -23.150497436523438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2749], device='cuda:0')), ('power', tensor([-23.5823], device='cuda:0'))])
epoch£º955	 i:0 	 global-step:19100	 l-p:0.15694278478622437
epoch£º955	 i:1 	 global-step:19101	 l-p:-0.02191232144832611
epoch£º955	 i:2 	 global-step:19102	 l-p:0.17237134277820587
epoch£º955	 i:3 	 global-step:19103	 l-p:0.15499962866306305
epoch£º955	 i:4 	 global-step:19104	 l-p:0.13061842322349548
epoch£º955	 i:5 	 global-step:19105	 l-p:0.2968391180038452
epoch£º955	 i:6 	 global-step:19106	 l-p:0.13635407388210297
epoch£º955	 i:7 	 global-step:19107	 l-p:0.05992349609732628
epoch£º955	 i:8 	 global-step:19108	 l-p:0.13195501267910004
epoch£º955	 i:9 	 global-step:19109	 l-p:0.1633814573287964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6532, 2.9292, 2.2907],
        [3.6532, 3.6520, 3.6532],
        [3.6532, 3.6511, 3.6531],
        [3.6532, 2.9591, 2.7429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.14227914810180664 
model_pd.l_d.mean(): -23.786460876464844 
model_pd.lagr.mean(): -23.644182205200195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1521], device='cuda:0')), ('power', tensor([-23.9385], device='cuda:0'))])
epoch£º956	 i:0 	 global-step:19120	 l-p:0.14227914810180664
epoch£º956	 i:1 	 global-step:19121	 l-p:0.14139246940612793
epoch£º956	 i:2 	 global-step:19122	 l-p:0.08387410640716553
epoch£º956	 i:3 	 global-step:19123	 l-p:0.13599000871181488
epoch£º956	 i:4 	 global-step:19124	 l-p:0.07415707409381866
epoch£º956	 i:5 	 global-step:19125	 l-p:-0.04471553862094879
epoch£º956	 i:6 	 global-step:19126	 l-p:0.13625739514827728
epoch£º956	 i:7 	 global-step:19127	 l-p:0.318104088306427
epoch£º956	 i:8 	 global-step:19128	 l-p:0.1314138025045395
epoch£º956	 i:9 	 global-step:19129	 l-p:0.10541018843650818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6338, 3.6331, 3.6338],
        [3.6338, 3.2762, 3.3880],
        [3.6338, 3.5940, 3.6286],
        [3.6338, 3.5908, 3.6279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.10927360504865646 
model_pd.l_d.mean(): -23.620771408081055 
model_pd.lagr.mean(): -23.511497497558594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2009], device='cuda:0')), ('power', tensor([-23.8217], device='cuda:0'))])
epoch£º957	 i:0 	 global-step:19140	 l-p:0.10927360504865646
epoch£º957	 i:1 	 global-step:19141	 l-p:0.13033978641033173
epoch£º957	 i:2 	 global-step:19142	 l-p:0.14072318375110626
epoch£º957	 i:3 	 global-step:19143	 l-p:0.10077844560146332
epoch£º957	 i:4 	 global-step:19144	 l-p:3.4683194160461426
epoch£º957	 i:5 	 global-step:19145	 l-p:0.11639959365129471
epoch£º957	 i:6 	 global-step:19146	 l-p:0.1447494477033615
epoch£º957	 i:7 	 global-step:19147	 l-p:0.09418898075819016
epoch£º957	 i:8 	 global-step:19148	 l-p:0.20292025804519653
epoch£º957	 i:9 	 global-step:19149	 l-p:0.2587195932865143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6413, 3.5983, 3.6354],
        [3.6413, 3.5074, 3.5999],
        [3.6413, 3.4920, 3.5911],
        [3.6413, 2.8731, 2.2475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 8.631020545959473 
model_pd.l_d.mean(): -23.675981521606445 
model_pd.lagr.mean(): -15.044960975646973 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1527], device='cuda:0')), ('power', tensor([-23.8287], device='cuda:0'))])
epoch£º958	 i:0 	 global-step:19160	 l-p:8.631020545959473
epoch£º958	 i:1 	 global-step:19161	 l-p:0.1423114389181137
epoch£º958	 i:2 	 global-step:19162	 l-p:0.11794526875019073
epoch£º958	 i:3 	 global-step:19163	 l-p:0.15767234563827515
epoch£º958	 i:4 	 global-step:19164	 l-p:0.13653835654258728
epoch£º958	 i:5 	 global-step:19165	 l-p:0.0009815454250201583
epoch£º958	 i:6 	 global-step:19166	 l-p:0.17520885169506073
epoch£º958	 i:7 	 global-step:19167	 l-p:0.14142996072769165
epoch£º958	 i:8 	 global-step:19168	 l-p:0.11750058829784393
epoch£º958	 i:9 	 global-step:19169	 l-p:0.12803223729133606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6547, 2.8596, 2.2809],
        [3.6547, 3.6547, 3.6547],
        [3.6547, 2.8634, 2.3787],
        [3.6547, 3.6494, 3.6545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.2322019338607788 
model_pd.l_d.mean(): -23.53529930114746 
model_pd.lagr.mean(): -23.303096771240234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1961], device='cuda:0')), ('power', tensor([-23.7314], device='cuda:0'))])
epoch£º959	 i:0 	 global-step:19180	 l-p:0.2322019338607788
epoch£º959	 i:1 	 global-step:19181	 l-p:0.12472528219223022
epoch£º959	 i:2 	 global-step:19182	 l-p:0.1469087153673172
epoch£º959	 i:3 	 global-step:19183	 l-p:0.1335979402065277
epoch£º959	 i:4 	 global-step:19184	 l-p:0.06190340965986252
epoch£º959	 i:5 	 global-step:19185	 l-p:0.1270999163389206
epoch£º959	 i:6 	 global-step:19186	 l-p:0.1395261287689209
epoch£º959	 i:7 	 global-step:19187	 l-p:0.13146011531352997
epoch£º959	 i:8 	 global-step:19188	 l-p:0.03501897677779198
epoch£º959	 i:9 	 global-step:19189	 l-p:-0.7303990721702576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6379, 3.6378, 3.6379],
        [3.6379, 3.6314, 3.6377],
        [3.6379, 3.1114, 2.4939],
        [3.6379, 3.1033, 2.4845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.07940018177032471 
model_pd.l_d.mean(): -22.923004150390625 
model_pd.lagr.mean(): -22.843603134155273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2729], device='cuda:0')), ('power', tensor([-23.1959], device='cuda:0'))])
epoch£º960	 i:0 	 global-step:19200	 l-p:0.07940018177032471
epoch£º960	 i:1 	 global-step:19201	 l-p:0.15746258199214935
epoch£º960	 i:2 	 global-step:19202	 l-p:0.12939666211605072
epoch£º960	 i:3 	 global-step:19203	 l-p:0.2523997128009796
epoch£º960	 i:4 	 global-step:19204	 l-p:0.1373254954814911
epoch£º960	 i:5 	 global-step:19205	 l-p:0.19225937128067017
epoch£º960	 i:6 	 global-step:19206	 l-p:0.07220031321048737
epoch£º960	 i:7 	 global-step:19207	 l-p:0.06551980972290039
epoch£º960	 i:8 	 global-step:19208	 l-p:0.3632502555847168
epoch£º960	 i:9 	 global-step:19209	 l-p:0.10434714704751968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6635, 2.8776, 2.2714],
        [3.6635, 3.5234, 3.6186],
        [3.6635, 2.8717, 2.2806],
        [3.6635, 3.6546, 3.6631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.12345952540636063 
model_pd.l_d.mean(): -22.95699691772461 
model_pd.lagr.mean(): -22.833538055419922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3684], device='cuda:0')), ('power', tensor([-23.3254], device='cuda:0'))])
epoch£º961	 i:0 	 global-step:19220	 l-p:0.12345952540636063
epoch£º961	 i:1 	 global-step:19221	 l-p:0.12110251188278198
epoch£º961	 i:2 	 global-step:19222	 l-p:0.12928298115730286
epoch£º961	 i:3 	 global-step:19223	 l-p:0.13334204256534576
epoch£º961	 i:4 	 global-step:19224	 l-p:0.14126133918762207
epoch£º961	 i:5 	 global-step:19225	 l-p:-0.15229997038841248
epoch£º961	 i:6 	 global-step:19226	 l-p:0.16367551684379578
epoch£º961	 i:7 	 global-step:19227	 l-p:0.13333697617053986
epoch£º961	 i:8 	 global-step:19228	 l-p:0.1892506182193756
epoch£º961	 i:9 	 global-step:19229	 l-p:0.24036546051502228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6692, 3.5784, 3.6482],
        [3.6692, 3.6692, 3.6692],
        [3.6692, 3.6667, 3.6692],
        [3.6692, 3.4753, 3.5891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.1784345656633377 
model_pd.l_d.mean(): -23.672157287597656 
model_pd.lagr.mean(): -23.493722915649414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1338], device='cuda:0')), ('power', tensor([-23.8060], device='cuda:0'))])
epoch£º962	 i:0 	 global-step:19240	 l-p:0.1784345656633377
epoch£º962	 i:1 	 global-step:19241	 l-p:0.18893487751483917
epoch£º962	 i:2 	 global-step:19242	 l-p:0.1252208799123764
epoch£º962	 i:3 	 global-step:19243	 l-p:0.13371330499649048
epoch£º962	 i:4 	 global-step:19244	 l-p:-0.43549931049346924
epoch£º962	 i:5 	 global-step:19245	 l-p:0.16770732402801514
epoch£º962	 i:6 	 global-step:19246	 l-p:-0.38150516152381897
epoch£º962	 i:7 	 global-step:19247	 l-p:0.3502561151981354
epoch£º962	 i:8 	 global-step:19248	 l-p:0.13007479906082153
epoch£º962	 i:9 	 global-step:19249	 l-p:0.15263044834136963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7021, 3.6976, 3.7019],
        [3.7021, 3.6992, 3.7020],
        [3.7021, 3.5993, 3.6760],
        [3.7021, 3.6212, 3.6848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.159707710146904 
model_pd.l_d.mean(): -23.22539520263672 
model_pd.lagr.mean(): -23.06568717956543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2331], device='cuda:0')), ('power', tensor([-23.4585], device='cuda:0'))])
epoch£º963	 i:0 	 global-step:19260	 l-p:0.159707710146904
epoch£º963	 i:1 	 global-step:19261	 l-p:0.1278342604637146
epoch£º963	 i:2 	 global-step:19262	 l-p:0.11993923038244247
epoch£º963	 i:3 	 global-step:19263	 l-p:0.8048930764198303
epoch£º963	 i:4 	 global-step:19264	 l-p:0.3823622763156891
epoch£º963	 i:5 	 global-step:19265	 l-p:0.1366090327501297
epoch£º963	 i:6 	 global-step:19266	 l-p:0.1044694036245346
epoch£º963	 i:7 	 global-step:19267	 l-p:0.13714013993740082
epoch£º963	 i:8 	 global-step:19268	 l-p:0.18012511730194092
epoch£º963	 i:9 	 global-step:19269	 l-p:0.1080661490559578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7118, 3.7088, 3.7117],
        [3.7118, 3.7112, 3.7117],
        [3.7118, 3.3237, 2.7249],
        [3.7118, 2.9216, 2.3672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.13203588128089905 
model_pd.l_d.mean(): -22.329124450683594 
model_pd.lagr.mean(): -22.19708824157715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3254], device='cuda:0')), ('power', tensor([-22.6545], device='cuda:0'))])
epoch£º964	 i:0 	 global-step:19280	 l-p:0.13203588128089905
epoch£º964	 i:1 	 global-step:19281	 l-p:0.13454146683216095
epoch£º964	 i:2 	 global-step:19282	 l-p:0.138502299785614
epoch£º964	 i:3 	 global-step:19283	 l-p:0.13287335634231567
epoch£º964	 i:4 	 global-step:19284	 l-p:-0.45805346965789795
epoch£º964	 i:5 	 global-step:19285	 l-p:0.1372157484292984
epoch£º964	 i:6 	 global-step:19286	 l-p:0.12424780428409576
epoch£º964	 i:7 	 global-step:19287	 l-p:0.1013675108551979
epoch£º964	 i:8 	 global-step:19288	 l-p:0.11395162343978882
epoch£º964	 i:9 	 global-step:19289	 l-p:0.19606544077396393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[3.6614, 2.8653, 2.3563],
        [3.6614, 3.0363, 2.9341],
        [3.6614, 2.8648, 2.3516],
        [3.6614, 3.0990, 3.0738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.1407923847436905 
model_pd.l_d.mean(): -23.488216400146484 
model_pd.lagr.mean(): -23.347423553466797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1872], device='cuda:0')), ('power', tensor([-23.6754], device='cuda:0'))])
epoch£º965	 i:0 	 global-step:19300	 l-p:0.1407923847436905
epoch£º965	 i:1 	 global-step:19301	 l-p:0.1994783729314804
epoch£º965	 i:2 	 global-step:19302	 l-p:0.11719159781932831
epoch£º965	 i:3 	 global-step:19303	 l-p:0.38106998801231384
epoch£º965	 i:4 	 global-step:19304	 l-p:0.1277795433998108
epoch£º965	 i:5 	 global-step:19305	 l-p:0.17094776034355164
epoch£º965	 i:6 	 global-step:19306	 l-p:0.0845625102519989
epoch£º965	 i:7 	 global-step:19307	 l-p:0.1349649727344513
epoch£º965	 i:8 	 global-step:19308	 l-p:0.12883973121643066
epoch£º965	 i:9 	 global-step:19309	 l-p:0.134963259100914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6342, 2.9815, 2.3463],
        [3.6342, 3.6333, 3.6342],
        [3.6342, 2.9620, 2.3251],
        [3.6342, 3.0857, 3.0777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.14981500804424286 
model_pd.l_d.mean(): -22.893896102905273 
model_pd.lagr.mean(): -22.744081497192383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3166], device='cuda:0')), ('power', tensor([-23.2105], device='cuda:0'))])
epoch£º966	 i:0 	 global-step:19320	 l-p:0.14981500804424286
epoch£º966	 i:1 	 global-step:19321	 l-p:0.10737260431051254
epoch£º966	 i:2 	 global-step:19322	 l-p:-0.047252338379621506
epoch£º966	 i:3 	 global-step:19323	 l-p:0.1545846164226532
epoch£º966	 i:4 	 global-step:19324	 l-p:0.13231156766414642
epoch£º966	 i:5 	 global-step:19325	 l-p:0.13657163083553314
epoch£º966	 i:6 	 global-step:19326	 l-p:0.13527549803256989
epoch£º966	 i:7 	 global-step:19327	 l-p:0.0868796780705452
epoch£º966	 i:8 	 global-step:19328	 l-p:0.13948525488376617
epoch£º966	 i:9 	 global-step:19329	 l-p:0.11874105036258698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6346, 3.6343, 3.6346],
        [3.6346, 3.6346, 3.6346],
        [3.6346, 3.6345, 3.6346],
        [3.6346, 2.8677, 2.4954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.16074509918689728 
model_pd.l_d.mean(): -23.745819091796875 
model_pd.lagr.mean(): -23.585073471069336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2187], device='cuda:0')), ('power', tensor([-23.9646], device='cuda:0'))])
epoch£º967	 i:0 	 global-step:19340	 l-p:0.16074509918689728
epoch£º967	 i:1 	 global-step:19341	 l-p:0.1409360021352768
epoch£º967	 i:2 	 global-step:19342	 l-p:0.1269685924053192
epoch£º967	 i:3 	 global-step:19343	 l-p:0.12399718910455704
epoch£º967	 i:4 	 global-step:19344	 l-p:0.08923841267824173
epoch£º967	 i:5 	 global-step:19345	 l-p:0.07049059122800827
epoch£º967	 i:6 	 global-step:19346	 l-p:0.20995372533798218
epoch£º967	 i:7 	 global-step:19347	 l-p:-0.00451801298186183
epoch£º967	 i:8 	 global-step:19348	 l-p:0.14133553206920624
epoch£º967	 i:9 	 global-step:19349	 l-p:0.28828921914100647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6385, 2.8826, 2.2489],
        [3.6385, 2.8451, 2.2396],
        [3.6385, 3.4736, 3.5788],
        [3.6385, 2.8679, 2.4849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): -0.40535834431648254 
model_pd.l_d.mean(): -23.388635635375977 
model_pd.lagr.mean(): -23.793994903564453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2908], device='cuda:0')), ('power', tensor([-23.6794], device='cuda:0'))])
epoch£º968	 i:0 	 global-step:19360	 l-p:-0.40535834431648254
epoch£º968	 i:1 	 global-step:19361	 l-p:0.1604834944009781
epoch£º968	 i:2 	 global-step:19362	 l-p:0.21418578922748566
epoch£º968	 i:3 	 global-step:19363	 l-p:0.17757804691791534
epoch£º968	 i:4 	 global-step:19364	 l-p:0.04062345251441002
epoch£º968	 i:5 	 global-step:19365	 l-p:0.1450445055961609
epoch£º968	 i:6 	 global-step:19366	 l-p:0.09389800578355789
epoch£º968	 i:7 	 global-step:19367	 l-p:0.14319221675395966
epoch£º968	 i:8 	 global-step:19368	 l-p:0.14494548738002777
epoch£º968	 i:9 	 global-step:19369	 l-p:0.13225524127483368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6835, 2.9018, 2.4609],
        [3.6835, 3.0143, 2.3704],
        [3.6835, 2.8967, 2.2895],
        [3.6835, 3.4432, 3.5655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.14441323280334473 
model_pd.l_d.mean(): -23.738128662109375 
model_pd.lagr.mean(): -23.59371566772461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1184], device='cuda:0')), ('power', tensor([-23.8565], device='cuda:0'))])
epoch£º969	 i:0 	 global-step:19380	 l-p:0.14441323280334473
epoch£º969	 i:1 	 global-step:19381	 l-p:0.12358619272708893
epoch£º969	 i:2 	 global-step:19382	 l-p:-0.08453884720802307
epoch£º969	 i:3 	 global-step:19383	 l-p:0.09864398837089539
epoch£º969	 i:4 	 global-step:19384	 l-p:0.22638313472270966
epoch£º969	 i:5 	 global-step:19385	 l-p:0.15424762666225433
epoch£º969	 i:6 	 global-step:19386	 l-p:0.11997054517269135
epoch£º969	 i:7 	 global-step:19387	 l-p:0.17513391375541687
epoch£º969	 i:8 	 global-step:19388	 l-p:0.142581969499588
epoch£º969	 i:9 	 global-step:19389	 l-p:-0.09972014278173447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6741, 3.6739, 3.6741],
        [3.6741, 3.1836, 3.2252],
        [3.6741, 3.3695, 3.4919],
        [3.6741, 3.0345, 2.9119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.1610507220029831 
model_pd.l_d.mean(): -23.57514762878418 
model_pd.lagr.mean(): -23.41409683227539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1825], device='cuda:0')), ('power', tensor([-23.7577], device='cuda:0'))])
epoch£º970	 i:0 	 global-step:19400	 l-p:0.1610507220029831
epoch£º970	 i:1 	 global-step:19401	 l-p:-0.13535164296627045
epoch£º970	 i:2 	 global-step:19402	 l-p:0.15415751934051514
epoch£º970	 i:3 	 global-step:19403	 l-p:0.14591249823570251
epoch£º970	 i:4 	 global-step:19404	 l-p:0.13010931015014648
epoch£º970	 i:5 	 global-step:19405	 l-p:0.17568929493427277
epoch£º970	 i:6 	 global-step:19406	 l-p:0.11835014075040817
epoch£º970	 i:7 	 global-step:19407	 l-p:-0.16817545890808105
epoch£º970	 i:8 	 global-step:19408	 l-p:0.2019338607788086
epoch£º970	 i:9 	 global-step:19409	 l-p:0.10934579372406006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6797, 3.5007, 3.6104],
        [3.6797, 3.5098, 3.6166],
        [3.6797, 3.6782, 3.6797],
        [3.6797, 2.9387, 2.6243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.1700112372636795 
model_pd.l_d.mean(): -23.638412475585938 
model_pd.lagr.mean(): -23.468400955200195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1567], device='cuda:0')), ('power', tensor([-23.7951], device='cuda:0'))])
epoch£º971	 i:0 	 global-step:19420	 l-p:0.1700112372636795
epoch£º971	 i:1 	 global-step:19421	 l-p:0.1313105821609497
epoch£º971	 i:2 	 global-step:19422	 l-p:-0.41279861330986023
epoch£º971	 i:3 	 global-step:19423	 l-p:0.13689996302127838
epoch£º971	 i:4 	 global-step:19424	 l-p:-0.016956128180027008
epoch£º971	 i:5 	 global-step:19425	 l-p:0.12673066556453705
epoch£º971	 i:6 	 global-step:19426	 l-p:0.140985369682312
epoch£º971	 i:7 	 global-step:19427	 l-p:0.18094240128993988
epoch£º971	 i:8 	 global-step:19428	 l-p:0.12488128989934921
epoch£º971	 i:9 	 global-step:19429	 l-p:-0.031056135892868042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6793, 3.5512, 3.6410],
        [3.6793, 3.6763, 3.6792],
        [3.6793, 3.1166, 3.0911],
        [3.6793, 3.6740, 3.6791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.12942062318325043 
model_pd.l_d.mean(): -23.14634132385254 
model_pd.lagr.mean(): -23.01692008972168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2787], device='cuda:0')), ('power', tensor([-23.4250], device='cuda:0'))])
epoch£º972	 i:0 	 global-step:19440	 l-p:0.12942062318325043
epoch£º972	 i:1 	 global-step:19441	 l-p:0.14604787528514862
epoch£º972	 i:2 	 global-step:19442	 l-p:0.13294516503810883
epoch£º972	 i:3 	 global-step:19443	 l-p:0.14025169610977173
epoch£º972	 i:4 	 global-step:19444	 l-p:0.14401598274707794
epoch£º972	 i:5 	 global-step:19445	 l-p:0.2606005370616913
epoch£º972	 i:6 	 global-step:19446	 l-p:0.0904984250664711
epoch£º972	 i:7 	 global-step:19447	 l-p:0.06049853190779686
epoch£º972	 i:8 	 global-step:19448	 l-p:0.10646949708461761
epoch£º972	 i:9 	 global-step:19449	 l-p:0.037666261196136475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6661, 3.6410, 3.6637],
        [3.6661, 3.2162, 3.2868],
        [3.6661, 3.4490, 3.5682],
        [3.6661, 2.9275, 2.2871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.1475857049226761 
model_pd.l_d.mean(): -23.18093490600586 
model_pd.lagr.mean(): -23.033349990844727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1921], device='cuda:0')), ('power', tensor([-23.3731], device='cuda:0'))])
epoch£º973	 i:0 	 global-step:19460	 l-p:0.1475857049226761
epoch£º973	 i:1 	 global-step:19461	 l-p:0.1779741644859314
epoch£º973	 i:2 	 global-step:19462	 l-p:0.054223425686359406
epoch£º973	 i:3 	 global-step:19463	 l-p:0.1287248730659485
epoch£º973	 i:4 	 global-step:19464	 l-p:0.14888812601566315
epoch£º973	 i:5 	 global-step:19465	 l-p:0.1192125752568245
epoch£º973	 i:6 	 global-step:19466	 l-p:0.12787793576717377
epoch£º973	 i:7 	 global-step:19467	 l-p:0.1492430418729782
epoch£º973	 i:8 	 global-step:19468	 l-p:0.1131167784333229
epoch£º973	 i:9 	 global-step:19469	 l-p:0.15469932556152344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6430, 3.0306, 2.9501],
        [3.6430, 3.5424, 3.6180],
        [3.6430, 3.1801, 3.2437],
        [3.6430, 2.9458, 2.3058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.135751411318779 
model_pd.l_d.mean(): -22.733606338500977 
model_pd.lagr.mean(): -22.597854614257812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2663], device='cuda:0')), ('power', tensor([-22.9999], device='cuda:0'))])
epoch£º974	 i:0 	 global-step:19480	 l-p:0.135751411318779
epoch£º974	 i:1 	 global-step:19481	 l-p:0.12025915086269379
epoch£º974	 i:2 	 global-step:19482	 l-p:-0.01596110872924328
epoch£º974	 i:3 	 global-step:19483	 l-p:0.7152509093284607
epoch£º974	 i:4 	 global-step:19484	 l-p:0.1345621645450592
epoch£º974	 i:5 	 global-step:19485	 l-p:0.2057158201932907
epoch£º974	 i:6 	 global-step:19486	 l-p:0.1335153728723526
epoch£º974	 i:7 	 global-step:19487	 l-p:0.09057560563087463
epoch£º974	 i:8 	 global-step:19488	 l-p:0.13843780755996704
epoch£º974	 i:9 	 global-step:19489	 l-p:0.1426374465227127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6237, 3.6237, 3.6237],
        [3.6237, 3.6237, 3.6237],
        [3.6237, 2.8896, 2.6069],
        [3.6237, 3.3623, 3.4870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.08576031774282455 
model_pd.l_d.mean(): -22.894145965576172 
model_pd.lagr.mean(): -22.808385848999023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2329], device='cuda:0')), ('power', tensor([-23.1271], device='cuda:0'))])
epoch£º975	 i:0 	 global-step:19500	 l-p:0.08576031774282455
epoch£º975	 i:1 	 global-step:19501	 l-p:0.16686281561851501
epoch£º975	 i:2 	 global-step:19502	 l-p:0.23589767515659332
epoch£º975	 i:3 	 global-step:19503	 l-p:0.1353515088558197
epoch£º975	 i:4 	 global-step:19504	 l-p:0.03996926173567772
epoch£º975	 i:5 	 global-step:19505	 l-p:0.14549987018108368
epoch£º975	 i:6 	 global-step:19506	 l-p:0.10408050566911697
epoch£º975	 i:7 	 global-step:19507	 l-p:0.04258539155125618
epoch£º975	 i:8 	 global-step:19508	 l-p:0.5480443239212036
epoch£º975	 i:9 	 global-step:19509	 l-p:0.14151521027088165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6271, 3.0299, 2.9710],
        [3.6271, 3.5787, 3.6200],
        [3.6271, 2.9302, 2.2923],
        [3.6271, 3.6271, 3.6271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.16301733255386353 
model_pd.l_d.mean(): -23.53006935119629 
model_pd.lagr.mean(): -23.36705207824707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2381], device='cuda:0')), ('power', tensor([-23.7681], device='cuda:0'))])
epoch£º976	 i:0 	 global-step:19520	 l-p:0.16301733255386353
epoch£º976	 i:1 	 global-step:19521	 l-p:0.24248746037483215
epoch£º976	 i:2 	 global-step:19522	 l-p:0.13263577222824097
epoch£º976	 i:3 	 global-step:19523	 l-p:0.9634132981300354
epoch£º976	 i:4 	 global-step:19524	 l-p:0.12577687203884125
epoch£º976	 i:5 	 global-step:19525	 l-p:0.12775975465774536
epoch£º976	 i:6 	 global-step:19526	 l-p:-0.02577579766511917
epoch£º976	 i:7 	 global-step:19527	 l-p:0.09194763004779816
epoch£º976	 i:8 	 global-step:19528	 l-p:0.09280923753976822
epoch£º976	 i:9 	 global-step:19529	 l-p:0.13655242323875427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6401, 3.6401, 3.6401],
        [3.6401, 3.6401, 3.6401],
        [3.6401, 3.4557, 3.5674],
        [3.6401, 3.0742, 3.0499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.15821316838264465 
model_pd.l_d.mean(): -23.610397338867188 
model_pd.lagr.mean(): -23.452184677124023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2240], device='cuda:0')), ('power', tensor([-23.8344], device='cuda:0'))])
epoch£º977	 i:0 	 global-step:19540	 l-p:0.15821316838264465
epoch£º977	 i:1 	 global-step:19541	 l-p:0.14513066411018372
epoch£º977	 i:2 	 global-step:19542	 l-p:0.18575812876224518
epoch£º977	 i:3 	 global-step:19543	 l-p:0.12272828817367554
epoch£º977	 i:4 	 global-step:19544	 l-p:0.1446324586868286
epoch£º977	 i:5 	 global-step:19545	 l-p:0.2395865023136139
epoch£º977	 i:6 	 global-step:19546	 l-p:0.03734099119901657
epoch£º977	 i:7 	 global-step:19547	 l-p:0.14314594864845276
epoch£º977	 i:8 	 global-step:19548	 l-p:-0.0828753262758255
epoch£º977	 i:9 	 global-step:19549	 l-p:0.13086533546447754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6354, 3.6342, 3.6354],
        [3.6354, 3.4802, 3.5820],
        [3.6354, 3.6354, 3.6354],
        [3.6354, 2.9205, 2.6773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.124180369079113 
model_pd.l_d.mean(): -23.373342514038086 
model_pd.lagr.mean(): -23.249162673950195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2429], device='cuda:0')), ('power', tensor([-23.6163], device='cuda:0'))])
epoch£º978	 i:0 	 global-step:19560	 l-p:0.124180369079113
epoch£º978	 i:1 	 global-step:19561	 l-p:0.15170426666736603
epoch£º978	 i:2 	 global-step:19562	 l-p:0.16631338000297546
epoch£º978	 i:3 	 global-step:19563	 l-p:0.066367506980896
epoch£º978	 i:4 	 global-step:19564	 l-p:0.1314968317747116
epoch£º978	 i:5 	 global-step:19565	 l-p:0.1333610564470291
epoch£º978	 i:6 	 global-step:19566	 l-p:0.13493956625461578
epoch£º978	 i:7 	 global-step:19567	 l-p:-0.21232940256595612
epoch£º978	 i:8 	 global-step:19568	 l-p:0.2130240947008133
epoch£º978	 i:9 	 global-step:19569	 l-p:0.048235416412353516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228]], device='cuda:0')
 pt:tensor([[3.6238, 3.1823, 2.5816],
        [3.6238, 2.8632, 2.5193],
        [3.6238, 2.8254, 2.3568],
        [3.6238, 2.9815, 2.8639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.14753888547420502 
model_pd.l_d.mean(): -23.113174438476562 
model_pd.lagr.mean(): -22.965635299682617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3291], device='cuda:0')), ('power', tensor([-23.4423], device='cuda:0'))])
epoch£º979	 i:0 	 global-step:19580	 l-p:0.14753888547420502
epoch£º979	 i:1 	 global-step:19581	 l-p:0.1832788586616516
epoch£º979	 i:2 	 global-step:19582	 l-p:0.1532558649778366
epoch£º979	 i:3 	 global-step:19583	 l-p:0.12607158720493317
epoch£º979	 i:4 	 global-step:19584	 l-p:0.08646903187036514
epoch£º979	 i:5 	 global-step:19585	 l-p:0.24686706066131592
epoch£º979	 i:6 	 global-step:19586	 l-p:0.7692467570304871
epoch£º979	 i:7 	 global-step:19587	 l-p:0.13816794753074646
epoch£º979	 i:8 	 global-step:19588	 l-p:0.029257087036967278
epoch£º979	 i:9 	 global-step:19589	 l-p:0.11056085675954819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6737, 3.6737, 3.6737],
        [3.6737, 3.6683, 3.6735],
        [3.6737, 3.5879, 3.6547],
        [3.6737, 3.6730, 3.6737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.15561430156230927 
model_pd.l_d.mean(): -23.580448150634766 
model_pd.lagr.mean(): -23.424833297729492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1745], device='cuda:0')), ('power', tensor([-23.7550], device='cuda:0'))])
epoch£º980	 i:0 	 global-step:19600	 l-p:0.15561430156230927
epoch£º980	 i:1 	 global-step:19601	 l-p:0.13403530418872833
epoch£º980	 i:2 	 global-step:19602	 l-p:0.12998642027378082
epoch£º980	 i:3 	 global-step:19603	 l-p:-0.026692790910601616
epoch£º980	 i:4 	 global-step:19604	 l-p:0.1749429851770401
epoch£º980	 i:5 	 global-step:19605	 l-p:0.14252175390720367
epoch£º980	 i:6 	 global-step:19606	 l-p:0.11421719193458557
epoch£º980	 i:7 	 global-step:19607	 l-p:-0.0006536674336530268
epoch£º980	 i:8 	 global-step:19608	 l-p:0.12455680221319199
epoch£º980	 i:9 	 global-step:19609	 l-p:0.24693073332309723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6680, 3.4265, 3.5495],
        [3.6680, 3.6680, 3.6680],
        [3.6680, 3.6641, 3.6679],
        [3.6680, 2.8757, 2.4111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.13202771544456482 
model_pd.l_d.mean(): -23.047273635864258 
model_pd.lagr.mean(): -22.915245056152344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2435], device='cuda:0')), ('power', tensor([-23.2908], device='cuda:0'))])
epoch£º981	 i:0 	 global-step:19620	 l-p:0.13202771544456482
epoch£º981	 i:1 	 global-step:19621	 l-p:0.13199228048324585
epoch£º981	 i:2 	 global-step:19622	 l-p:0.1333465725183487
epoch£º981	 i:3 	 global-step:19623	 l-p:0.12823742628097534
epoch£º981	 i:4 	 global-step:19624	 l-p:0.08197740465402603
epoch£º981	 i:5 	 global-step:19625	 l-p:0.09760793298482895
epoch£º981	 i:6 	 global-step:19626	 l-p:0.14020639657974243
epoch£º981	 i:7 	 global-step:19627	 l-p:0.14531831443309784
epoch£º981	 i:8 	 global-step:19628	 l-p:0.18155694007873535
epoch£º981	 i:9 	 global-step:19629	 l-p:0.15049736201763153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6321, 3.0802, 3.0723],
        [3.6321, 3.0077, 2.3751],
        [3.6321, 3.6321, 3.6322],
        [3.6321, 3.4657, 3.5718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.09900246560573578 
model_pd.l_d.mean(): -23.50274658203125 
model_pd.lagr.mean(): -23.403743743896484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2310], device='cuda:0')), ('power', tensor([-23.7337], device='cuda:0'))])
epoch£º982	 i:0 	 global-step:19640	 l-p:0.09900246560573578
epoch£º982	 i:1 	 global-step:19641	 l-p:0.15104341506958008
epoch£º982	 i:2 	 global-step:19642	 l-p:0.1405004858970642
epoch£º982	 i:3 	 global-step:19643	 l-p:0.14698204398155212
epoch£º982	 i:4 	 global-step:19644	 l-p:0.09562146663665771
epoch£º982	 i:5 	 global-step:19645	 l-p:0.6850107312202454
epoch£º982	 i:6 	 global-step:19646	 l-p:0.13082724809646606
epoch£º982	 i:7 	 global-step:19647	 l-p:0.12345438450574875
epoch£º982	 i:8 	 global-step:19648	 l-p:0.24559791386127472
epoch£º982	 i:9 	 global-step:19649	 l-p:0.1513882875442505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6164, 3.2716, 3.3893],
        [3.6164, 3.3540, 3.4793],
        [3.6164, 3.4497, 3.5560],
        [3.6164, 3.6110, 3.6162]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): -0.26384681463241577 
model_pd.l_d.mean(): -23.624248504638672 
model_pd.lagr.mean(): -23.88809585571289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2271], device='cuda:0')), ('power', tensor([-23.8513], device='cuda:0'))])
epoch£º983	 i:0 	 global-step:19660	 l-p:-0.26384681463241577
epoch£º983	 i:1 	 global-step:19661	 l-p:0.11920980364084244
epoch£º983	 i:2 	 global-step:19662	 l-p:0.0775907039642334
epoch£º983	 i:3 	 global-step:19663	 l-p:0.10487403720617294
epoch£º983	 i:4 	 global-step:19664	 l-p:0.13339512050151825
epoch£º983	 i:5 	 global-step:19665	 l-p:0.11482054740190506
epoch£º983	 i:6 	 global-step:19666	 l-p:0.11857752501964569
epoch£º983	 i:7 	 global-step:19667	 l-p:0.14423733949661255
epoch£º983	 i:8 	 global-step:19668	 l-p:0.17603227496147156
epoch£º983	 i:9 	 global-step:19669	 l-p:0.149210587143898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6084, 2.7972, 2.2171],
        [3.6084, 3.5972, 3.6078],
        [3.6084, 3.6083, 3.6084],
        [3.6084, 3.5990, 3.6079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.3124690353870392 
model_pd.l_d.mean(): -23.30306053161621 
model_pd.lagr.mean(): -22.990591049194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2718], device='cuda:0')), ('power', tensor([-23.5749], device='cuda:0'))])
epoch£º984	 i:0 	 global-step:19680	 l-p:0.3124690353870392
epoch£º984	 i:1 	 global-step:19681	 l-p:0.16448722779750824
epoch£º984	 i:2 	 global-step:19682	 l-p:0.10217449814081192
epoch£º984	 i:3 	 global-step:19683	 l-p:0.13879185914993286
epoch£º984	 i:4 	 global-step:19684	 l-p:0.0992710292339325
epoch£º984	 i:5 	 global-step:19685	 l-p:0.04255528375506401
epoch£º984	 i:6 	 global-step:19686	 l-p:0.631473183631897
epoch£º984	 i:7 	 global-step:19687	 l-p:0.11874561011791229
epoch£º984	 i:8 	 global-step:19688	 l-p:0.13512031733989716
epoch£º984	 i:9 	 global-step:19689	 l-p:0.13716554641723633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6582, 3.2074, 2.6002],
        [3.6582, 3.3147, 3.4317],
        [3.6582, 2.9631, 2.3202],
        [3.6582, 3.6582, 3.6582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.060973167419433594 
model_pd.l_d.mean(): -23.456987380981445 
model_pd.lagr.mean(): -23.396015167236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2210], device='cuda:0')), ('power', tensor([-23.6780], device='cuda:0'))])
epoch£º985	 i:0 	 global-step:19700	 l-p:0.060973167419433594
epoch£º985	 i:1 	 global-step:19701	 l-p:0.0648399144411087
epoch£º985	 i:2 	 global-step:19702	 l-p:0.021257352083921432
epoch£º985	 i:3 	 global-step:19703	 l-p:0.17967963218688965
epoch£º985	 i:4 	 global-step:19704	 l-p:0.1263624131679535
epoch£º985	 i:5 	 global-step:19705	 l-p:0.17223837971687317
epoch£º985	 i:6 	 global-step:19706	 l-p:0.12522634863853455
epoch£º985	 i:7 	 global-step:19707	 l-p:0.09703388065099716
epoch£º985	 i:8 	 global-step:19708	 l-p:0.13923285901546478
epoch£º985	 i:9 	 global-step:19709	 l-p:0.13184267282485962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7363, 3.7363, 3.7363],
        [3.7363, 3.5608, 3.6694],
        [3.7363, 3.7283, 3.7359],
        [3.7363, 3.4836, 3.6068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.13588427007198334 
model_pd.l_d.mean(): -23.468732833862305 
model_pd.lagr.mean(): -23.332849502563477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1832], device='cuda:0')), ('power', tensor([-23.6520], device='cuda:0'))])
epoch£º986	 i:0 	 global-step:19720	 l-p:0.13588427007198334
epoch£º986	 i:1 	 global-step:19721	 l-p:0.12848365306854248
epoch£º986	 i:2 	 global-step:19722	 l-p:0.14878755807876587
epoch£º986	 i:3 	 global-step:19723	 l-p:0.2107831835746765
epoch£º986	 i:4 	 global-step:19724	 l-p:0.13538265228271484
epoch£º986	 i:5 	 global-step:19725	 l-p:0.13482841849327087
epoch£º986	 i:6 	 global-step:19726	 l-p:0.14658096432685852
epoch£º986	 i:7 	 global-step:19727	 l-p:0.1279020458459854
epoch£º986	 i:8 	 global-step:19728	 l-p:0.9703691005706787
epoch£º986	 i:9 	 global-step:19729	 l-p:0.957899808883667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6984, 3.6984, 3.6984],
        [3.6984, 3.6984, 3.6984],
        [3.6984, 3.6070, 3.6772],
        [3.6984, 3.5328, 3.6383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.11339742690324783 
model_pd.l_d.mean(): -22.7591609954834 
model_pd.lagr.mean(): -22.645763397216797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2040], device='cuda:0')), ('power', tensor([-22.9631], device='cuda:0'))])
epoch£º987	 i:0 	 global-step:19740	 l-p:0.11339742690324783
epoch£º987	 i:1 	 global-step:19741	 l-p:-0.5213617086410522
epoch£º987	 i:2 	 global-step:19742	 l-p:1.1062333583831787
epoch£º987	 i:3 	 global-step:19743	 l-p:0.15475815534591675
epoch£º987	 i:4 	 global-step:19744	 l-p:0.13955257833003998
epoch£º987	 i:5 	 global-step:19745	 l-p:-0.18163718283176422
epoch£º987	 i:6 	 global-step:19746	 l-p:0.13008831441402435
epoch£º987	 i:7 	 global-step:19747	 l-p:0.13655315339565277
epoch£º987	 i:8 	 global-step:19748	 l-p:0.13908059895038605
epoch£º987	 i:9 	 global-step:19749	 l-p:0.13775180280208588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6859, 2.9375, 2.6125],
        [3.6859, 3.0137, 2.8435],
        [3.6859, 3.6852, 3.6859],
        [3.6859, 3.6859, 3.6859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.14709270000457764 
model_pd.l_d.mean(): -23.613941192626953 
model_pd.lagr.mean(): -23.466848373413086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1444], device='cuda:0')), ('power', tensor([-23.7583], device='cuda:0'))])
epoch£º988	 i:0 	 global-step:19760	 l-p:0.14709270000457764
epoch£º988	 i:1 	 global-step:19761	 l-p:0.20739634335041046
epoch£º988	 i:2 	 global-step:19762	 l-p:0.13034138083457947
epoch£º988	 i:3 	 global-step:19763	 l-p:0.0605190210044384
epoch£º988	 i:4 	 global-step:19764	 l-p:0.1467391401529312
epoch£º988	 i:5 	 global-step:19765	 l-p:0.10628154873847961
epoch£º988	 i:6 	 global-step:19766	 l-p:0.1452428251504898
epoch£º988	 i:7 	 global-step:19767	 l-p:0.13346147537231445
epoch£º988	 i:8 	 global-step:19768	 l-p:0.09113816916942596
epoch£º988	 i:9 	 global-step:19769	 l-p:0.18543675541877747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6408, 3.1999, 3.2790],
        [3.6408, 3.6350, 3.6406],
        [3.6408, 3.3855, 3.5103],
        [3.6408, 3.6408, 3.6408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.12838123738765717 
model_pd.l_d.mean(): -23.631624221801758 
model_pd.lagr.mean(): -23.50324249267578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1515], device='cuda:0')), ('power', tensor([-23.7831], device='cuda:0'))])
epoch£º989	 i:0 	 global-step:19780	 l-p:0.12838123738765717
epoch£º989	 i:1 	 global-step:19781	 l-p:0.11643754690885544
epoch£º989	 i:2 	 global-step:19782	 l-p:0.14696979522705078
epoch£º989	 i:3 	 global-step:19783	 l-p:-0.01598980836570263
epoch£º989	 i:4 	 global-step:19784	 l-p:0.10735048353672028
epoch£º989	 i:5 	 global-step:19785	 l-p:0.1479721963405609
epoch£º989	 i:6 	 global-step:19786	 l-p:0.1325373649597168
epoch£º989	 i:7 	 global-step:19787	 l-p:0.14212746918201447
epoch£º989	 i:8 	 global-step:19788	 l-p:0.27675530314445496
epoch£º989	 i:9 	 global-step:19789	 l-p:0.12813813984394073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228]], device='cuda:0')
 pt:tensor([[3.6365, 3.1008, 2.4805],
        [3.6365, 3.0037, 2.9011],
        [3.6365, 2.8327, 2.2328],
        [3.6365, 3.1191, 2.5020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): -0.10771119594573975 
model_pd.l_d.mean(): -22.69721794128418 
model_pd.lagr.mean(): -22.804929733276367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3799], device='cuda:0')), ('power', tensor([-23.0771], device='cuda:0'))])
epoch£º990	 i:0 	 global-step:19800	 l-p:-0.10771119594573975
epoch£º990	 i:1 	 global-step:19801	 l-p:0.0860358327627182
epoch£º990	 i:2 	 global-step:19802	 l-p:0.12277491390705109
epoch£º990	 i:3 	 global-step:19803	 l-p:0.276424378156662
epoch£º990	 i:4 	 global-step:19804	 l-p:0.11115545779466629
epoch£º990	 i:5 	 global-step:19805	 l-p:0.14682765305042267
epoch£º990	 i:6 	 global-step:19806	 l-p:0.146642804145813
epoch£º990	 i:7 	 global-step:19807	 l-p:0.137992262840271
epoch£º990	 i:8 	 global-step:19808	 l-p:0.10674585402011871
epoch£º990	 i:9 	 global-step:19809	 l-p:0.20182010531425476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6341, 3.5393, 3.6117],
        [3.6341, 2.9438, 2.7525],
        [3.6341, 3.4801, 3.5816],
        [3.6341, 3.6266, 3.6338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.11007465422153473 
model_pd.l_d.mean(): -23.739091873168945 
model_pd.lagr.mean(): -23.629016876220703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1436], device='cuda:0')), ('power', tensor([-23.8826], device='cuda:0'))])
epoch£º991	 i:0 	 global-step:19820	 l-p:0.11007465422153473
epoch£º991	 i:1 	 global-step:19821	 l-p:0.15302538871765137
epoch£º991	 i:2 	 global-step:19822	 l-p:0.1317952275276184
epoch£º991	 i:3 	 global-step:19823	 l-p:0.15565961599349976
epoch£º991	 i:4 	 global-step:19824	 l-p:0.12587589025497437
epoch£º991	 i:5 	 global-step:19825	 l-p:-0.0774911418557167
epoch£º991	 i:6 	 global-step:19826	 l-p:0.13499750196933746
epoch£º991	 i:7 	 global-step:19827	 l-p:0.1536189168691635
epoch£º991	 i:8 	 global-step:19828	 l-p:0.4477923512458801
epoch£º991	 i:9 	 global-step:19829	 l-p:0.10535070300102234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6254, 3.6252, 3.6254],
        [3.6254, 3.5981, 3.6227],
        [3.6254, 2.9229, 2.2838],
        [3.6254, 2.8225, 2.2156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.09648120403289795 
model_pd.l_d.mean(): -23.608308792114258 
model_pd.lagr.mean(): -23.51182746887207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1895], device='cuda:0')), ('power', tensor([-23.7978], device='cuda:0'))])
epoch£º992	 i:0 	 global-step:19840	 l-p:0.09648120403289795
epoch£º992	 i:1 	 global-step:19841	 l-p:0.05721631273627281
epoch£º992	 i:2 	 global-step:19842	 l-p:0.1002672016620636
epoch£º992	 i:3 	 global-step:19843	 l-p:0.10749354958534241
epoch£º992	 i:4 	 global-step:19844	 l-p:0.15289467573165894
epoch£º992	 i:5 	 global-step:19845	 l-p:0.7193548679351807
epoch£º992	 i:6 	 global-step:19846	 l-p:0.16050532460212708
epoch£º992	 i:7 	 global-step:19847	 l-p:0.13250328600406647
epoch£º992	 i:8 	 global-step:19848	 l-p:0.2332589328289032
epoch£º992	 i:9 	 global-step:19849	 l-p:0.1162400096654892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6245, 2.9608, 2.3245],
        [3.6245, 2.8875, 2.6080],
        [3.6245, 3.6245, 3.6245],
        [3.6245, 3.6044, 3.6229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.19792045652866364 
model_pd.l_d.mean(): -23.536645889282227 
model_pd.lagr.mean(): -23.338726043701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2582], device='cuda:0')), ('power', tensor([-23.7948], device='cuda:0'))])
epoch£º993	 i:0 	 global-step:19860	 l-p:0.19792045652866364
epoch£º993	 i:1 	 global-step:19861	 l-p:0.14592626690864563
epoch£º993	 i:2 	 global-step:19862	 l-p:0.12075959891080856
epoch£º993	 i:3 	 global-step:19863	 l-p:0.15032392740249634
epoch£º993	 i:4 	 global-step:19864	 l-p:8.674509048461914
epoch£º993	 i:5 	 global-step:19865	 l-p:0.23033969104290009
epoch£º993	 i:6 	 global-step:19866	 l-p:0.12033408880233765
epoch£º993	 i:7 	 global-step:19867	 l-p:0.12356500327587128
epoch£º993	 i:8 	 global-step:19868	 l-p:0.13522525131702423
epoch£º993	 i:9 	 global-step:19869	 l-p:0.05465518683195114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6146, 3.6057, 3.6142],
        [3.6146, 2.9034, 2.2655],
        [3.6146, 2.8026, 2.2897],
        [3.6146, 3.6143, 3.6147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.1325138509273529 
model_pd.l_d.mean(): -23.426589965820312 
model_pd.lagr.mean(): -23.294076919555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2450], device='cuda:0')), ('power', tensor([-23.6715], device='cuda:0'))])
epoch£º994	 i:0 	 global-step:19880	 l-p:0.1325138509273529
epoch£º994	 i:1 	 global-step:19881	 l-p:0.09319721162319183
epoch£º994	 i:2 	 global-step:19882	 l-p:0.11015569418668747
epoch£º994	 i:3 	 global-step:19883	 l-p:0.0699811577796936
epoch£º994	 i:4 	 global-step:19884	 l-p:0.49403971433639526
epoch£º994	 i:5 	 global-step:19885	 l-p:0.13237310945987701
epoch£º994	 i:6 	 global-step:19886	 l-p:0.03678499907255173
epoch£º994	 i:7 	 global-step:19887	 l-p:0.15167565643787384
epoch£º994	 i:8 	 global-step:19888	 l-p:0.15810653567314148
epoch£º994	 i:9 	 global-step:19889	 l-p:0.1429099291563034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5995, 3.3655, 3.4888],
        [3.5995, 2.7832, 2.2043],
        [3.5995, 3.1934, 3.2922],
        [3.5995, 3.5957, 3.5994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.14831681549549103 
model_pd.l_d.mean(): -23.65099334716797 
model_pd.lagr.mean(): -23.502676010131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2200], device='cuda:0')), ('power', tensor([-23.8710], device='cuda:0'))])
epoch£º995	 i:0 	 global-step:19900	 l-p:0.14831681549549103
epoch£º995	 i:1 	 global-step:19901	 l-p:0.08670041710138321
epoch£º995	 i:2 	 global-step:19902	 l-p:0.126471608877182
epoch£º995	 i:3 	 global-step:19903	 l-p:0.20425306260585785
epoch£º995	 i:4 	 global-step:19904	 l-p:0.15534362196922302
epoch£º995	 i:5 	 global-step:19905	 l-p:0.1629708856344223
epoch£º995	 i:6 	 global-step:19906	 l-p:0.12185452878475189
epoch£º995	 i:7 	 global-step:19907	 l-p:0.048180706799030304
epoch£º995	 i:8 	 global-step:19908	 l-p:0.0676192194223404
epoch£º995	 i:9 	 global-step:19909	 l-p:0.11628584563732147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5761, 3.1072, 3.1734],
        [3.5761, 3.0502, 2.4401],
        [3.5761, 2.8363, 2.5636],
        [3.5761, 3.5748, 3.5761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.012468070723116398 
model_pd.l_d.mean(): -22.40165901184082 
model_pd.lagr.mean(): -22.389190673828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3998], device='cuda:0')), ('power', tensor([-22.8015], device='cuda:0'))])
epoch£º996	 i:0 	 global-step:19920	 l-p:0.012468070723116398
epoch£º996	 i:1 	 global-step:19921	 l-p:0.17449986934661865
epoch£º996	 i:2 	 global-step:19922	 l-p:0.03232776001095772
epoch£º996	 i:3 	 global-step:19923	 l-p:0.13849341869354248
epoch£º996	 i:4 	 global-step:19924	 l-p:0.15775111317634583
epoch£º996	 i:5 	 global-step:19925	 l-p:0.13525964319705963
epoch£º996	 i:6 	 global-step:19926	 l-p:0.13090504705905914
epoch£º996	 i:7 	 global-step:19927	 l-p:0.14223964512348175
epoch£º996	 i:8 	 global-step:19928	 l-p:0.13160736858844757
epoch£º996	 i:9 	 global-step:19929	 l-p:0.11534358561038971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6080, 3.4590, 3.5587],
        [3.6080, 3.6081, 3.6080],
        [3.6080, 2.9264, 2.7563],
        [3.6080, 2.9198, 2.2838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.14278803765773773 
model_pd.l_d.mean(): -23.179332733154297 
model_pd.lagr.mean(): -23.036544799804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3374], device='cuda:0')), ('power', tensor([-23.5167], device='cuda:0'))])
epoch£º997	 i:0 	 global-step:19940	 l-p:0.14278803765773773
epoch£º997	 i:1 	 global-step:19941	 l-p:0.12485162168741226
epoch£º997	 i:2 	 global-step:19942	 l-p:0.06446545571088791
epoch£º997	 i:3 	 global-step:19943	 l-p:0.11135044693946838
epoch£º997	 i:4 	 global-step:19944	 l-p:0.14591863751411438
epoch£º997	 i:5 	 global-step:19945	 l-p:0.08101774752140045
epoch£º997	 i:6 	 global-step:19946	 l-p:-0.44665342569351196
epoch£º997	 i:7 	 global-step:19947	 l-p:0.13420706987380981
epoch£º997	 i:8 	 global-step:19948	 l-p:0.10980941355228424
epoch£º997	 i:9 	 global-step:19949	 l-p:0.16441746056079865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5997, 2.7854, 2.1927],
        [3.5997, 3.5967, 3.5996],
        [3.5997, 3.5997, 3.5997],
        [3.5997, 3.5288, 3.5862]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.13987289369106293 
model_pd.l_d.mean(): -23.816923141479492 
model_pd.lagr.mean(): -23.67704963684082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1352], device='cuda:0')), ('power', tensor([-23.9521], device='cuda:0'))])
epoch£º998	 i:0 	 global-step:19960	 l-p:0.13987289369106293
epoch£º998	 i:1 	 global-step:19961	 l-p:0.040905024856328964
epoch£º998	 i:2 	 global-step:19962	 l-p:0.15783098340034485
epoch£º998	 i:3 	 global-step:19963	 l-p:0.1839187741279602
epoch£º998	 i:4 	 global-step:19964	 l-p:5.177377223968506
epoch£º998	 i:5 	 global-step:19965	 l-p:0.14084237813949585
epoch£º998	 i:6 	 global-step:19966	 l-p:0.15150988101959229
epoch£º998	 i:7 	 global-step:19967	 l-p:0.058997344225645065
epoch£º998	 i:8 	 global-step:19968	 l-p:-0.0919913649559021
epoch£º998	 i:9 	 global-step:19969	 l-p:0.140316441655159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6240, 3.6236, 3.6240],
        [3.6240, 3.2111, 3.3064],
        [3.6240, 3.6188, 3.6238],
        [3.6240, 3.6159, 3.6237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.1373269259929657 
model_pd.l_d.mean(): -22.001752853393555 
model_pd.lagr.mean(): -21.864425659179688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3803], device='cuda:0')), ('power', tensor([-22.3821], device='cuda:0'))])
epoch£º999	 i:0 	 global-step:19980	 l-p:0.1373269259929657
epoch£º999	 i:1 	 global-step:19981	 l-p:0.0738995224237442
epoch£º999	 i:2 	 global-step:19982	 l-p:0.11671393364667892
epoch£º999	 i:3 	 global-step:19983	 l-p:0.11766564846038818
epoch£º999	 i:4 	 global-step:19984	 l-p:0.19288131594657898
epoch£º999	 i:5 	 global-step:19985	 l-p:0.13491250574588776
epoch£º999	 i:6 	 global-step:19986	 l-p:0.21671761572360992
epoch£º999	 i:7 	 global-step:19987	 l-p:0.3145228624343872
epoch£º999	 i:8 	 global-step:19988	 l-p:0.0739508867263794
epoch£º999	 i:9 	 global-step:19989	 l-p:0.1115463599562645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6872, 3.5207, 3.6268],
        [3.6872, 2.8824, 2.3530],
        [3.6872, 3.5101, 3.6197],
        [3.6872, 2.9913, 2.3433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.11998916417360306 
model_pd.l_d.mean(): -23.474536895751953 
model_pd.lagr.mean(): -23.35454750061035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1752], device='cuda:0')), ('power', tensor([-23.6498], device='cuda:0'))])
epoch£º1000	 i:0 	 global-step:20000	 l-p:0.11998916417360306
epoch£º1000	 i:1 	 global-step:20001	 l-p:0.12290029972791672
epoch£º1000	 i:2 	 global-step:20002	 l-p:-0.06878682971000671
epoch£º1000	 i:3 	 global-step:20003	 l-p:-0.03069242462515831
epoch£º1000	 i:4 	 global-step:20004	 l-p:0.12707018852233887
epoch£º1000	 i:5 	 global-step:20005	 l-p:-11.749722480773926
epoch£º1000	 i:6 	 global-step:20006	 l-p:0.1301647126674652
epoch£º1000	 i:7 	 global-step:20007	 l-p:0.22150039672851562
epoch£º1000	 i:8 	 global-step:20008	 l-p:0.153911292552948
epoch£º1000	 i:9 	 global-step:20009	 l-p:0.14161185920238495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6973, 3.6952, 3.6972],
        [3.6973, 3.1480, 3.1407],
        [3.6973, 3.3712, 3.4917],
        [3.6973, 3.6972, 3.6973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): -0.5516107678413391 
model_pd.l_d.mean(): -22.81976318359375 
model_pd.lagr.mean(): -23.371374130249023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1975], device='cuda:0')), ('power', tensor([-23.0172], device='cuda:0'))])
epoch£º1001	 i:0 	 global-step:20020	 l-p:-0.5516107678413391
epoch£º1001	 i:1 	 global-step:20021	 l-p:0.1369992345571518
epoch£º1001	 i:2 	 global-step:20022	 l-p:0.10979685187339783
epoch£º1001	 i:3 	 global-step:20023	 l-p:0.15651032328605652
epoch£º1001	 i:4 	 global-step:20024	 l-p:0.14029069244861603
epoch£º1001	 i:5 	 global-step:20025	 l-p:0.5375863313674927
epoch£º1001	 i:6 	 global-step:20026	 l-p:-0.2806592583656311
epoch£º1001	 i:7 	 global-step:20027	 l-p:0.14382030069828033
epoch£º1001	 i:8 	 global-step:20028	 l-p:0.15374493598937988
epoch£º1001	 i:9 	 global-step:20029	 l-p:0.15030306577682495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7011, 3.6615, 3.6960],
        [3.7011, 3.2371, 3.3003],
        [3.7011, 3.2485, 2.6348],
        [3.7011, 3.4394, 3.5641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.1526661068201065 
model_pd.l_d.mean(): -23.46621322631836 
model_pd.lagr.mean(): -23.313547134399414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1855], device='cuda:0')), ('power', tensor([-23.6517], device='cuda:0'))])
epoch£º1002	 i:0 	 global-step:20040	 l-p:0.1526661068201065
epoch£º1002	 i:1 	 global-step:20041	 l-p:0.15143397450447083
epoch£º1002	 i:2 	 global-step:20042	 l-p:-0.47661399841308594
epoch£º1002	 i:3 	 global-step:20043	 l-p:0.10718514025211334
epoch£º1002	 i:4 	 global-step:20044	 l-p:0.17197935283184052
epoch£º1002	 i:5 	 global-step:20045	 l-p:34.321407318115234
epoch£º1002	 i:6 	 global-step:20046	 l-p:-0.057744357734918594
epoch£º1002	 i:7 	 global-step:20047	 l-p:0.16457806527614594
epoch£º1002	 i:8 	 global-step:20048	 l-p:0.12855133414268494
epoch£º1002	 i:9 	 global-step:20049	 l-p:0.12797123193740845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6953, 2.8980, 2.2923],
        [3.6953, 3.6953, 3.6953],
        [3.6953, 3.6953, 3.6953],
        [3.6953, 3.6168, 3.6791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.1279592365026474 
model_pd.l_d.mean(): -23.01280403137207 
model_pd.lagr.mean(): -22.884845733642578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2999], device='cuda:0')), ('power', tensor([-23.3127], device='cuda:0'))])
epoch£º1003	 i:0 	 global-step:20060	 l-p:0.1279592365026474
epoch£º1003	 i:1 	 global-step:20061	 l-p:0.13420219719409943
epoch£º1003	 i:2 	 global-step:20062	 l-p:0.140079528093338
epoch£º1003	 i:3 	 global-step:20063	 l-p:0.13163548707962036
epoch£º1003	 i:4 	 global-step:20064	 l-p:0.12184791266918182
epoch£º1003	 i:5 	 global-step:20065	 l-p:0.20906776189804077
epoch£º1003	 i:6 	 global-step:20066	 l-p:-0.49477577209472656
epoch£º1003	 i:7 	 global-step:20067	 l-p:-0.20428386330604553
epoch£º1003	 i:8 	 global-step:20068	 l-p:0.12951986491680145
epoch£º1003	 i:9 	 global-step:20069	 l-p:0.1762867569923401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6887, 2.9488, 2.6511],
        [3.6887, 2.9693, 2.3211],
        [3.6887, 3.1594, 3.1721],
        [3.6887, 3.6883, 3.6887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.21935191750526428 
model_pd.l_d.mean(): -23.383447647094727 
model_pd.lagr.mean(): -23.164094924926758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2634], device='cuda:0')), ('power', tensor([-23.6469], device='cuda:0'))])
epoch£º1004	 i:0 	 global-step:20080	 l-p:0.21935191750526428
epoch£º1004	 i:1 	 global-step:20081	 l-p:0.13448476791381836
epoch£º1004	 i:2 	 global-step:20082	 l-p:0.1228625476360321
epoch£º1004	 i:3 	 global-step:20083	 l-p:0.1251542866230011
epoch£º1004	 i:4 	 global-step:20084	 l-p:0.13002417981624603
epoch£º1004	 i:5 	 global-step:20085	 l-p:0.11895398050546646
epoch£º1004	 i:6 	 global-step:20086	 l-p:0.16504202783107758
epoch£º1004	 i:7 	 global-step:20087	 l-p:-0.06568297743797302
epoch£º1004	 i:8 	 global-step:20088	 l-p:0.11869628727436066
epoch£º1004	 i:9 	 global-step:20089	 l-p:-0.06045302376151085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6797, 3.4609, 3.5810],
        [3.6797, 3.1443, 3.1520],
        [3.6797, 3.6772, 3.6797],
        [3.6797, 3.3352, 3.4524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.1260305792093277 
model_pd.l_d.mean(): -23.46561050415039 
model_pd.lagr.mean(): -23.339580535888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1587], device='cuda:0')), ('power', tensor([-23.6243], device='cuda:0'))])
epoch£º1005	 i:0 	 global-step:20100	 l-p:0.1260305792093277
epoch£º1005	 i:1 	 global-step:20101	 l-p:0.11105575412511826
epoch£º1005	 i:2 	 global-step:20102	 l-p:-0.0902051106095314
epoch£º1005	 i:3 	 global-step:20103	 l-p:0.1392909586429596
epoch£º1005	 i:4 	 global-step:20104	 l-p:0.14229218661785126
epoch£º1005	 i:5 	 global-step:20105	 l-p:0.13418500125408173
epoch£º1005	 i:6 	 global-step:20106	 l-p:0.1483275443315506
epoch£º1005	 i:7 	 global-step:20107	 l-p:0.18323607742786407
epoch£º1005	 i:8 	 global-step:20108	 l-p:0.18166020512580872
epoch£º1005	 i:9 	 global-step:20109	 l-p:0.10798370093107224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]], device='cuda:0')
 pt:tensor([[3.6721, 3.1558, 2.5334],
        [3.6721, 2.8707, 2.2663],
        [3.6721, 3.0496, 2.4102],
        [3.6721, 3.0104, 2.3667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.017394935712218285 
model_pd.l_d.mean(): -22.688743591308594 
model_pd.lagr.mean(): -22.671348571777344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3855], device='cuda:0')), ('power', tensor([-23.0743], device='cuda:0'))])
epoch£º1006	 i:0 	 global-step:20120	 l-p:0.017394935712218285
epoch£º1006	 i:1 	 global-step:20121	 l-p:0.3040161430835724
epoch£º1006	 i:2 	 global-step:20122	 l-p:0.057307593524456024
epoch£º1006	 i:3 	 global-step:20123	 l-p:0.0910675972700119
epoch£º1006	 i:4 	 global-step:20124	 l-p:0.007827343419194221
epoch£º1006	 i:5 	 global-step:20125	 l-p:0.14283065497875214
epoch£º1006	 i:6 	 global-step:20126	 l-p:0.15643678605556488
epoch£º1006	 i:7 	 global-step:20127	 l-p:0.14047765731811523
epoch£º1006	 i:8 	 global-step:20128	 l-p:0.1344442218542099
epoch£º1006	 i:9 	 global-step:20129	 l-p:0.11645294725894928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6919, 3.5882, 3.6657],
        [3.6919, 3.6907, 3.6919],
        [3.6919, 3.6919, 3.6919],
        [3.6919, 3.6517, 3.6867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): -0.10025051236152649 
model_pd.l_d.mean(): -23.175762176513672 
model_pd.lagr.mean(): -23.276012420654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2784], device='cuda:0')), ('power', tensor([-23.4542], device='cuda:0'))])
epoch£º1007	 i:0 	 global-step:20140	 l-p:-0.10025051236152649
epoch£º1007	 i:1 	 global-step:20141	 l-p:0.14929170906543732
epoch£º1007	 i:2 	 global-step:20142	 l-p:0.12766823172569275
epoch£º1007	 i:3 	 global-step:20143	 l-p:0.11450101435184479
epoch£º1007	 i:4 	 global-step:20144	 l-p:0.1381765604019165
epoch£º1007	 i:5 	 global-step:20145	 l-p:0.11188697814941406
epoch£º1007	 i:6 	 global-step:20146	 l-p:0.2170938104391098
epoch£º1007	 i:7 	 global-step:20147	 l-p:0.15019957721233368
epoch£º1007	 i:8 	 global-step:20148	 l-p:0.13201378285884857
epoch£º1007	 i:9 	 global-step:20149	 l-p:-0.13094325363636017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6788, 3.2751, 3.3728],
        [3.6788, 3.6159, 3.6677],
        [3.6788, 2.8731, 2.2869],
        [3.6788, 2.8776, 2.3904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.21536527574062347 
model_pd.l_d.mean(): -23.614900588989258 
model_pd.lagr.mean(): -23.3995361328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2185], device='cuda:0')), ('power', tensor([-23.8334], device='cuda:0'))])
epoch£º1008	 i:0 	 global-step:20160	 l-p:0.21536527574062347
epoch£º1008	 i:1 	 global-step:20161	 l-p:0.24891433119773865
epoch£º1008	 i:2 	 global-step:20162	 l-p:0.09270419925451279
epoch£º1008	 i:3 	 global-step:20163	 l-p:0.12758760154247284
epoch£º1008	 i:4 	 global-step:20164	 l-p:-0.05026058852672577
epoch£º1008	 i:5 	 global-step:20165	 l-p:0.1455295830965042
epoch£º1008	 i:6 	 global-step:20166	 l-p:-0.09008482098579407
epoch£º1008	 i:7 	 global-step:20167	 l-p:0.1325528621673584
epoch£º1008	 i:8 	 global-step:20168	 l-p:0.13990721106529236
epoch£º1008	 i:9 	 global-step:20169	 l-p:0.12625154852867126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6759, 3.6707, 3.6758],
        [3.6759, 3.6758, 3.6759],
        [3.6759, 3.2901, 3.3951],
        [3.6759, 3.4411, 3.5639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.15159021317958832 
model_pd.l_d.mean(): -23.654285430908203 
model_pd.lagr.mean(): -23.502695083618164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1851], device='cuda:0')), ('power', tensor([-23.8394], device='cuda:0'))])
epoch£º1009	 i:0 	 global-step:20180	 l-p:0.15159021317958832
epoch£º1009	 i:1 	 global-step:20181	 l-p:0.174521803855896
epoch£º1009	 i:2 	 global-step:20182	 l-p:0.13931840658187866
epoch£º1009	 i:3 	 global-step:20183	 l-p:0.025971628725528717
epoch£º1009	 i:4 	 global-step:20184	 l-p:0.14132054150104523
epoch£º1009	 i:5 	 global-step:20185	 l-p:0.11838960647583008
epoch£º1009	 i:6 	 global-step:20186	 l-p:0.1329842507839203
epoch£º1009	 i:7 	 global-step:20187	 l-p:0.1234658807516098
epoch£º1009	 i:8 	 global-step:20188	 l-p:0.29919570684432983
epoch£º1009	 i:9 	 global-step:20189	 l-p:0.14695224165916443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6588, 2.8580, 2.2463],
        [3.6588, 3.6585, 3.6587],
        [3.6588, 3.6588, 3.6588],
        [3.6588, 3.6184, 3.6535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.12234474718570709 
model_pd.l_d.mean(): -23.26198387145996 
model_pd.lagr.mean(): -23.139638900756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2467], device='cuda:0')), ('power', tensor([-23.5087], device='cuda:0'))])
epoch£º1010	 i:0 	 global-step:20200	 l-p:0.12234474718570709
epoch£º1010	 i:1 	 global-step:20201	 l-p:0.1442020833492279
epoch£º1010	 i:2 	 global-step:20202	 l-p:0.11943120509386063
epoch£º1010	 i:3 	 global-step:20203	 l-p:0.1535165011882782
epoch£º1010	 i:4 	 global-step:20204	 l-p:0.1657971292734146
epoch£º1010	 i:5 	 global-step:20205	 l-p:0.12630939483642578
epoch£º1010	 i:6 	 global-step:20206	 l-p:0.10451341420412064
epoch£º1010	 i:7 	 global-step:20207	 l-p:-0.03562149778008461
epoch£º1010	 i:8 	 global-step:20208	 l-p:0.047471802681684494
epoch£º1010	 i:9 	 global-step:20209	 l-p:0.557339072227478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6320, 2.9470, 2.3071],
        [3.6320, 2.9665, 2.3283],
        [3.6320, 2.9599, 2.3211],
        [3.6320, 3.6311, 3.6320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.16843856871128082 
model_pd.l_d.mean(): -22.274429321289062 
model_pd.lagr.mean(): -22.10599136352539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4182], device='cuda:0')), ('power', tensor([-22.6926], device='cuda:0'))])
epoch£º1011	 i:0 	 global-step:20220	 l-p:0.16843856871128082
epoch£º1011	 i:1 	 global-step:20221	 l-p:0.20774775743484497
epoch£º1011	 i:2 	 global-step:20222	 l-p:0.016873054206371307
epoch£º1011	 i:3 	 global-step:20223	 l-p:0.08542558550834656
epoch£º1011	 i:4 	 global-step:20224	 l-p:0.359456330537796
epoch£º1011	 i:5 	 global-step:20225	 l-p:0.09792125225067139
epoch£º1011	 i:6 	 global-step:20226	 l-p:0.12464972585439682
epoch£º1011	 i:7 	 global-step:20227	 l-p:0.09504862874746323
epoch£º1011	 i:8 	 global-step:20228	 l-p:0.1233360767364502
epoch£º1011	 i:9 	 global-step:20229	 l-p:0.1406198889017105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6519, 3.3431, 3.4674],
        [3.6519, 3.6434, 3.6515],
        [3.6519, 2.8950, 2.5683],
        [3.6519, 3.4260, 3.5478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.12889231741428375 
model_pd.l_d.mean(): -23.656871795654297 
model_pd.lagr.mean(): -23.527978897094727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1785], device='cuda:0')), ('power', tensor([-23.8353], device='cuda:0'))])
epoch£º1012	 i:0 	 global-step:20240	 l-p:0.12889231741428375
epoch£º1012	 i:1 	 global-step:20241	 l-p:0.12956510484218597
epoch£º1012	 i:2 	 global-step:20242	 l-p:0.14581869542598724
epoch£º1012	 i:3 	 global-step:20243	 l-p:0.14666658639907837
epoch£º1012	 i:4 	 global-step:20244	 l-p:0.09139112383127213
epoch£º1012	 i:5 	 global-step:20245	 l-p:0.1889295130968094
epoch£º1012	 i:6 	 global-step:20246	 l-p:0.12494514137506485
epoch£º1012	 i:7 	 global-step:20247	 l-p:0.14361119270324707
epoch£º1012	 i:8 	 global-step:20248	 l-p:0.08260016143321991
epoch£º1012	 i:9 	 global-step:20249	 l-p:-0.04375534877181053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6439, 2.8579, 2.2286],
        [3.6439, 3.6439, 3.6439],
        [3.6439, 2.9361, 2.7166],
        [3.6439, 2.9002, 2.6073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.09448862075805664 
model_pd.l_d.mean(): -23.575395584106445 
model_pd.lagr.mean(): -23.480907440185547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2240], device='cuda:0')), ('power', tensor([-23.7994], device='cuda:0'))])
epoch£º1013	 i:0 	 global-step:20260	 l-p:0.09448862075805664
epoch£º1013	 i:1 	 global-step:20261	 l-p:0.2035684585571289
epoch£º1013	 i:2 	 global-step:20262	 l-p:0.12545400857925415
epoch£º1013	 i:3 	 global-step:20263	 l-p:0.12154525518417358
epoch£º1013	 i:4 	 global-step:20264	 l-p:0.24049854278564453
epoch£º1013	 i:5 	 global-step:20265	 l-p:0.10646333545446396
epoch£º1013	 i:6 	 global-step:20266	 l-p:2.9761223793029785
epoch£º1013	 i:7 	 global-step:20267	 l-p:0.13722841441631317
epoch£º1013	 i:8 	 global-step:20268	 l-p:0.14195673167705536
epoch£º1013	 i:9 	 global-step:20269	 l-p:0.07501997798681259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6599, 3.2427, 2.6412],
        [3.6599, 3.6593, 3.6599],
        [3.6599, 3.6599, 3.6599],
        [3.6599, 2.9236, 2.6432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.08337770402431488 
model_pd.l_d.mean(): -23.124332427978516 
model_pd.lagr.mean(): -23.04095458984375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2371], device='cuda:0')), ('power', tensor([-23.3614], device='cuda:0'))])
epoch£º1014	 i:0 	 global-step:20280	 l-p:0.08337770402431488
epoch£º1014	 i:1 	 global-step:20281	 l-p:0.23594416677951813
epoch£º1014	 i:2 	 global-step:20282	 l-p:0.10584463179111481
epoch£º1014	 i:3 	 global-step:20283	 l-p:-0.03546728938817978
epoch£º1014	 i:4 	 global-step:20284	 l-p:0.12325706332921982
epoch£º1014	 i:5 	 global-step:20285	 l-p:0.1476924866437912
epoch£º1014	 i:6 	 global-step:20286	 l-p:0.12010661512613297
epoch£º1014	 i:7 	 global-step:20287	 l-p:0.2753121256828308
epoch£º1014	 i:8 	 global-step:20288	 l-p:0.16034534573554993
epoch£º1014	 i:9 	 global-step:20289	 l-p:0.13766881823539734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6727, 2.8755, 2.2592],
        [3.6727, 3.6617, 3.6721],
        [3.6727, 3.6692, 3.6726],
        [3.6727, 3.2135, 2.6017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.14117498695850372 
model_pd.l_d.mean(): -23.324365615844727 
model_pd.lagr.mean(): -23.183191299438477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2785], device='cuda:0')), ('power', tensor([-23.6028], device='cuda:0'))])
epoch£º1015	 i:0 	 global-step:20300	 l-p:0.14117498695850372
epoch£º1015	 i:1 	 global-step:20301	 l-p:0.04745874181389809
epoch£º1015	 i:2 	 global-step:20302	 l-p:0.11417331546545029
epoch£º1015	 i:3 	 global-step:20303	 l-p:0.06608240306377411
epoch£º1015	 i:4 	 global-step:20304	 l-p:0.2247859537601471
epoch£º1015	 i:5 	 global-step:20305	 l-p:0.15488003194332123
epoch£º1015	 i:6 	 global-step:20306	 l-p:0.1465798020362854
epoch£º1015	 i:7 	 global-step:20307	 l-p:0.12583881616592407
epoch£º1015	 i:8 	 global-step:20308	 l-p:0.11542464792728424
epoch£º1015	 i:9 	 global-step:20309	 l-p:0.11750325560569763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6765, 3.6758, 3.6765],
        [3.6765, 3.6765, 3.6765],
        [3.6765, 3.5719, 3.6499],
        [3.6765, 3.6736, 3.6764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.13413523137569427 
model_pd.l_d.mean(): -22.629087448120117 
model_pd.lagr.mean(): -22.494953155517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2254], device='cuda:0')), ('power', tensor([-22.8544], device='cuda:0'))])
epoch£º1016	 i:0 	 global-step:20320	 l-p:0.13413523137569427
epoch£º1016	 i:1 	 global-step:20321	 l-p:0.1556958109140396
epoch£º1016	 i:2 	 global-step:20322	 l-p:0.23693229258060455
epoch£º1016	 i:3 	 global-step:20323	 l-p:0.058044251054525375
epoch£º1016	 i:4 	 global-step:20324	 l-p:0.15156368911266327
epoch£º1016	 i:5 	 global-step:20325	 l-p:0.09222280234098434
epoch£º1016	 i:6 	 global-step:20326	 l-p:0.14767144620418549
epoch£º1016	 i:7 	 global-step:20327	 l-p:0.13681648671627045
epoch£º1016	 i:8 	 global-step:20328	 l-p:0.14759977161884308
epoch£º1016	 i:9 	 global-step:20329	 l-p:-0.03878568857908249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6757, 3.6756, 3.6757],
        [3.6757, 3.5012, 3.6102],
        [3.6757, 3.1809, 3.2246],
        [3.6757, 3.6757, 3.6757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.1108466312289238 
model_pd.l_d.mean(): -23.24530601501465 
model_pd.lagr.mean(): -23.134458541870117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2402], device='cuda:0')), ('power', tensor([-23.4855], device='cuda:0'))])
epoch£º1017	 i:0 	 global-step:20340	 l-p:0.1108466312289238
epoch£º1017	 i:1 	 global-step:20341	 l-p:0.1260787397623062
epoch£º1017	 i:2 	 global-step:20342	 l-p:-0.01221105083823204
epoch£º1017	 i:3 	 global-step:20343	 l-p:0.15154002606868744
epoch£º1017	 i:4 	 global-step:20344	 l-p:0.14148090779781342
epoch£º1017	 i:5 	 global-step:20345	 l-p:0.061626333743333817
epoch£º1017	 i:6 	 global-step:20346	 l-p:0.29865169525146484
epoch£º1017	 i:7 	 global-step:20347	 l-p:0.20163314044475555
epoch£º1017	 i:8 	 global-step:20348	 l-p:0.14069436490535736
epoch£º1017	 i:9 	 global-step:20349	 l-p:0.10440592467784882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6693, 3.6608, 3.6689],
        [3.6693, 3.5137, 3.6159],
        [3.6693, 2.9348, 2.2890],
        [3.6693, 3.0372, 2.9364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.11814466863870621 
model_pd.l_d.mean(): -22.953609466552734 
model_pd.lagr.mean(): -22.835464477539062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2440], device='cuda:0')), ('power', tensor([-23.1976], device='cuda:0'))])
epoch£º1018	 i:0 	 global-step:20360	 l-p:0.11814466863870621
epoch£º1018	 i:1 	 global-step:20361	 l-p:0.16429950296878815
epoch£º1018	 i:2 	 global-step:20362	 l-p:0.03643479198217392
epoch£º1018	 i:3 	 global-step:20363	 l-p:0.1371127814054489
epoch£º1018	 i:4 	 global-step:20364	 l-p:0.12852893769741058
epoch£º1018	 i:5 	 global-step:20365	 l-p:0.10679536312818527
epoch£º1018	 i:6 	 global-step:20366	 l-p:0.2099502831697464
epoch£º1018	 i:7 	 global-step:20367	 l-p:0.05515393242239952
epoch£º1018	 i:8 	 global-step:20368	 l-p:0.11978377401828766
epoch£º1018	 i:9 	 global-step:20369	 l-p:0.3209107220172882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6672, 3.6477, 3.6657],
        [3.6672, 3.0743, 3.0237],
        [3.6672, 2.9635, 2.3173],
        [3.6672, 3.2308, 2.6243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.13679470121860504 
model_pd.l_d.mean(): -23.50053596496582 
model_pd.lagr.mean(): -23.363740921020508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1703], device='cuda:0')), ('power', tensor([-23.6708], device='cuda:0'))])
epoch£º1019	 i:0 	 global-step:20380	 l-p:0.13679470121860504
epoch£º1019	 i:1 	 global-step:20381	 l-p:0.14966072142124176
epoch£º1019	 i:2 	 global-step:20382	 l-p:0.1463114470243454
epoch£º1019	 i:3 	 global-step:20383	 l-p:0.3261254131793976
epoch£º1019	 i:4 	 global-step:20384	 l-p:0.12270303815603256
epoch£º1019	 i:5 	 global-step:20385	 l-p:0.13219690322875977
epoch£º1019	 i:6 	 global-step:20386	 l-p:0.0518634133040905
epoch£º1019	 i:7 	 global-step:20387	 l-p:0.08548067510128021
epoch£º1019	 i:8 	 global-step:20388	 l-p:0.1299576312303543
epoch£º1019	 i:9 	 global-step:20389	 l-p:0.1806141585111618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6649, 2.8696, 2.4238],
        [3.6649, 3.4927, 3.6010],
        [3.6649, 2.9284, 2.6477],
        [3.6649, 2.8731, 2.2483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.3461570143699646 
model_pd.l_d.mean(): -23.47892189025879 
model_pd.lagr.mean(): -23.13276481628418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2176], device='cuda:0')), ('power', tensor([-23.6965], device='cuda:0'))])
epoch£º1020	 i:0 	 global-step:20400	 l-p:0.3461570143699646
epoch£º1020	 i:1 	 global-step:20401	 l-p:0.13365110754966736
epoch£º1020	 i:2 	 global-step:20402	 l-p:0.1517651379108429
epoch£º1020	 i:3 	 global-step:20403	 l-p:0.1579250991344452
epoch£º1020	 i:4 	 global-step:20404	 l-p:-0.05250917002558708
epoch£º1020	 i:5 	 global-step:20405	 l-p:0.0792379379272461
epoch£º1020	 i:6 	 global-step:20406	 l-p:0.17578789591789246
epoch£º1020	 i:7 	 global-step:20407	 l-p:0.1501273214817047
epoch£º1020	 i:8 	 global-step:20408	 l-p:0.11647800356149673
epoch£º1020	 i:9 	 global-step:20409	 l-p:0.12649303674697876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6734, 2.9818, 2.7877],
        [3.6734, 3.6734, 3.6734],
        [3.6734, 2.8923, 2.4928],
        [3.6734, 3.1032, 3.0779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.12674663960933685 
model_pd.l_d.mean(): -23.64522361755371 
model_pd.lagr.mean(): -23.518476486206055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1884], device='cuda:0')), ('power', tensor([-23.8337], device='cuda:0'))])
epoch£º1021	 i:0 	 global-step:20420	 l-p:0.12674663960933685
epoch£º1021	 i:1 	 global-step:20421	 l-p:0.1216680109500885
epoch£º1021	 i:2 	 global-step:20422	 l-p:0.04389393702149391
epoch£º1021	 i:3 	 global-step:20423	 l-p:0.2033299207687378
epoch£º1021	 i:4 	 global-step:20424	 l-p:0.12437858432531357
epoch£º1021	 i:5 	 global-step:20425	 l-p:0.10376328974962234
epoch£º1021	 i:6 	 global-step:20426	 l-p:0.1545676290988922
epoch£º1021	 i:7 	 global-step:20427	 l-p:0.07313879579305649
epoch£º1021	 i:8 	 global-step:20428	 l-p:0.3044929802417755
epoch£º1021	 i:9 	 global-step:20429	 l-p:0.13140133023262024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6672, 3.6670, 3.6672],
        [3.6672, 2.8719, 2.4261],
        [3.6672, 3.2529, 3.3467],
        [3.6672, 3.6672, 3.6672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.13172313570976257 
model_pd.l_d.mean(): -23.5877628326416 
model_pd.lagr.mean(): -23.456039428710938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1715], device='cuda:0')), ('power', tensor([-23.7592], device='cuda:0'))])
epoch£º1022	 i:0 	 global-step:20440	 l-p:0.13172313570976257
epoch£º1022	 i:1 	 global-step:20441	 l-p:0.1819853037595749
epoch£º1022	 i:2 	 global-step:20442	 l-p:0.1309075802564621
epoch£º1022	 i:3 	 global-step:20443	 l-p:0.12425345182418823
epoch£º1022	 i:4 	 global-step:20444	 l-p:0.09750863909721375
epoch£º1022	 i:5 	 global-step:20445	 l-p:0.19939064979553223
epoch£º1022	 i:6 	 global-step:20446	 l-p:0.14207947254180908
epoch£º1022	 i:7 	 global-step:20447	 l-p:0.5369613766670227
epoch£º1022	 i:8 	 global-step:20448	 l-p:0.1600753366947174
epoch£º1022	 i:9 	 global-step:20449	 l-p:0.016654271632432938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6559, 3.6547, 3.6559],
        [3.6559, 2.8799, 2.5032],
        [3.6559, 3.6559, 3.6559],
        [3.6559, 3.3219, 3.4425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.1375122368335724 
model_pd.l_d.mean(): -23.8065242767334 
model_pd.lagr.mean(): -23.66901206970215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0972], device='cuda:0')), ('power', tensor([-23.9037], device='cuda:0'))])
epoch£º1023	 i:0 	 global-step:20460	 l-p:0.1375122368335724
epoch£º1023	 i:1 	 global-step:20461	 l-p:2.3658604621887207
epoch£º1023	 i:2 	 global-step:20462	 l-p:0.11060599982738495
epoch£º1023	 i:3 	 global-step:20463	 l-p:0.15563705563545227
epoch£º1023	 i:4 	 global-step:20464	 l-p:0.14540418982505798
epoch£º1023	 i:5 	 global-step:20465	 l-p:0.23850157856941223
epoch£º1023	 i:6 	 global-step:20466	 l-p:0.120906300842762
epoch£º1023	 i:7 	 global-step:20467	 l-p:0.12774638831615448
epoch£º1023	 i:8 	 global-step:20468	 l-p:0.08258918672800064
epoch£º1023	 i:9 	 global-step:20469	 l-p:0.0667993575334549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6566, 3.6566, 3.6566],
        [3.6566, 2.9188, 2.2744],
        [3.6566, 2.9044, 2.2618],
        [3.6566, 3.6566, 3.6566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.14826080203056335 
model_pd.l_d.mean(): -23.019805908203125 
model_pd.lagr.mean(): -22.871545791625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3048], device='cuda:0')), ('power', tensor([-23.3246], device='cuda:0'))])
epoch£º1024	 i:0 	 global-step:20480	 l-p:0.14826080203056335
epoch£º1024	 i:1 	 global-step:20481	 l-p:0.7295787930488586
epoch£º1024	 i:2 	 global-step:20482	 l-p:0.0746299996972084
epoch£º1024	 i:3 	 global-step:20483	 l-p:0.1491154134273529
epoch£º1024	 i:4 	 global-step:20484	 l-p:0.12565015256404877
epoch£º1024	 i:5 	 global-step:20485	 l-p:0.17040245234966278
epoch£º1024	 i:6 	 global-step:20486	 l-p:0.14464031159877777
epoch£º1024	 i:7 	 global-step:20487	 l-p:0.08760461956262589
epoch£º1024	 i:8 	 global-step:20488	 l-p:0.061672620475292206
epoch£º1024	 i:9 	 global-step:20489	 l-p:0.13934923708438873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6694, 3.6668, 3.6693],
        [3.6694, 3.6597, 3.6689],
        [3.6694, 3.6694, 3.6694],
        [3.6694, 3.6678, 3.6694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.029252519831061363 
model_pd.l_d.mean(): -23.58784294128418 
model_pd.lagr.mean(): -23.558589935302734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1353], device='cuda:0')), ('power', tensor([-23.7232], device='cuda:0'))])
epoch£º1025	 i:0 	 global-step:20500	 l-p:0.029252519831061363
epoch£º1025	 i:1 	 global-step:20501	 l-p:0.14104247093200684
epoch£º1025	 i:2 	 global-step:20502	 l-p:0.13072769343852997
epoch£º1025	 i:3 	 global-step:20503	 l-p:0.11850868165493011
epoch£º1025	 i:4 	 global-step:20504	 l-p:0.13070352375507355
epoch£º1025	 i:5 	 global-step:20505	 l-p:0.12123381346464157
epoch£º1025	 i:6 	 global-step:20506	 l-p:0.06902169436216354
epoch£º1025	 i:7 	 global-step:20507	 l-p:0.11742370575666428
epoch£º1025	 i:8 	 global-step:20508	 l-p:0.2066994458436966
epoch£º1025	 i:9 	 global-step:20509	 l-p:0.3167056441307068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7151,  0.6395,  1.0000,  0.5719,
          1.0000,  0.8943, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4406,  0.3353,  1.0000,  0.2551,
          1.0000,  0.7609, 31.6228]], device='cuda:0')
 pt:tensor([[3.6680, 3.2309, 2.6241],
        [3.6680, 2.9152, 2.5973],
        [3.6680, 2.9673, 2.3209],
        [3.6680, 2.8670, 2.3986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.097136951982975 
model_pd.l_d.mean(): -23.15760612487793 
model_pd.lagr.mean(): -23.060468673706055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3033], device='cuda:0')), ('power', tensor([-23.4609], device='cuda:0'))])
epoch£º1026	 i:0 	 global-step:20520	 l-p:0.097136951982975
epoch£º1026	 i:1 	 global-step:20521	 l-p:0.12539096176624298
epoch£º1026	 i:2 	 global-step:20522	 l-p:0.14215604960918427
epoch£º1026	 i:3 	 global-step:20523	 l-p:0.13589835166931152
epoch£º1026	 i:4 	 global-step:20524	 l-p:0.1426670253276825
epoch£º1026	 i:5 	 global-step:20525	 l-p:0.06618988513946533
epoch£º1026	 i:6 	 global-step:20526	 l-p:0.18542170524597168
epoch£º1026	 i:7 	 global-step:20527	 l-p:0.31467533111572266
epoch£º1026	 i:8 	 global-step:20528	 l-p:0.12929970026016235
epoch£º1026	 i:9 	 global-step:20529	 l-p:0.04795941337943077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6692, 2.9487, 2.7021],
        [3.6692, 2.8680, 2.2547],
        [3.6692, 3.4987, 3.6064],
        [3.6692, 3.0972, 3.0708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.11631831526756287 
model_pd.l_d.mean(): -23.188892364501953 
model_pd.lagr.mean(): -23.072574615478516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3150], device='cuda:0')), ('power', tensor([-23.5039], device='cuda:0'))])
epoch£º1027	 i:0 	 global-step:20540	 l-p:0.11631831526756287
epoch£º1027	 i:1 	 global-step:20541	 l-p:0.13126859068870544
epoch£º1027	 i:2 	 global-step:20542	 l-p:0.07174340635538101
epoch£º1027	 i:3 	 global-step:20543	 l-p:0.12875014543533325
epoch£º1027	 i:4 	 global-step:20544	 l-p:0.17959384620189667
epoch£º1027	 i:5 	 global-step:20545	 l-p:0.1465304046869278
epoch£º1027	 i:6 	 global-step:20546	 l-p:0.32063040137290955
epoch£º1027	 i:7 	 global-step:20547	 l-p:0.16939052939414978
epoch£º1027	 i:8 	 global-step:20548	 l-p:0.031071385368704796
epoch£º1027	 i:9 	 global-step:20549	 l-p:0.07963409274816513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6738, 3.6726, 3.6738],
        [3.6738, 3.6738, 3.6738],
        [3.6738, 3.6656, 3.6734],
        [3.6738, 3.6339, 3.6687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.14868465065956116 
model_pd.l_d.mean(): -23.73464012145996 
model_pd.lagr.mean(): -23.585954666137695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0932], device='cuda:0')), ('power', tensor([-23.8278], device='cuda:0'))])
epoch£º1028	 i:0 	 global-step:20560	 l-p:0.14868465065956116
epoch£º1028	 i:1 	 global-step:20561	 l-p:0.1492234468460083
epoch£º1028	 i:2 	 global-step:20562	 l-p:0.04591800272464752
epoch£º1028	 i:3 	 global-step:20563	 l-p:0.23361337184906006
epoch£º1028	 i:4 	 global-step:20564	 l-p:0.13881371915340424
epoch£º1028	 i:5 	 global-step:20565	 l-p:0.1329498142004013
epoch£º1028	 i:6 	 global-step:20566	 l-p:0.1413234919309616
epoch£º1028	 i:7 	 global-step:20567	 l-p:0.024615230038762093
epoch£º1028	 i:8 	 global-step:20568	 l-p:-0.08765487372875214
epoch£º1028	 i:9 	 global-step:20569	 l-p:0.17161205410957336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6882, 2.8928, 2.4382],
        [3.6882, 3.6882, 3.6882],
        [3.6882, 3.6882, 3.6882],
        [3.6882, 3.2552, 3.3391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.12890271842479706 
model_pd.l_d.mean(): -23.67142677307129 
model_pd.lagr.mean(): -23.542524337768555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1308], device='cuda:0')), ('power', tensor([-23.8022], device='cuda:0'))])
epoch£º1029	 i:0 	 global-step:20580	 l-p:0.12890271842479706
epoch£º1029	 i:1 	 global-step:20581	 l-p:0.12387729436159134
epoch£º1029	 i:2 	 global-step:20582	 l-p:-0.6133856177330017
epoch£º1029	 i:3 	 global-step:20583	 l-p:0.1400333195924759
epoch£º1029	 i:4 	 global-step:20584	 l-p:0.17528854310512543
epoch£º1029	 i:5 	 global-step:20585	 l-p:0.13239580392837524
epoch£º1029	 i:6 	 global-step:20586	 l-p:0.18937568366527557
epoch£º1029	 i:7 	 global-step:20587	 l-p:0.12696769833564758
epoch£º1029	 i:8 	 global-step:20588	 l-p:-1.2156115770339966
epoch£º1029	 i:9 	 global-step:20589	 l-p:0.17092400789260864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7068, 3.6835, 3.7047],
        [3.7068, 3.6378, 3.6938],
        [3.7068, 3.0481, 2.3991],
        [3.7068, 3.2281, 3.2824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.12517066299915314 
model_pd.l_d.mean(): -23.306245803833008 
model_pd.lagr.mean(): -23.181076049804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2192], device='cuda:0')), ('power', tensor([-23.5255], device='cuda:0'))])
epoch£º1030	 i:0 	 global-step:20600	 l-p:0.12517066299915314
epoch£º1030	 i:1 	 global-step:20601	 l-p:-4.133732318878174
epoch£º1030	 i:2 	 global-step:20602	 l-p:0.12872463464736938
epoch£º1030	 i:3 	 global-step:20603	 l-p:0.3153078854084015
epoch£º1030	 i:4 	 global-step:20604	 l-p:0.09100047498941422
epoch£º1030	 i:5 	 global-step:20605	 l-p:0.131739541888237
epoch£º1030	 i:6 	 global-step:20606	 l-p:0.12112760543823242
epoch£º1030	 i:7 	 global-step:20607	 l-p:0.1483909636735916
epoch£º1030	 i:8 	 global-step:20608	 l-p:0.17189036309719086
epoch£º1030	 i:9 	 global-step:20609	 l-p:0.13625608384609222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7111, 3.1805, 2.5495],
        [3.7111, 2.9246, 2.2968],
        [3.7111, 3.0983, 3.0194],
        [3.7111, 3.6708, 3.7059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.11832387745380402 
model_pd.l_d.mean(): -23.125015258789062 
model_pd.lagr.mean(): -23.006690979003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2303], device='cuda:0')), ('power', tensor([-23.3553], device='cuda:0'))])
epoch£º1031	 i:0 	 global-step:20620	 l-p:0.11832387745380402
epoch£º1031	 i:1 	 global-step:20621	 l-p:0.12472675740718842
epoch£º1031	 i:2 	 global-step:20622	 l-p:0.1369594782590866
epoch£º1031	 i:3 	 global-step:20623	 l-p:0.15022258460521698
epoch£º1031	 i:4 	 global-step:20624	 l-p:0.12145982682704926
epoch£º1031	 i:5 	 global-step:20625	 l-p:0.021662510931491852
epoch£º1031	 i:6 	 global-step:20626	 l-p:-0.09762579947710037
epoch£º1031	 i:7 	 global-step:20627	 l-p:0.1312086135149002
epoch£º1031	 i:8 	 global-step:20628	 l-p:0.1567007452249527
epoch£º1031	 i:9 	 global-step:20629	 l-p:-0.29846611618995667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6908, 3.1828, 3.2157],
        [3.6908, 3.5466, 3.6441],
        [3.6908, 3.6900, 3.6908],
        [3.6908, 3.4122, 3.5378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.140757218003273 
model_pd.l_d.mean(): -23.29232406616211 
model_pd.lagr.mean(): -23.151567459106445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2022], device='cuda:0')), ('power', tensor([-23.4945], device='cuda:0'))])
epoch£º1032	 i:0 	 global-step:20640	 l-p:0.140757218003273
epoch£º1032	 i:1 	 global-step:20641	 l-p:0.11058107018470764
epoch£º1032	 i:2 	 global-step:20642	 l-p:0.17754127085208893
epoch£º1032	 i:3 	 global-step:20643	 l-p:-0.3648894429206848
epoch£º1032	 i:4 	 global-step:20644	 l-p:0.1232113242149353
epoch£º1032	 i:5 	 global-step:20645	 l-p:0.13721013069152832
epoch£º1032	 i:6 	 global-step:20646	 l-p:-0.18904557824134827
epoch£º1032	 i:7 	 global-step:20647	 l-p:0.13667425513267517
epoch£º1032	 i:8 	 global-step:20648	 l-p:0.23155365884304047
epoch£º1032	 i:9 	 global-step:20649	 l-p:0.13169394433498383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6892, 3.6891, 3.6892],
        [3.6892, 2.9841, 2.7642],
        [3.6892, 3.2369, 3.3097],
        [3.6892, 3.6871, 3.6892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): -0.2523496747016907 
model_pd.l_d.mean(): -23.525474548339844 
model_pd.lagr.mean(): -23.77782440185547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2120], device='cuda:0')), ('power', tensor([-23.7375], device='cuda:0'))])
epoch£º1033	 i:0 	 global-step:20660	 l-p:-0.2523496747016907
epoch£º1033	 i:1 	 global-step:20661	 l-p:0.13186490535736084
epoch£º1033	 i:2 	 global-step:20662	 l-p:0.22857634723186493
epoch£º1033	 i:3 	 global-step:20663	 l-p:0.13526451587677002
epoch£º1033	 i:4 	 global-step:20664	 l-p:0.1310388147830963
epoch£º1033	 i:5 	 global-step:20665	 l-p:0.14025560021400452
epoch£º1033	 i:6 	 global-step:20666	 l-p:0.12144433706998825
epoch£º1033	 i:7 	 global-step:20667	 l-p:0.12358324974775314
epoch£º1033	 i:8 	 global-step:20668	 l-p:0.03364598751068115
epoch£º1033	 i:9 	 global-step:20669	 l-p:0.08037517219781876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6760, 2.8690, 2.3664],
        [3.6760, 3.6558, 3.6744],
        [3.6760, 3.5718, 3.6497],
        [3.6760, 3.6708, 3.6758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.1621876358985901 
model_pd.l_d.mean(): -23.701217651367188 
model_pd.lagr.mean(): -23.539030075073242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1116], device='cuda:0')), ('power', tensor([-23.8128], device='cuda:0'))])
epoch£º1034	 i:0 	 global-step:20680	 l-p:0.1621876358985901
epoch£º1034	 i:1 	 global-step:20681	 l-p:0.13665853440761566
epoch£º1034	 i:2 	 global-step:20682	 l-p:0.1905209720134735
epoch£º1034	 i:3 	 global-step:20683	 l-p:0.1377338320016861
epoch£º1034	 i:4 	 global-step:20684	 l-p:0.15643846988677979
epoch£º1034	 i:5 	 global-step:20685	 l-p:0.09446431696414948
epoch£º1034	 i:6 	 global-step:20686	 l-p:0.3549613058567047
epoch£º1034	 i:7 	 global-step:20687	 l-p:0.0299068596214056
epoch£º1034	 i:8 	 global-step:20688	 l-p:0.030992358922958374
epoch£º1034	 i:9 	 global-step:20689	 l-p:0.1035052239894867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6722, 3.6483, 3.6700],
        [3.6722, 3.6561, 3.6710],
        [3.6722, 2.8761, 2.4296],
        [3.6722, 3.6381, 3.6682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.1575668901205063 
model_pd.l_d.mean(): -22.994733810424805 
model_pd.lagr.mean(): -22.837167739868164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2354], device='cuda:0')), ('power', tensor([-23.2301], device='cuda:0'))])
epoch£º1035	 i:0 	 global-step:20700	 l-p:0.1575668901205063
epoch£º1035	 i:1 	 global-step:20701	 l-p:0.19158773124217987
epoch£º1035	 i:2 	 global-step:20702	 l-p:0.1474585235118866
epoch£º1035	 i:3 	 global-step:20703	 l-p:0.10702572762966156
epoch£º1035	 i:4 	 global-step:20704	 l-p:0.015067811124026775
epoch£º1035	 i:5 	 global-step:20705	 l-p:-0.015042705461382866
epoch£º1035	 i:6 	 global-step:20706	 l-p:0.1361387073993683
epoch£º1035	 i:7 	 global-step:20707	 l-p:0.039956219494342804
epoch£º1035	 i:8 	 global-step:20708	 l-p:0.24787844717502594
epoch£º1035	 i:9 	 global-step:20709	 l-p:0.11455614864826202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6875, 3.6439, 3.6815],
        [3.6875, 3.2766, 2.6728],
        [3.6875, 3.2974, 3.4013],
        [3.6875, 3.3375, 3.4541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): -0.011472906917333603 
model_pd.l_d.mean(): -22.83619499206543 
model_pd.lagr.mean(): -22.847667694091797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3061], device='cuda:0')), ('power', tensor([-23.1423], device='cuda:0'))])
epoch£º1036	 i:0 	 global-step:20720	 l-p:-0.011472906917333603
epoch£º1036	 i:1 	 global-step:20721	 l-p:0.13704727590084076
epoch£º1036	 i:2 	 global-step:20722	 l-p:0.14510631561279297
epoch£º1036	 i:3 	 global-step:20723	 l-p:0.08172670006752014
epoch£º1036	 i:4 	 global-step:20724	 l-p:0.000665802916046232
epoch£º1036	 i:5 	 global-step:20725	 l-p:0.15055008232593536
epoch£º1036	 i:6 	 global-step:20726	 l-p:0.20554660260677338
epoch£º1036	 i:7 	 global-step:20727	 l-p:-0.6141852140426636
epoch£º1036	 i:8 	 global-step:20728	 l-p:0.1760040670633316
epoch£º1036	 i:9 	 global-step:20729	 l-p:0.1340441107749939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6995, 3.6618, 3.6949],
        [3.6995, 3.3386, 3.4519],
        [3.6995, 2.9975, 2.7819],
        [3.6995, 3.6920, 3.6992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): -0.35285940766334534 
model_pd.l_d.mean(): -22.550512313842773 
model_pd.lagr.mean(): -22.903371810913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2920], device='cuda:0')), ('power', tensor([-22.8426], device='cuda:0'))])
epoch£º1037	 i:0 	 global-step:20740	 l-p:-0.35285940766334534
epoch£º1037	 i:1 	 global-step:20741	 l-p:0.13200856745243073
epoch£º1037	 i:2 	 global-step:20742	 l-p:0.14023734629154205
epoch£º1037	 i:3 	 global-step:20743	 l-p:-0.3115403354167938
epoch£º1037	 i:4 	 global-step:20744	 l-p:0.16738319396972656
epoch£º1037	 i:5 	 global-step:20745	 l-p:0.09598420560359955
epoch£º1037	 i:6 	 global-step:20746	 l-p:0.13183000683784485
epoch£º1037	 i:7 	 global-step:20747	 l-p:0.3464159667491913
epoch£º1037	 i:8 	 global-step:20748	 l-p:0.12381812930107117
epoch£º1037	 i:9 	 global-step:20749	 l-p:0.13097143173217773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7082, 3.1631, 3.1618],
        [3.7082, 2.9338, 2.2950],
        [3.7082, 3.1777, 3.1903],
        [3.7082, 3.7082, 3.7082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.1355614811182022 
model_pd.l_d.mean(): -23.057361602783203 
model_pd.lagr.mean(): -22.92180061340332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1672], device='cuda:0')), ('power', tensor([-23.2245], device='cuda:0'))])
epoch£º1038	 i:0 	 global-step:20760	 l-p:0.1355614811182022
epoch£º1038	 i:1 	 global-step:20761	 l-p:0.14619001746177673
epoch£º1038	 i:2 	 global-step:20762	 l-p:0.11783454567193985
epoch£º1038	 i:3 	 global-step:20763	 l-p:0.14372634887695312
epoch£º1038	 i:4 	 global-step:20764	 l-p:-1.125936508178711
epoch£º1038	 i:5 	 global-step:20765	 l-p:0.6526544094085693
epoch£º1038	 i:6 	 global-step:20766	 l-p:0.1269868016242981
epoch£º1038	 i:7 	 global-step:20767	 l-p:-0.1477697640657425
epoch£º1038	 i:8 	 global-step:20768	 l-p:0.14697785675525665
epoch£º1038	 i:9 	 global-step:20769	 l-p:0.10162946581840515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6979, 3.5259, 3.6340],
        [3.6979, 3.3679, 3.4886],
        [3.6979, 3.5964, 3.6727],
        [3.6979, 2.8919, 2.3016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): -0.23917613923549652 
model_pd.l_d.mean(): -23.571889877319336 
model_pd.lagr.mean(): -23.811065673828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2257], device='cuda:0')), ('power', tensor([-23.7976], device='cuda:0'))])
epoch£º1039	 i:0 	 global-step:20780	 l-p:-0.23917613923549652
epoch£º1039	 i:1 	 global-step:20781	 l-p:0.11867080628871918
epoch£º1039	 i:2 	 global-step:20782	 l-p:0.11816733330488205
epoch£º1039	 i:3 	 global-step:20783	 l-p:0.1132122129201889
epoch£º1039	 i:4 	 global-step:20784	 l-p:0.19975000619888306
epoch£º1039	 i:5 	 global-step:20785	 l-p:0.12493199110031128
epoch£º1039	 i:6 	 global-step:20786	 l-p:0.13832439482212067
epoch£º1039	 i:7 	 global-step:20787	 l-p:0.1208016574382782
epoch£º1039	 i:8 	 global-step:20788	 l-p:-0.02724035270512104
epoch£º1039	 i:9 	 global-step:20789	 l-p:0.14098326861858368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6776, 3.6776, 3.6776],
        [3.6776, 3.6135, 3.6662],
        [3.6776, 2.8950, 2.4948],
        [3.6776, 3.6721, 3.6774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.01955253630876541 
model_pd.l_d.mean(): -23.265275955200195 
model_pd.lagr.mean(): -23.245723724365234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2943], device='cuda:0')), ('power', tensor([-23.5596], device='cuda:0'))])
epoch£º1040	 i:0 	 global-step:20800	 l-p:0.01955253630876541
epoch£º1040	 i:1 	 global-step:20801	 l-p:0.1317407339811325
epoch£º1040	 i:2 	 global-step:20802	 l-p:0.1310630440711975
epoch£º1040	 i:3 	 global-step:20803	 l-p:0.1407245546579361
epoch£º1040	 i:4 	 global-step:20804	 l-p:0.056772783398628235
epoch£º1040	 i:5 	 global-step:20805	 l-p:0.050896625965833664
epoch£º1040	 i:6 	 global-step:20806	 l-p:0.12690302729606628
epoch£º1040	 i:7 	 global-step:20807	 l-p:0.15069106221199036
epoch£º1040	 i:8 	 global-step:20808	 l-p:0.40949636697769165
epoch£º1040	 i:9 	 global-step:20809	 l-p:0.12269794195890427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6656, 3.6655, 3.6656],
        [3.6656, 3.1097, 3.1015],
        [3.6656, 3.6578, 3.6652],
        [3.6656, 3.6219, 3.6596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.11725577712059021 
model_pd.l_d.mean(): -22.500268936157227 
model_pd.lagr.mean(): -22.383012771606445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3041], device='cuda:0')), ('power', tensor([-22.8043], device='cuda:0'))])
epoch£º1041	 i:0 	 global-step:20820	 l-p:0.11725577712059021
epoch£º1041	 i:1 	 global-step:20821	 l-p:0.3562884032726288
epoch£º1041	 i:2 	 global-step:20822	 l-p:0.06059938296675682
epoch£º1041	 i:3 	 global-step:20823	 l-p:0.20386306941509247
epoch£º1041	 i:4 	 global-step:20824	 l-p:0.140620157122612
epoch£º1041	 i:5 	 global-step:20825	 l-p:0.17080752551555634
epoch£º1041	 i:6 	 global-step:20826	 l-p:0.1304396241903305
epoch£º1041	 i:7 	 global-step:20827	 l-p:0.1527114063501358
epoch£º1041	 i:8 	 global-step:20828	 l-p:0.07715509831905365
epoch£º1041	 i:9 	 global-step:20829	 l-p:0.1344173550605774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6587, 3.6587, 3.6587],
        [3.6587, 3.3117, 3.4300],
        [3.6587, 3.6584, 3.6587],
        [3.6587, 2.8454, 2.2606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.13100981712341309 
model_pd.l_d.mean(): -23.44000244140625 
model_pd.lagr.mean(): -23.308992385864258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2109], device='cuda:0')), ('power', tensor([-23.6509], device='cuda:0'))])
epoch£º1042	 i:0 	 global-step:20840	 l-p:0.13100981712341309
epoch£º1042	 i:1 	 global-step:20841	 l-p:0.15390445291996002
epoch£º1042	 i:2 	 global-step:20842	 l-p:0.09254930913448334
epoch£º1042	 i:3 	 global-step:20843	 l-p:0.11015857756137848
epoch£º1042	 i:4 	 global-step:20844	 l-p:0.13627801835536957
epoch£º1042	 i:5 	 global-step:20845	 l-p:0.12593208253383636
epoch£º1042	 i:6 	 global-step:20846	 l-p:0.48753949999809265
epoch£º1042	 i:7 	 global-step:20847	 l-p:0.13547047972679138
epoch£º1042	 i:8 	 global-step:20848	 l-p:0.14795400202274323
epoch£º1042	 i:9 	 global-step:20849	 l-p:0.13448359072208405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6644, 3.6643, 3.6644],
        [3.6644, 3.4987, 3.6049],
        [3.6644, 3.1698, 2.5518],
        [3.6644, 3.1722, 3.2198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.14673328399658203 
model_pd.l_d.mean(): -23.563447952270508 
model_pd.lagr.mean(): -23.41671371459961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2126], device='cuda:0')), ('power', tensor([-23.7760], device='cuda:0'))])
epoch£º1043	 i:0 	 global-step:20860	 l-p:0.14673328399658203
epoch£º1043	 i:1 	 global-step:20861	 l-p:0.11234273761510849
epoch£º1043	 i:2 	 global-step:20862	 l-p:0.14725199341773987
epoch£º1043	 i:3 	 global-step:20863	 l-p:1.043558120727539
epoch£º1043	 i:4 	 global-step:20864	 l-p:0.13577498495578766
epoch£º1043	 i:5 	 global-step:20865	 l-p:0.058612603694200516
epoch£º1043	 i:6 	 global-step:20866	 l-p:0.2853643298149109
epoch£º1043	 i:7 	 global-step:20867	 l-p:0.14737452566623688
epoch£º1043	 i:8 	 global-step:20868	 l-p:0.11372163146734238
epoch£º1043	 i:9 	 global-step:20869	 l-p:0.060568325221538544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6485, 3.6465, 3.6484],
        [3.6485, 3.6482, 3.6485],
        [3.6485, 3.5433, 3.6218],
        [3.6485, 2.8704, 2.2340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.16209106147289276 
model_pd.l_d.mean(): -23.36925506591797 
model_pd.lagr.mean(): -23.207164764404297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2183], device='cuda:0')), ('power', tensor([-23.5876], device='cuda:0'))])
epoch£º1044	 i:0 	 global-step:20880	 l-p:0.16209106147289276
epoch£º1044	 i:1 	 global-step:20881	 l-p:0.14036230742931366
epoch£º1044	 i:2 	 global-step:20882	 l-p:0.32180923223495483
epoch£º1044	 i:3 	 global-step:20883	 l-p:0.1330387443304062
epoch£º1044	 i:4 	 global-step:20884	 l-p:0.1261453926563263
epoch£º1044	 i:5 	 global-step:20885	 l-p:0.13235357403755188
epoch£º1044	 i:6 	 global-step:20886	 l-p:0.029743412509560585
epoch£º1044	 i:7 	 global-step:20887	 l-p:0.09483790397644043
epoch£º1044	 i:8 	 global-step:20888	 l-p:0.057807788252830505
epoch£º1044	 i:9 	 global-step:20889	 l-p:0.127219095826149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6348, 3.1049, 2.4847],
        [3.6348, 3.4601, 3.5695],
        [3.6348, 3.6238, 3.6342],
        [3.6348, 2.8649, 2.5173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.1404908001422882 
model_pd.l_d.mean(): -22.61501121520996 
model_pd.lagr.mean(): -22.474519729614258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3084], device='cuda:0')), ('power', tensor([-22.9234], device='cuda:0'))])
epoch£º1045	 i:0 	 global-step:20900	 l-p:0.1404908001422882
epoch£º1045	 i:1 	 global-step:20901	 l-p:0.03423056751489639
epoch£º1045	 i:2 	 global-step:20902	 l-p:0.09666899591684341
epoch£º1045	 i:3 	 global-step:20903	 l-p:0.16993845999240875
epoch£º1045	 i:4 	 global-step:20904	 l-p:0.11271147429943085
epoch£º1045	 i:5 	 global-step:20905	 l-p:0.0993696078658104
epoch£º1045	 i:6 	 global-step:20906	 l-p:0.18932929635047913
epoch£º1045	 i:7 	 global-step:20907	 l-p:0.13887475430965424
epoch£º1045	 i:8 	 global-step:20908	 l-p:0.25269800424575806
epoch£º1045	 i:9 	 global-step:20909	 l-p:0.08582606911659241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6531, 3.0013, 2.8774],
        [3.6531, 3.6519, 3.6531],
        [3.6531, 2.8434, 2.2389],
        [3.6531, 3.6532, 3.6532]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): -0.9937732219696045 
model_pd.l_d.mean(): -23.38935089111328 
model_pd.lagr.mean(): -24.38312339782715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2275], device='cuda:0')), ('power', tensor([-23.6168], device='cuda:0'))])
epoch£º1046	 i:0 	 global-step:20920	 l-p:-0.9937732219696045
epoch£º1046	 i:1 	 global-step:20921	 l-p:0.17204706370830536
epoch£º1046	 i:2 	 global-step:20922	 l-p:0.1746094822883606
epoch£º1046	 i:3 	 global-step:20923	 l-p:0.1467687338590622
epoch£º1046	 i:4 	 global-step:20924	 l-p:0.07295029610395432
epoch£º1046	 i:5 	 global-step:20925	 l-p:0.10462135076522827
epoch£º1046	 i:6 	 global-step:20926	 l-p:0.05686516314744949
epoch£º1046	 i:7 	 global-step:20927	 l-p:0.1345139592885971
epoch£º1046	 i:8 	 global-step:20928	 l-p:0.1450301855802536
epoch£º1046	 i:9 	 global-step:20929	 l-p:0.12707750499248505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6811, 3.3903, 3.5159],
        [3.6811, 3.6323, 3.6739],
        [3.6811, 2.9763, 2.7606],
        [3.6811, 3.5511, 3.6423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.09614007920026779 
model_pd.l_d.mean(): -23.15443992614746 
model_pd.lagr.mean(): -23.058300018310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2056], device='cuda:0')), ('power', tensor([-23.3600], device='cuda:0'))])
epoch£º1047	 i:0 	 global-step:20940	 l-p:0.09614007920026779
epoch£º1047	 i:1 	 global-step:20941	 l-p:0.16547083854675293
epoch£º1047	 i:2 	 global-step:20942	 l-p:0.12925635278224945
epoch£º1047	 i:3 	 global-step:20943	 l-p:0.12547805905342102
epoch£º1047	 i:4 	 global-step:20944	 l-p:0.1816943734884262
epoch£º1047	 i:5 	 global-step:20945	 l-p:0.15903404355049133
epoch£º1047	 i:6 	 global-step:20946	 l-p:0.03124253638088703
epoch£º1047	 i:7 	 global-step:20947	 l-p:0.13809676468372345
epoch£º1047	 i:8 	 global-step:20948	 l-p:0.05001585930585861
epoch£º1047	 i:9 	 global-step:20949	 l-p:0.3151355981826782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6678, 3.0412, 2.4012],
        [3.6678, 3.3038, 3.4176],
        [3.6678, 3.6678, 3.6678],
        [3.6678, 2.8752, 2.4487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.10128596425056458 
model_pd.l_d.mean(): -23.117441177368164 
model_pd.lagr.mean(): -23.016155242919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2844], device='cuda:0')), ('power', tensor([-23.4019], device='cuda:0'))])
epoch£º1048	 i:0 	 global-step:20960	 l-p:0.10128596425056458
epoch£º1048	 i:1 	 global-step:20961	 l-p:0.20592699944972992
epoch£º1048	 i:2 	 global-step:20962	 l-p:0.14095765352249146
epoch£º1048	 i:3 	 global-step:20963	 l-p:0.12713861465454102
epoch£º1048	 i:4 	 global-step:20964	 l-p:0.1188158392906189
epoch£º1048	 i:5 	 global-step:20965	 l-p:0.1743299812078476
epoch£º1048	 i:6 	 global-step:20966	 l-p:0.11053048819303513
epoch£º1048	 i:7 	 global-step:20967	 l-p:0.15088506042957306
epoch£º1048	 i:8 	 global-step:20968	 l-p:0.13737864792346954
epoch£º1048	 i:9 	 global-step:20969	 l-p:0.1389809101819992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6652, 3.5223, 3.6194],
        [3.6652, 2.8564, 2.2528],
        [3.6652, 3.2062, 2.5949],
        [3.6652, 3.2735, 3.3780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.12479279190301895 
model_pd.l_d.mean(): -22.861719131469727 
model_pd.lagr.mean(): -22.736927032470703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2718], device='cuda:0')), ('power', tensor([-23.1336], device='cuda:0'))])
epoch£º1049	 i:0 	 global-step:20980	 l-p:0.12479279190301895
epoch£º1049	 i:1 	 global-step:20981	 l-p:0.4345248341560364
epoch£º1049	 i:2 	 global-step:20982	 l-p:0.07858005911111832
epoch£º1049	 i:3 	 global-step:20983	 l-p:0.14107848703861237
epoch£º1049	 i:4 	 global-step:20984	 l-p:0.10710199177265167
epoch£º1049	 i:5 	 global-step:20985	 l-p:0.19808797538280487
epoch£º1049	 i:6 	 global-step:20986	 l-p:0.13214196264743805
epoch£º1049	 i:7 	 global-step:20987	 l-p:0.12642771005630493
epoch£º1049	 i:8 	 global-step:20988	 l-p:0.1315603256225586
epoch£º1049	 i:9 	 global-step:20989	 l-p:0.13741527497768402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6568, 3.0379, 2.9593],
        [3.6568, 3.5721, 3.6385],
        [3.6568, 3.6543, 3.6568],
        [3.6568, 3.5874, 3.6438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 2.3191728591918945 
model_pd.l_d.mean(): -23.735855102539062 
model_pd.lagr.mean(): -21.416683197021484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1721], device='cuda:0')), ('power', tensor([-23.9080], device='cuda:0'))])
epoch£º1050	 i:0 	 global-step:21000	 l-p:2.3191728591918945
epoch£º1050	 i:1 	 global-step:21001	 l-p:0.10808130353689194
epoch£º1050	 i:2 	 global-step:21002	 l-p:0.11090709269046783
epoch£º1050	 i:3 	 global-step:21003	 l-p:0.14335300028324127
epoch£º1050	 i:4 	 global-step:21004	 l-p:0.12931975722312927
epoch£º1050	 i:5 	 global-step:21005	 l-p:0.20119547843933105
epoch£º1050	 i:6 	 global-step:21006	 l-p:0.331186443567276
epoch£º1050	 i:7 	 global-step:21007	 l-p:0.10180328041315079
epoch£º1050	 i:8 	 global-step:21008	 l-p:0.11962658911943436
epoch£º1050	 i:9 	 global-step:21009	 l-p:0.11950426548719406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6386, 2.9133, 2.6673],
        [3.6386, 3.6366, 3.6386],
        [3.6386, 2.8995, 2.6249],
        [3.6386, 2.9567, 2.7890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.13322260975837708 
model_pd.l_d.mean(): -22.611770629882812 
model_pd.lagr.mean(): -22.478548049926758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3194], device='cuda:0')), ('power', tensor([-22.9312], device='cuda:0'))])
epoch£º1051	 i:0 	 global-step:21020	 l-p:0.13322260975837708
epoch£º1051	 i:1 	 global-step:21021	 l-p:-0.016668539494276047
epoch£º1051	 i:2 	 global-step:21022	 l-p:0.14782603085041046
epoch£º1051	 i:3 	 global-step:21023	 l-p:0.21008586883544922
epoch£º1051	 i:4 	 global-step:21024	 l-p:0.14556659758090973
epoch£º1051	 i:5 	 global-step:21025	 l-p:0.4002819061279297
epoch£º1051	 i:6 	 global-step:21026	 l-p:0.12628379464149475
epoch£º1051	 i:7 	 global-step:21027	 l-p:0.056397922337055206
epoch£º1051	 i:8 	 global-step:21028	 l-p:0.1312202513217926
epoch£º1051	 i:9 	 global-step:21029	 l-p:0.14808794856071472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6373, 3.6373, 3.6373],
        [3.6373, 3.6373, 3.6373],
        [3.6373, 3.3642, 3.4910],
        [3.6373, 3.1013, 3.1151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.20407874882221222 
model_pd.l_d.mean(): -23.585826873779297 
model_pd.lagr.mean(): -23.38174819946289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1808], device='cuda:0')), ('power', tensor([-23.7667], device='cuda:0'))])
epoch£º1052	 i:0 	 global-step:21040	 l-p:0.20407874882221222
epoch£º1052	 i:1 	 global-step:21041	 l-p:0.12990975379943848
epoch£º1052	 i:2 	 global-step:21042	 l-p:0.1382606327533722
epoch£º1052	 i:3 	 global-step:21043	 l-p:0.13146156072616577
epoch£º1052	 i:4 	 global-step:21044	 l-p:0.1743847280740738
epoch£º1052	 i:5 	 global-step:21045	 l-p:0.06962008774280548
epoch£º1052	 i:6 	 global-step:21046	 l-p:-0.7658474445343018
epoch£º1052	 i:7 	 global-step:21047	 l-p:0.17885395884513855
epoch£º1052	 i:8 	 global-step:21048	 l-p:0.13294512033462524
epoch£º1052	 i:9 	 global-step:21049	 l-p:0.031880710273981094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228]], device='cuda:0')
 pt:tensor([[3.6260, 2.8048, 2.2571],
        [3.6260, 3.0804, 2.4587],
        [3.6260, 3.0489, 3.0233],
        [3.6260, 3.2082, 3.3033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.14230556786060333 
model_pd.l_d.mean(): -23.521268844604492 
model_pd.lagr.mean(): -23.378963470458984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2325], device='cuda:0')), ('power', tensor([-23.7537], device='cuda:0'))])
epoch£º1053	 i:0 	 global-step:21060	 l-p:0.14230556786060333
epoch£º1053	 i:1 	 global-step:21061	 l-p:0.09548502415418625
epoch£º1053	 i:2 	 global-step:21062	 l-p:0.14499756693840027
epoch£º1053	 i:3 	 global-step:21063	 l-p:0.2844104468822479
epoch£º1053	 i:4 	 global-step:21064	 l-p:0.08946970105171204
epoch£º1053	 i:5 	 global-step:21065	 l-p:0.049270495772361755
epoch£º1053	 i:6 	 global-step:21066	 l-p:0.09634663164615631
epoch£º1053	 i:7 	 global-step:21067	 l-p:3.381960153579712
epoch£º1053	 i:8 	 global-step:21068	 l-p:0.14998002350330353
epoch£º1053	 i:9 	 global-step:21069	 l-p:0.12082013487815857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6373, 3.6318, 3.6370],
        [3.6373, 3.6373, 3.6373],
        [3.6373, 3.6366, 3.6373],
        [3.6373, 3.6295, 3.6369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.12062191963195801 
model_pd.l_d.mean(): -21.81206703186035 
model_pd.lagr.mean(): -21.691444396972656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4020], device='cuda:0')), ('power', tensor([-22.2141], device='cuda:0'))])
epoch£º1054	 i:0 	 global-step:21080	 l-p:0.12062191963195801
epoch£º1054	 i:1 	 global-step:21081	 l-p:0.12107299268245697
epoch£º1054	 i:2 	 global-step:21082	 l-p:0.12129226326942444
epoch£º1054	 i:3 	 global-step:21083	 l-p:0.07378464192152023
epoch£º1054	 i:4 	 global-step:21084	 l-p:0.16608235239982605
epoch£º1054	 i:5 	 global-step:21085	 l-p:0.22583390772342682
epoch£º1054	 i:6 	 global-step:21086	 l-p:0.1352039873600006
epoch£º1054	 i:7 	 global-step:21087	 l-p:0.13478046655654907
epoch£º1054	 i:8 	 global-step:21088	 l-p:0.14679405093193054
epoch£º1054	 i:9 	 global-step:21089	 l-p:0.17485029995441437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6373, 3.6266, 3.6367],
        [3.6373, 3.5494, 3.6178],
        [3.6373, 2.8664, 2.5183],
        [3.6373, 3.3013, 3.4228]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.13991117477416992 
model_pd.l_d.mean(): -22.866426467895508 
model_pd.lagr.mean(): -22.72651481628418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3680], device='cuda:0')), ('power', tensor([-23.2345], device='cuda:0'))])
epoch£º1055	 i:0 	 global-step:21100	 l-p:0.13991117477416992
epoch£º1055	 i:1 	 global-step:21101	 l-p:0.16018448770046234
epoch£º1055	 i:2 	 global-step:21102	 l-p:0.1396552473306656
epoch£º1055	 i:3 	 global-step:21103	 l-p:0.1511312872171402
epoch£º1055	 i:4 	 global-step:21104	 l-p:0.5027228593826294
epoch£º1055	 i:5 	 global-step:21105	 l-p:0.13178184628486633
epoch£º1055	 i:6 	 global-step:21106	 l-p:0.08041293919086456
epoch£º1055	 i:7 	 global-step:21107	 l-p:-0.04957187548279762
epoch£º1055	 i:8 	 global-step:21108	 l-p:0.1479467898607254
epoch£º1055	 i:9 	 global-step:21109	 l-p:0.13620001077651978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6407, 3.6407, 3.6407],
        [3.6407, 3.6398, 3.6407],
        [3.6407, 3.6269, 3.6398],
        [3.6407, 3.6406, 3.6407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.15824589133262634 
model_pd.l_d.mean(): -23.79545783996582 
model_pd.lagr.mean(): -23.6372127532959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1180], device='cuda:0')), ('power', tensor([-23.9134], device='cuda:0'))])
epoch£º1056	 i:0 	 global-step:21120	 l-p:0.15824589133262634
epoch£º1056	 i:1 	 global-step:21121	 l-p:0.12584957480430603
epoch£º1056	 i:2 	 global-step:21122	 l-p:0.1567540317773819
epoch£º1056	 i:3 	 global-step:21123	 l-p:0.07702987641096115
epoch£º1056	 i:4 	 global-step:21124	 l-p:0.12046531587839127
epoch£º1056	 i:5 	 global-step:21125	 l-p:0.15741926431655884
epoch£º1056	 i:6 	 global-step:21126	 l-p:0.016790350899100304
epoch£º1056	 i:7 	 global-step:21127	 l-p:0.14195133745670319
epoch£º1056	 i:8 	 global-step:21128	 l-p:0.8076880574226379
epoch£º1056	 i:9 	 global-step:21129	 l-p:0.1573510766029358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6344, 3.5173, 3.6022],
        [3.6344, 2.9447, 2.3031],
        [3.6344, 3.6344, 3.6344],
        [3.6344, 3.4609, 3.5700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.14599265158176422 
model_pd.l_d.mean(): -23.49070167541504 
model_pd.lagr.mean(): -23.344709396362305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2096], device='cuda:0')), ('power', tensor([-23.7003], device='cuda:0'))])
epoch£º1057	 i:0 	 global-step:21140	 l-p:0.14599265158176422
epoch£º1057	 i:1 	 global-step:21141	 l-p:0.1323283612728119
epoch£º1057	 i:2 	 global-step:21142	 l-p:0.0940920040011406
epoch£º1057	 i:3 	 global-step:21143	 l-p:0.08089334517717361
epoch£º1057	 i:4 	 global-step:21144	 l-p:0.7203591465950012
epoch£º1057	 i:5 	 global-step:21145	 l-p:0.11568082869052887
epoch£º1057	 i:6 	 global-step:21146	 l-p:0.09420271962881088
epoch£º1057	 i:7 	 global-step:21147	 l-p:0.1585329920053482
epoch£º1057	 i:8 	 global-step:21148	 l-p:0.1438795030117035
epoch£º1057	 i:9 	 global-step:21149	 l-p:0.1307530403137207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6468, 3.6125, 3.6429],
        [3.6468, 3.1524, 3.2005],
        [3.6468, 3.0594, 3.0209],
        [3.6468, 3.5414, 3.6201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.13717889785766602 
model_pd.l_d.mean(): -23.659141540527344 
model_pd.lagr.mean(): -23.521963119506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1675], device='cuda:0')), ('power', tensor([-23.8266], device='cuda:0'))])
epoch£º1058	 i:0 	 global-step:21160	 l-p:0.13717889785766602
epoch£º1058	 i:1 	 global-step:21161	 l-p:0.10832761228084564
epoch£º1058	 i:2 	 global-step:21162	 l-p:-0.07270412147045135
epoch£º1058	 i:3 	 global-step:21163	 l-p:0.12919597327709198
epoch£º1058	 i:4 	 global-step:21164	 l-p:0.15235935151576996
epoch£º1058	 i:5 	 global-step:21165	 l-p:0.16328735649585724
epoch£º1058	 i:6 	 global-step:21166	 l-p:0.0869026705622673
epoch£º1058	 i:7 	 global-step:21167	 l-p:0.546805202960968
epoch£º1058	 i:8 	 global-step:21168	 l-p:0.1281217783689499
epoch£º1058	 i:9 	 global-step:21169	 l-p:0.16351519525051117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6347, 3.6281, 3.6345],
        [3.6347, 3.2446, 3.3511],
        [3.6347, 3.6004, 3.6308],
        [3.6347, 3.5940, 3.6295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.12413676828145981 
model_pd.l_d.mean(): -23.53965950012207 
model_pd.lagr.mean(): -23.415523529052734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2011], device='cuda:0')), ('power', tensor([-23.7408], device='cuda:0'))])
epoch£º1059	 i:0 	 global-step:21180	 l-p:0.12413676828145981
epoch£º1059	 i:1 	 global-step:21181	 l-p:1.0154258012771606
epoch£º1059	 i:2 	 global-step:21182	 l-p:0.05660954490303993
epoch£º1059	 i:3 	 global-step:21183	 l-p:0.16285374760627747
epoch£º1059	 i:4 	 global-step:21184	 l-p:0.18236704170703888
epoch£º1059	 i:5 	 global-step:21185	 l-p:0.1396384835243225
epoch£º1059	 i:6 	 global-step:21186	 l-p:0.14092566072940826
epoch£º1059	 i:7 	 global-step:21187	 l-p:0.09160470962524414
epoch£º1059	 i:8 	 global-step:21188	 l-p:0.21325165033340454
epoch£º1059	 i:9 	 global-step:21189	 l-p:0.012988156639039516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6356, 3.6301, 3.6354],
        [3.6356, 2.8452, 2.4439],
        [3.6356, 2.8164, 2.2300],
        [3.6356, 3.6347, 3.6356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.13274940848350525 
model_pd.l_d.mean(): -23.127702713012695 
model_pd.lagr.mean(): -22.994953155517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1968], device='cuda:0')), ('power', tensor([-23.3246], device='cuda:0'))])
epoch£º1060	 i:0 	 global-step:21200	 l-p:0.13274940848350525
epoch£º1060	 i:1 	 global-step:21201	 l-p:0.1527189463376999
epoch£º1060	 i:2 	 global-step:21202	 l-p:0.11012418568134308
epoch£º1060	 i:3 	 global-step:21203	 l-p:0.15115554630756378
epoch£º1060	 i:4 	 global-step:21204	 l-p:0.08575865626335144
epoch£º1060	 i:5 	 global-step:21205	 l-p:0.5687859654426575
epoch£º1060	 i:6 	 global-step:21206	 l-p:0.21356482803821564
epoch£º1060	 i:7 	 global-step:21207	 l-p:0.11261089146137238
epoch£º1060	 i:8 	 global-step:21208	 l-p:-0.05126972496509552
epoch£º1060	 i:9 	 global-step:21209	 l-p:0.13802917301654816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6495, 3.6495, 3.6496],
        [3.6495, 3.2565, 3.3615],
        [3.6495, 3.1781, 2.5661],
        [3.6495, 3.0291, 2.9508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.12614484131336212 
model_pd.l_d.mean(): -23.030296325683594 
model_pd.lagr.mean(): -22.904151916503906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2954], device='cuda:0')), ('power', tensor([-23.3257], device='cuda:0'))])
epoch£º1061	 i:0 	 global-step:21220	 l-p:0.12614484131336212
epoch£º1061	 i:1 	 global-step:21221	 l-p:0.1020519807934761
epoch£º1061	 i:2 	 global-step:21222	 l-p:0.13948175311088562
epoch£º1061	 i:3 	 global-step:21223	 l-p:1.306397557258606
epoch£º1061	 i:4 	 global-step:21224	 l-p:0.10011960566043854
epoch£º1061	 i:5 	 global-step:21225	 l-p:0.2014647275209427
epoch£º1061	 i:6 	 global-step:21226	 l-p:0.11517328023910522
epoch£º1061	 i:7 	 global-step:21227	 l-p:0.12394769489765167
epoch£º1061	 i:8 	 global-step:21228	 l-p:0.15287534892559052
epoch£º1061	 i:9 	 global-step:21229	 l-p:0.13526596128940582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6807, 3.6799, 3.6807],
        [3.6807, 3.5960, 3.6623],
        [3.6807, 3.3291, 3.4463],
        [3.6807, 3.4007, 3.5269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.21781684458255768 
model_pd.l_d.mean(): -23.31535530090332 
model_pd.lagr.mean(): -23.097537994384766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2370], device='cuda:0')), ('power', tensor([-23.5524], device='cuda:0'))])
epoch£º1062	 i:0 	 global-step:21240	 l-p:0.21781684458255768
epoch£º1062	 i:1 	 global-step:21241	 l-p:0.0531199648976326
epoch£º1062	 i:2 	 global-step:21242	 l-p:0.02245006524026394
epoch£º1062	 i:3 	 global-step:21243	 l-p:0.149992436170578
epoch£º1062	 i:4 	 global-step:21244	 l-p:0.10039878636598587
epoch£º1062	 i:5 	 global-step:21245	 l-p:-0.09929279237985611
epoch£º1062	 i:6 	 global-step:21246	 l-p:0.130033478140831
epoch£º1062	 i:7 	 global-step:21247	 l-p:0.11973024159669876
epoch£º1062	 i:8 	 global-step:21248	 l-p:0.17563380300998688
epoch£º1062	 i:9 	 global-step:21249	 l-p:0.16374030709266663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6891, 3.6886, 3.6891],
        [3.6891, 3.1331, 3.1245],
        [3.6891, 3.6891, 3.6891],
        [3.6891, 3.0549, 2.9535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.13032545149326324 
model_pd.l_d.mean(): -23.538776397705078 
model_pd.lagr.mean(): -23.408451080322266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1520], device='cuda:0')), ('power', tensor([-23.6908], device='cuda:0'))])
epoch£º1063	 i:0 	 global-step:21260	 l-p:0.13032545149326324
epoch£º1063	 i:1 	 global-step:21261	 l-p:-0.2290259748697281
epoch£º1063	 i:2 	 global-step:21262	 l-p:0.1366979330778122
epoch£º1063	 i:3 	 global-step:21263	 l-p:0.13464364409446716
epoch£º1063	 i:4 	 global-step:21264	 l-p:0.11298809945583344
epoch£º1063	 i:5 	 global-step:21265	 l-p:0.1604396551847458
epoch£º1063	 i:6 	 global-step:21266	 l-p:0.1836446076631546
epoch£º1063	 i:7 	 global-step:21267	 l-p:0.17455047369003296
epoch£º1063	 i:8 	 global-step:21268	 l-p:0.11358347535133362
epoch£º1063	 i:9 	 global-step:21269	 l-p:0.13656331598758698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6718, 3.6714, 3.6719],
        [3.6718, 2.9594, 2.7341],
        [3.6718, 3.2143, 3.2863],
        [3.6718, 3.5268, 3.6249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.15833373367786407 
model_pd.l_d.mean(): -23.52541160583496 
model_pd.lagr.mean(): -23.36707878112793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1615], device='cuda:0')), ('power', tensor([-23.6869], device='cuda:0'))])
epoch£º1064	 i:0 	 global-step:21280	 l-p:0.15833373367786407
epoch£º1064	 i:1 	 global-step:21281	 l-p:0.07749686390161514
epoch£º1064	 i:2 	 global-step:21282	 l-p:0.06372641772031784
epoch£º1064	 i:3 	 global-step:21283	 l-p:0.1285569816827774
epoch£º1064	 i:4 	 global-step:21284	 l-p:0.1253388375043869
epoch£º1064	 i:5 	 global-step:21285	 l-p:0.11965589225292206
epoch£º1064	 i:6 	 global-step:21286	 l-p:1.0289620161056519
epoch£º1064	 i:7 	 global-step:21287	 l-p:0.1740022450685501
epoch£º1064	 i:8 	 global-step:21288	 l-p:0.23874175548553467
epoch£º1064	 i:9 	 global-step:21289	 l-p:0.1316644847393036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6569, 3.6569, 3.6569],
        [3.6569, 3.6569, 3.6569],
        [3.6569, 2.8666, 2.2340],
        [3.6569, 3.6556, 3.6569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.13157859444618225 
model_pd.l_d.mean(): -23.108383178710938 
model_pd.lagr.mean(): -22.976804733276367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2535], device='cuda:0')), ('power', tensor([-23.3619], device='cuda:0'))])
epoch£º1065	 i:0 	 global-step:21300	 l-p:0.13157859444618225
epoch£º1065	 i:1 	 global-step:21301	 l-p:0.1625613421201706
epoch£º1065	 i:2 	 global-step:21302	 l-p:0.2206401824951172
epoch£º1065	 i:3 	 global-step:21303	 l-p:0.12578698992729187
epoch£º1065	 i:4 	 global-step:21304	 l-p:0.15604563057422638
epoch£º1065	 i:5 	 global-step:21305	 l-p:0.08539697527885437
epoch£º1065	 i:6 	 global-step:21306	 l-p:0.12593582272529602
epoch£º1065	 i:7 	 global-step:21307	 l-p:-0.19033199548721313
epoch£º1065	 i:8 	 global-step:21308	 l-p:0.12193324416875839
epoch£º1065	 i:9 	 global-step:21309	 l-p:0.13193392753601074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6474, 3.1749, 2.5630],
        [3.6474, 2.8281, 2.2838],
        [3.6474, 3.4284, 3.5496],
        [3.6474, 2.8307, 2.3131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.15422841906547546 
model_pd.l_d.mean(): -23.491424560546875 
model_pd.lagr.mean(): -23.337196350097656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1932], device='cuda:0')), ('power', tensor([-23.6846], device='cuda:0'))])
epoch£º1066	 i:0 	 global-step:21320	 l-p:0.15422841906547546
epoch£º1066	 i:1 	 global-step:21321	 l-p:0.11461739987134933
epoch£º1066	 i:2 	 global-step:21322	 l-p:-0.07121569663286209
epoch£º1066	 i:3 	 global-step:21323	 l-p:0.12124522775411606
epoch£º1066	 i:4 	 global-step:21324	 l-p:0.15619365870952606
epoch£º1066	 i:5 	 global-step:21325	 l-p:0.1075238361954689
epoch£º1066	 i:6 	 global-step:21326	 l-p:1.0400642156600952
epoch£º1066	 i:7 	 global-step:21327	 l-p:0.125701442360878
epoch£º1066	 i:8 	 global-step:21328	 l-p:0.147072896361351
epoch£º1066	 i:9 	 global-step:21329	 l-p:0.22464296221733093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6284, 3.6284, 3.6284],
        [3.6284, 3.6262, 3.6284],
        [3.6284, 3.6189, 3.6279],
        [3.6284, 3.1204, 2.5045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.1422988474369049 
model_pd.l_d.mean(): -23.48470115661621 
model_pd.lagr.mean(): -23.3424015045166 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2327], device='cuda:0')), ('power', tensor([-23.7174], device='cuda:0'))])
epoch£º1067	 i:0 	 global-step:21340	 l-p:0.1422988474369049
epoch£º1067	 i:1 	 global-step:21341	 l-p:0.1356453001499176
epoch£º1067	 i:2 	 global-step:21342	 l-p:0.14882543683052063
epoch£º1067	 i:3 	 global-step:21343	 l-p:0.01042252965271473
epoch£º1067	 i:4 	 global-step:21344	 l-p:-0.0480206198990345
epoch£º1067	 i:5 	 global-step:21345	 l-p:0.14891639351844788
epoch£º1067	 i:6 	 global-step:21346	 l-p:0.15234063565731049
epoch£º1067	 i:7 	 global-step:21347	 l-p:0.3898044228553772
epoch£º1067	 i:8 	 global-step:21348	 l-p:0.11183346807956696
epoch£º1067	 i:9 	 global-step:21349	 l-p:0.1552470326423645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6091, 3.6091, 3.6091],
        [3.6091, 3.6082, 3.6091],
        [3.6091, 3.6090, 3.6091],
        [3.6091, 3.6091, 3.6091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.16693748533725739 
model_pd.l_d.mean(): -22.90437126159668 
model_pd.lagr.mean(): -22.73743438720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2609], device='cuda:0')), ('power', tensor([-23.1653], device='cuda:0'))])
epoch£º1068	 i:0 	 global-step:21360	 l-p:0.16693748533725739
epoch£º1068	 i:1 	 global-step:21361	 l-p:0.16184690594673157
epoch£º1068	 i:2 	 global-step:21362	 l-p:0.12946389615535736
epoch£º1068	 i:3 	 global-step:21363	 l-p:0.143812894821167
epoch£º1068	 i:4 	 global-step:21364	 l-p:-2.2720510959625244
epoch£º1068	 i:5 	 global-step:21365	 l-p:0.1235615685582161
epoch£º1068	 i:6 	 global-step:21366	 l-p:0.054800089448690414
epoch£º1068	 i:7 	 global-step:21367	 l-p:0.10894569009542465
epoch£º1068	 i:8 	 global-step:21368	 l-p:0.16387613117694855
epoch£º1068	 i:9 	 global-step:21369	 l-p:0.07847040146589279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6100, 3.0722, 3.0877],
        [3.6100, 3.5852, 3.6077],
        [3.6100, 3.5718, 3.6052],
        [3.6100, 3.3392, 3.4666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.10341741144657135 
model_pd.l_d.mean(): -23.650238037109375 
model_pd.lagr.mean(): -23.54681968688965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1726], device='cuda:0')), ('power', tensor([-23.8228], device='cuda:0'))])
epoch£º1069	 i:0 	 global-step:21380	 l-p:0.10341741144657135
epoch£º1069	 i:1 	 global-step:21381	 l-p:0.5891941785812378
epoch£º1069	 i:2 	 global-step:21382	 l-p:0.14910772442817688
epoch£º1069	 i:3 	 global-step:21383	 l-p:0.0033154773991554976
epoch£º1069	 i:4 	 global-step:21384	 l-p:0.13213974237442017
epoch£º1069	 i:5 	 global-step:21385	 l-p:0.12363570928573608
epoch£º1069	 i:6 	 global-step:21386	 l-p:0.12102369219064713
epoch£º1069	 i:7 	 global-step:21387	 l-p:0.07973671704530716
epoch£º1069	 i:8 	 global-step:21388	 l-p:0.16127756237983704
epoch£º1069	 i:9 	 global-step:21389	 l-p:0.13958638906478882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6258, 2.9772, 2.3407],
        [3.6258, 3.6232, 3.6257],
        [3.6258, 3.5560, 3.6127],
        [3.6258, 2.9321, 2.2913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): -0.3547460734844208 
model_pd.l_d.mean(): -22.887290954589844 
model_pd.lagr.mean(): -23.242036819458008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3409], device='cuda:0')), ('power', tensor([-23.2282], device='cuda:0'))])
epoch£º1070	 i:0 	 global-step:21400	 l-p:-0.3547460734844208
epoch£º1070	 i:1 	 global-step:21401	 l-p:0.13540199398994446
epoch£º1070	 i:2 	 global-step:21402	 l-p:0.11667729914188385
epoch£º1070	 i:3 	 global-step:21403	 l-p:0.1428590714931488
epoch£º1070	 i:4 	 global-step:21404	 l-p:0.10346719622612
epoch£º1070	 i:5 	 global-step:21405	 l-p:0.10612743347883224
epoch£º1070	 i:6 	 global-step:21406	 l-p:0.20942646265029907
epoch£º1070	 i:7 	 global-step:21407	 l-p:0.14345592260360718
epoch£º1070	 i:8 	 global-step:21408	 l-p:0.1468375027179718
epoch£º1070	 i:9 	 global-step:21409	 l-p:0.1128922700881958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6459, 3.6459, 3.6459],
        [3.6459, 3.6127, 3.6421],
        [3.6459, 3.3396, 3.4654],
        [3.6459, 3.1145, 3.1332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.08469412475824356 
model_pd.l_d.mean(): -23.208154678344727 
model_pd.lagr.mean(): -23.12346076965332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2462], device='cuda:0')), ('power', tensor([-23.4544], device='cuda:0'))])
epoch£º1071	 i:0 	 global-step:21420	 l-p:0.08469412475824356
epoch£º1071	 i:1 	 global-step:21421	 l-p:-0.12378860265016556
epoch£º1071	 i:2 	 global-step:21422	 l-p:0.13476915657520294
epoch£º1071	 i:3 	 global-step:21423	 l-p:0.10863987356424332
epoch£º1071	 i:4 	 global-step:21424	 l-p:0.13004732131958008
epoch£º1071	 i:5 	 global-step:21425	 l-p:0.17961253225803375
epoch£º1071	 i:6 	 global-step:21426	 l-p:0.2200203239917755
epoch£º1071	 i:7 	 global-step:21427	 l-p:0.05932033434510231
epoch£º1071	 i:8 	 global-step:21428	 l-p:0.16317015886306763
epoch£º1071	 i:9 	 global-step:21429	 l-p:0.13018810749053955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6672, 3.6672, 3.6672],
        [3.6672, 3.2061, 2.5936],
        [3.6672, 3.6657, 3.6672],
        [3.6672, 2.9177, 2.6178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.22276671230793 
model_pd.l_d.mean(): -22.928016662597656 
model_pd.lagr.mean(): -22.705249786376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3330], device='cuda:0')), ('power', tensor([-23.2610], device='cuda:0'))])
epoch£º1072	 i:0 	 global-step:21440	 l-p:0.22276671230793
epoch£º1072	 i:1 	 global-step:21441	 l-p:0.17613621056079865
epoch£º1072	 i:2 	 global-step:21442	 l-p:0.2527475357055664
epoch£º1072	 i:3 	 global-step:21443	 l-p:0.017955364659428596
epoch£º1072	 i:4 	 global-step:21444	 l-p:0.10969127714633942
epoch£º1072	 i:5 	 global-step:21445	 l-p:0.14856478571891785
epoch£º1072	 i:6 	 global-step:21446	 l-p:0.056442130357027054
epoch£º1072	 i:7 	 global-step:21447	 l-p:0.11571945250034332
epoch£º1072	 i:8 	 global-step:21448	 l-p:0.12155274301767349
epoch£º1072	 i:9 	 global-step:21449	 l-p:0.13825371861457825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6840, 3.6439, 3.6788],
        [3.6840, 3.6717, 3.6832],
        [3.6840, 2.9175, 2.2727],
        [3.6840, 3.6550, 3.6810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.16603416204452515 
model_pd.l_d.mean(): -22.997577667236328 
model_pd.lagr.mean(): -22.83154296875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3060], device='cuda:0')), ('power', tensor([-23.3036], device='cuda:0'))])
epoch£º1073	 i:0 	 global-step:21460	 l-p:0.16603416204452515
epoch£º1073	 i:1 	 global-step:21461	 l-p:0.12622587382793427
epoch£º1073	 i:2 	 global-step:21462	 l-p:0.11705906689167023
epoch£º1073	 i:3 	 global-step:21463	 l-p:0.11313007026910782
epoch£º1073	 i:4 	 global-step:21464	 l-p:0.11025481671094894
epoch£º1073	 i:5 	 global-step:21465	 l-p:-0.09921226650476456
epoch£º1073	 i:6 	 global-step:21466	 l-p:0.1462882161140442
epoch£º1073	 i:7 	 global-step:21467	 l-p:0.13099829852581024
epoch£º1073	 i:8 	 global-step:21468	 l-p:0.1969674676656723
epoch£º1073	 i:9 	 global-step:21469	 l-p:0.1404944807291031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6759, 3.6759, 3.6759],
        [3.6759, 2.9966, 2.8304],
        [3.6759, 3.5002, 3.6099],
        [3.6759, 2.9929, 2.8207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): -0.07584365457296371 
model_pd.l_d.mean(): -23.086862564086914 
model_pd.lagr.mean(): -23.16270637512207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2543], device='cuda:0')), ('power', tensor([-23.3412], device='cuda:0'))])
epoch£º1074	 i:0 	 global-step:21480	 l-p:-0.07584365457296371
epoch£º1074	 i:1 	 global-step:21481	 l-p:0.1214413270354271
epoch£º1074	 i:2 	 global-step:21482	 l-p:0.129250630736351
epoch£º1074	 i:3 	 global-step:21483	 l-p:0.1318279653787613
epoch£º1074	 i:4 	 global-step:21484	 l-p:0.13925981521606445
epoch£º1074	 i:5 	 global-step:21485	 l-p:0.218925341963768
epoch£º1074	 i:6 	 global-step:21486	 l-p:0.08026402443647385
epoch£º1074	 i:7 	 global-step:21487	 l-p:0.14001236855983734
epoch£º1074	 i:8 	 global-step:21488	 l-p:1.3402949571609497
epoch£º1074	 i:9 	 global-step:21489	 l-p:0.1762528121471405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6595, 3.4722, 3.5856],
        [3.6595, 3.6525, 3.6592],
        [3.6595, 3.6588, 3.6595],
        [3.6595, 3.1648, 3.2128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.17340388894081116 
model_pd.l_d.mean(): -23.66703987121582 
model_pd.lagr.mean(): -23.493635177612305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1450], device='cuda:0')), ('power', tensor([-23.8120], device='cuda:0'))])
epoch£º1075	 i:0 	 global-step:21500	 l-p:0.17340388894081116
epoch£º1075	 i:1 	 global-step:21501	 l-p:0.1360742747783661
epoch£º1075	 i:2 	 global-step:21502	 l-p:0.12960617244243622
epoch£º1075	 i:3 	 global-step:21503	 l-p:0.07992318272590637
epoch£º1075	 i:4 	 global-step:21504	 l-p:0.17910854518413544
epoch£º1075	 i:5 	 global-step:21505	 l-p:0.1277303546667099
epoch£º1075	 i:6 	 global-step:21506	 l-p:0.06242050975561142
epoch£º1075	 i:7 	 global-step:21507	 l-p:0.13780640065670013
epoch£º1075	 i:8 	 global-step:21508	 l-p:0.8786030411720276
epoch£º1075	 i:9 	 global-step:21509	 l-p:0.12403073906898499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6623, 3.6273, 3.6582],
        [3.6623, 3.1025, 2.4728],
        [3.6623, 3.1242, 2.4982],
        [3.6623, 3.5977, 3.6508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.7807430624961853 
model_pd.l_d.mean(): -23.412120819091797 
model_pd.lagr.mean(): -22.631378173828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2513], device='cuda:0')), ('power', tensor([-23.6634], device='cuda:0'))])
epoch£º1076	 i:0 	 global-step:21520	 l-p:0.7807430624961853
epoch£º1076	 i:1 	 global-step:21521	 l-p:0.07569598406553268
epoch£º1076	 i:2 	 global-step:21522	 l-p:0.13528048992156982
epoch£º1076	 i:3 	 global-step:21523	 l-p:0.12577278912067413
epoch£º1076	 i:4 	 global-step:21524	 l-p:0.12873657047748566
epoch£º1076	 i:5 	 global-step:21525	 l-p:0.10449294745922089
epoch£º1076	 i:6 	 global-step:21526	 l-p:0.13730306923389435
epoch£º1076	 i:7 	 global-step:21527	 l-p:0.12436580657958984
epoch£º1076	 i:8 	 global-step:21528	 l-p:0.16875216364860535
epoch£º1076	 i:9 	 global-step:21529	 l-p:0.1308572143316269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6528, 3.0082, 2.8985],
        [3.6528, 3.0693, 3.0361],
        [3.6528, 3.6436, 3.6523],
        [3.6528, 3.6516, 3.6528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.1455921232700348 
model_pd.l_d.mean(): -23.213865280151367 
model_pd.lagr.mean(): -23.068273544311523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1701], device='cuda:0')), ('power', tensor([-23.3839], device='cuda:0'))])
epoch£º1077	 i:0 	 global-step:21540	 l-p:0.1455921232700348
epoch£º1077	 i:1 	 global-step:21541	 l-p:0.2390013486146927
epoch£º1077	 i:2 	 global-step:21542	 l-p:0.12620486319065094
epoch£º1077	 i:3 	 global-step:21543	 l-p:0.11927773058414459
epoch£º1077	 i:4 	 global-step:21544	 l-p:0.22856944799423218
epoch£º1077	 i:5 	 global-step:21545	 l-p:0.09490783512592316
epoch£º1077	 i:6 	 global-step:21546	 l-p:0.1260458528995514
epoch£º1077	 i:7 	 global-step:21547	 l-p:0.13638444244861603
epoch£º1077	 i:8 	 global-step:21548	 l-p:-0.04356404021382332
epoch£º1077	 i:9 	 global-step:21549	 l-p:0.1240442544221878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6438, 2.9599, 2.7917],
        [3.6438, 3.2606, 3.3699],
        [3.6438, 3.1004, 3.1087],
        [3.6438, 3.6439, 3.6438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.10799075663089752 
model_pd.l_d.mean(): -22.81085777282715 
model_pd.lagr.mean(): -22.70286750793457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3141], device='cuda:0')), ('power', tensor([-23.1250], device='cuda:0'))])
epoch£º1078	 i:0 	 global-step:21560	 l-p:0.10799075663089752
epoch£º1078	 i:1 	 global-step:21561	 l-p:0.13804639875888824
epoch£º1078	 i:2 	 global-step:21562	 l-p:0.12686055898666382
epoch£º1078	 i:3 	 global-step:21563	 l-p:0.3524002134799957
epoch£º1078	 i:4 	 global-step:21564	 l-p:0.12066131085157394
epoch£º1078	 i:5 	 global-step:21565	 l-p:0.1980007141828537
epoch£º1078	 i:6 	 global-step:21566	 l-p:0.08163025975227356
epoch£º1078	 i:7 	 global-step:21567	 l-p:0.10243798792362213
epoch£º1078	 i:8 	 global-step:21568	 l-p:-0.05073942989110947
epoch£º1078	 i:9 	 global-step:21569	 l-p:0.13018999993801117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6506, 3.5780, 3.6365],
        [3.6506, 2.8566, 2.2247],
        [3.6506, 3.6421, 3.6502],
        [3.6506, 2.9346, 2.7090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.16698504984378815 
model_pd.l_d.mean(): -23.263805389404297 
model_pd.lagr.mean(): -23.096820831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2602], device='cuda:0')), ('power', tensor([-23.5240], device='cuda:0'))])
epoch£º1079	 i:0 	 global-step:21580	 l-p:0.16698504984378815
epoch£º1079	 i:1 	 global-step:21581	 l-p:0.19340534508228302
epoch£º1079	 i:2 	 global-step:21582	 l-p:-0.612972617149353
epoch£º1079	 i:3 	 global-step:21583	 l-p:0.13397067785263062
epoch£º1079	 i:4 	 global-step:21584	 l-p:0.0331864133477211
epoch£º1079	 i:5 	 global-step:21585	 l-p:0.13380733132362366
epoch£º1079	 i:6 	 global-step:21586	 l-p:0.13118019700050354
epoch£º1079	 i:7 	 global-step:21587	 l-p:0.056587547063827515
epoch£º1079	 i:8 	 global-step:21588	 l-p:0.1354137510061264
epoch£º1079	 i:9 	 global-step:21589	 l-p:0.1362670511007309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6706, 3.6189, 3.6627],
        [3.6706, 3.4799, 3.5943],
        [3.6706, 3.1229, 2.4940],
        [3.6706, 2.8539, 2.2708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.11078603565692902 
model_pd.l_d.mean(): -23.323381423950195 
model_pd.lagr.mean(): -23.212594985961914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2397], device='cuda:0')), ('power', tensor([-23.5630], device='cuda:0'))])
epoch£º1080	 i:0 	 global-step:21600	 l-p:0.11078603565692902
epoch£º1080	 i:1 	 global-step:21601	 l-p:0.11252717673778534
epoch£º1080	 i:2 	 global-step:21602	 l-p:0.1884070187807083
epoch£º1080	 i:3 	 global-step:21603	 l-p:0.06683630496263504
epoch£º1080	 i:4 	 global-step:21604	 l-p:0.15206073224544525
epoch£º1080	 i:5 	 global-step:21605	 l-p:0.019700637087225914
epoch£º1080	 i:6 	 global-step:21606	 l-p:0.1326860636472702
epoch£º1080	 i:7 	 global-step:21607	 l-p:0.330011785030365
epoch£º1080	 i:8 	 global-step:21608	 l-p:0.15038087964057922
epoch£º1080	 i:9 	 global-step:21609	 l-p:0.13519762456417084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6721, 3.6718, 3.6721],
        [3.6721, 3.6721, 3.6721],
        [3.6721, 3.6695, 3.6720],
        [3.6721, 3.6566, 3.6710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.11143671721220016 
model_pd.l_d.mean(): -22.76401138305664 
model_pd.lagr.mean(): -22.65257453918457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2734], device='cuda:0')), ('power', tensor([-23.0374], device='cuda:0'))])
epoch£º1081	 i:0 	 global-step:21620	 l-p:0.11143671721220016
epoch£º1081	 i:1 	 global-step:21621	 l-p:0.16793884336948395
epoch£º1081	 i:2 	 global-step:21622	 l-p:0.1275595724582672
epoch£º1081	 i:3 	 global-step:21623	 l-p:0.07423435151576996
epoch£º1081	 i:4 	 global-step:21624	 l-p:0.15161919593811035
epoch£º1081	 i:5 	 global-step:21625	 l-p:0.23018832504749298
epoch£º1081	 i:6 	 global-step:21626	 l-p:0.05029609426856041
epoch£º1081	 i:7 	 global-step:21627	 l-p:0.12480875104665756
epoch£º1081	 i:8 	 global-step:21628	 l-p:2.601738929748535
epoch£º1081	 i:9 	 global-step:21629	 l-p:0.14133650064468384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6613, 3.6613, 3.6613],
        [3.6613, 3.4239, 3.5481],
        [3.6613, 2.8491, 2.3580],
        [3.6613, 2.8525, 2.2371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.10065647214651108 
model_pd.l_d.mean(): -22.81044578552246 
model_pd.lagr.mean(): -22.709789276123047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3100], device='cuda:0')), ('power', tensor([-23.1204], device='cuda:0'))])
epoch£º1082	 i:0 	 global-step:21640	 l-p:0.10065647214651108
epoch£º1082	 i:1 	 global-step:21641	 l-p:0.11402320861816406
epoch£º1082	 i:2 	 global-step:21642	 l-p:0.13281618058681488
epoch£º1082	 i:3 	 global-step:21643	 l-p:0.13237015902996063
epoch£º1082	 i:4 	 global-step:21644	 l-p:0.2259986847639084
epoch£º1082	 i:5 	 global-step:21645	 l-p:0.10598231852054596
epoch£º1082	 i:6 	 global-step:21646	 l-p:0.10488651692867279
epoch£º1082	 i:7 	 global-step:21647	 l-p:0.13714605569839478
epoch£º1082	 i:8 	 global-step:21648	 l-p:0.14147132635116577
epoch£º1082	 i:9 	 global-step:21649	 l-p:1.3382854461669922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6611, 3.6558, 3.6609],
        [3.6611, 3.1185, 3.1266],
        [3.6611, 3.1327, 2.5083],
        [3.6611, 2.8611, 2.2341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.21657869219779968 
model_pd.l_d.mean(): -23.307348251342773 
model_pd.lagr.mean(): -23.090768814086914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2854], device='cuda:0')), ('power', tensor([-23.5928], device='cuda:0'))])
epoch£º1083	 i:0 	 global-step:21660	 l-p:0.21657869219779968
epoch£º1083	 i:1 	 global-step:21661	 l-p:0.13605782389640808
epoch£º1083	 i:2 	 global-step:21662	 l-p:0.7863135933876038
epoch£º1083	 i:3 	 global-step:21663	 l-p:0.1278376430273056
epoch£º1083	 i:4 	 global-step:21664	 l-p:0.16588923335075378
epoch£º1083	 i:5 	 global-step:21665	 l-p:0.10522359609603882
epoch£º1083	 i:6 	 global-step:21666	 l-p:0.16291314363479614
epoch£º1083	 i:7 	 global-step:21667	 l-p:0.13803063333034515
epoch£º1083	 i:8 	 global-step:21668	 l-p:-0.04265327379107475
epoch£º1083	 i:9 	 global-step:21669	 l-p:0.1489466279745102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6603, 3.1454, 2.5235],
        [3.6603, 3.6345, 3.6578],
        [3.6603, 2.9008, 2.5810],
        [3.6603, 2.8601, 2.2331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.11939767748117447 
model_pd.l_d.mean(): -23.329458236694336 
model_pd.lagr.mean(): -23.210060119628906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2818], device='cuda:0')), ('power', tensor([-23.6112], device='cuda:0'))])
epoch£º1084	 i:0 	 global-step:21680	 l-p:0.11939767748117447
epoch£º1084	 i:1 	 global-step:21681	 l-p:0.13784641027450562
epoch£º1084	 i:2 	 global-step:21682	 l-p:0.12316714972257614
epoch£º1084	 i:3 	 global-step:21683	 l-p:0.13558469712734222
epoch£º1084	 i:4 	 global-step:21684	 l-p:-0.15279589593410492
epoch£º1084	 i:5 	 global-step:21685	 l-p:0.11179521679878235
epoch£º1084	 i:6 	 global-step:21686	 l-p:0.16766755282878876
epoch£º1084	 i:7 	 global-step:21687	 l-p:0.40240082144737244
epoch£º1084	 i:8 	 global-step:21688	 l-p:0.15993686020374298
epoch£º1084	 i:9 	 global-step:21689	 l-p:0.17759890854358673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6395, 3.4113, 3.5344],
        [3.6395, 2.8360, 2.3960],
        [3.6395, 3.1384, 3.1832],
        [3.6395, 3.6345, 3.6393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.14901937544345856 
model_pd.l_d.mean(): -22.98358917236328 
model_pd.lagr.mean(): -22.834569931030273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2542], device='cuda:0')), ('power', tensor([-23.2378], device='cuda:0'))])
epoch£º1085	 i:0 	 global-step:21700	 l-p:0.14901937544345856
epoch£º1085	 i:1 	 global-step:21701	 l-p:0.12664535641670227
epoch£º1085	 i:2 	 global-step:21702	 l-p:0.1395564079284668
epoch£º1085	 i:3 	 global-step:21703	 l-p:0.13104863464832306
epoch£º1085	 i:4 	 global-step:21704	 l-p:0.7493765950202942
epoch£º1085	 i:5 	 global-step:21705	 l-p:0.1587544083595276
epoch£º1085	 i:6 	 global-step:21706	 l-p:0.022681618109345436
epoch£º1085	 i:7 	 global-step:21707	 l-p:0.08968101441860199
epoch£º1085	 i:8 	 global-step:21708	 l-p:0.20093949139118195
epoch£º1085	 i:9 	 global-step:21709	 l-p:0.1752912849187851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6368, 3.4646, 3.5734],
        [3.6368, 2.9607, 2.3194],
        [3.6368, 3.6368, 3.6368],
        [3.6368, 3.6366, 3.6368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.14533324539661407 
model_pd.l_d.mean(): -23.080364227294922 
model_pd.lagr.mean(): -22.93503189086914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1597], device='cuda:0')), ('power', tensor([-23.2401], device='cuda:0'))])
epoch£º1086	 i:0 	 global-step:21720	 l-p:0.14533324539661407
epoch£º1086	 i:1 	 global-step:21721	 l-p:0.13222162425518036
epoch£º1086	 i:2 	 global-step:21722	 l-p:0.7303144931793213
epoch£º1086	 i:3 	 global-step:21723	 l-p:0.10203710198402405
epoch£º1086	 i:4 	 global-step:21724	 l-p:0.07881268858909607
epoch£º1086	 i:5 	 global-step:21725	 l-p:0.15025794506072998
epoch£º1086	 i:6 	 global-step:21726	 l-p:0.16565890610218048
epoch£º1086	 i:7 	 global-step:21727	 l-p:-0.07581549137830734
epoch£º1086	 i:8 	 global-step:21728	 l-p:0.1267731785774231
epoch£º1086	 i:9 	 global-step:21729	 l-p:0.15196363627910614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6483, 3.6480, 3.6483],
        [3.6483, 3.5868, 3.6377],
        [3.6483, 3.6133, 3.6442],
        [3.6483, 2.8355, 2.2234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.14120829105377197 
model_pd.l_d.mean(): -22.917612075805664 
model_pd.lagr.mean(): -22.776403427124023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2908], device='cuda:0')), ('power', tensor([-23.2084], device='cuda:0'))])
epoch£º1087	 i:0 	 global-step:21740	 l-p:0.14120829105377197
epoch£º1087	 i:1 	 global-step:21741	 l-p:0.12719202041625977
epoch£º1087	 i:2 	 global-step:21742	 l-p:0.32679617404937744
epoch£º1087	 i:3 	 global-step:21743	 l-p:0.13170892000198364
epoch£º1087	 i:4 	 global-step:21744	 l-p:0.10821303725242615
epoch£º1087	 i:5 	 global-step:21745	 l-p:0.13075599074363708
epoch£º1087	 i:6 	 global-step:21746	 l-p:0.06478038430213928
epoch£º1087	 i:7 	 global-step:21747	 l-p:0.1343182474374771
epoch£º1087	 i:8 	 global-step:21748	 l-p:0.050688982009887695
epoch£º1087	 i:9 	 global-step:21749	 l-p:0.1544228494167328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6405, 3.6208, 3.6389],
        [3.6405, 2.9005, 2.2559],
        [3.6405, 3.0918, 2.4669],
        [3.6405, 2.8753, 2.2330]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.13439954817295074 
model_pd.l_d.mean(): -23.266626358032227 
model_pd.lagr.mean(): -23.132226943969727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2699], device='cuda:0')), ('power', tensor([-23.5365], device='cuda:0'))])
epoch£º1088	 i:0 	 global-step:21760	 l-p:0.13439954817295074
epoch£º1088	 i:1 	 global-step:21761	 l-p:0.13422496616840363
epoch£º1088	 i:2 	 global-step:21762	 l-p:0.14698955416679382
epoch£º1088	 i:3 	 global-step:21763	 l-p:0.10137031972408295
epoch£º1088	 i:4 	 global-step:21764	 l-p:-2.52532958984375
epoch£º1088	 i:5 	 global-step:21765	 l-p:0.1527971476316452
epoch£º1088	 i:6 	 global-step:21766	 l-p:0.08919811993837357
epoch£º1088	 i:7 	 global-step:21767	 l-p:0.1717950850725174
epoch£º1088	 i:8 	 global-step:21768	 l-p:0.05254512280225754
epoch£º1088	 i:9 	 global-step:21769	 l-p:0.10542341321706772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6335, 3.4593, 3.5689],
        [3.6335, 3.4500, 3.5626],
        [3.6335, 2.8286, 2.3883],
        [3.6335, 3.6335, 3.6335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.05326953902840614 
model_pd.l_d.mean(): -23.633974075317383 
model_pd.lagr.mean(): -23.580703735351562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2089], device='cuda:0')), ('power', tensor([-23.8429], device='cuda:0'))])
epoch£º1089	 i:0 	 global-step:21780	 l-p:0.05326953902840614
epoch£º1089	 i:1 	 global-step:21781	 l-p:0.12932242453098297
epoch£º1089	 i:2 	 global-step:21782	 l-p:0.643307089805603
epoch£º1089	 i:3 	 global-step:21783	 l-p:0.13944774866104126
epoch£º1089	 i:4 	 global-step:21784	 l-p:0.08555034548044205
epoch£º1089	 i:5 	 global-step:21785	 l-p:0.11794530600309372
epoch£º1089	 i:6 	 global-step:21786	 l-p:0.12445922940969467
epoch£º1089	 i:7 	 global-step:21787	 l-p:0.03547334298491478
epoch£º1089	 i:8 	 global-step:21788	 l-p:0.132209911942482
epoch£º1089	 i:9 	 global-step:21789	 l-p:0.16635127365589142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6539, 2.9584, 2.7708],
        [3.6539, 2.8637, 2.4642],
        [3.6539, 2.9645, 2.3193],
        [3.6539, 3.6513, 3.6538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.14608705043792725 
model_pd.l_d.mean(): -23.594158172607422 
model_pd.lagr.mean(): -23.448070526123047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1799], device='cuda:0')), ('power', tensor([-23.7741], device='cuda:0'))])
epoch£º1090	 i:0 	 global-step:21800	 l-p:0.14608705043792725
epoch£º1090	 i:1 	 global-step:21801	 l-p:0.21298323571681976
epoch£º1090	 i:2 	 global-step:21802	 l-p:-0.1126471683382988
epoch£º1090	 i:3 	 global-step:21803	 l-p:0.11832983046770096
epoch£º1090	 i:4 	 global-step:21804	 l-p:0.13507720828056335
epoch£º1090	 i:5 	 global-step:21805	 l-p:0.10250857472419739
epoch£º1090	 i:6 	 global-step:21806	 l-p:0.09505939483642578
epoch£º1090	 i:7 	 global-step:21807	 l-p:0.08904962986707687
epoch£º1090	 i:8 	 global-step:21808	 l-p:0.1191415935754776
epoch£º1090	 i:9 	 global-step:21809	 l-p:0.11769088357686996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6550, 2.8654, 2.2297],
        [3.6550, 2.8679, 2.4776],
        [3.6550, 2.8403, 2.2331],
        [3.6550, 3.6547, 3.6550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.14412814378738403 
model_pd.l_d.mean(): -23.37712860107422 
model_pd.lagr.mean(): -23.232999801635742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1866], device='cuda:0')), ('power', tensor([-23.5637], device='cuda:0'))])
epoch£º1091	 i:0 	 global-step:21820	 l-p:0.14412814378738403
epoch£º1091	 i:1 	 global-step:21821	 l-p:0.13495583832263947
epoch£º1091	 i:2 	 global-step:21822	 l-p:0.13442353904247284
epoch£º1091	 i:3 	 global-step:21823	 l-p:0.13928261399269104
epoch£º1091	 i:4 	 global-step:21824	 l-p:0.3410290479660034
epoch£º1091	 i:5 	 global-step:21825	 l-p:-0.05929712951183319
epoch£º1091	 i:6 	 global-step:21826	 l-p:0.09554634243249893
epoch£º1091	 i:7 	 global-step:21827	 l-p:0.1913379579782486
epoch£º1091	 i:8 	 global-step:21828	 l-p:0.09749706834554672
epoch£º1091	 i:9 	 global-step:21829	 l-p:0.11681254953145981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6470, 3.3572, 3.4843],
        [3.6470, 3.6126, 3.6430],
        [3.6470, 2.8827, 2.2394],
        [3.6470, 3.1031, 3.1118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.08439700305461884 
model_pd.l_d.mean(): -23.523006439208984 
model_pd.lagr.mean(): -23.438610076904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2459], device='cuda:0')), ('power', tensor([-23.7689], device='cuda:0'))])
epoch£º1092	 i:0 	 global-step:21840	 l-p:0.08439700305461884
epoch£º1092	 i:1 	 global-step:21841	 l-p:0.2024446278810501
epoch£º1092	 i:2 	 global-step:21842	 l-p:0.13541704416275024
epoch£º1092	 i:3 	 global-step:21843	 l-p:0.13082581758499146
epoch£º1092	 i:4 	 global-step:21844	 l-p:0.15889856219291687
epoch£º1092	 i:5 	 global-step:21845	 l-p:0.13294608891010284
epoch£º1092	 i:6 	 global-step:21846	 l-p:0.1935136914253235
epoch£º1092	 i:7 	 global-step:21847	 l-p:0.14576376974582672
epoch£º1092	 i:8 	 global-step:21848	 l-p:-0.3205817639827728
epoch£º1092	 i:9 	 global-step:21849	 l-p:0.12505680322647095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6544, 2.8914, 2.2471],
        [3.6544, 3.6542, 3.6544],
        [3.6544, 3.6544, 3.6544],
        [3.6544, 3.6543, 3.6544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.1493266075849533 
model_pd.l_d.mean(): -23.493755340576172 
model_pd.lagr.mean(): -23.34442901611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2225], device='cuda:0')), ('power', tensor([-23.7163], device='cuda:0'))])
epoch£º1093	 i:0 	 global-step:21860	 l-p:0.1493266075849533
epoch£º1093	 i:1 	 global-step:21861	 l-p:0.18664497137069702
epoch£º1093	 i:2 	 global-step:21862	 l-p:0.1015474870800972
epoch£º1093	 i:3 	 global-step:21863	 l-p:-0.22683577239513397
epoch£º1093	 i:4 	 global-step:21864	 l-p:0.10210820287466049
epoch£º1093	 i:5 	 global-step:21865	 l-p:0.0620083324611187
epoch£º1093	 i:6 	 global-step:21866	 l-p:0.13669957220554352
epoch£º1093	 i:7 	 global-step:21867	 l-p:0.1998569667339325
epoch£º1093	 i:8 	 global-step:21868	 l-p:0.16039234399795532
epoch£º1093	 i:9 	 global-step:21869	 l-p:0.14777667820453644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6575, 3.5045, 3.6061],
        [3.6575, 3.4585, 3.5753],
        [3.6575, 3.6575, 3.6575],
        [3.6575, 3.1250, 3.1433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.14258287847042084 
model_pd.l_d.mean(): -23.683712005615234 
model_pd.lagr.mean(): -23.54113006591797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1478], device='cuda:0')), ('power', tensor([-23.8315], device='cuda:0'))])
epoch£º1094	 i:0 	 global-step:21880	 l-p:0.14258287847042084
epoch£º1094	 i:1 	 global-step:21881	 l-p:0.08798791468143463
epoch£º1094	 i:2 	 global-step:21882	 l-p:0.18255376815795898
epoch£º1094	 i:3 	 global-step:21883	 l-p:0.1297827661037445
epoch£º1094	 i:4 	 global-step:21884	 l-p:0.15185795724391937
epoch£º1094	 i:5 	 global-step:21885	 l-p:0.12645062804222107
epoch£º1094	 i:6 	 global-step:21886	 l-p:-0.33669158816337585
epoch£º1094	 i:7 	 global-step:21887	 l-p:0.1503751277923584
epoch£º1094	 i:8 	 global-step:21888	 l-p:0.09924831241369247
epoch£º1094	 i:9 	 global-step:21889	 l-p:0.28433871269226074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6507, 3.6466, 3.6506],
        [3.6507, 3.3396, 3.4652],
        [3.6507, 3.1067, 3.1150],
        [3.6507, 3.6507, 3.6507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.13897672295570374 
model_pd.l_d.mean(): -23.429052352905273 
model_pd.lagr.mean(): -23.290075302124023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2342], device='cuda:0')), ('power', tensor([-23.6632], device='cuda:0'))])
epoch£º1095	 i:0 	 global-step:21900	 l-p:0.13897672295570374
epoch£º1095	 i:1 	 global-step:21901	 l-p:0.18570928275585175
epoch£º1095	 i:2 	 global-step:21902	 l-p:0.09966401755809784
epoch£º1095	 i:3 	 global-step:21903	 l-p:0.09022024273872375
epoch£º1095	 i:4 	 global-step:21904	 l-p:0.13659128546714783
epoch£º1095	 i:5 	 global-step:21905	 l-p:0.2395966351032257
epoch£º1095	 i:6 	 global-step:21906	 l-p:0.07676167041063309
epoch£º1095	 i:7 	 global-step:21907	 l-p:-1.980143666267395
epoch£º1095	 i:8 	 global-step:21908	 l-p:0.15274982154369354
epoch£º1095	 i:9 	 global-step:21909	 l-p:0.13284696638584137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6673, 3.1354, 3.1535],
        [3.6673, 3.1238, 3.1314],
        [3.6673, 3.6670, 3.6673],
        [3.6673, 3.2228, 3.3041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.1883901208639145 
model_pd.l_d.mean(): -21.853960037231445 
model_pd.lagr.mean(): -21.665569305419922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4045], device='cuda:0')), ('power', tensor([-22.2585], device='cuda:0'))])
epoch£º1096	 i:0 	 global-step:21920	 l-p:0.1883901208639145
epoch£º1096	 i:1 	 global-step:21921	 l-p:0.3683038651943207
epoch£º1096	 i:2 	 global-step:21922	 l-p:0.14056454598903656
epoch£º1096	 i:3 	 global-step:21923	 l-p:0.15767322480678558
epoch£º1096	 i:4 	 global-step:21924	 l-p:0.07694225013256073
epoch£º1096	 i:5 	 global-step:21925	 l-p:0.11153025180101395
epoch£º1096	 i:6 	 global-step:21926	 l-p:-0.12697802484035492
epoch£º1096	 i:7 	 global-step:21927	 l-p:0.13497987389564514
epoch£º1096	 i:8 	 global-step:21928	 l-p:0.14117635786533356
epoch£º1096	 i:9 	 global-step:21929	 l-p:0.1423109620809555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6900, 3.6900, 3.6900],
        [3.6900, 3.6870, 3.6899],
        [3.6900, 3.6900, 3.6900],
        [3.6900, 3.0500, 2.4041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.16914048790931702 
model_pd.l_d.mean(): -22.935556411743164 
model_pd.lagr.mean(): -22.766416549682617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2235], device='cuda:0')), ('power', tensor([-23.1591], device='cuda:0'))])
epoch£º1097	 i:0 	 global-step:21940	 l-p:0.16914048790931702
epoch£º1097	 i:1 	 global-step:21941	 l-p:0.14160476624965668
epoch£º1097	 i:2 	 global-step:21942	 l-p:-0.15965621173381805
epoch£º1097	 i:3 	 global-step:21943	 l-p:0.1226261705160141
epoch£º1097	 i:4 	 global-step:21944	 l-p:0.12376274168491364
epoch£º1097	 i:5 	 global-step:21945	 l-p:0.1255323737859726
epoch£º1097	 i:6 	 global-step:21946	 l-p:0.2121521383523941
epoch£º1097	 i:7 	 global-step:21947	 l-p:0.12521156668663025
epoch£º1097	 i:8 	 global-step:21948	 l-p:-0.021782325580716133
epoch£º1097	 i:9 	 global-step:21949	 l-p:0.02435566671192646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6958, 3.6873, 3.6954],
        [3.6958, 3.6725, 3.6937],
        [3.6958, 2.9829, 2.7567],
        [3.6958, 3.2441, 3.3198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.1444643884897232 
model_pd.l_d.mean(): -23.66676139831543 
model_pd.lagr.mean(): -23.522296905517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1295], device='cuda:0')), ('power', tensor([-23.7963], device='cuda:0'))])
epoch£º1098	 i:0 	 global-step:21960	 l-p:0.1444643884897232
epoch£º1098	 i:1 	 global-step:21961	 l-p:0.1393955647945404
epoch£º1098	 i:2 	 global-step:21962	 l-p:0.14237196743488312
epoch£º1098	 i:3 	 global-step:21963	 l-p:0.1425291746854782
epoch£º1098	 i:4 	 global-step:21964	 l-p:0.12900950014591217
epoch£º1098	 i:5 	 global-step:21965	 l-p:0.11330466717481613
epoch£º1098	 i:6 	 global-step:21966	 l-p:-0.020368384197354317
epoch£º1098	 i:7 	 global-step:21967	 l-p:-0.029383763670921326
epoch£º1098	 i:8 	 global-step:21968	 l-p:0.11411991715431213
epoch£º1098	 i:9 	 global-step:21969	 l-p:0.29986053705215454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6795, 2.9468, 2.2966],
        [3.6795, 3.6792, 3.6795],
        [3.6795, 3.5340, 3.6324],
        [3.6795, 3.6794, 3.6795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.05256563052535057 
model_pd.l_d.mean(): -23.51890754699707 
model_pd.lagr.mean(): -23.466341018676758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2007], device='cuda:0')), ('power', tensor([-23.7196], device='cuda:0'))])
epoch£º1099	 i:0 	 global-step:21980	 l-p:0.05256563052535057
epoch£º1099	 i:1 	 global-step:21981	 l-p:0.16336143016815186
epoch£º1099	 i:2 	 global-step:21982	 l-p:0.05975382775068283
epoch£º1099	 i:3 	 global-step:21983	 l-p:0.25212860107421875
epoch£º1099	 i:4 	 global-step:21984	 l-p:0.04707302898168564
epoch£º1099	 i:5 	 global-step:21985	 l-p:0.15759001672267914
epoch£º1099	 i:6 	 global-step:21986	 l-p:0.1550808846950531
epoch£º1099	 i:7 	 global-step:21987	 l-p:0.15387564897537231
epoch£º1099	 i:8 	 global-step:21988	 l-p:0.10475033521652222
epoch£º1099	 i:9 	 global-step:21989	 l-p:0.1109892800450325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6849, 3.6849, 3.6849],
        [3.6849, 3.1935, 2.5723],
        [3.6849, 3.0152, 2.3668],
        [3.6849, 3.1340, 3.1332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): -0.14490777254104614 
model_pd.l_d.mean(): -23.41934585571289 
model_pd.lagr.mean(): -23.564252853393555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2241], device='cuda:0')), ('power', tensor([-23.6434], device='cuda:0'))])
epoch£º1100	 i:0 	 global-step:22000	 l-p:-0.14490777254104614
epoch£º1100	 i:1 	 global-step:22001	 l-p:0.2596288323402405
epoch£º1100	 i:2 	 global-step:22002	 l-p:0.10087861865758896
epoch£º1100	 i:3 	 global-step:22003	 l-p:0.14854931831359863
epoch£º1100	 i:4 	 global-step:22004	 l-p:0.12322957813739777
epoch£º1100	 i:5 	 global-step:22005	 l-p:0.03044416569173336
epoch£º1100	 i:6 	 global-step:22006	 l-p:0.17843768000602722
epoch£º1100	 i:7 	 global-step:22007	 l-p:0.14558331668376923
epoch£º1100	 i:8 	 global-step:22008	 l-p:0.17300018668174744
epoch£º1100	 i:9 	 global-step:22009	 l-p:0.13425005972385406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6844, 2.8711, 2.3651],
        [3.6844, 3.6353, 3.6772],
        [3.6844, 3.6752, 3.6840],
        [3.6844, 2.8667, 2.3158]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.18057747185230255 
model_pd.l_d.mean(): -23.403318405151367 
model_pd.lagr.mean(): -23.222740173339844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2407], device='cuda:0')), ('power', tensor([-23.6440], device='cuda:0'))])
epoch£º1101	 i:0 	 global-step:22020	 l-p:0.18057747185230255
epoch£º1101	 i:1 	 global-step:22021	 l-p:0.2336733192205429
epoch£º1101	 i:2 	 global-step:22022	 l-p:0.12556269764900208
epoch£º1101	 i:3 	 global-step:22023	 l-p:0.1508404165506363
epoch£º1101	 i:4 	 global-step:22024	 l-p:0.05317405238747597
epoch£º1101	 i:5 	 global-step:22025	 l-p:-0.08173228055238724
epoch£º1101	 i:6 	 global-step:22026	 l-p:0.10383781045675278
epoch£º1101	 i:7 	 global-step:22027	 l-p:-0.027954019606113434
epoch£º1101	 i:8 	 global-step:22028	 l-p:0.1123809814453125
epoch£º1101	 i:9 	 global-step:22029	 l-p:0.17173701524734497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6971, 3.1227, 3.0970],
        [3.6971, 2.8874, 2.2792],
        [3.6971, 3.6816, 3.6960],
        [3.6971, 2.9850, 2.7611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.12418517470359802 
model_pd.l_d.mean(): -22.970518112182617 
model_pd.lagr.mean(): -22.846332550048828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2045], device='cuda:0')), ('power', tensor([-23.1751], device='cuda:0'))])
epoch£º1102	 i:0 	 global-step:22040	 l-p:0.12418517470359802
epoch£º1102	 i:1 	 global-step:22041	 l-p:0.1807037889957428
epoch£º1102	 i:2 	 global-step:22042	 l-p:0.1376405656337738
epoch£º1102	 i:3 	 global-step:22043	 l-p:-0.32159116864204407
epoch£º1102	 i:4 	 global-step:22044	 l-p:0.13495789468288422
epoch£º1102	 i:5 	 global-step:22045	 l-p:0.18912746012210846
epoch£º1102	 i:6 	 global-step:22046	 l-p:0.1254379004240036
epoch£º1102	 i:7 	 global-step:22047	 l-p:-0.04508239030838013
epoch£º1102	 i:8 	 global-step:22048	 l-p:0.021055011078715324
epoch£º1102	 i:9 	 global-step:22049	 l-p:0.1257256269454956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6987, 3.6585, 3.6935],
        [3.6987, 3.3770, 3.5005],
        [3.6987, 2.9742, 2.3212],
        [3.6987, 3.6987, 3.6987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.11792654544115067 
model_pd.l_d.mean(): -23.461341857910156 
model_pd.lagr.mean(): -23.343416213989258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2056], device='cuda:0')), ('power', tensor([-23.6669], device='cuda:0'))])
epoch£º1103	 i:0 	 global-step:22060	 l-p:0.11792654544115067
epoch£º1103	 i:1 	 global-step:22061	 l-p:0.12791359424591064
epoch£º1103	 i:2 	 global-step:22062	 l-p:-0.08918160945177078
epoch£º1103	 i:3 	 global-step:22063	 l-p:0.13623341917991638
epoch£º1103	 i:4 	 global-step:22064	 l-p:-1.74672532081604
epoch£º1103	 i:5 	 global-step:22065	 l-p:0.18356603384017944
epoch£º1103	 i:6 	 global-step:22066	 l-p:0.18051108717918396
epoch£º1103	 i:7 	 global-step:22067	 l-p:0.12158779054880142
epoch£º1103	 i:8 	 global-step:22068	 l-p:-0.14363469183444977
epoch£º1103	 i:9 	 global-step:22069	 l-p:0.14320474863052368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7084, 3.1913, 2.5620],
        [3.7084, 3.5379, 3.6459],
        [3.7084, 3.7084, 3.7084],
        [3.7084, 3.7006, 3.7080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.1735333651304245 
model_pd.l_d.mean(): -23.509098052978516 
model_pd.lagr.mean(): -23.3355655670166 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2047], device='cuda:0')), ('power', tensor([-23.7138], device='cuda:0'))])
epoch£º1104	 i:0 	 global-step:22080	 l-p:0.1735333651304245
epoch£º1104	 i:1 	 global-step:22081	 l-p:0.13100691139698029
epoch£º1104	 i:2 	 global-step:22082	 l-p:4.061194896697998
epoch£º1104	 i:3 	 global-step:22083	 l-p:0.3509759306907654
epoch£º1104	 i:4 	 global-step:22084	 l-p:0.14144937694072723
epoch£º1104	 i:5 	 global-step:22085	 l-p:0.13264788687229156
epoch£º1104	 i:6 	 global-step:22086	 l-p:0.14350861310958862
epoch£º1104	 i:7 	 global-step:22087	 l-p:0.11138913780450821
epoch£º1104	 i:8 	 global-step:22088	 l-p:0.1549060195684433
epoch£º1104	 i:9 	 global-step:22089	 l-p:0.12565122544765472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7158, 2.9052, 2.3108],
        [3.7158, 3.7020, 3.7150],
        [3.7158, 2.9435, 2.2969],
        [3.7158, 3.3683, 3.4865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.1704675704240799 
model_pd.l_d.mean(): -23.420913696289062 
model_pd.lagr.mean(): -23.250446319580078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2058], device='cuda:0')), ('power', tensor([-23.6267], device='cuda:0'))])
epoch£º1105	 i:0 	 global-step:22100	 l-p:0.1704675704240799
epoch£º1105	 i:1 	 global-step:22101	 l-p:0.12581811845302582
epoch£º1105	 i:2 	 global-step:22102	 l-p:-1.209721326828003
epoch£º1105	 i:3 	 global-step:22103	 l-p:0.13043512403964996
epoch£º1105	 i:4 	 global-step:22104	 l-p:0.13212844729423523
epoch£º1105	 i:5 	 global-step:22105	 l-p:0.13288816809654236
epoch£º1105	 i:6 	 global-step:22106	 l-p:0.5680742859840393
epoch£º1105	 i:7 	 global-step:22107	 l-p:-0.3892510235309601
epoch£º1105	 i:8 	 global-step:22108	 l-p:0.2176184058189392
epoch£º1105	 i:9 	 global-step:22109	 l-p:0.13412553071975708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7044, 3.0402, 2.3893],
        [3.7044, 3.6990, 3.7042],
        [3.7044, 3.6605, 3.6984],
        [3.7044, 3.6429, 3.6938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.1637246161699295 
model_pd.l_d.mean(): -23.340606689453125 
model_pd.lagr.mean(): -23.176881790161133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1924], device='cuda:0')), ('power', tensor([-23.5330], device='cuda:0'))])
epoch£º1106	 i:0 	 global-step:22120	 l-p:0.1637246161699295
epoch£º1106	 i:1 	 global-step:22121	 l-p:-0.12325464934110641
epoch£º1106	 i:2 	 global-step:22122	 l-p:0.11993061006069183
epoch£º1106	 i:3 	 global-step:22123	 l-p:0.12142961472272873
epoch£º1106	 i:4 	 global-step:22124	 l-p:-0.45015203952789307
epoch£º1106	 i:5 	 global-step:22125	 l-p:0.2091878056526184
epoch£º1106	 i:6 	 global-step:22126	 l-p:0.12913715839385986
epoch£º1106	 i:7 	 global-step:22127	 l-p:0.1321551352739334
epoch£º1106	 i:8 	 global-step:22128	 l-p:0.7007977366447449
epoch£º1106	 i:9 	 global-step:22129	 l-p:0.13673870265483856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7059, 3.0801, 2.4334],
        [3.7059, 3.7059, 3.7059],
        [3.7059, 3.5175, 3.6311],
        [3.7059, 3.7003, 3.7057]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.754776120185852 
model_pd.l_d.mean(): -23.478910446166992 
model_pd.lagr.mean(): -22.72413444519043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2142], device='cuda:0')), ('power', tensor([-23.6932], device='cuda:0'))])
epoch£º1107	 i:0 	 global-step:22140	 l-p:0.754776120185852
epoch£º1107	 i:1 	 global-step:22141	 l-p:-0.0805416852235794
epoch£º1107	 i:2 	 global-step:22142	 l-p:0.12974412739276886
epoch£º1107	 i:3 	 global-step:22143	 l-p:0.10935220122337341
epoch£º1107	 i:4 	 global-step:22144	 l-p:0.1294177621603012
epoch£º1107	 i:5 	 global-step:22145	 l-p:-0.3397279381752014
epoch£º1107	 i:6 	 global-step:22146	 l-p:0.12363578379154205
epoch£º1107	 i:7 	 global-step:22147	 l-p:0.16544798016548157
epoch£º1107	 i:8 	 global-step:22148	 l-p:0.09285073727369308
epoch£º1107	 i:9 	 global-step:22149	 l-p:0.13921910524368286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7000, 3.0319, 2.8837],
        [3.7000, 3.7000, 3.7000],
        [3.7000, 3.2012, 3.2451],
        [3.7000, 2.9241, 2.5541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.13579939305782318 
model_pd.l_d.mean(): -23.3502197265625 
model_pd.lagr.mean(): -23.214420318603516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2392], device='cuda:0')), ('power', tensor([-23.5894], device='cuda:0'))])
epoch£º1108	 i:0 	 global-step:22160	 l-p:0.13579939305782318
epoch£º1108	 i:1 	 global-step:22161	 l-p:0.1321386992931366
epoch£º1108	 i:2 	 global-step:22162	 l-p:0.13743682205677032
epoch£º1108	 i:3 	 global-step:22163	 l-p:-0.10187561810016632
epoch£º1108	 i:4 	 global-step:22164	 l-p:0.14574109017848969
epoch£º1108	 i:5 	 global-step:22165	 l-p:0.17169946432113647
epoch£º1108	 i:6 	 global-step:22166	 l-p:0.13592667877674103
epoch£º1108	 i:7 	 global-step:22167	 l-p:0.03276316821575165
epoch£º1108	 i:8 	 global-step:22168	 l-p:0.11841358244419098
epoch£º1108	 i:9 	 global-step:22169	 l-p:0.18608756363391876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6855, 3.0188, 2.3703],
        [3.6855, 3.6853, 3.6855],
        [3.6855, 3.2919, 3.3969],
        [3.6855, 3.2256, 3.2973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.1254526674747467 
model_pd.l_d.mean(): -23.356307983398438 
model_pd.lagr.mean(): -23.23085594177246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2079], device='cuda:0')), ('power', tensor([-23.5642], device='cuda:0'))])
epoch£º1109	 i:0 	 global-step:22180	 l-p:0.1254526674747467
epoch£º1109	 i:1 	 global-step:22181	 l-p:0.1637255847454071
epoch£º1109	 i:2 	 global-step:22182	 l-p:0.15989667177200317
epoch£º1109	 i:3 	 global-step:22183	 l-p:0.13229717314243317
epoch£º1109	 i:4 	 global-step:22184	 l-p:0.13424888253211975
epoch£º1109	 i:5 	 global-step:22185	 l-p:0.020870687440037727
epoch£º1109	 i:6 	 global-step:22186	 l-p:0.17709395289421082
epoch£º1109	 i:7 	 global-step:22187	 l-p:0.13133348524570465
epoch£º1109	 i:8 	 global-step:22188	 l-p:0.11813100427389145
epoch£º1109	 i:9 	 global-step:22189	 l-p:-0.04269814491271973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228]], device='cuda:0')
 pt:tensor([[3.6894, 3.3248, 3.4394],
        [3.6894, 2.8824, 2.2639],
        [3.6894, 2.9859, 2.3342],
        [3.6894, 3.0176, 2.3681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.05182803049683571 
model_pd.l_d.mean(): -23.32093620300293 
model_pd.lagr.mean(): -23.269107818603516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2688], device='cuda:0')), ('power', tensor([-23.5897], device='cuda:0'))])
epoch£º1110	 i:0 	 global-step:22200	 l-p:0.05182803049683571
epoch£º1110	 i:1 	 global-step:22201	 l-p:0.17940063774585724
epoch£º1110	 i:2 	 global-step:22202	 l-p:-0.03368362411856651
epoch£º1110	 i:3 	 global-step:22203	 l-p:0.13400660455226898
epoch£º1110	 i:4 	 global-step:22204	 l-p:-0.9420351982116699
epoch£º1110	 i:5 	 global-step:22205	 l-p:0.11545801162719727
epoch£º1110	 i:6 	 global-step:22206	 l-p:0.1261882781982422
epoch£º1110	 i:7 	 global-step:22207	 l-p:0.2126779705286026
epoch£º1110	 i:8 	 global-step:22208	 l-p:0.11538730561733246
epoch£º1110	 i:9 	 global-step:22209	 l-p:0.13092480599880219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7088, 2.9719, 2.3180],
        [3.7088, 3.6376, 3.6952],
        [3.7088, 3.7088, 3.7088],
        [3.7088, 2.9961, 2.7696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.13101068139076233 
model_pd.l_d.mean(): -22.540739059448242 
model_pd.lagr.mean(): -22.40972900390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3031], device='cuda:0')), ('power', tensor([-22.8438], device='cuda:0'))])
epoch£º1111	 i:0 	 global-step:22220	 l-p:0.13101068139076233
epoch£º1111	 i:1 	 global-step:22221	 l-p:0.5960702896118164
epoch£º1111	 i:2 	 global-step:22222	 l-p:0.1385784149169922
epoch£º1111	 i:3 	 global-step:22223	 l-p:0.14003044366836548
epoch£º1111	 i:4 	 global-step:22224	 l-p:0.12780879437923431
epoch£º1111	 i:5 	 global-step:22225	 l-p:-0.049768559634685516
epoch£º1111	 i:6 	 global-step:22226	 l-p:0.11227632313966751
epoch£º1111	 i:7 	 global-step:22227	 l-p:-0.14058439433574677
epoch£º1111	 i:8 	 global-step:22228	 l-p:0.16902092099189758
epoch£º1111	 i:9 	 global-step:22229	 l-p:0.1403014212846756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[3.6990, 2.8845, 2.3604],
        [3.6990, 2.8823, 2.3158],
        [3.6990, 3.1232, 3.0964],
        [3.6990, 2.8844, 2.3601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.16695065796375275 
model_pd.l_d.mean(): -23.09878921508789 
model_pd.lagr.mean(): -22.931838989257812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2406], device='cuda:0')), ('power', tensor([-23.3394], device='cuda:0'))])
epoch£º1112	 i:0 	 global-step:22240	 l-p:0.16695065796375275
epoch£º1112	 i:1 	 global-step:22241	 l-p:0.14058201014995575
epoch£º1112	 i:2 	 global-step:22242	 l-p:0.11961094290018082
epoch£º1112	 i:3 	 global-step:22243	 l-p:0.1466861367225647
epoch£º1112	 i:4 	 global-step:22244	 l-p:0.21945436298847198
epoch£º1112	 i:5 	 global-step:22245	 l-p:-0.551213264465332
epoch£º1112	 i:6 	 global-step:22246	 l-p:-0.2607022523880005
epoch£º1112	 i:7 	 global-step:22247	 l-p:0.1281200647354126
epoch£º1112	 i:8 	 global-step:22248	 l-p:0.129985049366951
epoch£º1112	 i:9 	 global-step:22249	 l-p:0.12460103631019592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7011, 3.6989, 3.7010],
        [3.7011, 3.7001, 3.7011],
        [3.7011, 3.7011, 3.7011],
        [3.7011, 3.1669, 3.1815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.13545207679271698 
model_pd.l_d.mean(): -22.81965446472168 
model_pd.lagr.mean(): -22.684202194213867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2745], device='cuda:0')), ('power', tensor([-23.0942], device='cuda:0'))])
epoch£º1113	 i:0 	 global-step:22260	 l-p:0.13545207679271698
epoch£º1113	 i:1 	 global-step:22261	 l-p:0.20410968363285065
epoch£º1113	 i:2 	 global-step:22262	 l-p:0.12454816699028015
epoch£º1113	 i:3 	 global-step:22263	 l-p:0.12173020839691162
epoch£º1113	 i:4 	 global-step:22264	 l-p:-0.0027317332569509745
epoch£º1113	 i:5 	 global-step:22265	 l-p:0.13296473026275635
epoch£º1113	 i:6 	 global-step:22266	 l-p:0.11020102351903915
epoch£º1113	 i:7 	 global-step:22267	 l-p:-0.08154937624931335
epoch£º1113	 i:8 	 global-step:22268	 l-p:0.1896546334028244
epoch£º1113	 i:9 	 global-step:22269	 l-p:0.06531914323568344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6890, 3.5332, 3.6359],
        [3.6890, 2.9687, 2.7321],
        [3.6890, 2.8923, 2.4630],
        [3.6890, 3.6837, 3.6888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.12430531531572342 
model_pd.l_d.mean(): -22.28990936279297 
model_pd.lagr.mean(): -22.165603637695312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2890], device='cuda:0')), ('power', tensor([-22.5789], device='cuda:0'))])
epoch£º1114	 i:0 	 global-step:22280	 l-p:0.12430531531572342
epoch£º1114	 i:1 	 global-step:22281	 l-p:0.1388138383626938
epoch£º1114	 i:2 	 global-step:22282	 l-p:0.03733623027801514
epoch£º1114	 i:3 	 global-step:22283	 l-p:0.1638597846031189
epoch£º1114	 i:4 	 global-step:22284	 l-p:0.12266016006469727
epoch£º1114	 i:5 	 global-step:22285	 l-p:0.13085435330867767
epoch£º1114	 i:6 	 global-step:22286	 l-p:0.1952507197856903
epoch£º1114	 i:7 	 global-step:22287	 l-p:0.13822238147258759
epoch£º1114	 i:8 	 global-step:22288	 l-p:-0.11060933768749237
epoch£º1114	 i:9 	 global-step:22289	 l-p:0.20251086354255676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6869, 2.8741, 2.2659],
        [3.6869, 3.6746, 3.6862],
        [3.6869, 3.0138, 2.3644],
        [3.6869, 3.6607, 3.6844]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.16734303534030914 
model_pd.l_d.mean(): -23.26863670349121 
model_pd.lagr.mean(): -23.101293563842773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2481], device='cuda:0')), ('power', tensor([-23.5167], device='cuda:0'))])
epoch£º1115	 i:0 	 global-step:22300	 l-p:0.16734303534030914
epoch£º1115	 i:1 	 global-step:22301	 l-p:-0.04833468422293663
epoch£º1115	 i:2 	 global-step:22302	 l-p:0.1280822604894638
epoch£º1115	 i:3 	 global-step:22303	 l-p:0.1790117770433426
epoch£º1115	 i:4 	 global-step:22304	 l-p:0.14603975415229797
epoch£º1115	 i:5 	 global-step:22305	 l-p:0.14287184178829193
epoch£º1115	 i:6 	 global-step:22306	 l-p:0.012899436987936497
epoch£º1115	 i:7 	 global-step:22307	 l-p:0.10456249862909317
epoch£º1115	 i:8 	 global-step:22308	 l-p:0.12387599796056747
epoch£º1115	 i:9 	 global-step:22309	 l-p:0.039108019322156906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6949, 3.2237, 3.2882],
        [3.6949, 3.1634, 3.1812],
        [3.6949, 2.8815, 2.3747],
        [3.6949, 3.6949, 3.6949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.12223973870277405 
model_pd.l_d.mean(): -23.476966857910156 
model_pd.lagr.mean(): -23.354726791381836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1800], device='cuda:0')), ('power', tensor([-23.6570], device='cuda:0'))])
epoch£º1116	 i:0 	 global-step:22320	 l-p:0.12223973870277405
epoch£º1116	 i:1 	 global-step:22321	 l-p:0.14501953125
epoch£º1116	 i:2 	 global-step:22322	 l-p:0.12164464592933655
epoch£º1116	 i:3 	 global-step:22323	 l-p:0.1301122009754181
epoch£º1116	 i:4 	 global-step:22324	 l-p:0.05068676173686981
epoch£º1116	 i:5 	 global-step:22325	 l-p:0.12554599344730377
epoch£º1116	 i:6 	 global-step:22326	 l-p:0.17864462733268738
epoch£º1116	 i:7 	 global-step:22327	 l-p:0.04306546226143837
epoch£º1116	 i:8 	 global-step:22328	 l-p:0.13258376717567444
epoch£º1116	 i:9 	 global-step:22329	 l-p:0.18926040828227997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6800, 3.5626, 3.6478],
        [3.6800, 3.6652, 3.6790],
        [3.6800, 3.1214, 3.1147],
        [3.6800, 3.6799, 3.6800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.12818141281604767 
model_pd.l_d.mean(): -23.603132247924805 
model_pd.lagr.mean(): -23.474950790405273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1549], device='cuda:0')), ('power', tensor([-23.7581], device='cuda:0'))])
epoch£º1117	 i:0 	 global-step:22340	 l-p:0.12818141281604767
epoch£º1117	 i:1 	 global-step:22341	 l-p:0.1207849383354187
epoch£º1117	 i:2 	 global-step:22342	 l-p:0.05881654471158981
epoch£º1117	 i:3 	 global-step:22343	 l-p:0.24011145532131195
epoch£º1117	 i:4 	 global-step:22344	 l-p:0.14058545231819153
epoch£º1117	 i:5 	 global-step:22345	 l-p:0.15474973618984222
epoch£º1117	 i:6 	 global-step:22346	 l-p:0.1889975666999817
epoch£º1117	 i:7 	 global-step:22347	 l-p:0.05671299621462822
epoch£º1117	 i:8 	 global-step:22348	 l-p:0.13772204518318176
epoch£º1117	 i:9 	 global-step:22349	 l-p:0.13044427335262299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6753, 3.6654, 3.6748],
        [3.6753, 3.3257, 3.4448],
        [3.6753, 3.5040, 3.6125],
        [3.6753, 3.6703, 3.6751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.05674760788679123 
model_pd.l_d.mean(): -22.71413803100586 
model_pd.lagr.mean(): -22.657390594482422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2450], device='cuda:0')), ('power', tensor([-22.9591], device='cuda:0'))])
epoch£º1118	 i:0 	 global-step:22360	 l-p:0.05674760788679123
epoch£º1118	 i:1 	 global-step:22361	 l-p:0.12053735554218292
epoch£º1118	 i:2 	 global-step:22362	 l-p:0.07148479670286179
epoch£º1118	 i:3 	 global-step:22363	 l-p:0.12469267845153809
epoch£º1118	 i:4 	 global-step:22364	 l-p:0.13137951493263245
epoch£º1118	 i:5 	 global-step:22365	 l-p:0.1487249881029129
epoch£º1118	 i:6 	 global-step:22366	 l-p:0.14999878406524658
epoch£º1118	 i:7 	 global-step:22367	 l-p:0.14704008400440216
epoch£º1118	 i:8 	 global-step:22368	 l-p:0.12028899788856506
epoch£º1118	 i:9 	 global-step:22369	 l-p:0.9673464894294739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6623, 3.1471, 3.1811],
        [3.6623, 3.0175, 2.3748],
        [3.6623, 3.6528, 3.6618],
        [3.6623, 3.1852, 2.5695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 3.4369051456451416 
model_pd.l_d.mean(): -22.77461051940918 
model_pd.lagr.mean(): -19.337705612182617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3490], device='cuda:0')), ('power', tensor([-23.1236], device='cuda:0'))])
epoch£º1119	 i:0 	 global-step:22380	 l-p:3.4369051456451416
epoch£º1119	 i:1 	 global-step:22381	 l-p:0.12020814418792725
epoch£º1119	 i:2 	 global-step:22382	 l-p:0.1412367969751358
epoch£º1119	 i:3 	 global-step:22383	 l-p:0.25932490825653076
epoch£º1119	 i:4 	 global-step:22384	 l-p:0.11080869287252426
epoch£º1119	 i:5 	 global-step:22385	 l-p:0.19414088129997253
epoch£º1119	 i:6 	 global-step:22386	 l-p:0.05482179671525955
epoch£º1119	 i:7 	 global-step:22387	 l-p:0.13762156665325165
epoch£º1119	 i:8 	 global-step:22388	 l-p:0.1472133845090866
epoch£º1119	 i:9 	 global-step:22389	 l-p:0.136666938662529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6541, 2.8630, 2.4682],
        [3.6541, 3.4116, 3.5369],
        [3.6541, 3.5615, 3.6328],
        [3.6541, 2.9815, 2.3371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.21684105694293976 
model_pd.l_d.mean(): -22.833751678466797 
model_pd.lagr.mean(): -22.616910934448242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3128], device='cuda:0')), ('power', tensor([-23.1465], device='cuda:0'))])
epoch£º1120	 i:0 	 global-step:22400	 l-p:0.21684105694293976
epoch£º1120	 i:1 	 global-step:22401	 l-p:0.156233549118042
epoch£º1120	 i:2 	 global-step:22402	 l-p:0.1415310651063919
epoch£º1120	 i:3 	 global-step:22403	 l-p:0.1229456290602684
epoch£º1120	 i:4 	 global-step:22404	 l-p:0.13624009490013123
epoch£º1120	 i:5 	 global-step:22405	 l-p:-0.0610162615776062
epoch£º1120	 i:6 	 global-step:22406	 l-p:0.1408580094575882
epoch£º1120	 i:7 	 global-step:22407	 l-p:0.12411203235387802
epoch£º1120	 i:8 	 global-step:22408	 l-p:0.13389794528484344
epoch£º1120	 i:9 	 global-step:22409	 l-p:0.11367819458246231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6403, 2.8407, 2.2082],
        [3.6403, 2.9199, 2.2744],
        [3.6403, 3.6205, 3.6387],
        [3.6403, 3.6396, 3.6402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.1002238318324089 
model_pd.l_d.mean(): -23.759180068969727 
model_pd.lagr.mean(): -23.65895652770996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1627], device='cuda:0')), ('power', tensor([-23.9219], device='cuda:0'))])
epoch£º1121	 i:0 	 global-step:22420	 l-p:0.1002238318324089
epoch£º1121	 i:1 	 global-step:22421	 l-p:0.10718264430761337
epoch£º1121	 i:2 	 global-step:22422	 l-p:0.7509078979492188
epoch£º1121	 i:3 	 global-step:22423	 l-p:0.13043145835399628
epoch£º1121	 i:4 	 global-step:22424	 l-p:0.12878142297267914
epoch£º1121	 i:5 	 global-step:22425	 l-p:0.08140653371810913
epoch£º1121	 i:6 	 global-step:22426	 l-p:0.15582434833049774
epoch£º1121	 i:7 	 global-step:22427	 l-p:0.11262599378824234
epoch£º1121	 i:8 	 global-step:22428	 l-p:0.1258065551519394
epoch£º1121	 i:9 	 global-step:22429	 l-p:0.15895824134349823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6396, 3.5597, 3.6231],
        [3.6396, 3.5779, 3.6290],
        [3.6396, 3.4707, 3.5786],
        [3.6396, 3.0792, 2.4520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.14886629581451416 
model_pd.l_d.mean(): -23.236724853515625 
model_pd.lagr.mean(): -23.087858200073242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3225], device='cuda:0')), ('power', tensor([-23.5592], device='cuda:0'))])
epoch£º1122	 i:0 	 global-step:22440	 l-p:0.14886629581451416
epoch£º1122	 i:1 	 global-step:22441	 l-p:0.10957776755094528
epoch£º1122	 i:2 	 global-step:22442	 l-p:-0.012527589686214924
epoch£º1122	 i:3 	 global-step:22443	 l-p:0.6519352793693542
epoch£º1122	 i:4 	 global-step:22444	 l-p:0.13277901709079742
epoch£º1122	 i:5 	 global-step:22445	 l-p:0.1406116783618927
epoch£º1122	 i:6 	 global-step:22446	 l-p:0.15351539850234985
epoch£º1122	 i:7 	 global-step:22447	 l-p:0.13375769555568695
epoch£º1122	 i:8 	 global-step:22448	 l-p:0.22005830705165863
epoch£º1122	 i:9 	 global-step:22449	 l-p:0.07265058159828186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6453, 2.8540, 2.4625],
        [3.6453, 2.8207, 2.2324],
        [3.6453, 3.4763, 3.5842],
        [3.6453, 3.6354, 3.6447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.18669036030769348 
model_pd.l_d.mean(): -23.4354305267334 
model_pd.lagr.mean(): -23.24873924255371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2603], device='cuda:0')), ('power', tensor([-23.6957], device='cuda:0'))])
epoch£º1123	 i:0 	 global-step:22460	 l-p:0.18669036030769348
epoch£º1123	 i:1 	 global-step:22461	 l-p:0.07946579903364182
epoch£º1123	 i:2 	 global-step:22462	 l-p:0.13851229846477509
epoch£º1123	 i:3 	 global-step:22463	 l-p:0.07899576425552368
epoch£º1123	 i:4 	 global-step:22464	 l-p:0.11968831717967987
epoch£º1123	 i:5 	 global-step:22465	 l-p:-0.21779415011405945
epoch£º1123	 i:6 	 global-step:22466	 l-p:0.24075831472873688
epoch£º1123	 i:7 	 global-step:22467	 l-p:0.14828814566135406
epoch£º1123	 i:8 	 global-step:22468	 l-p:0.16004954278469086
epoch£º1123	 i:9 	 global-step:22469	 l-p:0.12426595389842987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6611, 2.8457, 2.2329],
        [3.6611, 3.4324, 3.5557],
        [3.6611, 3.0660, 3.0223],
        [3.6611, 3.4616, 3.5787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.14455056190490723 
model_pd.l_d.mean(): -23.581510543823242 
model_pd.lagr.mean(): -23.436960220336914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1687], device='cuda:0')), ('power', tensor([-23.7503], device='cuda:0'))])
epoch£º1124	 i:0 	 global-step:22480	 l-p:0.14455056190490723
epoch£º1124	 i:1 	 global-step:22481	 l-p:0.1937437206506729
epoch£º1124	 i:2 	 global-step:22482	 l-p:0.1305868923664093
epoch£º1124	 i:3 	 global-step:22483	 l-p:0.07447681576013565
epoch£º1124	 i:4 	 global-step:22484	 l-p:0.13415305316448212
epoch£º1124	 i:5 	 global-step:22485	 l-p:0.17715062201023102
epoch£º1124	 i:6 	 global-step:22486	 l-p:0.07311832904815674
epoch£º1124	 i:7 	 global-step:22487	 l-p:-0.5547259449958801
epoch£º1124	 i:8 	 global-step:22488	 l-p:0.1484449952840805
epoch£º1124	 i:9 	 global-step:22489	 l-p:0.13891300559043884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6621, 3.4851, 3.5956],
        [3.6621, 2.9154, 2.6319],
        [3.6621, 2.9447, 2.7204],
        [3.6621, 3.6459, 3.6609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.15400204062461853 
model_pd.l_d.mean(): -23.760374069213867 
model_pd.lagr.mean(): -23.606372833251953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1818], device='cuda:0')), ('power', tensor([-23.9422], device='cuda:0'))])
epoch£º1125	 i:0 	 global-step:22500	 l-p:0.15400204062461853
epoch£º1125	 i:1 	 global-step:22501	 l-p:0.12849222123622894
epoch£º1125	 i:2 	 global-step:22502	 l-p:0.06570310145616531
epoch£º1125	 i:3 	 global-step:22503	 l-p:0.2276395708322525
epoch£º1125	 i:4 	 global-step:22504	 l-p:0.12928670644760132
epoch£º1125	 i:5 	 global-step:22505	 l-p:1.2368195056915283
epoch£º1125	 i:6 	 global-step:22506	 l-p:0.14419634640216827
epoch£º1125	 i:7 	 global-step:22507	 l-p:0.11018954962491989
epoch£º1125	 i:8 	 global-step:22508	 l-p:0.13824191689491272
epoch£º1125	 i:9 	 global-step:22509	 l-p:0.13643991947174072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6658, 3.6658, 3.6658],
        [3.6658, 3.4964, 3.6043],
        [3.6658, 3.3336, 3.4566],
        [3.6658, 2.9930, 2.8449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.15717406570911407 
model_pd.l_d.mean(): -23.649322509765625 
model_pd.lagr.mean(): -23.492149353027344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1940], device='cuda:0')), ('power', tensor([-23.8434], device='cuda:0'))])
epoch£º1126	 i:0 	 global-step:22520	 l-p:0.15717406570911407
epoch£º1126	 i:1 	 global-step:22521	 l-p:0.15450555086135864
epoch£º1126	 i:2 	 global-step:22522	 l-p:0.10147987306118011
epoch£º1126	 i:3 	 global-step:22523	 l-p:0.052713893353939056
epoch£º1126	 i:4 	 global-step:22524	 l-p:0.11618185043334961
epoch£º1126	 i:5 	 global-step:22525	 l-p:0.12897859513759613
epoch£º1126	 i:6 	 global-step:22526	 l-p:0.1436174511909485
epoch£º1126	 i:7 	 global-step:22527	 l-p:0.4172571003437042
epoch£º1126	 i:8 	 global-step:22528	 l-p:0.13005177676677704
epoch£º1126	 i:9 	 global-step:22529	 l-p:0.17118310928344727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6743, 2.9733, 2.7772],
        [3.6743, 3.6734, 3.6743],
        [3.6743, 3.6740, 3.6743],
        [3.6743, 2.8581, 2.2503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.15838943421840668 
model_pd.l_d.mean(): -23.66689682006836 
model_pd.lagr.mean(): -23.508506774902344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2345], device='cuda:0')), ('power', tensor([-23.9014], device='cuda:0'))])
epoch£º1127	 i:0 	 global-step:22540	 l-p:0.15838943421840668
epoch£º1127	 i:1 	 global-step:22541	 l-p:0.14302276074886322
epoch£º1127	 i:2 	 global-step:22542	 l-p:0.12260381132364273
epoch£º1127	 i:3 	 global-step:22543	 l-p:0.11005982756614685
epoch£º1127	 i:4 	 global-step:22544	 l-p:0.12832188606262207
epoch£º1127	 i:5 	 global-step:22545	 l-p:0.14052623510360718
epoch£º1127	 i:6 	 global-step:22546	 l-p:0.04679200053215027
epoch£º1127	 i:7 	 global-step:22547	 l-p:0.34053295850753784
epoch£º1127	 i:8 	 global-step:22548	 l-p:0.06044551357626915
epoch£º1127	 i:9 	 global-step:22549	 l-p:0.14572574198246002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6793, 3.6793, 3.6793],
        [3.6793, 3.6793, 3.6793],
        [3.6793, 3.5766, 3.6538],
        [3.6793, 2.9505, 2.7014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.2136540412902832 
model_pd.l_d.mean(): -22.488544464111328 
model_pd.lagr.mean(): -22.274890899658203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3249], device='cuda:0')), ('power', tensor([-22.8134], device='cuda:0'))])
epoch£º1128	 i:0 	 global-step:22560	 l-p:0.2136540412902832
epoch£º1128	 i:1 	 global-step:22561	 l-p:0.28939181566238403
epoch£º1128	 i:2 	 global-step:22562	 l-p:0.1081421822309494
epoch£º1128	 i:3 	 global-step:22563	 l-p:-0.0836486965417862
epoch£º1128	 i:4 	 global-step:22564	 l-p:0.12187937647104263
epoch£º1128	 i:5 	 global-step:22565	 l-p:0.14425818622112274
epoch£º1128	 i:6 	 global-step:22566	 l-p:0.09576216340065002
epoch£º1128	 i:7 	 global-step:22567	 l-p:0.08651519566774368
epoch£º1128	 i:8 	 global-step:22568	 l-p:0.13441285490989685
epoch£º1128	 i:9 	 global-step:22569	 l-p:0.12184873223304749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6841, 3.6841, 3.6841],
        [3.6841, 3.5100, 3.6195],
        [3.6841, 3.6618, 3.6822],
        [3.6841, 3.6550, 3.6811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.17421239614486694 
model_pd.l_d.mean(): -23.00919532775879 
model_pd.lagr.mean(): -22.834983825683594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2038], device='cuda:0')), ('power', tensor([-23.2130], device='cuda:0'))])
epoch£º1129	 i:0 	 global-step:22580	 l-p:0.17421239614486694
epoch£º1129	 i:1 	 global-step:22581	 l-p:0.14005547761917114
epoch£º1129	 i:2 	 global-step:22582	 l-p:0.1004587784409523
epoch£º1129	 i:3 	 global-step:22583	 l-p:0.12142125517129898
epoch£º1129	 i:4 	 global-step:22584	 l-p:0.12961001694202423
epoch£º1129	 i:5 	 global-step:22585	 l-p:0.13495135307312012
epoch£º1129	 i:6 	 global-step:22586	 l-p:0.15464113652706146
epoch£º1129	 i:7 	 global-step:22587	 l-p:0.11499033868312836
epoch£º1129	 i:8 	 global-step:22588	 l-p:0.14748486876487732
epoch£º1129	 i:9 	 global-step:22589	 l-p:0.1720726191997528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6607, 3.0948, 2.4635],
        [3.6607, 3.6606, 3.6607],
        [3.6607, 3.1166, 2.4889],
        [3.6607, 3.1266, 3.1454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.14107246696949005 
model_pd.l_d.mean(): -22.726308822631836 
model_pd.lagr.mean(): -22.585235595703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3356], device='cuda:0')), ('power', tensor([-23.0619], device='cuda:0'))])
epoch£º1130	 i:0 	 global-step:22600	 l-p:0.14107246696949005
epoch£º1130	 i:1 	 global-step:22601	 l-p:0.09518742561340332
epoch£º1130	 i:2 	 global-step:22602	 l-p:0.12456227838993073
epoch£º1130	 i:3 	 global-step:22603	 l-p:-1.7704734802246094
epoch£º1130	 i:4 	 global-step:22604	 l-p:0.11583779752254486
epoch£º1130	 i:5 	 global-step:22605	 l-p:0.14330841600894928
epoch£º1130	 i:6 	 global-step:22606	 l-p:0.10486476868391037
epoch£º1130	 i:7 	 global-step:22607	 l-p:0.1995382457971573
epoch£º1130	 i:8 	 global-step:22608	 l-p:0.1243690773844719
epoch£º1130	 i:9 	 global-step:22609	 l-p:0.1376684308052063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6681, 3.6681, 3.6681],
        [3.6681, 3.5883, 3.6516],
        [3.6681, 2.9781, 2.3298],
        [3.6681, 3.4879, 3.5995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.14015692472457886 
model_pd.l_d.mean(): -23.126787185668945 
model_pd.lagr.mean(): -22.986629486083984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2328], device='cuda:0')), ('power', tensor([-23.3596], device='cuda:0'))])
epoch£º1131	 i:0 	 global-step:22620	 l-p:0.14015692472457886
epoch£º1131	 i:1 	 global-step:22621	 l-p:0.12177087366580963
epoch£º1131	 i:2 	 global-step:22622	 l-p:0.1300247311592102
epoch£º1131	 i:3 	 global-step:22623	 l-p:0.2204590141773224
epoch£º1131	 i:4 	 global-step:22624	 l-p:0.13117805123329163
epoch£º1131	 i:5 	 global-step:22625	 l-p:0.15073344111442566
epoch£º1131	 i:6 	 global-step:22626	 l-p:0.06218385696411133
epoch£º1131	 i:7 	 global-step:22627	 l-p:-0.7859922051429749
epoch£º1131	 i:8 	 global-step:22628	 l-p:0.1512613445520401
epoch£º1131	 i:9 	 global-step:22629	 l-p:0.12224998325109482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228]], device='cuda:0')
 pt:tensor([[3.6576, 3.1007, 2.4713],
        [3.6576, 3.1115, 3.1198],
        [3.6576, 2.8395, 2.3460],
        [3.6576, 3.0299, 2.9480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.12463632225990295 
model_pd.l_d.mean(): -23.547943115234375 
model_pd.lagr.mean(): -23.423307418823242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2023], device='cuda:0')), ('power', tensor([-23.7502], device='cuda:0'))])
epoch£º1132	 i:0 	 global-step:22640	 l-p:0.12463632225990295
epoch£º1132	 i:1 	 global-step:22641	 l-p:0.14892567694187164
epoch£º1132	 i:2 	 global-step:22642	 l-p:0.14336848258972168
epoch£º1132	 i:3 	 global-step:22643	 l-p:0.13388332724571228
epoch£º1132	 i:4 	 global-step:22644	 l-p:0.15155018866062164
epoch£º1132	 i:5 	 global-step:22645	 l-p:0.597396194934845
epoch£º1132	 i:6 	 global-step:22646	 l-p:0.1330142319202423
epoch£º1132	 i:7 	 global-step:22647	 l-p:0.10111629217863083
epoch£º1132	 i:8 	 global-step:22648	 l-p:0.03897379711270332
epoch£º1132	 i:9 	 global-step:22649	 l-p:0.12396036088466644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6378, 3.6378, 3.6378],
        [3.6378, 2.8090, 2.2425],
        [3.6378, 3.1897, 3.2720],
        [3.6378, 3.5468, 3.6172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 3.0793936252593994 
model_pd.l_d.mean(): -23.210006713867188 
model_pd.lagr.mean(): -20.130613327026367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2687], device='cuda:0')), ('power', tensor([-23.4787], device='cuda:0'))])
epoch£º1133	 i:0 	 global-step:22660	 l-p:3.0793936252593994
epoch£º1133	 i:1 	 global-step:22661	 l-p:0.2519221007823944
epoch£º1133	 i:2 	 global-step:22662	 l-p:0.1367865353822708
epoch£º1133	 i:3 	 global-step:22663	 l-p:0.1273549199104309
epoch£º1133	 i:4 	 global-step:22664	 l-p:0.13656285405158997
epoch£º1133	 i:5 	 global-step:22665	 l-p:0.07336407154798508
epoch£º1133	 i:6 	 global-step:22666	 l-p:0.09265896677970886
epoch£º1133	 i:7 	 global-step:22667	 l-p:0.14384305477142334
epoch£º1133	 i:8 	 global-step:22668	 l-p:0.11717505007982254
epoch£º1133	 i:9 	 global-step:22669	 l-p:0.1466657966375351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6474, 3.1116, 3.1303],
        [3.6474, 3.1738, 3.2402],
        [3.6474, 3.4753, 3.5844],
        [3.6474, 3.2916, 3.4105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.150113046169281 
model_pd.l_d.mean(): -23.401554107666016 
model_pd.lagr.mean(): -23.251441955566406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2717], device='cuda:0')), ('power', tensor([-23.6732], device='cuda:0'))])
epoch£º1134	 i:0 	 global-step:22680	 l-p:0.150113046169281
epoch£º1134	 i:1 	 global-step:22681	 l-p:0.1292862892150879
epoch£º1134	 i:2 	 global-step:22682	 l-p:0.13193781673908234
epoch£º1134	 i:3 	 global-step:22683	 l-p:0.08832389861345291
epoch£º1134	 i:4 	 global-step:22684	 l-p:0.1586780846118927
epoch£º1134	 i:5 	 global-step:22685	 l-p:0.11992446333169937
epoch£º1134	 i:6 	 global-step:22686	 l-p:0.08599042892456055
epoch£º1134	 i:7 	 global-step:22687	 l-p:0.025358805432915688
epoch£º1134	 i:8 	 global-step:22688	 l-p:0.7535422444343567
epoch£º1134	 i:9 	 global-step:22689	 l-p:0.21344539523124695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6457, 3.0965, 2.4700],
        [3.6457, 2.8429, 2.4196],
        [3.6457, 3.5502, 3.6233],
        [3.6457, 2.9241, 2.6973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): -0.004793443717062473 
model_pd.l_d.mean(): -23.36886978149414 
model_pd.lagr.mean(): -23.3736629486084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2933], device='cuda:0')), ('power', tensor([-23.6622], device='cuda:0'))])
epoch£º1135	 i:0 	 global-step:22700	 l-p:-0.004793443717062473
epoch£º1135	 i:1 	 global-step:22701	 l-p:0.09151065349578857
epoch£º1135	 i:2 	 global-step:22702	 l-p:0.12880657613277435
epoch£º1135	 i:3 	 global-step:22703	 l-p:0.11639567464590073
epoch£º1135	 i:4 	 global-step:22704	 l-p:0.13619086146354675
epoch£º1135	 i:5 	 global-step:22705	 l-p:0.1321144849061966
epoch£º1135	 i:6 	 global-step:22706	 l-p:0.10400235652923584
epoch£º1135	 i:7 	 global-step:22707	 l-p:0.24451006948947906
epoch£º1135	 i:8 	 global-step:22708	 l-p:0.16078132390975952
epoch£º1135	 i:9 	 global-step:22709	 l-p:0.1336067020893097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6648, 3.6127, 3.6568],
        [3.6648, 3.6612, 3.6647],
        [3.6648, 3.3264, 3.4486],
        [3.6648, 2.9474, 2.7252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.07115630060434341 
model_pd.l_d.mean(): -23.6680908203125 
model_pd.lagr.mean(): -23.596935272216797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1245], device='cuda:0')), ('power', tensor([-23.7926], device='cuda:0'))])
epoch£º1136	 i:0 	 global-step:22720	 l-p:0.07115630060434341
epoch£º1136	 i:1 	 global-step:22721	 l-p:0.20751845836639404
epoch£º1136	 i:2 	 global-step:22722	 l-p:0.12819668650627136
epoch£º1136	 i:3 	 global-step:22723	 l-p:0.12891298532485962
epoch£º1136	 i:4 	 global-step:22724	 l-p:0.4924354553222656
epoch£º1136	 i:5 	 global-step:22725	 l-p:0.1829705834388733
epoch£º1136	 i:6 	 global-step:22726	 l-p:0.14125995337963104
epoch£º1136	 i:7 	 global-step:22727	 l-p:0.1332325041294098
epoch£º1136	 i:8 	 global-step:22728	 l-p:0.06255902349948883
epoch£º1136	 i:9 	 global-step:22729	 l-p:0.15803661942481995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6726, 3.6718, 3.6726],
        [3.6726, 3.6726, 3.6726],
        [3.6726, 3.6633, 3.6721],
        [3.6726, 2.9174, 2.6154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.4740505516529083 
model_pd.l_d.mean(): -23.34888458251953 
model_pd.lagr.mean(): -22.874834060668945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2221], device='cuda:0')), ('power', tensor([-23.5710], device='cuda:0'))])
epoch£º1137	 i:0 	 global-step:22740	 l-p:0.4740505516529083
epoch£º1137	 i:1 	 global-step:22741	 l-p:0.029703082516789436
epoch£º1137	 i:2 	 global-step:22742	 l-p:0.17375509440898895
epoch£º1137	 i:3 	 global-step:22743	 l-p:0.1283775418996811
epoch£º1137	 i:4 	 global-step:22744	 l-p:0.14984886348247528
epoch£º1137	 i:5 	 global-step:22745	 l-p:0.15149323642253876
epoch£º1137	 i:6 	 global-step:22746	 l-p:0.13696709275245667
epoch£º1137	 i:7 	 global-step:22747	 l-p:0.1130245178937912
epoch£º1137	 i:8 	 global-step:22748	 l-p:0.15323416888713837
epoch£º1137	 i:9 	 global-step:22749	 l-p:0.0058049545623362064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6687, 3.6610, 3.6683],
        [3.6687, 3.6687, 3.6687],
        [3.6687, 3.5670, 3.6437],
        [3.6687, 2.9053, 2.2573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.6102758646011353 
model_pd.l_d.mean(): -22.879074096679688 
model_pd.lagr.mean(): -22.268798828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2545], device='cuda:0')), ('power', tensor([-23.1336], device='cuda:0'))])
epoch£º1138	 i:0 	 global-step:22760	 l-p:0.6102758646011353
epoch£º1138	 i:1 	 global-step:22761	 l-p:0.14256514608860016
epoch£º1138	 i:2 	 global-step:22762	 l-p:0.14926545321941376
epoch£º1138	 i:3 	 global-step:22763	 l-p:0.12504246830940247
epoch£º1138	 i:4 	 global-step:22764	 l-p:0.154770165681839
epoch£º1138	 i:5 	 global-step:22765	 l-p:0.12863726913928986
epoch£º1138	 i:6 	 global-step:22766	 l-p:0.08656802773475647
epoch£º1138	 i:7 	 global-step:22767	 l-p:0.13911062479019165
epoch£º1138	 i:8 	 global-step:22768	 l-p:0.278591126203537
epoch£º1138	 i:9 	 global-step:22769	 l-p:0.07588735222816467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6551, 3.6551, 3.6552],
        [3.6551, 3.6303, 3.6528],
        [3.6551, 2.8351, 2.3373],
        [3.6551, 3.6259, 3.6521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.12917211651802063 
model_pd.l_d.mean(): -23.295812606811523 
model_pd.lagr.mean(): -23.166641235351562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2525], device='cuda:0')), ('power', tensor([-23.5483], device='cuda:0'))])
epoch£º1139	 i:0 	 global-step:22780	 l-p:0.12917211651802063
epoch£º1139	 i:1 	 global-step:22781	 l-p:0.1186368316411972
epoch£º1139	 i:2 	 global-step:22782	 l-p:0.1436830461025238
epoch£º1139	 i:3 	 global-step:22783	 l-p:0.10402831435203552
epoch£º1139	 i:4 	 global-step:22784	 l-p:0.14290183782577515
epoch£º1139	 i:5 	 global-step:22785	 l-p:0.13203021883964539
epoch£º1139	 i:6 	 global-step:22786	 l-p:0.1771526336669922
epoch£º1139	 i:7 	 global-step:22787	 l-p:0.2650412321090698
epoch£º1139	 i:8 	 global-step:22788	 l-p:0.08315744996070862
epoch£º1139	 i:9 	 global-step:22789	 l-p:-0.08836095780134201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6571, 3.6571, 3.6571],
        [3.6571, 3.5634, 3.6354],
        [3.6571, 3.6551, 3.6571],
        [3.6571, 3.6571, 3.6571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.1520281881093979 
model_pd.l_d.mean(): -23.74535369873047 
model_pd.lagr.mean(): -23.593324661254883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1248], device='cuda:0')), ('power', tensor([-23.8701], device='cuda:0'))])
epoch£º1140	 i:0 	 global-step:22800	 l-p:0.1520281881093979
epoch£º1140	 i:1 	 global-step:22801	 l-p:0.13467440009117126
epoch£º1140	 i:2 	 global-step:22802	 l-p:0.2524343729019165
epoch£º1140	 i:3 	 global-step:22803	 l-p:0.1774170696735382
epoch£º1140	 i:4 	 global-step:22804	 l-p:0.1264493316411972
epoch£º1140	 i:5 	 global-step:22805	 l-p:0.117430180311203
epoch£º1140	 i:6 	 global-step:22806	 l-p:0.13718447089195251
epoch£º1140	 i:7 	 global-step:22807	 l-p:-0.3541179895401001
epoch£º1140	 i:8 	 global-step:22808	 l-p:0.1134934276342392
epoch£º1140	 i:9 	 global-step:22809	 l-p:0.12882766127586365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6556, 3.4026, 3.5293],
        [3.6556, 3.6543, 3.6555],
        [3.6556, 2.8631, 2.2241],
        [3.6556, 3.1807, 3.2462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): -0.18619219958782196 
model_pd.l_d.mean(): -22.642221450805664 
model_pd.lagr.mean(): -22.828413009643555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3884], device='cuda:0')), ('power', tensor([-23.0306], device='cuda:0'))])
epoch£º1141	 i:0 	 global-step:22820	 l-p:-0.18619219958782196
epoch£º1141	 i:1 	 global-step:22821	 l-p:0.16765186190605164
epoch£º1141	 i:2 	 global-step:22822	 l-p:0.13781821727752686
epoch£º1141	 i:3 	 global-step:22823	 l-p:0.08239378780126572
epoch£º1141	 i:4 	 global-step:22824	 l-p:0.12626436352729797
epoch£º1141	 i:5 	 global-step:22825	 l-p:0.14177359640598297
epoch£º1141	 i:6 	 global-step:22826	 l-p:0.14112521708011627
epoch£º1141	 i:7 	 global-step:22827	 l-p:0.37999868392944336
epoch£º1141	 i:8 	 global-step:22828	 l-p:0.09319864213466644
epoch£º1141	 i:9 	 global-step:22829	 l-p:0.07874520123004913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6576, 3.6567, 3.6576],
        [3.6576, 3.1867, 2.5723],
        [3.6576, 3.6574, 3.6576],
        [3.6576, 2.8894, 2.2430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.1273292750120163 
model_pd.l_d.mean(): -23.04790687561035 
model_pd.lagr.mean(): -22.920578002929688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1936], device='cuda:0')), ('power', tensor([-23.2415], device='cuda:0'))])
epoch£º1142	 i:0 	 global-step:22840	 l-p:0.1273292750120163
epoch£º1142	 i:1 	 global-step:22841	 l-p:0.09604605287313461
epoch£º1142	 i:2 	 global-step:22842	 l-p:0.18773232400417328
epoch£º1142	 i:3 	 global-step:22843	 l-p:0.13399112224578857
epoch£º1142	 i:4 	 global-step:22844	 l-p:1.4692480564117432
epoch£º1142	 i:5 	 global-step:22845	 l-p:0.11762436479330063
epoch£º1142	 i:6 	 global-step:22846	 l-p:0.13166704773902893
epoch£º1142	 i:7 	 global-step:22847	 l-p:0.2177421748638153
epoch£º1142	 i:8 	 global-step:22848	 l-p:0.047474734485149384
epoch£º1142	 i:9 	 global-step:22849	 l-p:0.11122548580169678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6737, 3.1289, 2.4989],
        [3.6737, 3.6117, 3.6630],
        [3.6737, 3.2776, 3.3834],
        [3.6737, 3.6714, 3.6736]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.038638561964035034 
model_pd.l_d.mean(): -23.2344913482666 
model_pd.lagr.mean(): -23.195852279663086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1596], device='cuda:0')), ('power', tensor([-23.3941], device='cuda:0'))])
epoch£º1143	 i:0 	 global-step:22860	 l-p:0.038638561964035034
epoch£º1143	 i:1 	 global-step:22861	 l-p:0.13152925670146942
epoch£º1143	 i:2 	 global-step:22862	 l-p:0.1436738818883896
epoch£º1143	 i:3 	 global-step:22863	 l-p:0.12371933460235596
epoch£º1143	 i:4 	 global-step:22864	 l-p:0.18745486438274384
epoch£º1143	 i:5 	 global-step:22865	 l-p:0.11615927517414093
epoch£º1143	 i:6 	 global-step:22866	 l-p:0.21739301085472107
epoch£º1143	 i:7 	 global-step:22867	 l-p:0.12985171377658844
epoch£º1143	 i:8 	 global-step:22868	 l-p:0.14453257620334625
epoch£º1143	 i:9 	 global-step:22869	 l-p:0.13239076733589172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6828, 3.6828, 3.6828],
        [3.6828, 3.6828, 3.6828],
        [3.6828, 3.6825, 3.6828],
        [3.6828, 2.9587, 2.7213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.1564638912677765 
model_pd.l_d.mean(): -23.43825912475586 
model_pd.lagr.mean(): -23.281795501708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2025], device='cuda:0')), ('power', tensor([-23.6408], device='cuda:0'))])
epoch£º1144	 i:0 	 global-step:22880	 l-p:0.1564638912677765
epoch£º1144	 i:1 	 global-step:22881	 l-p:0.14198558032512665
epoch£º1144	 i:2 	 global-step:22882	 l-p:0.10072079300880432
epoch£º1144	 i:3 	 global-step:22883	 l-p:0.06115001067519188
epoch£º1144	 i:4 	 global-step:22884	 l-p:0.04984145984053612
epoch£º1144	 i:5 	 global-step:22885	 l-p:0.13898944854736328
epoch£º1144	 i:6 	 global-step:22886	 l-p:0.14386244118213654
epoch£º1144	 i:7 	 global-step:22887	 l-p:0.18169763684272766
epoch£º1144	 i:8 	 global-step:22888	 l-p:0.1615113914012909
epoch£º1144	 i:9 	 global-step:22889	 l-p:0.13463442027568817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6899, 3.5204, 3.6284],
        [3.6899, 3.4150, 3.5424],
        [3.6899, 3.6784, 3.6892],
        [3.6899, 3.6701, 3.6883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.017757195979356766 
model_pd.l_d.mean(): -23.148038864135742 
model_pd.lagr.mean(): -23.130281448364258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2675], device='cuda:0')), ('power', tensor([-23.4156], device='cuda:0'))])
epoch£º1145	 i:0 	 global-step:22900	 l-p:0.017757195979356766
epoch£º1145	 i:1 	 global-step:22901	 l-p:0.11276933550834656
epoch£º1145	 i:2 	 global-step:22902	 l-p:0.11902705579996109
epoch£º1145	 i:3 	 global-step:22903	 l-p:0.142173632979393
epoch£º1145	 i:4 	 global-step:22904	 l-p:0.1263100802898407
epoch£º1145	 i:5 	 global-step:22905	 l-p:0.14190156757831573
epoch£º1145	 i:6 	 global-step:22906	 l-p:-0.018305014818906784
epoch£º1145	 i:7 	 global-step:22907	 l-p:0.2648535668849945
epoch£º1145	 i:8 	 global-step:22908	 l-p:0.1280899941921234
epoch£º1145	 i:9 	 global-step:22909	 l-p:0.13575035333633423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6825, 3.6825, 3.6825],
        [3.6825, 3.2368, 2.6242],
        [3.6825, 2.9684, 2.7505],
        [3.6825, 2.9846, 2.7958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.1892199069261551 
model_pd.l_d.mean(): -23.158288955688477 
model_pd.lagr.mean(): -22.96906852722168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3379], device='cuda:0')), ('power', tensor([-23.4962], device='cuda:0'))])
epoch£º1146	 i:0 	 global-step:22920	 l-p:0.1892199069261551
epoch£º1146	 i:1 	 global-step:22921	 l-p:0.12779341638088226
epoch£º1146	 i:2 	 global-step:22922	 l-p:0.056432049721479416
epoch£º1146	 i:3 	 global-step:22923	 l-p:0.2888485789299011
epoch£º1146	 i:4 	 global-step:22924	 l-p:0.13714945316314697
epoch£º1146	 i:5 	 global-step:22925	 l-p:0.028118470683693886
epoch£º1146	 i:6 	 global-step:22926	 l-p:0.12515710294246674
epoch£º1146	 i:7 	 global-step:22927	 l-p:0.1395139843225479
epoch£º1146	 i:8 	 global-step:22928	 l-p:0.04181509092450142
epoch£º1146	 i:9 	 global-step:22929	 l-p:0.20391349494457245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6840, 3.0408, 2.3944],
        [3.6840, 3.6830, 3.6839],
        [3.6840, 3.6820, 3.6839],
        [3.6840, 2.8665, 2.2569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.3075639605522156 
model_pd.l_d.mean(): -23.339670181274414 
model_pd.lagr.mean(): -23.032106399536133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2592], device='cuda:0')), ('power', tensor([-23.5989], device='cuda:0'))])
epoch£º1147	 i:0 	 global-step:22940	 l-p:0.3075639605522156
epoch£º1147	 i:1 	 global-step:22941	 l-p:0.06361791491508484
epoch£º1147	 i:2 	 global-step:22942	 l-p:0.17527513206005096
epoch£º1147	 i:3 	 global-step:22943	 l-p:0.10889551788568497
epoch£º1147	 i:4 	 global-step:22944	 l-p:0.12337446212768555
epoch£º1147	 i:5 	 global-step:22945	 l-p:0.14023688435554504
epoch£º1147	 i:6 	 global-step:22946	 l-p:0.1276671141386032
epoch£º1147	 i:7 	 global-step:22947	 l-p:0.12588612735271454
epoch£º1147	 i:8 	 global-step:22948	 l-p:-0.25050464272499084
epoch£º1147	 i:9 	 global-step:22949	 l-p:0.13261374831199646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6940, 3.1453, 2.5115],
        [3.6940, 3.5405, 3.6424],
        [3.6940, 3.4715, 3.5937],
        [3.6940, 3.6862, 3.6936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.1151038110256195 
model_pd.l_d.mean(): -23.012205123901367 
model_pd.lagr.mean(): -22.8971004486084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2595], device='cuda:0')), ('power', tensor([-23.2717], device='cuda:0'))])
epoch£º1148	 i:0 	 global-step:22960	 l-p:0.1151038110256195
epoch£º1148	 i:1 	 global-step:22961	 l-p:0.1225254163146019
epoch£º1148	 i:2 	 global-step:22962	 l-p:0.14370815455913544
epoch£º1148	 i:3 	 global-step:22963	 l-p:0.0014903258997946978
epoch£º1148	 i:4 	 global-step:22964	 l-p:0.13884606957435608
epoch£º1148	 i:5 	 global-step:22965	 l-p:0.0990789607167244
epoch£º1148	 i:6 	 global-step:22966	 l-p:0.12337325513362885
epoch£º1148	 i:7 	 global-step:22967	 l-p:0.11527658253908157
epoch£º1148	 i:8 	 global-step:22968	 l-p:0.20481014251708984
epoch£º1148	 i:9 	 global-step:22969	 l-p:0.363467276096344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6729, 2.8655, 2.2362],
        [3.6729, 2.8522, 2.3428],
        [3.6729, 3.4133, 3.5404],
        [3.6729, 3.4488, 3.5715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.15498259663581848 
model_pd.l_d.mean(): -23.144227981567383 
model_pd.lagr.mean(): -22.98924446105957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3047], device='cuda:0')), ('power', tensor([-23.4489], device='cuda:0'))])
epoch£º1149	 i:0 	 global-step:22980	 l-p:0.15498259663581848
epoch£º1149	 i:1 	 global-step:22981	 l-p:0.5148076415061951
epoch£º1149	 i:2 	 global-step:22982	 l-p:0.15258093178272247
epoch£º1149	 i:3 	 global-step:22983	 l-p:0.13283860683441162
epoch£º1149	 i:4 	 global-step:22984	 l-p:0.035833779722452164
epoch£º1149	 i:5 	 global-step:22985	 l-p:0.12264495342969894
epoch£º1149	 i:6 	 global-step:22986	 l-p:0.1345576047897339
epoch£º1149	 i:7 	 global-step:22987	 l-p:0.15347228944301605
epoch£º1149	 i:8 	 global-step:22988	 l-p:0.10503724962472916
epoch£º1149	 i:9 	 global-step:22989	 l-p:0.2045755684375763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6572, 3.6572, 3.6572],
        [3.6572, 3.6566, 3.6572],
        [3.6572, 3.6568, 3.6572],
        [3.6572, 3.6564, 3.6572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.11465645581483841 
model_pd.l_d.mean(): -23.4511661529541 
model_pd.lagr.mean(): -23.336509704589844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1465], device='cuda:0')), ('power', tensor([-23.5977], device='cuda:0'))])
epoch£º1150	 i:0 	 global-step:23000	 l-p:0.11465645581483841
epoch£º1150	 i:1 	 global-step:23001	 l-p:0.08360861241817474
epoch£º1150	 i:2 	 global-step:23002	 l-p:0.17106249928474426
epoch£º1150	 i:3 	 global-step:23003	 l-p:0.13670481741428375
epoch£º1150	 i:4 	 global-step:23004	 l-p:0.16196635365486145
epoch£º1150	 i:5 	 global-step:23005	 l-p:0.09803422540426254
epoch£º1150	 i:6 	 global-step:23006	 l-p:-0.09551578015089035
epoch£º1150	 i:7 	 global-step:23007	 l-p:0.13811996579170227
epoch£º1150	 i:8 	 global-step:23008	 l-p:0.11004313826560974
epoch£º1150	 i:9 	 global-step:23009	 l-p:0.2951542139053345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6571, 3.6571, 3.6571],
        [3.6571, 3.6571, 3.6571],
        [3.6571, 3.4878, 3.5959],
        [3.6571, 3.0681, 3.0344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.1374148577451706 
model_pd.l_d.mean(): -23.59926986694336 
model_pd.lagr.mean(): -23.461854934692383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1784], device='cuda:0')), ('power', tensor([-23.7776], device='cuda:0'))])
epoch£º1151	 i:0 	 global-step:23020	 l-p:0.1374148577451706
epoch£º1151	 i:1 	 global-step:23021	 l-p:0.1897396743297577
epoch£º1151	 i:2 	 global-step:23022	 l-p:0.08129463344812393
epoch£º1151	 i:3 	 global-step:23023	 l-p:0.0788666233420372
epoch£º1151	 i:4 	 global-step:23024	 l-p:0.12191200256347656
epoch£º1151	 i:5 	 global-step:23025	 l-p:0.13523246347904205
epoch£º1151	 i:6 	 global-step:23026	 l-p:0.2173348069190979
epoch£º1151	 i:7 	 global-step:23027	 l-p:0.08478900790214539
epoch£º1151	 i:8 	 global-step:23028	 l-p:0.12590384483337402
epoch£º1151	 i:9 	 global-step:23029	 l-p:0.48546743392944336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6743, 3.6731, 3.6743],
        [3.6743, 2.9417, 2.6907],
        [3.6743, 3.6743, 3.6743],
        [3.6743, 3.2888, 3.3986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.1338931769132614 
model_pd.l_d.mean(): -23.699142456054688 
model_pd.lagr.mean(): -23.565248489379883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1048], device='cuda:0')), ('power', tensor([-23.8039], device='cuda:0'))])
epoch£º1152	 i:0 	 global-step:23040	 l-p:0.1338931769132614
epoch£º1152	 i:1 	 global-step:23041	 l-p:0.35494741797447205
epoch£º1152	 i:2 	 global-step:23042	 l-p:-0.02445775642991066
epoch£º1152	 i:3 	 global-step:23043	 l-p:0.07201584428548813
epoch£º1152	 i:4 	 global-step:23044	 l-p:0.13157588243484497
epoch£º1152	 i:5 	 global-step:23045	 l-p:0.21068379282951355
epoch£º1152	 i:6 	 global-step:23046	 l-p:0.11359695345163345
epoch£º1152	 i:7 	 global-step:23047	 l-p:0.13812513649463654
epoch£º1152	 i:8 	 global-step:23048	 l-p:0.14369456470012665
epoch£º1152	 i:9 	 global-step:23049	 l-p:0.1560351848602295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6866, 3.6847, 3.6866],
        [3.6866, 3.5888, 3.6632],
        [3.6866, 3.4497, 3.5744],
        [3.6866, 3.6609, 3.6842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.10821589827537537 
model_pd.l_d.mean(): -23.466136932373047 
model_pd.lagr.mean(): -23.357921600341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2198], device='cuda:0')), ('power', tensor([-23.6860], device='cuda:0'))])
epoch£º1153	 i:0 	 global-step:23060	 l-p:0.10821589827537537
epoch£º1153	 i:1 	 global-step:23061	 l-p:0.14008460938930511
epoch£º1153	 i:2 	 global-step:23062	 l-p:0.10842181742191315
epoch£º1153	 i:3 	 global-step:23063	 l-p:0.018857937306165695
epoch£º1153	 i:4 	 global-step:23064	 l-p:0.11972714960575104
epoch£º1153	 i:5 	 global-step:23065	 l-p:0.1549375057220459
epoch£º1153	 i:6 	 global-step:23066	 l-p:0.12257195264101028
epoch£º1153	 i:7 	 global-step:23067	 l-p:0.19499240815639496
epoch£º1153	 i:8 	 global-step:23068	 l-p:0.13776478171348572
epoch£º1153	 i:9 	 global-step:23069	 l-p:0.1479164958000183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6763, 3.6032, 3.6622],
        [3.6763, 3.4289, 3.5549],
        [3.6763, 3.6763, 3.6763],
        [3.6763, 3.6143, 3.6656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.13414259254932404 
model_pd.l_d.mean(): -23.602882385253906 
model_pd.lagr.mean(): -23.468740463256836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1954], device='cuda:0')), ('power', tensor([-23.7983], device='cuda:0'))])
epoch£º1154	 i:0 	 global-step:23080	 l-p:0.13414259254932404
epoch£º1154	 i:1 	 global-step:23081	 l-p:0.40783077478408813
epoch£º1154	 i:2 	 global-step:23082	 l-p:0.12190103530883789
epoch£º1154	 i:3 	 global-step:23083	 l-p:0.2250126153230667
epoch£º1154	 i:4 	 global-step:23084	 l-p:0.08222663402557373
epoch£º1154	 i:5 	 global-step:23085	 l-p:0.08748552203178406
epoch£º1154	 i:6 	 global-step:23086	 l-p:0.07957883924245834
epoch£º1154	 i:7 	 global-step:23087	 l-p:0.13694724440574646
epoch£º1154	 i:8 	 global-step:23088	 l-p:0.11458513140678406
epoch£º1154	 i:9 	 global-step:23089	 l-p:0.13097989559173584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6748, 3.6662, 3.6743],
        [3.6748, 3.6748, 3.6748],
        [3.6748, 3.6748, 3.6748],
        [3.6748, 3.1590, 2.5337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.36865484714508057 
model_pd.l_d.mean(): -22.773094177246094 
model_pd.lagr.mean(): -22.40443992614746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2974], device='cuda:0')), ('power', tensor([-23.0705], device='cuda:0'))])
epoch£º1155	 i:0 	 global-step:23100	 l-p:0.36865484714508057
epoch£º1155	 i:1 	 global-step:23101	 l-p:0.12491580843925476
epoch£º1155	 i:2 	 global-step:23102	 l-p:0.11411704123020172
epoch£º1155	 i:3 	 global-step:23103	 l-p:0.16372451186180115
epoch£º1155	 i:4 	 global-step:23104	 l-p:0.13301865756511688
epoch£º1155	 i:5 	 global-step:23105	 l-p:0.13936112821102142
epoch£º1155	 i:6 	 global-step:23106	 l-p:0.23157453536987305
epoch£º1155	 i:7 	 global-step:23107	 l-p:0.07206252217292786
epoch£º1155	 i:8 	 global-step:23108	 l-p:0.05881461501121521
epoch£º1155	 i:9 	 global-step:23109	 l-p:0.1281299889087677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6689, 3.5854, 3.6511],
        [3.6689, 3.6689, 3.6689],
        [3.6689, 3.5957, 3.6547],
        [3.6689, 2.8589, 2.4066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.1446039080619812 
model_pd.l_d.mean(): -23.80420684814453 
model_pd.lagr.mean(): -23.659603118896484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1260], device='cuda:0')), ('power', tensor([-23.9302], device='cuda:0'))])
epoch£º1156	 i:0 	 global-step:23120	 l-p:0.1446039080619812
epoch£º1156	 i:1 	 global-step:23121	 l-p:0.12809167802333832
epoch£º1156	 i:2 	 global-step:23122	 l-p:0.09987539052963257
epoch£º1156	 i:3 	 global-step:23123	 l-p:0.15380176901817322
epoch£º1156	 i:4 	 global-step:23124	 l-p:0.12761300802230835
epoch£º1156	 i:5 	 global-step:23125	 l-p:0.14037397503852844
epoch£º1156	 i:6 	 global-step:23126	 l-p:0.3393613398075104
epoch£º1156	 i:7 	 global-step:23127	 l-p:0.08185987174510956
epoch£º1156	 i:8 	 global-step:23128	 l-p:0.04545246437191963
epoch£º1156	 i:9 	 global-step:23129	 l-p:0.07199197262525558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6488, 3.6488, 3.6488],
        [3.6488, 3.0911, 3.0911],
        [3.6488, 3.6434, 3.6486],
        [3.6488, 3.4567, 3.5722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.21044200658798218 
model_pd.l_d.mean(): -23.34178352355957 
model_pd.lagr.mean(): -23.1313419342041 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2700], device='cuda:0')), ('power', tensor([-23.6117], device='cuda:0'))])
epoch£º1157	 i:0 	 global-step:23140	 l-p:0.21044200658798218
epoch£º1157	 i:1 	 global-step:23141	 l-p:0.15298070013523102
epoch£º1157	 i:2 	 global-step:23142	 l-p:0.07302749156951904
epoch£º1157	 i:3 	 global-step:23143	 l-p:0.1556924283504486
epoch£º1157	 i:4 	 global-step:23144	 l-p:0.1108488067984581
epoch£º1157	 i:5 	 global-step:23145	 l-p:0.12867450714111328
epoch£º1157	 i:6 	 global-step:23146	 l-p:0.15152454376220703
epoch£º1157	 i:7 	 global-step:23147	 l-p:0.29610416293144226
epoch£º1157	 i:8 	 global-step:23148	 l-p:0.12566238641738892
epoch£º1157	 i:9 	 global-step:23149	 l-p:-0.1277509182691574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6551, 3.4315, 3.5544],
        [3.6551, 3.6550, 3.6551],
        [3.6551, 3.6551, 3.6552],
        [3.6551, 3.0566, 3.0130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.13813742995262146 
model_pd.l_d.mean(): -23.207651138305664 
model_pd.lagr.mean(): -23.06951332092285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2557], device='cuda:0')), ('power', tensor([-23.4633], device='cuda:0'))])
epoch£º1158	 i:0 	 global-step:23160	 l-p:0.13813742995262146
epoch£º1158	 i:1 	 global-step:23161	 l-p:0.12412124127149582
epoch£º1158	 i:2 	 global-step:23162	 l-p:0.13032491505146027
epoch£º1158	 i:3 	 global-step:23163	 l-p:0.3943006098270416
epoch£º1158	 i:4 	 global-step:23164	 l-p:0.11864328384399414
epoch£º1158	 i:5 	 global-step:23165	 l-p:0.1002492904663086
epoch£º1158	 i:6 	 global-step:23166	 l-p:0.10208921134471893
epoch£º1158	 i:7 	 global-step:23167	 l-p:0.1499306857585907
epoch£º1158	 i:8 	 global-step:23168	 l-p:0.08375375717878342
epoch£º1158	 i:9 	 global-step:23169	 l-p:-0.03494905307888985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6530, 2.8848, 2.5625],
        [3.6530, 3.6528, 3.6530],
        [3.6530, 3.6523, 3.6530],
        [3.6530, 3.5470, 3.6262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.14506292343139648 
model_pd.l_d.mean(): -23.533546447753906 
model_pd.lagr.mean(): -23.38848304748535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2070], device='cuda:0')), ('power', tensor([-23.7405], device='cuda:0'))])
epoch£º1159	 i:0 	 global-step:23180	 l-p:0.14506292343139648
epoch£º1159	 i:1 	 global-step:23181	 l-p:0.13802634179592133
epoch£º1159	 i:2 	 global-step:23182	 l-p:0.2075316160917282
epoch£º1159	 i:3 	 global-step:23183	 l-p:-0.10643037408590317
epoch£º1159	 i:4 	 global-step:23184	 l-p:0.09042887389659882
epoch£º1159	 i:5 	 global-step:23185	 l-p:0.10845605283975601
epoch£º1159	 i:6 	 global-step:23186	 l-p:0.2809014916419983
epoch£º1159	 i:7 	 global-step:23187	 l-p:0.14266012609004974
epoch£º1159	 i:8 	 global-step:23188	 l-p:0.11834094673395157
epoch£º1159	 i:9 	 global-step:23189	 l-p:0.10268644243478775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6599, 2.8461, 2.3849],
        [3.6599, 3.6499, 3.6594],
        [3.6599, 2.8315, 2.2819],
        [3.6599, 3.6597, 3.6599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.12990936636924744 
model_pd.l_d.mean(): -23.256786346435547 
model_pd.lagr.mean(): -23.126876831054688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2076], device='cuda:0')), ('power', tensor([-23.4644], device='cuda:0'))])
epoch£º1160	 i:0 	 global-step:23200	 l-p:0.12990936636924744
epoch£º1160	 i:1 	 global-step:23201	 l-p:0.2134711742401123
epoch£º1160	 i:2 	 global-step:23202	 l-p:0.14616306126117706
epoch£º1160	 i:3 	 global-step:23203	 l-p:0.06923680007457733
epoch£º1160	 i:4 	 global-step:23204	 l-p:0.1422780156135559
epoch£º1160	 i:5 	 global-step:23205	 l-p:0.1975167840719223
epoch£º1160	 i:6 	 global-step:23206	 l-p:0.12339412420988083
epoch£º1160	 i:7 	 global-step:23207	 l-p:0.07546467334032059
epoch£º1160	 i:8 	 global-step:23208	 l-p:0.15962187945842743
epoch£º1160	 i:9 	 global-step:23209	 l-p:-0.36297744512557983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6610, 2.8367, 2.2327],
        [3.6610, 3.6610, 3.6610],
        [3.6610, 3.4683, 3.5840],
        [3.6610, 3.2037, 3.2809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): -0.318535178899765 
model_pd.l_d.mean(): -22.394241333007812 
model_pd.lagr.mean(): -22.71277618408203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4318], device='cuda:0')), ('power', tensor([-22.8261], device='cuda:0'))])
epoch£º1161	 i:0 	 global-step:23220	 l-p:-0.318535178899765
epoch£º1161	 i:1 	 global-step:23221	 l-p:0.13075074553489685
epoch£º1161	 i:2 	 global-step:23222	 l-p:0.18681102991104126
epoch£º1161	 i:3 	 global-step:23223	 l-p:0.08049193024635315
epoch£º1161	 i:4 	 global-step:23224	 l-p:0.06559864431619644
epoch£º1161	 i:5 	 global-step:23225	 l-p:0.1780112385749817
epoch£º1161	 i:6 	 global-step:23226	 l-p:0.15024514496326447
epoch£º1161	 i:7 	 global-step:23227	 l-p:0.13615208864212036
epoch£º1161	 i:8 	 global-step:23228	 l-p:0.06288233399391174
epoch£º1161	 i:9 	 global-step:23229	 l-p:0.1255003958940506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228]], device='cuda:0')
 pt:tensor([[3.6699, 2.9038, 2.2548],
        [3.6699, 3.3976, 3.5256],
        [3.6699, 3.1219, 3.1297],
        [3.6699, 2.8514, 2.3640]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.1577676236629486 
model_pd.l_d.mean(): -23.366878509521484 
model_pd.lagr.mean(): -23.209110260009766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3091], device='cuda:0')), ('power', tensor([-23.6760], device='cuda:0'))])
epoch£º1162	 i:0 	 global-step:23240	 l-p:0.1577676236629486
epoch£º1162	 i:1 	 global-step:23241	 l-p:0.20356781780719757
epoch£º1162	 i:2 	 global-step:23242	 l-p:0.12215554714202881
epoch£º1162	 i:3 	 global-step:23243	 l-p:0.07782410085201263
epoch£º1162	 i:4 	 global-step:23244	 l-p:0.1222975105047226
epoch£º1162	 i:5 	 global-step:23245	 l-p:0.1195337101817131
epoch£º1162	 i:6 	 global-step:23246	 l-p:0.16794435679912567
epoch£º1162	 i:7 	 global-step:23247	 l-p:0.08394167572259903
epoch£º1162	 i:8 	 global-step:23248	 l-p:-21.138431549072266
epoch£º1162	 i:9 	 global-step:23249	 l-p:0.13082903623580933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6664, 2.8585, 2.2272],
        [3.6664, 3.6664, 3.6664],
        [3.6664, 3.6664, 3.6664],
        [3.6664, 3.1596, 2.5370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.1831405907869339 
model_pd.l_d.mean(): -23.099714279174805 
model_pd.lagr.mean(): -22.916574478149414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2475], device='cuda:0')), ('power', tensor([-23.3472], device='cuda:0'))])
epoch£º1163	 i:0 	 global-step:23260	 l-p:0.1831405907869339
epoch£º1163	 i:1 	 global-step:23261	 l-p:0.06703558564186096
epoch£º1163	 i:2 	 global-step:23262	 l-p:0.1345505565404892
epoch£º1163	 i:3 	 global-step:23263	 l-p:0.1115834191441536
epoch£º1163	 i:4 	 global-step:23264	 l-p:0.14802882075309753
epoch£º1163	 i:5 	 global-step:23265	 l-p:0.14266715943813324
epoch£º1163	 i:6 	 global-step:23266	 l-p:0.5003209114074707
epoch£º1163	 i:7 	 global-step:23267	 l-p:0.10141951590776443
epoch£º1163	 i:8 	 global-step:23268	 l-p:0.14862795174121857
epoch£º1163	 i:9 	 global-step:23269	 l-p:0.14380450546741486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1270,  0.0638,  1.0000,  0.0321,
          1.0000,  0.5026, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1497,  0.0795,  1.0000,  0.0422,
          1.0000,  0.5310, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6301, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[3.6731, 3.4135, 3.5408],
        [3.6731, 3.3481, 3.4730],
        [3.6731, 3.0928, 3.0681],
        [3.6731, 3.2039, 2.5875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.13041594624519348 
model_pd.l_d.mean(): -23.210311889648438 
model_pd.lagr.mean(): -23.07989501953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2507], device='cuda:0')), ('power', tensor([-23.4611], device='cuda:0'))])
epoch£º1164	 i:0 	 global-step:23280	 l-p:0.13041594624519348
epoch£º1164	 i:1 	 global-step:23281	 l-p:0.06329253315925598
epoch£º1164	 i:2 	 global-step:23282	 l-p:0.11466971784830093
epoch£º1164	 i:3 	 global-step:23283	 l-p:0.23249799013137817
epoch£º1164	 i:4 	 global-step:23284	 l-p:0.1307620108127594
epoch£º1164	 i:5 	 global-step:23285	 l-p:0.14880309998989105
epoch£º1164	 i:6 	 global-step:23286	 l-p:0.13223108649253845
epoch£º1164	 i:7 	 global-step:23287	 l-p:0.5819699168205261
epoch£º1164	 i:8 	 global-step:23288	 l-p:0.14218585193157196
epoch£º1164	 i:9 	 global-step:23289	 l-p:0.06361394375562668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6693, 2.8441, 2.3173],
        [3.6693, 2.8688, 2.2316],
        [3.6693, 3.2124, 3.2894],
        [3.6693, 3.6694, 3.6693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.11682950705289841 
model_pd.l_d.mean(): -23.171892166137695 
model_pd.lagr.mean(): -23.055063247680664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2894], device='cuda:0')), ('power', tensor([-23.4613], device='cuda:0'))])
epoch£º1165	 i:0 	 global-step:23300	 l-p:0.11682950705289841
epoch£º1165	 i:1 	 global-step:23301	 l-p:1.1601208448410034
epoch£º1165	 i:2 	 global-step:23302	 l-p:0.1398020088672638
epoch£º1165	 i:3 	 global-step:23303	 l-p:0.16248248517513275
epoch£º1165	 i:4 	 global-step:23304	 l-p:0.04947267472743988
epoch£º1165	 i:5 	 global-step:23305	 l-p:0.06949436664581299
epoch£º1165	 i:6 	 global-step:23306	 l-p:0.19626224040985107
epoch£º1165	 i:7 	 global-step:23307	 l-p:0.17334607243537903
epoch£º1165	 i:8 	 global-step:23308	 l-p:0.15199512243270874
epoch£º1165	 i:9 	 global-step:23309	 l-p:0.04483635723590851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6746, 3.6746, 3.6746],
        [3.6746, 3.3987, 3.5267],
        [3.6746, 3.6486, 3.6721],
        [3.6746, 3.4150, 3.5423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.19060198962688446 
model_pd.l_d.mean(): -23.48895263671875 
model_pd.lagr.mean(): -23.298351287841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2010], device='cuda:0')), ('power', tensor([-23.6899], device='cuda:0'))])
epoch£º1166	 i:0 	 global-step:23320	 l-p:0.19060198962688446
epoch£º1166	 i:1 	 global-step:23321	 l-p:0.1392401158809662
epoch£º1166	 i:2 	 global-step:23322	 l-p:0.13348902761936188
epoch£º1166	 i:3 	 global-step:23323	 l-p:0.039548251777887344
epoch£º1166	 i:4 	 global-step:23324	 l-p:0.07783971726894379
epoch£º1166	 i:5 	 global-step:23325	 l-p:0.05080384761095047
epoch£º1166	 i:6 	 global-step:23326	 l-p:0.19097812473773956
epoch£º1166	 i:7 	 global-step:23327	 l-p:0.26742231845855713
epoch£º1166	 i:8 	 global-step:23328	 l-p:0.1384958028793335
epoch£º1166	 i:9 	 global-step:23329	 l-p:0.12782436609268188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6904, 3.4902, 3.6077],
        [3.6904, 3.6499, 3.6852],
        [3.6904, 3.1111, 3.0861],
        [3.6904, 3.2304, 3.3047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.12559489905834198 
model_pd.l_d.mean(): -23.509136199951172 
model_pd.lagr.mean(): -23.383541107177734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2767], device='cuda:0')), ('power', tensor([-23.7859], device='cuda:0'))])
epoch£º1167	 i:0 	 global-step:23340	 l-p:0.12559489905834198
epoch£º1167	 i:1 	 global-step:23341	 l-p:0.2572903335094452
epoch£º1167	 i:2 	 global-step:23342	 l-p:0.13477776944637299
epoch£º1167	 i:3 	 global-step:23343	 l-p:0.14728721976280212
epoch£º1167	 i:4 	 global-step:23344	 l-p:0.0686044692993164
epoch£º1167	 i:5 	 global-step:23345	 l-p:0.12442600727081299
epoch£º1167	 i:6 	 global-step:23346	 l-p:0.12680809199810028
epoch£º1167	 i:7 	 global-step:23347	 l-p:0.1259738951921463
epoch£º1167	 i:8 	 global-step:23348	 l-p:0.013103919103741646
epoch£º1167	 i:9 	 global-step:23349	 l-p:0.07660990208387375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6852, 3.6770, 3.6848],
        [3.6852, 2.9688, 2.3155],
        [3.6852, 3.6743, 3.6846],
        [3.6852, 3.2686, 2.6613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.13630612194538116 
model_pd.l_d.mean(): -21.65247344970703 
model_pd.lagr.mean(): -21.51616668701172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4418], device='cuda:0')), ('power', tensor([-22.0943], device='cuda:0'))])
epoch£º1168	 i:0 	 global-step:23360	 l-p:0.13630612194538116
epoch£º1168	 i:1 	 global-step:23361	 l-p:0.1144508644938469
epoch£º1168	 i:2 	 global-step:23362	 l-p:0.13067561388015747
epoch£º1168	 i:3 	 global-step:23363	 l-p:0.17057816684246063
epoch£º1168	 i:4 	 global-step:23364	 l-p:0.05150208994746208
epoch£º1168	 i:5 	 global-step:23365	 l-p:0.13303819298744202
epoch£º1168	 i:6 	 global-step:23366	 l-p:0.005810849368572235
epoch£º1168	 i:7 	 global-step:23367	 l-p:0.18644538521766663
epoch£º1168	 i:8 	 global-step:23368	 l-p:0.12818005681037903
epoch£º1168	 i:9 	 global-step:23369	 l-p:0.3004600405693054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6804, 2.8736, 2.2412],
        [3.6804, 3.6763, 3.6803],
        [3.6804, 2.9137, 2.2634],
        [3.6804, 3.4801, 3.5977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.13357728719711304 
model_pd.l_d.mean(): -23.562564849853516 
model_pd.lagr.mean(): -23.428987503051758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1600], device='cuda:0')), ('power', tensor([-23.7226], device='cuda:0'))])
epoch£º1169	 i:0 	 global-step:23380	 l-p:0.13357728719711304
epoch£º1169	 i:1 	 global-step:23381	 l-p:0.2037179321050644
epoch£º1169	 i:2 	 global-step:23382	 l-p:0.05144398659467697
epoch£º1169	 i:3 	 global-step:23383	 l-p:0.05101744458079338
epoch£º1169	 i:4 	 global-step:23384	 l-p:0.1326468288898468
epoch£º1169	 i:5 	 global-step:23385	 l-p:0.04249304533004761
epoch£º1169	 i:6 	 global-step:23386	 l-p:0.29646530747413635
epoch£º1169	 i:7 	 global-step:23387	 l-p:0.1384819895029068
epoch£º1169	 i:8 	 global-step:23388	 l-p:0.13113795220851898
epoch£º1169	 i:9 	 global-step:23389	 l-p:0.11760413646697998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6895, 3.6895, 3.6895],
        [3.6895, 3.0861, 3.0343],
        [3.6895, 3.1859, 3.2305],
        [3.6895, 2.8666, 2.3393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): -0.005490932147949934 
model_pd.l_d.mean(): -23.232820510864258 
model_pd.lagr.mean(): -23.238311767578125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2712], device='cuda:0')), ('power', tensor([-23.5040], device='cuda:0'))])
epoch£º1170	 i:0 	 global-step:23400	 l-p:-0.005490932147949934
epoch£º1170	 i:1 	 global-step:23401	 l-p:0.10150796920061111
epoch£º1170	 i:2 	 global-step:23402	 l-p:0.12054181843996048
epoch£º1170	 i:3 	 global-step:23403	 l-p:0.29651662707328796
epoch£º1170	 i:4 	 global-step:23404	 l-p:0.12129077315330505
epoch£º1170	 i:5 	 global-step:23405	 l-p:0.1446034163236618
epoch£º1170	 i:6 	 global-step:23406	 l-p:0.058086756616830826
epoch£º1170	 i:7 	 global-step:23407	 l-p:0.14814580976963043
epoch£º1170	 i:8 	 global-step:23408	 l-p:0.14104074239730835
epoch£º1170	 i:9 	 global-step:23409	 l-p:0.11008308082818985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6786, 3.6774, 3.6786],
        [3.6786, 2.8840, 2.2426],
        [3.6786, 2.8791, 2.2406],
        [3.6786, 3.4351, 3.5609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.1315355896949768 
model_pd.l_d.mean(): -23.442180633544922 
model_pd.lagr.mean(): -23.310644149780273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2012], device='cuda:0')), ('power', tensor([-23.6434], device='cuda:0'))])
epoch£º1171	 i:0 	 global-step:23420	 l-p:0.1315355896949768
epoch£º1171	 i:1 	 global-step:23421	 l-p:0.06969977915287018
epoch£º1171	 i:2 	 global-step:23422	 l-p:0.10801778733730316
epoch£º1171	 i:3 	 global-step:23423	 l-p:0.1226193904876709
epoch£º1171	 i:4 	 global-step:23424	 l-p:0.15027311444282532
epoch£º1171	 i:5 	 global-step:23425	 l-p:0.15221577882766724
epoch£º1171	 i:6 	 global-step:23426	 l-p:0.14331288635730743
epoch£º1171	 i:7 	 global-step:23427	 l-p:0.1867145448923111
epoch£º1171	 i:8 	 global-step:23428	 l-p:-0.6842477321624756
epoch£º1171	 i:9 	 global-step:23429	 l-p:0.10000642389059067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6634, 3.6580, 3.6632],
        [3.6634, 3.6578, 3.6632],
        [3.6634, 3.6562, 3.6631],
        [3.6634, 3.3870, 3.5152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.0804896429181099 
model_pd.l_d.mean(): -23.668148040771484 
model_pd.lagr.mean(): -23.587657928466797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1591], device='cuda:0')), ('power', tensor([-23.8273], device='cuda:0'))])
epoch£º1172	 i:0 	 global-step:23440	 l-p:0.0804896429181099
epoch£º1172	 i:1 	 global-step:23441	 l-p:0.14537295699119568
epoch£º1172	 i:2 	 global-step:23442	 l-p:0.12161235511302948
epoch£º1172	 i:3 	 global-step:23443	 l-p:-1.1832026243209839
epoch£º1172	 i:4 	 global-step:23444	 l-p:0.14101047813892365
epoch£º1172	 i:5 	 global-step:23445	 l-p:0.13129200041294098
epoch£º1172	 i:6 	 global-step:23446	 l-p:0.12524375319480896
epoch£º1172	 i:7 	 global-step:23447	 l-p:0.1540682315826416
epoch£º1172	 i:8 	 global-step:23448	 l-p:0.09349477291107178
epoch£º1172	 i:9 	 global-step:23449	 l-p:0.1424032598733902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6612, 2.9125, 2.6344],
        [3.6612, 3.6612, 3.6612],
        [3.6612, 3.0307, 2.9488],
        [3.6612, 3.6612, 3.6612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.20133307576179504 
model_pd.l_d.mean(): -23.26856803894043 
model_pd.lagr.mean(): -23.06723403930664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2254], device='cuda:0')), ('power', tensor([-23.4940], device='cuda:0'))])
epoch£º1173	 i:0 	 global-step:23460	 l-p:0.20133307576179504
epoch£º1173	 i:1 	 global-step:23461	 l-p:-0.5074335336685181
epoch£º1173	 i:2 	 global-step:23462	 l-p:0.10428575426340103
epoch£º1173	 i:3 	 global-step:23463	 l-p:0.12497957050800323
epoch£º1173	 i:4 	 global-step:23464	 l-p:0.16697107255458832
epoch£º1173	 i:5 	 global-step:23465	 l-p:0.10635050386190414
epoch£º1173	 i:6 	 global-step:23466	 l-p:0.12530529499053955
epoch£º1173	 i:7 	 global-step:23467	 l-p:0.12545248866081238
epoch£º1173	 i:8 	 global-step:23468	 l-p:0.1422278881072998
epoch£º1173	 i:9 	 global-step:23469	 l-p:0.14932619035243988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6667, 3.3301, 3.4535],
        [3.6667, 3.4184, 3.5450],
        [3.6667, 2.8723, 2.4787],
        [3.6667, 2.9705, 2.3209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.18987613916397095 
model_pd.l_d.mean(): -23.018600463867188 
model_pd.lagr.mean(): -22.828723907470703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2802], device='cuda:0')), ('power', tensor([-23.2988], device='cuda:0'))])
epoch£º1174	 i:0 	 global-step:23480	 l-p:0.18987613916397095
epoch£º1174	 i:1 	 global-step:23481	 l-p:0.14995308220386505
epoch£º1174	 i:2 	 global-step:23482	 l-p:0.08201099932193756
epoch£º1174	 i:3 	 global-step:23483	 l-p:0.12452059239149094
epoch£º1174	 i:4 	 global-step:23484	 l-p:0.2545643746852875
epoch£º1174	 i:5 	 global-step:23485	 l-p:0.11892326176166534
epoch£º1174	 i:6 	 global-step:23486	 l-p:0.07920975238084793
epoch£º1174	 i:7 	 global-step:23487	 l-p:6.902714729309082
epoch£º1174	 i:8 	 global-step:23488	 l-p:0.12883061170578003
epoch£º1174	 i:9 	 global-step:23489	 l-p:0.07724977284669876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6701, 3.1995, 2.5830],
        [3.6701, 3.3491, 3.4748],
        [3.6701, 2.9650, 2.7696],
        [3.6701, 3.6699, 3.6701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.06763334572315216 
model_pd.l_d.mean(): -22.60380744934082 
model_pd.lagr.mean(): -22.536174774169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3672], device='cuda:0')), ('power', tensor([-22.9710], device='cuda:0'))])
epoch£º1175	 i:0 	 global-step:23500	 l-p:0.06763334572315216
epoch£º1175	 i:1 	 global-step:23501	 l-p:0.15336480736732483
epoch£º1175	 i:2 	 global-step:23502	 l-p:0.5306461453437805
epoch£º1175	 i:3 	 global-step:23503	 l-p:0.11495267599821091
epoch£º1175	 i:4 	 global-step:23504	 l-p:0.13542306423187256
epoch£º1175	 i:5 	 global-step:23505	 l-p:0.10844361782073975
epoch£º1175	 i:6 	 global-step:23506	 l-p:0.19206871092319489
epoch£º1175	 i:7 	 global-step:23507	 l-p:0.14537286758422852
epoch£º1175	 i:8 	 global-step:23508	 l-p:0.15171077847480774
epoch£º1175	 i:9 	 global-step:23509	 l-p:0.04075128585100174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6786, 3.1303, 3.1380],
        [3.6786, 2.9443, 2.6926],
        [3.6786, 3.6786, 3.6786],
        [3.6786, 3.1368, 3.1505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.12343822419643402 
model_pd.l_d.mean(): -23.135608673095703 
model_pd.lagr.mean(): -23.012170791625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1678], device='cuda:0')), ('power', tensor([-23.3034], device='cuda:0'))])
epoch£º1176	 i:0 	 global-step:23520	 l-p:0.12343822419643402
epoch£º1176	 i:1 	 global-step:23521	 l-p:0.19445326924324036
epoch£º1176	 i:2 	 global-step:23522	 l-p:0.09611819684505463
epoch£º1176	 i:3 	 global-step:23523	 l-p:0.3177604079246521
epoch£º1176	 i:4 	 global-step:23524	 l-p:0.005641424562782049
epoch£º1176	 i:5 	 global-step:23525	 l-p:0.04258480295538902
epoch£º1176	 i:6 	 global-step:23526	 l-p:0.15012070536613464
epoch£º1176	 i:7 	 global-step:23527	 l-p:0.13149233162403107
epoch£º1176	 i:8 	 global-step:23528	 l-p:0.17463281750679016
epoch£º1176	 i:9 	 global-step:23529	 l-p:0.1418457180261612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6927, 3.6393, 3.6844],
        [3.6927, 3.5869, 3.6659],
        [3.6927, 3.6880, 3.6925],
        [3.6927, 3.0156, 2.3636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.12992946803569794 
model_pd.l_d.mean(): -23.41208839416504 
model_pd.lagr.mean(): -23.28215980529785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2165], device='cuda:0')), ('power', tensor([-23.6286], device='cuda:0'))])
epoch£º1177	 i:0 	 global-step:23540	 l-p:0.12992946803569794
epoch£º1177	 i:1 	 global-step:23541	 l-p:0.12060993909835815
epoch£º1177	 i:2 	 global-step:23542	 l-p:0.1286446452140808
epoch£º1177	 i:3 	 global-step:23543	 l-p:0.13054877519607544
epoch£º1177	 i:4 	 global-step:23544	 l-p:0.1766360104084015
epoch£º1177	 i:5 	 global-step:23545	 l-p:0.11760403960943222
epoch£º1177	 i:6 	 global-step:23546	 l-p:0.16255784034729004
epoch£º1177	 i:7 	 global-step:23547	 l-p:-0.02250383235514164
epoch£º1177	 i:8 	 global-step:23548	 l-p:0.013297238387167454
epoch£º1177	 i:9 	 global-step:23549	 l-p:0.1347651481628418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6979, 3.6979, 3.6979],
        [3.6979, 3.6484, 3.6906],
        [3.6979, 3.0079, 2.8338],
        [3.6979, 3.6979, 3.6979]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.1478985995054245 
model_pd.l_d.mean(): -23.7426815032959 
model_pd.lagr.mean(): -23.594783782958984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1244], device='cuda:0')), ('power', tensor([-23.8671], device='cuda:0'))])
epoch£º1178	 i:0 	 global-step:23560	 l-p:0.1478985995054245
epoch£º1178	 i:1 	 global-step:23561	 l-p:-0.03277693688869476
epoch£º1178	 i:2 	 global-step:23562	 l-p:0.16345958411693573
epoch£º1178	 i:3 	 global-step:23563	 l-p:0.13409677147865295
epoch£º1178	 i:4 	 global-step:23564	 l-p:0.2001769244670868
epoch£º1178	 i:5 	 global-step:23565	 l-p:-0.3081933856010437
epoch£º1178	 i:6 	 global-step:23566	 l-p:0.13419154286384583
epoch£º1178	 i:7 	 global-step:23567	 l-p:0.16199365258216858
epoch£º1178	 i:8 	 global-step:23568	 l-p:-0.02389228716492653
epoch£º1178	 i:9 	 global-step:23569	 l-p:0.13011014461517334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7108, 3.3598, 3.4793],
        [3.7108, 3.6756, 3.7067],
        [3.7108, 2.9492, 2.2947],
        [3.7108, 3.6488, 3.7001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.12668289244174957 
model_pd.l_d.mean(): -23.598873138427734 
model_pd.lagr.mean(): -23.472190856933594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1603], device='cuda:0')), ('power', tensor([-23.7592], device='cuda:0'))])
epoch£º1179	 i:0 	 global-step:23580	 l-p:0.12668289244174957
epoch£º1179	 i:1 	 global-step:23581	 l-p:0.10956299304962158
epoch£º1179	 i:2 	 global-step:23582	 l-p:0.24539481103420258
epoch£º1179	 i:3 	 global-step:23583	 l-p:-0.30674299597740173
epoch£º1179	 i:4 	 global-step:23584	 l-p:0.6973356604576111
epoch£º1179	 i:5 	 global-step:23585	 l-p:0.11868708580732346
epoch£º1179	 i:6 	 global-step:23586	 l-p:0.12199664860963821
epoch£º1179	 i:7 	 global-step:23587	 l-p:0.14840452373027802
epoch£º1179	 i:8 	 global-step:23588	 l-p:0.12543077766895294
epoch£º1179	 i:9 	 global-step:23589	 l-p:-0.14863289892673492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7121, 3.6959, 3.7110],
        [3.7121, 3.6064, 3.6854],
        [3.7121, 3.5741, 3.6694],
        [3.7121, 3.6767, 3.7079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.12765751779079437 
model_pd.l_d.mean(): -22.997411727905273 
model_pd.lagr.mean(): -22.869754791259766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1965], device='cuda:0')), ('power', tensor([-23.1939], device='cuda:0'))])
epoch£º1180	 i:0 	 global-step:23600	 l-p:0.12765751779079437
epoch£º1180	 i:1 	 global-step:23601	 l-p:0.12060663104057312
epoch£º1180	 i:2 	 global-step:23602	 l-p:0.12871846556663513
epoch£º1180	 i:3 	 global-step:23603	 l-p:0.13493244349956512
epoch£º1180	 i:4 	 global-step:23604	 l-p:0.13141751289367676
epoch£º1180	 i:5 	 global-step:23605	 l-p:0.045578353106975555
epoch£º1180	 i:6 	 global-step:23606	 l-p:0.09772656857967377
epoch£º1180	 i:7 	 global-step:23607	 l-p:0.1035437062382698
epoch£º1180	 i:8 	 global-step:23608	 l-p:0.01940964162349701
epoch£º1180	 i:9 	 global-step:23609	 l-p:0.15428228676319122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6894, 3.1723, 2.5443],
        [3.6894, 3.6739, 3.6883],
        [3.6894, 3.6488, 3.6841],
        [3.6894, 3.1359, 2.5016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.11487922072410583 
model_pd.l_d.mean(): -23.186464309692383 
model_pd.lagr.mean(): -23.071584701538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2911], device='cuda:0')), ('power', tensor([-23.4776], device='cuda:0'))])
epoch£º1181	 i:0 	 global-step:23620	 l-p:0.11487922072410583
epoch£º1181	 i:1 	 global-step:23621	 l-p:0.1453552097082138
epoch£º1181	 i:2 	 global-step:23622	 l-p:0.06767094135284424
epoch£º1181	 i:3 	 global-step:23623	 l-p:0.2009381204843521
epoch£º1181	 i:4 	 global-step:23624	 l-p:0.14152474701404572
epoch£º1181	 i:5 	 global-step:23625	 l-p:0.16309839487075806
epoch£º1181	 i:6 	 global-step:23626	 l-p:0.11416684091091156
epoch£º1181	 i:7 	 global-step:23627	 l-p:0.21667835116386414
epoch£º1181	 i:8 	 global-step:23628	 l-p:0.14480461180210114
epoch£º1181	 i:9 	 global-step:23629	 l-p:0.1468668133020401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6736, 2.8747, 2.2348],
        [3.6736, 3.3212, 3.4413],
        [3.6736, 3.2609, 3.3611],
        [3.6736, 3.6736, 3.6736]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.12566742300987244 
model_pd.l_d.mean(): -23.55723762512207 
model_pd.lagr.mean(): -23.431570053100586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1696], device='cuda:0')), ('power', tensor([-23.7268], device='cuda:0'))])
epoch£º1182	 i:0 	 global-step:23640	 l-p:0.12566742300987244
epoch£º1182	 i:1 	 global-step:23641	 l-p:0.13400167226791382
epoch£º1182	 i:2 	 global-step:23642	 l-p:0.1324155181646347
epoch£º1182	 i:3 	 global-step:23643	 l-p:0.1208549216389656
epoch£º1182	 i:4 	 global-step:23644	 l-p:-0.5647411942481995
epoch£º1182	 i:5 	 global-step:23645	 l-p:0.14268681406974792
epoch£º1182	 i:6 	 global-step:23646	 l-p:0.2788940668106079
epoch£º1182	 i:7 	 global-step:23647	 l-p:0.10655468702316284
epoch£º1182	 i:8 	 global-step:23648	 l-p:0.13050401210784912
epoch£º1182	 i:9 	 global-step:23649	 l-p:0.15072907507419586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6557, 3.1008, 2.4712],
        [3.6557, 3.0273, 2.3867],
        [3.6557, 3.6557, 3.6557],
        [3.6557, 2.8591, 2.4652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.06572726368904114 
model_pd.l_d.mean(): -23.35260772705078 
model_pd.lagr.mean(): -23.286880493164062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2787], device='cuda:0')), ('power', tensor([-23.6313], device='cuda:0'))])
epoch£º1183	 i:0 	 global-step:23660	 l-p:0.06572726368904114
epoch£º1183	 i:1 	 global-step:23661	 l-p:0.08826430886983871
epoch£º1183	 i:2 	 global-step:23662	 l-p:0.18023525178432465
epoch£º1183	 i:3 	 global-step:23663	 l-p:0.1419193297624588
epoch£º1183	 i:4 	 global-step:23664	 l-p:0.15261226892471313
epoch£º1183	 i:5 	 global-step:23665	 l-p:0.3317034840583801
epoch£º1183	 i:6 	 global-step:23666	 l-p:0.10919618606567383
epoch£º1183	 i:7 	 global-step:23667	 l-p:0.12919458746910095
epoch£º1183	 i:8 	 global-step:23668	 l-p:-0.11314869672060013
epoch£º1183	 i:9 	 global-step:23669	 l-p:0.13929957151412964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6580, 2.8872, 2.2392],
        [3.6580, 3.6430, 3.6570],
        [3.6580, 2.9518, 2.3027],
        [3.6580, 3.1518, 3.1970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.14888833463191986 
model_pd.l_d.mean(): -23.553356170654297 
model_pd.lagr.mean(): -23.404468536376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1876], device='cuda:0')), ('power', tensor([-23.7410], device='cuda:0'))])
epoch£º1184	 i:0 	 global-step:23680	 l-p:0.14888833463191986
epoch£º1184	 i:1 	 global-step:23681	 l-p:0.14328435063362122
epoch£º1184	 i:2 	 global-step:23682	 l-p:0.1626378744840622
epoch£º1184	 i:3 	 global-step:23683	 l-p:0.08657510578632355
epoch£º1184	 i:4 	 global-step:23684	 l-p:0.09877300262451172
epoch£º1184	 i:5 	 global-step:23685	 l-p:0.13774923980236053
epoch£º1184	 i:6 	 global-step:23686	 l-p:-0.011523256078362465
epoch£º1184	 i:7 	 global-step:23687	 l-p:0.15746593475341797
epoch£º1184	 i:8 	 global-step:23688	 l-p:0.15222591161727905
epoch£º1184	 i:9 	 global-step:23689	 l-p:0.38602572679519653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6512, 2.9586, 2.7886],
        [3.6512, 3.1857, 3.2594],
        [3.6512, 3.1769, 3.2452],
        [3.6512, 3.4126, 3.5383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.14613889157772064 
model_pd.l_d.mean(): -23.511634826660156 
model_pd.lagr.mean(): -23.365495681762695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2150], device='cuda:0')), ('power', tensor([-23.7267], device='cuda:0'))])
epoch£º1185	 i:0 	 global-step:23700	 l-p:0.14613889157772064
epoch£º1185	 i:1 	 global-step:23701	 l-p:0.1230618804693222
epoch£º1185	 i:2 	 global-step:23702	 l-p:0.15065503120422363
epoch£º1185	 i:3 	 global-step:23703	 l-p:0.12135676294565201
epoch£º1185	 i:4 	 global-step:23704	 l-p:0.6750699281692505
epoch£º1185	 i:5 	 global-step:23705	 l-p:0.13082918524742126
epoch£º1185	 i:6 	 global-step:23706	 l-p:0.10722463577985764
epoch£º1185	 i:7 	 global-step:23707	 l-p:0.1026538535952568
epoch£º1185	 i:8 	 global-step:23708	 l-p:0.20523497462272644
epoch£º1185	 i:9 	 global-step:23709	 l-p:-0.008665580302476883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6514, 3.5709, 3.6348],
        [3.6514, 3.5544, 3.6285],
        [3.6514, 3.0598, 3.0265],
        [3.6514, 2.8903, 2.2418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.1942649781703949 
model_pd.l_d.mean(): -23.480491638183594 
model_pd.lagr.mean(): -23.286226272583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2153], device='cuda:0')), ('power', tensor([-23.6958], device='cuda:0'))])
epoch£º1186	 i:0 	 global-step:23720	 l-p:0.1942649781703949
epoch£º1186	 i:1 	 global-step:23721	 l-p:0.12134671956300735
epoch£º1186	 i:2 	 global-step:23722	 l-p:0.30350184440612793
epoch£º1186	 i:3 	 global-step:23723	 l-p:0.14414888620376587
epoch£º1186	 i:4 	 global-step:23724	 l-p:-0.2541467845439911
epoch£º1186	 i:5 	 global-step:23725	 l-p:0.09322793781757355
epoch£º1186	 i:6 	 global-step:23726	 l-p:0.06698185205459595
epoch£º1186	 i:7 	 global-step:23727	 l-p:0.13289794325828552
epoch£º1186	 i:8 	 global-step:23728	 l-p:0.11890971660614014
epoch£º1186	 i:9 	 global-step:23729	 l-p:0.09971259534358978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6758, 3.3771, 3.5050],
        [3.6758, 3.0262, 2.3797],
        [3.6758, 3.5214, 3.6239],
        [3.6758, 3.6732, 3.6757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.4643125832080841 
model_pd.l_d.mean(): -23.484895706176758 
model_pd.lagr.mean(): -23.02058219909668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2136], device='cuda:0')), ('power', tensor([-23.6985], device='cuda:0'))])
epoch£º1187	 i:0 	 global-step:23740	 l-p:0.4643125832080841
epoch£º1187	 i:1 	 global-step:23741	 l-p:0.13219070434570312
epoch£º1187	 i:2 	 global-step:23742	 l-p:0.11501134932041168
epoch£º1187	 i:3 	 global-step:23743	 l-p:0.13807792961597443
epoch£º1187	 i:4 	 global-step:23744	 l-p:0.12098763883113861
epoch£º1187	 i:5 	 global-step:23745	 l-p:0.1347440630197525
epoch£º1187	 i:6 	 global-step:23746	 l-p:-0.01925436221063137
epoch£º1187	 i:7 	 global-step:23747	 l-p:0.06836704909801483
epoch£º1187	 i:8 	 global-step:23748	 l-p:0.24634811282157898
epoch£º1187	 i:9 	 global-step:23749	 l-p:0.14803025126457214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6776, 3.5020, 3.6124],
        [3.6776, 3.6776, 3.6776],
        [3.6776, 3.2424, 2.6318],
        [3.6776, 3.5938, 3.6598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.12633419036865234 
model_pd.l_d.mean(): -23.446340560913086 
model_pd.lagr.mean(): -23.32000732421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1389], device='cuda:0')), ('power', tensor([-23.5853], device='cuda:0'))])
epoch£º1188	 i:0 	 global-step:23760	 l-p:0.12633419036865234
epoch£º1188	 i:1 	 global-step:23761	 l-p:0.1452079862356186
epoch£º1188	 i:2 	 global-step:23762	 l-p:0.11183822154998779
epoch£º1188	 i:3 	 global-step:23763	 l-p:0.2192254215478897
epoch£º1188	 i:4 	 global-step:23764	 l-p:0.060917019844055176
epoch£º1188	 i:5 	 global-step:23765	 l-p:0.8435057401657104
epoch£º1188	 i:6 	 global-step:23766	 l-p:0.11062432080507278
epoch£º1188	 i:7 	 global-step:23767	 l-p:0.1540410965681076
epoch£º1188	 i:8 	 global-step:23768	 l-p:0.07205004245042801
epoch£º1188	 i:9 	 global-step:23769	 l-p:0.1189756914973259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6756, 3.4027, 3.5310],
        [3.6756, 3.2176, 3.2948],
        [3.6756, 3.6756, 3.6756],
        [3.6756, 3.6723, 3.6755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.06817694753408432 
model_pd.l_d.mean(): -23.7706298828125 
model_pd.lagr.mean(): -23.70245361328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0970], device='cuda:0')), ('power', tensor([-23.8676], device='cuda:0'))])
epoch£º1189	 i:0 	 global-step:23780	 l-p:0.06817694753408432
epoch£º1189	 i:1 	 global-step:23781	 l-p:0.4560036063194275
epoch£º1189	 i:2 	 global-step:23782	 l-p:0.14719468355178833
epoch£º1189	 i:3 	 global-step:23783	 l-p:0.022163715213537216
epoch£º1189	 i:4 	 global-step:23784	 l-p:0.13913501799106598
epoch£º1189	 i:5 	 global-step:23785	 l-p:0.12346864491701126
epoch£º1189	 i:6 	 global-step:23786	 l-p:0.15421372652053833
epoch£º1189	 i:7 	 global-step:23787	 l-p:0.18464381992816925
epoch£º1189	 i:8 	 global-step:23788	 l-p:0.11846305429935455
epoch£º1189	 i:9 	 global-step:23789	 l-p:0.09268873929977417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6791, 3.6791, 3.6791],
        [3.6791, 3.6444, 3.6751],
        [3.6791, 3.6791, 3.6791],
        [3.6791, 3.6497, 3.6761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.16626639664173126 
model_pd.l_d.mean(): -22.998584747314453 
model_pd.lagr.mean(): -22.832319259643555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2473], device='cuda:0')), ('power', tensor([-23.2459], device='cuda:0'))])
epoch£º1190	 i:0 	 global-step:23800	 l-p:0.16626639664173126
epoch£º1190	 i:1 	 global-step:23801	 l-p:0.13206976652145386
epoch£º1190	 i:2 	 global-step:23802	 l-p:0.16116756200790405
epoch£º1190	 i:3 	 global-step:23803	 l-p:0.14926967024803162
epoch£º1190	 i:4 	 global-step:23804	 l-p:0.08579394221305847
epoch£º1190	 i:5 	 global-step:23805	 l-p:0.16914403438568115
epoch£º1190	 i:6 	 global-step:23806	 l-p:0.12993325293064117
epoch£º1190	 i:7 	 global-step:23807	 l-p:0.1305253505706787
epoch£º1190	 i:8 	 global-step:23808	 l-p:-0.03256376087665558
epoch£º1190	 i:9 	 global-step:23809	 l-p:0.3940560817718506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6778, 3.1971, 2.5774],
        [3.6778, 3.6777, 3.6778],
        [3.6778, 3.5974, 3.6612],
        [3.6778, 3.2157, 3.2906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.16972100734710693 
model_pd.l_d.mean(): -22.72383689880371 
model_pd.lagr.mean(): -22.554115295410156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3350], device='cuda:0')), ('power', tensor([-23.0589], device='cuda:0'))])
epoch£º1191	 i:0 	 global-step:23820	 l-p:0.16972100734710693
epoch£º1191	 i:1 	 global-step:23821	 l-p:0.07622247189283371
epoch£º1191	 i:2 	 global-step:23822	 l-p:0.04226085543632507
epoch£º1191	 i:3 	 global-step:23823	 l-p:0.14930205047130585
epoch£º1191	 i:4 	 global-step:23824	 l-p:0.15885810554027557
epoch£º1191	 i:5 	 global-step:23825	 l-p:0.03710624575614929
epoch£º1191	 i:6 	 global-step:23826	 l-p:0.14270269870758057
epoch£º1191	 i:7 	 global-step:23827	 l-p:0.23464076220989227
epoch£º1191	 i:8 	 global-step:23828	 l-p:0.1367952674627304
epoch£º1191	 i:9 	 global-step:23829	 l-p:0.1489623785018921
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6946, 3.5476, 3.6470],
        [3.6946, 3.5221, 3.6314],
        [3.6946, 3.3782, 3.5044],
        [3.6946, 3.6855, 3.6942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.1265907883644104 
model_pd.l_d.mean(): -23.009937286376953 
model_pd.lagr.mean(): -22.883346557617188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2634], device='cuda:0')), ('power', tensor([-23.2733], device='cuda:0'))])
epoch£º1192	 i:0 	 global-step:23840	 l-p:0.1265907883644104
epoch£º1192	 i:1 	 global-step:23841	 l-p:0.14420825242996216
epoch£º1192	 i:2 	 global-step:23842	 l-p:0.12996242940425873
epoch£º1192	 i:3 	 global-step:23843	 l-p:0.13702791929244995
epoch£º1192	 i:4 	 global-step:23844	 l-p:0.010954656638205051
epoch£º1192	 i:5 	 global-step:23845	 l-p:0.1514316350221634
epoch£º1192	 i:6 	 global-step:23846	 l-p:0.11624917387962341
epoch£º1192	 i:7 	 global-step:23847	 l-p:0.17141221463680267
epoch£º1192	 i:8 	 global-step:23848	 l-p:0.05999603122472763
epoch£º1192	 i:9 	 global-step:23849	 l-p:0.17529280483722687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6880, 2.8978, 2.5143],
        [3.6880, 3.5845, 3.6623],
        [3.6880, 3.6793, 3.6876],
        [3.6880, 3.6468, 3.6826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.1185208186507225 
model_pd.l_d.mean(): -23.291383743286133 
model_pd.lagr.mean(): -23.172863006591797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2847], device='cuda:0')), ('power', tensor([-23.5761], device='cuda:0'))])
epoch£º1193	 i:0 	 global-step:23860	 l-p:0.1185208186507225
epoch£º1193	 i:1 	 global-step:23861	 l-p:0.09776239097118378
epoch£º1193	 i:2 	 global-step:23862	 l-p:0.16061171889305115
epoch£º1193	 i:3 	 global-step:23863	 l-p:-0.008641538210213184
epoch£º1193	 i:4 	 global-step:23864	 l-p:0.12554311752319336
epoch£º1193	 i:5 	 global-step:23865	 l-p:0.14456969499588013
epoch£º1193	 i:6 	 global-step:23866	 l-p:0.04182777926325798
epoch£º1193	 i:7 	 global-step:23867	 l-p:0.14503605663776398
epoch£º1193	 i:8 	 global-step:23868	 l-p:0.1237793117761612
epoch£º1193	 i:9 	 global-step:23869	 l-p:0.22982840240001678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6960, 2.8713, 2.3424],
        [3.6960, 3.6941, 3.6960],
        [3.6960, 2.8891, 2.2539],
        [3.6960, 2.9304, 2.6095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.11777521669864655 
model_pd.l_d.mean(): -23.491989135742188 
model_pd.lagr.mean(): -23.37421417236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1987], device='cuda:0')), ('power', tensor([-23.6906], device='cuda:0'))])
epoch£º1194	 i:0 	 global-step:23880	 l-p:0.11777521669864655
epoch£º1194	 i:1 	 global-step:23881	 l-p:0.0482604056596756
epoch£º1194	 i:2 	 global-step:23882	 l-p:0.12952378392219543
epoch£º1194	 i:3 	 global-step:23883	 l-p:0.14179159700870514
epoch£º1194	 i:4 	 global-step:23884	 l-p:0.12767855823040009
epoch£º1194	 i:5 	 global-step:23885	 l-p:0.1983751505613327
epoch£º1194	 i:6 	 global-step:23886	 l-p:0.029058750718832016
epoch£º1194	 i:7 	 global-step:23887	 l-p:0.15444254875183105
epoch£º1194	 i:8 	 global-step:23888	 l-p:0.13175618648529053
epoch£º1194	 i:9 	 global-step:23889	 l-p:0.10210037976503372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6921, 3.6828, 3.6917],
        [3.6921, 3.6914, 3.6921],
        [3.6921, 3.4320, 3.5595],
        [3.6921, 3.0936, 3.0494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.18713971972465515 
model_pd.l_d.mean(): -23.48562240600586 
model_pd.lagr.mean(): -23.29848289489746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1944], device='cuda:0')), ('power', tensor([-23.6800], device='cuda:0'))])
epoch£º1195	 i:0 	 global-step:23900	 l-p:0.18713971972465515
epoch£º1195	 i:1 	 global-step:23901	 l-p:-0.05589962750673294
epoch£º1195	 i:2 	 global-step:23902	 l-p:0.10192909091711044
epoch£º1195	 i:3 	 global-step:23903	 l-p:0.13584762811660767
epoch£º1195	 i:4 	 global-step:23904	 l-p:0.12180648744106293
epoch£º1195	 i:5 	 global-step:23905	 l-p:-0.11423509567975998
epoch£º1195	 i:6 	 global-step:23906	 l-p:0.13276420533657074
epoch£º1195	 i:7 	 global-step:23907	 l-p:0.1771213710308075
epoch£º1195	 i:8 	 global-step:23908	 l-p:0.12617477774620056
epoch£º1195	 i:9 	 global-step:23909	 l-p:0.14714482426643372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7045, 3.5503, 3.6527],
        [3.7045, 3.6474, 3.6953],
        [3.7045, 3.1307, 2.4904],
        [3.7045, 3.0295, 2.3754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): -0.5498856902122498 
model_pd.l_d.mean(): -23.341737747192383 
model_pd.lagr.mean(): -23.89162254333496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2269], device='cuda:0')), ('power', tensor([-23.5687], device='cuda:0'))])
epoch£º1196	 i:0 	 global-step:23920	 l-p:-0.5498856902122498
epoch£º1196	 i:1 	 global-step:23921	 l-p:0.13037490844726562
epoch£º1196	 i:2 	 global-step:23922	 l-p:0.15710145235061646
epoch£º1196	 i:3 	 global-step:23923	 l-p:0.1431436985731125
epoch£º1196	 i:4 	 global-step:23924	 l-p:0.18606318533420563
epoch£º1196	 i:5 	 global-step:23925	 l-p:0.17378610372543335
epoch£º1196	 i:6 	 global-step:23926	 l-p:0.13318859040737152
epoch£º1196	 i:7 	 global-step:23927	 l-p:0.1216132789850235
epoch£º1196	 i:8 	 global-step:23928	 l-p:0.12754620611667633
epoch£º1196	 i:9 	 global-step:23929	 l-p:-5.8455465477891266e-05
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7036, 3.0745, 2.9916],
        [3.7036, 3.5464, 3.6500],
        [3.7036, 3.2040, 2.5770],
        [3.7036, 2.8797, 2.2826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.13946489989757538 
model_pd.l_d.mean(): -23.291004180908203 
model_pd.lagr.mean(): -23.151538848876953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2697], device='cuda:0')), ('power', tensor([-23.5607], device='cuda:0'))])
epoch£º1197	 i:0 	 global-step:23940	 l-p:0.13946489989757538
epoch£º1197	 i:1 	 global-step:23941	 l-p:0.17116549611091614
epoch£º1197	 i:2 	 global-step:23942	 l-p:0.056355785578489304
epoch£º1197	 i:3 	 global-step:23943	 l-p:0.12640081346035004
epoch£º1197	 i:4 	 global-step:23944	 l-p:0.1281697303056717
epoch£º1197	 i:5 	 global-step:23945	 l-p:0.12928752601146698
epoch£º1197	 i:6 	 global-step:23946	 l-p:0.13760297000408173
epoch£º1197	 i:7 	 global-step:23947	 l-p:0.12929381430149078
epoch£º1197	 i:8 	 global-step:23948	 l-p:-0.06409964710474014
epoch£º1197	 i:9 	 global-step:23949	 l-p:0.04884712025523186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6955, 2.9543, 2.2986],
        [3.6955, 3.6956, 3.6955],
        [3.6955, 3.6877, 3.6952],
        [3.6955, 3.6651, 3.6923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): -0.1609019637107849 
model_pd.l_d.mean(): -23.180063247680664 
model_pd.lagr.mean(): -23.340965270996094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2364], device='cuda:0')), ('power', tensor([-23.4164], device='cuda:0'))])
epoch£º1198	 i:0 	 global-step:23960	 l-p:-0.1609019637107849
epoch£º1198	 i:1 	 global-step:23961	 l-p:0.18572714924812317
epoch£º1198	 i:2 	 global-step:23962	 l-p:0.05880989879369736
epoch£º1198	 i:3 	 global-step:23963	 l-p:0.13690797984600067
epoch£º1198	 i:4 	 global-step:23964	 l-p:0.12554508447647095
epoch£º1198	 i:5 	 global-step:23965	 l-p:0.11914056539535522
epoch£º1198	 i:6 	 global-step:23966	 l-p:0.1228618323802948
epoch£º1198	 i:7 	 global-step:23967	 l-p:0.10555240511894226
epoch£º1198	 i:8 	 global-step:23968	 l-p:0.15638919174671173
epoch£º1198	 i:9 	 global-step:23969	 l-p:0.23680315911769867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6944, 3.6918, 3.6943],
        [3.6944, 3.6944, 3.6944],
        [3.6944, 3.6928, 3.6944],
        [3.6944, 3.6944, 3.6944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.09846735000610352 
model_pd.l_d.mean(): -22.73891258239746 
model_pd.lagr.mean(): -22.640445709228516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3069], device='cuda:0')), ('power', tensor([-23.0458], device='cuda:0'))])
epoch£º1199	 i:0 	 global-step:23980	 l-p:0.09846735000610352
epoch£º1199	 i:1 	 global-step:23981	 l-p:0.05737939849495888
epoch£º1199	 i:2 	 global-step:23982	 l-p:0.15844540297985077
epoch£º1199	 i:3 	 global-step:23983	 l-p:0.14039713144302368
epoch£º1199	 i:4 	 global-step:23984	 l-p:0.12922607362270355
epoch£º1199	 i:5 	 global-step:23985	 l-p:0.13943666219711304
epoch£º1199	 i:6 	 global-step:23986	 l-p:0.18220175802707672
epoch£º1199	 i:7 	 global-step:23987	 l-p:0.20615676045417786
epoch£º1199	 i:8 	 global-step:23988	 l-p:0.13338598608970642
epoch£º1199	 i:9 	 global-step:23989	 l-p:0.01974361762404442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6855, 3.5354, 3.6362],
        [3.6855, 3.6854, 3.6855],
        [3.6855, 3.2087, 3.2743],
        [3.6855, 3.1953, 3.2517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.14638204872608185 
model_pd.l_d.mean(): -23.357254028320312 
model_pd.lagr.mean(): -23.210872650146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2160], device='cuda:0')), ('power', tensor([-23.5733], device='cuda:0'))])
epoch£º1200	 i:0 	 global-step:24000	 l-p:0.14638204872608185
epoch£º1200	 i:1 	 global-step:24001	 l-p:0.1151159480214119
epoch£º1200	 i:2 	 global-step:24002	 l-p:0.18371562659740448
epoch£º1200	 i:3 	 global-step:24003	 l-p:0.05035540834069252
epoch£º1200	 i:4 	 global-step:24004	 l-p:0.13370200991630554
epoch£º1200	 i:5 	 global-step:24005	 l-p:0.1280907541513443
epoch£º1200	 i:6 	 global-step:24006	 l-p:0.15226155519485474
epoch£º1200	 i:7 	 global-step:24007	 l-p:0.16645850241184235
epoch£º1200	 i:8 	 global-step:24008	 l-p:0.10143382847309113
epoch£º1200	 i:9 	 global-step:24009	 l-p:0.13424824178218842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6832, 3.6833, 3.6832],
        [3.6832, 3.6388, 3.6772],
        [3.6832, 2.8918, 2.5081],
        [3.6832, 2.9107, 2.5776]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.1362125426530838 
model_pd.l_d.mean(): -22.6249942779541 
model_pd.lagr.mean(): -22.488780975341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3077], device='cuda:0')), ('power', tensor([-22.9327], device='cuda:0'))])
epoch£º1201	 i:0 	 global-step:24020	 l-p:0.1362125426530838
epoch£º1201	 i:1 	 global-step:24021	 l-p:0.20924125611782074
epoch£º1201	 i:2 	 global-step:24022	 l-p:0.13834360241889954
epoch£º1201	 i:3 	 global-step:24023	 l-p:0.10432912409305573
epoch£º1201	 i:4 	 global-step:24024	 l-p:0.07557649165391922
epoch£º1201	 i:5 	 global-step:24025	 l-p:0.05590086430311203
epoch£º1201	 i:6 	 global-step:24026	 l-p:0.12097137421369553
epoch£º1201	 i:7 	 global-step:24027	 l-p:0.4686984419822693
epoch£º1201	 i:8 	 global-step:24028	 l-p:0.19267520308494568
epoch£º1201	 i:9 	 global-step:24029	 l-p:0.06183701753616333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6768, 3.5191, 3.6230],
        [3.6768, 3.0302, 2.9278],
        [3.6768, 3.6768, 3.6768],
        [3.6768, 3.1397, 2.5097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.14141425490379333 
model_pd.l_d.mean(): -23.111862182617188 
model_pd.lagr.mean(): -22.970447540283203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2851], device='cuda:0')), ('power', tensor([-23.3970], device='cuda:0'))])
epoch£º1202	 i:0 	 global-step:24040	 l-p:0.14141425490379333
epoch£º1202	 i:1 	 global-step:24041	 l-p:0.1286323368549347
epoch£º1202	 i:2 	 global-step:24042	 l-p:0.14271870255470276
epoch£º1202	 i:3 	 global-step:24043	 l-p:0.13410964608192444
epoch£º1202	 i:4 	 global-step:24044	 l-p:0.4026559293270111
epoch£º1202	 i:5 	 global-step:24045	 l-p:0.10121690481901169
epoch£º1202	 i:6 	 global-step:24046	 l-p:0.12440498918294907
epoch£º1202	 i:7 	 global-step:24047	 l-p:0.16863271594047546
epoch£º1202	 i:8 	 global-step:24048	 l-p:0.1275712251663208
epoch£º1202	 i:9 	 global-step:24049	 l-p:0.07362952083349228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6782, 3.6348, 3.6724],
        [3.6782, 3.6776, 3.6782],
        [3.6782, 2.8579, 2.2367],
        [3.6782, 2.8501, 2.3213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.07952612638473511 
model_pd.l_d.mean(): -23.47521209716797 
model_pd.lagr.mean(): -23.39568519592285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2672], device='cuda:0')), ('power', tensor([-23.7424], device='cuda:0'))])
epoch£º1203	 i:0 	 global-step:24060	 l-p:0.07952612638473511
epoch£º1203	 i:1 	 global-step:24061	 l-p:0.1803232580423355
epoch£º1203	 i:2 	 global-step:24062	 l-p:0.0618203729391098
epoch£º1203	 i:3 	 global-step:24063	 l-p:0.13001245260238647
epoch£º1203	 i:4 	 global-step:24064	 l-p:0.12144910544157028
epoch£º1203	 i:5 	 global-step:24065	 l-p:0.14053183794021606
epoch£º1203	 i:6 	 global-step:24066	 l-p:0.3163325786590576
epoch£º1203	 i:7 	 global-step:24067	 l-p:0.1301964819431305
epoch£º1203	 i:8 	 global-step:24068	 l-p:0.19069136679172516
epoch£º1203	 i:9 	 global-step:24069	 l-p:0.06988855451345444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6827, 3.6664, 3.6816],
        [3.6827, 3.6291, 3.6744],
        [3.6827, 3.0236, 2.9027],
        [3.6827, 3.3684, 3.4953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.08058962970972061 
model_pd.l_d.mean(): -23.631118774414062 
model_pd.lagr.mean(): -23.55052947998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1725], device='cuda:0')), ('power', tensor([-23.8037], device='cuda:0'))])
epoch£º1204	 i:0 	 global-step:24080	 l-p:0.08058962970972061
epoch£º1204	 i:1 	 global-step:24081	 l-p:0.06035543233156204
epoch£º1204	 i:2 	 global-step:24082	 l-p:0.03277765214443207
epoch£º1204	 i:3 	 global-step:24083	 l-p:0.28281456232070923
epoch£º1204	 i:4 	 global-step:24084	 l-p:0.1214776411652565
epoch£º1204	 i:5 	 global-step:24085	 l-p:0.16461516916751862
epoch£º1204	 i:6 	 global-step:24086	 l-p:0.212116077542305
epoch£º1204	 i:7 	 global-step:24087	 l-p:0.12032157927751541
epoch£º1204	 i:8 	 global-step:24088	 l-p:0.09855128824710846
epoch£º1204	 i:9 	 global-step:24089	 l-p:0.14423568546772003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6904, 2.8646, 2.3396],
        [3.6904, 3.5921, 3.6669],
        [3.6904, 3.0427, 2.3938],
        [3.6904, 2.8617, 2.2901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.24677294492721558 
model_pd.l_d.mean(): -23.533554077148438 
model_pd.lagr.mean(): -23.286781311035156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2277], device='cuda:0')), ('power', tensor([-23.7612], device='cuda:0'))])
epoch£º1205	 i:0 	 global-step:24100	 l-p:0.24677294492721558
epoch£º1205	 i:1 	 global-step:24101	 l-p:0.13172124326229095
epoch£º1205	 i:2 	 global-step:24102	 l-p:0.12387580424547195
epoch£º1205	 i:3 	 global-step:24103	 l-p:0.14447633922100067
epoch£º1205	 i:4 	 global-step:24104	 l-p:0.17065975069999695
epoch£º1205	 i:5 	 global-step:24105	 l-p:0.03885215148329735
epoch£º1205	 i:6 	 global-step:24106	 l-p:0.06132485345005989
epoch£º1205	 i:7 	 global-step:24107	 l-p:0.12954507768154144
epoch£º1205	 i:8 	 global-step:24108	 l-p:0.15300364792346954
epoch£º1205	 i:9 	 global-step:24109	 l-p:0.049229755997657776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6884, 3.6884, 3.6884],
        [3.6884, 2.9365, 2.6504],
        [3.6884, 3.5942, 3.6666],
        [3.6884, 3.0829, 3.0316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.14531885087490082 
model_pd.l_d.mean(): -23.633872985839844 
model_pd.lagr.mean(): -23.488554000854492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1516], device='cuda:0')), ('power', tensor([-23.7855], device='cuda:0'))])
epoch£º1206	 i:0 	 global-step:24120	 l-p:0.14531885087490082
epoch£º1206	 i:1 	 global-step:24121	 l-p:0.12311044335365295
epoch£º1206	 i:2 	 global-step:24122	 l-p:0.21015460789203644
epoch£º1206	 i:3 	 global-step:24123	 l-p:0.10842256247997284
epoch£º1206	 i:4 	 global-step:24124	 l-p:0.0355166532099247
epoch£º1206	 i:5 	 global-step:24125	 l-p:0.1438375562429428
epoch£º1206	 i:6 	 global-step:24126	 l-p:0.15216928720474243
epoch£º1206	 i:7 	 global-step:24127	 l-p:-0.02817176841199398
epoch£º1206	 i:8 	 global-step:24128	 l-p:0.28114914894104004
epoch£º1206	 i:9 	 global-step:24129	 l-p:0.1436937153339386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6848, 3.3274, 3.4467],
        [3.6848, 2.8699, 2.4069],
        [3.6848, 3.6826, 3.6848],
        [3.6848, 2.9634, 2.7398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.3031345307826996 
model_pd.l_d.mean(): -23.315895080566406 
model_pd.lagr.mean(): -23.012760162353516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2679], device='cuda:0')), ('power', tensor([-23.5838], device='cuda:0'))])
epoch£º1207	 i:0 	 global-step:24140	 l-p:0.3031345307826996
epoch£º1207	 i:1 	 global-step:24141	 l-p:0.06739621609449387
epoch£º1207	 i:2 	 global-step:24142	 l-p:0.115817591547966
epoch£º1207	 i:3 	 global-step:24143	 l-p:0.1355612725019455
epoch£º1207	 i:4 	 global-step:24144	 l-p:0.1434721052646637
epoch£º1207	 i:5 	 global-step:24145	 l-p:0.05971553176641464
epoch£º1207	 i:6 	 global-step:24146	 l-p:0.20620913803577423
epoch£º1207	 i:7 	 global-step:24147	 l-p:0.07835917174816132
epoch£º1207	 i:8 	 global-step:24148	 l-p:0.11273181438446045
epoch£º1207	 i:9 	 global-step:24149	 l-p:0.11708465218544006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6847, 2.8655, 2.2432],
        [3.6847, 3.4306, 3.5578],
        [3.6847, 3.6847, 3.6847],
        [3.6847, 3.3921, 3.5204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.03663252294063568 
model_pd.l_d.mean(): -23.259708404541016 
model_pd.lagr.mean(): -23.22307586669922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2811], device='cuda:0')), ('power', tensor([-23.5408], device='cuda:0'))])
epoch£º1208	 i:0 	 global-step:24160	 l-p:0.03663252294063568
epoch£º1208	 i:1 	 global-step:24161	 l-p:0.19607287645339966
epoch£º1208	 i:2 	 global-step:24162	 l-p:0.11937233060598373
epoch£º1208	 i:3 	 global-step:24163	 l-p:0.06840682774782181
epoch£º1208	 i:4 	 global-step:24164	 l-p:0.06044100597500801
epoch£º1208	 i:5 	 global-step:24165	 l-p:0.1478978842496872
epoch£º1208	 i:6 	 global-step:24166	 l-p:0.26877906918525696
epoch£º1208	 i:7 	 global-step:24167	 l-p:0.15612901747226715
epoch£º1208	 i:8 	 global-step:24168	 l-p:0.1290111541748047
epoch£º1208	 i:9 	 global-step:24169	 l-p:0.10487962514162064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6926, 3.5144, 3.6257],
        [3.6926, 3.6701, 3.6907],
        [3.6926, 3.6926, 3.6926],
        [3.6926, 3.1591, 2.5274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.13143399357795715 
model_pd.l_d.mean(): -22.942996978759766 
model_pd.lagr.mean(): -22.81156349182129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2000], device='cuda:0')), ('power', tensor([-23.1430], device='cuda:0'))])
epoch£º1209	 i:0 	 global-step:24180	 l-p:0.13143399357795715
epoch£º1209	 i:1 	 global-step:24181	 l-p:-0.03416875749826431
epoch£º1209	 i:2 	 global-step:24182	 l-p:0.14051032066345215
epoch£º1209	 i:3 	 global-step:24183	 l-p:0.17036937177181244
epoch£º1209	 i:4 	 global-step:24184	 l-p:-0.008460178039968014
epoch£º1209	 i:5 	 global-step:24185	 l-p:0.11085103452205658
epoch£º1209	 i:6 	 global-step:24186	 l-p:0.10377562046051025
epoch£º1209	 i:7 	 global-step:24187	 l-p:0.14929068088531494
epoch£º1209	 i:8 	 global-step:24188	 l-p:0.1752503514289856
epoch£º1209	 i:9 	 global-step:24189	 l-p:0.229220911860466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6938, 3.6907, 3.6937],
        [3.6938, 3.1162, 2.4768],
        [3.6938, 3.6842, 3.6933],
        [3.6938, 3.5979, 3.6713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.022112959995865822 
model_pd.l_d.mean(): -23.21750831604004 
model_pd.lagr.mean(): -23.19539451599121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2854], device='cuda:0')), ('power', tensor([-23.5029], device='cuda:0'))])
epoch£º1210	 i:0 	 global-step:24200	 l-p:0.022112959995865822
epoch£º1210	 i:1 	 global-step:24201	 l-p:0.1231023371219635
epoch£º1210	 i:2 	 global-step:24202	 l-p:0.12071048468351364
epoch£º1210	 i:3 	 global-step:24203	 l-p:0.25403186678886414
epoch£º1210	 i:4 	 global-step:24204	 l-p:0.017507415264844894
epoch£º1210	 i:5 	 global-step:24205	 l-p:0.16925595700740814
epoch£º1210	 i:6 	 global-step:24206	 l-p:0.17431941628456116
epoch£º1210	 i:7 	 global-step:24207	 l-p:-0.04837922379374504
epoch£º1210	 i:8 	 global-step:24208	 l-p:0.1600462943315506
epoch£º1210	 i:9 	 global-step:24209	 l-p:0.11473366618156433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6996, 3.6996, 3.6996],
        [3.6996, 3.6737, 3.6971],
        [3.6996, 3.6996, 3.6996],
        [3.6996, 3.6995, 3.6996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.14933133125305176 
model_pd.l_d.mean(): -23.538124084472656 
model_pd.lagr.mean(): -23.388792037963867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2054], device='cuda:0')), ('power', tensor([-23.7435], device='cuda:0'))])
epoch£º1211	 i:0 	 global-step:24220	 l-p:0.14933133125305176
epoch£º1211	 i:1 	 global-step:24221	 l-p:-0.23236067593097687
epoch£º1211	 i:2 	 global-step:24222	 l-p:0.12835869193077087
epoch£º1211	 i:3 	 global-step:24223	 l-p:0.11447733640670776
epoch£º1211	 i:4 	 global-step:24224	 l-p:0.04290517792105675
epoch£º1211	 i:5 	 global-step:24225	 l-p:0.13410301506519318
epoch£º1211	 i:6 	 global-step:24226	 l-p:0.12604846060276031
epoch£º1211	 i:7 	 global-step:24227	 l-p:0.29048919677734375
epoch£º1211	 i:8 	 global-step:24228	 l-p:0.11719521880149841
epoch£º1211	 i:9 	 global-step:24229	 l-p:0.11990197747945786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6973, 3.6862, 3.6967],
        [3.6973, 3.6940, 3.6972],
        [3.6973, 3.3617, 3.4852],
        [3.6973, 3.6972, 3.6973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.11818082630634308 
model_pd.l_d.mean(): -22.74195098876953 
model_pd.lagr.mean(): -22.623769760131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3012], device='cuda:0')), ('power', tensor([-23.0431], device='cuda:0'))])
epoch£º1212	 i:0 	 global-step:24240	 l-p:0.11818082630634308
epoch£º1212	 i:1 	 global-step:24241	 l-p:0.15161821246147156
epoch£º1212	 i:2 	 global-step:24242	 l-p:0.13667286932468414
epoch£º1212	 i:3 	 global-step:24243	 l-p:0.2567897439002991
epoch£º1212	 i:4 	 global-step:24244	 l-p:0.04551814869046211
epoch£º1212	 i:5 	 global-step:24245	 l-p:0.15647897124290466
epoch£º1212	 i:6 	 global-step:24246	 l-p:0.15971730649471283
epoch£º1212	 i:7 	 global-step:24247	 l-p:0.047291770577430725
epoch£º1212	 i:8 	 global-step:24248	 l-p:0.0976618081331253
epoch£º1212	 i:9 	 global-step:24249	 l-p:-0.008013691753149033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6921, 3.6918, 3.6921],
        [3.6921, 3.6919, 3.6921],
        [3.6921, 3.5333, 3.6376],
        [3.6921, 3.4435, 3.5701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): -0.07556033134460449 
model_pd.l_d.mean(): -23.415563583374023 
model_pd.lagr.mean(): -23.49112319946289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2165], device='cuda:0')), ('power', tensor([-23.6321], device='cuda:0'))])
epoch£º1213	 i:0 	 global-step:24260	 l-p:-0.07556033134460449
epoch£º1213	 i:1 	 global-step:24261	 l-p:0.13098588585853577
epoch£º1213	 i:2 	 global-step:24262	 l-p:0.1691737174987793
epoch£º1213	 i:3 	 global-step:24263	 l-p:0.007727517746388912
epoch£º1213	 i:4 	 global-step:24264	 l-p:0.17187190055847168
epoch£º1213	 i:5 	 global-step:24265	 l-p:0.12071863561868668
epoch£º1213	 i:6 	 global-step:24266	 l-p:0.2417728304862976
epoch£º1213	 i:7 	 global-step:24267	 l-p:0.12991495430469513
epoch£º1213	 i:8 	 global-step:24268	 l-p:0.12741811573505402
epoch£º1213	 i:9 	 global-step:24269	 l-p:0.15847642719745636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6944, 3.3982, 3.5262],
        [3.6944, 2.8679, 2.2709],
        [3.6944, 3.5753, 3.6615],
        [3.6944, 3.5491, 3.6478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.1349361538887024 
model_pd.l_d.mean(): -23.469572067260742 
model_pd.lagr.mean(): -23.334636688232422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2039], device='cuda:0')), ('power', tensor([-23.6735], device='cuda:0'))])
epoch£º1214	 i:0 	 global-step:24280	 l-p:0.1349361538887024
epoch£º1214	 i:1 	 global-step:24281	 l-p:0.014720873907208443
epoch£º1214	 i:2 	 global-step:24282	 l-p:0.1677752435207367
epoch£º1214	 i:3 	 global-step:24283	 l-p:-0.006193466018885374
epoch£º1214	 i:4 	 global-step:24284	 l-p:0.13282084465026855
epoch£º1214	 i:5 	 global-step:24285	 l-p:0.12615108489990234
epoch£º1214	 i:6 	 global-step:24286	 l-p:0.27910125255584717
epoch£º1214	 i:7 	 global-step:24287	 l-p:0.1371583342552185
epoch£º1214	 i:8 	 global-step:24288	 l-p:0.11203610152006149
epoch£º1214	 i:9 	 global-step:24289	 l-p:0.06777849048376083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6926, 2.8688, 2.2594],
        [3.6926, 2.8743, 2.2516],
        [3.6926, 3.1918, 3.2403],
        [3.6926, 3.6926, 3.6926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.12915989756584167 
model_pd.l_d.mean(): -23.595359802246094 
model_pd.lagr.mean(): -23.46619987487793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1894], device='cuda:0')), ('power', tensor([-23.7847], device='cuda:0'))])
epoch£º1215	 i:0 	 global-step:24300	 l-p:0.12915989756584167
epoch£º1215	 i:1 	 global-step:24301	 l-p:-0.07881008088588715
epoch£º1215	 i:2 	 global-step:24302	 l-p:0.11680900305509567
epoch£º1215	 i:3 	 global-step:24303	 l-p:0.1316918432712555
epoch£º1215	 i:4 	 global-step:24304	 l-p:0.18491868674755096
epoch£º1215	 i:5 	 global-step:24305	 l-p:0.1615445911884308
epoch£º1215	 i:6 	 global-step:24306	 l-p:0.14317303895950317
epoch£º1215	 i:7 	 global-step:24307	 l-p:0.14715753495693207
epoch£º1215	 i:8 	 global-step:24308	 l-p:0.02965128980576992
epoch£º1215	 i:9 	 global-step:24309	 l-p:0.25253844261169434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6898, 2.8710, 2.2485],
        [3.6898, 2.9342, 2.2797],
        [3.6898, 3.5115, 3.6229],
        [3.6898, 3.6892, 3.6898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.1210322380065918 
model_pd.l_d.mean(): -22.842971801757812 
model_pd.lagr.mean(): -22.721939086914062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2103], device='cuda:0')), ('power', tensor([-23.0533], device='cuda:0'))])
epoch£º1216	 i:0 	 global-step:24320	 l-p:0.1210322380065918
epoch£º1216	 i:1 	 global-step:24321	 l-p:0.13036035001277924
epoch£º1216	 i:2 	 global-step:24322	 l-p:0.1442410796880722
epoch£º1216	 i:3 	 global-step:24323	 l-p:0.12798629701137543
epoch£º1216	 i:4 	 global-step:24324	 l-p:0.1818179041147232
epoch£º1216	 i:5 	 global-step:24325	 l-p:0.19590726494789124
epoch£º1216	 i:6 	 global-step:24326	 l-p:0.12085199356079102
epoch£º1216	 i:7 	 global-step:24327	 l-p:0.053487446159124374
epoch£º1216	 i:8 	 global-step:24328	 l-p:0.1459789127111435
epoch£º1216	 i:9 	 global-step:24329	 l-p:0.08441619575023651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6864, 3.6808, 3.6862],
        [3.6864, 2.9950, 2.8245],
        [3.6864, 2.8765, 2.2417],
        [3.6864, 3.3160, 3.4318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.13139460980892181 
model_pd.l_d.mean(): -22.864700317382812 
model_pd.lagr.mean(): -22.733304977416992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2918], device='cuda:0')), ('power', tensor([-23.1565], device='cuda:0'))])
epoch£º1217	 i:0 	 global-step:24340	 l-p:0.13139460980892181
epoch£º1217	 i:1 	 global-step:24341	 l-p:0.12951034307479858
epoch£º1217	 i:2 	 global-step:24342	 l-p:0.13421228528022766
epoch£º1217	 i:3 	 global-step:24343	 l-p:0.15582942962646484
epoch£º1217	 i:4 	 global-step:24344	 l-p:0.21045291423797607
epoch£º1217	 i:5 	 global-step:24345	 l-p:0.12540075182914734
epoch£º1217	 i:6 	 global-step:24346	 l-p:0.14280438423156738
epoch£º1217	 i:7 	 global-step:24347	 l-p:0.12783882021903992
epoch£º1217	 i:8 	 global-step:24348	 l-p:0.1334601491689682
epoch£º1217	 i:9 	 global-step:24349	 l-p:0.07910730689764023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6804, 2.8543, 2.2458],
        [3.6804, 2.9439, 2.6928],
        [3.6804, 3.6803, 3.6804],
        [3.6804, 3.4584, 3.5812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.10770910233259201 
model_pd.l_d.mean(): -23.12310028076172 
model_pd.lagr.mean(): -23.015390396118164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2524], device='cuda:0')), ('power', tensor([-23.3755], device='cuda:0'))])
epoch£º1218	 i:0 	 global-step:24360	 l-p:0.10770910233259201
epoch£º1218	 i:1 	 global-step:24361	 l-p:0.14875295758247375
epoch£º1218	 i:2 	 global-step:24362	 l-p:0.14863038063049316
epoch£º1218	 i:3 	 global-step:24363	 l-p:0.13272416591644287
epoch£º1218	 i:4 	 global-step:24364	 l-p:0.1390572488307953
epoch£º1218	 i:5 	 global-step:24365	 l-p:0.4458197057247162
epoch£º1218	 i:6 	 global-step:24366	 l-p:0.14245723187923431
epoch£º1218	 i:7 	 global-step:24367	 l-p:0.20910829305648804
epoch£º1218	 i:8 	 global-step:24368	 l-p:0.12966085970401764
epoch£º1218	 i:9 	 global-step:24369	 l-p:0.14250420033931732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6700, 2.9076, 2.6035],
        [3.6700, 3.5122, 3.6162],
        [3.6700, 3.5977, 3.6562],
        [3.6700, 3.6701, 3.6701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.12456051260232925 
model_pd.l_d.mean(): -22.782079696655273 
model_pd.lagr.mean(): -22.65751838684082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2605], device='cuda:0')), ('power', tensor([-23.0426], device='cuda:0'))])
epoch£º1219	 i:0 	 global-step:24380	 l-p:0.12456051260232925
epoch£º1219	 i:1 	 global-step:24381	 l-p:0.13175572454929352
epoch£º1219	 i:2 	 global-step:24382	 l-p:-1.6250722408294678
epoch£º1219	 i:3 	 global-step:24383	 l-p:0.13994403183460236
epoch£º1219	 i:4 	 global-step:24384	 l-p:0.2386365383863449
epoch£º1219	 i:5 	 global-step:24385	 l-p:0.1462850421667099
epoch£º1219	 i:6 	 global-step:24386	 l-p:0.10435447096824646
epoch£º1219	 i:7 	 global-step:24387	 l-p:0.11971056461334229
epoch£º1219	 i:8 	 global-step:24388	 l-p:0.15403302013874054
epoch£º1219	 i:9 	 global-step:24389	 l-p:0.07868398725986481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6635, 3.6634, 3.6636],
        [3.6635, 3.0772, 2.4410],
        [3.6635, 2.8779, 2.2309],
        [3.6635, 2.8404, 2.2195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.1360498070716858 
model_pd.l_d.mean(): -23.648529052734375 
model_pd.lagr.mean(): -23.512479782104492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1817], device='cuda:0')), ('power', tensor([-23.8303], device='cuda:0'))])
epoch£º1220	 i:0 	 global-step:24400	 l-p:0.1360498070716858
epoch£º1220	 i:1 	 global-step:24401	 l-p:0.13303181529045105
epoch£º1220	 i:2 	 global-step:24402	 l-p:0.2804374396800995
epoch£º1220	 i:3 	 global-step:24403	 l-p:0.09415990859270096
epoch£º1220	 i:4 	 global-step:24404	 l-p:0.12123355269432068
epoch£º1220	 i:5 	 global-step:24405	 l-p:0.12021481990814209
epoch£º1220	 i:6 	 global-step:24406	 l-p:-0.18126516044139862
epoch£º1220	 i:7 	 global-step:24407	 l-p:0.12709733843803406
epoch£º1220	 i:8 	 global-step:24408	 l-p:0.1284625083208084
epoch£º1220	 i:9 	 global-step:24409	 l-p:0.07958497107028961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6628, 3.6608, 3.6627],
        [3.6628, 3.6627, 3.6628],
        [3.6628, 2.9133, 2.2618],
        [3.6628, 2.9052, 2.6143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): -0.2679802179336548 
model_pd.l_d.mean(): -23.461328506469727 
model_pd.lagr.mean(): -23.72930908203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2577], device='cuda:0')), ('power', tensor([-23.7190], device='cuda:0'))])
epoch£º1221	 i:0 	 global-step:24420	 l-p:-0.2679802179336548
epoch£º1221	 i:1 	 global-step:24421	 l-p:0.07116024941205978
epoch£º1221	 i:2 	 global-step:24422	 l-p:0.13565853238105774
epoch£º1221	 i:3 	 global-step:24423	 l-p:0.1481335163116455
epoch£º1221	 i:4 	 global-step:24424	 l-p:0.128923237323761
epoch£º1221	 i:5 	 global-step:24425	 l-p:0.1360761970281601
epoch£º1221	 i:6 	 global-step:24426	 l-p:0.1471669226884842
epoch£º1221	 i:7 	 global-step:24427	 l-p:0.15513132512569427
epoch£º1221	 i:8 	 global-step:24428	 l-p:0.25062233209609985
epoch£º1221	 i:9 	 global-step:24429	 l-p:0.11923303455114365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6639, 3.6639, 3.6639],
        [3.6639, 3.5600, 3.6381],
        [3.6639, 3.6333, 3.6607],
        [3.6639, 3.6630, 3.6639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.12152410298585892 
model_pd.l_d.mean(): -23.65996551513672 
model_pd.lagr.mean(): -23.538440704345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1559], device='cuda:0')), ('power', tensor([-23.8158], device='cuda:0'))])
epoch£º1222	 i:0 	 global-step:24440	 l-p:0.12152410298585892
epoch£º1222	 i:1 	 global-step:24441	 l-p:0.13311681151390076
epoch£º1222	 i:2 	 global-step:24442	 l-p:0.1368652731180191
epoch£º1222	 i:3 	 global-step:24443	 l-p:0.2640400230884552
epoch£º1222	 i:4 	 global-step:24444	 l-p:0.07040269672870636
epoch£º1222	 i:5 	 global-step:24445	 l-p:-0.19060109555721283
epoch£º1222	 i:6 	 global-step:24446	 l-p:0.12628838419914246
epoch£º1222	 i:7 	 global-step:24447	 l-p:0.14116360247135162
epoch£º1222	 i:8 	 global-step:24448	 l-p:0.18913128972053528
epoch£º1222	 i:9 	 global-step:24449	 l-p:0.12892191112041473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6603, 3.5667, 3.6387],
        [3.6603, 3.6352, 3.6579],
        [3.6603, 3.5097, 3.6109],
        [3.6603, 3.2069, 3.2885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.11722411960363388 
model_pd.l_d.mean(): -22.87039566040039 
model_pd.lagr.mean(): -22.753171920776367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2666], device='cuda:0')), ('power', tensor([-23.1370], device='cuda:0'))])
epoch£º1223	 i:0 	 global-step:24460	 l-p:0.11722411960363388
epoch£º1223	 i:1 	 global-step:24461	 l-p:-0.10134246945381165
epoch£º1223	 i:2 	 global-step:24462	 l-p:0.1356777399778366
epoch£º1223	 i:3 	 global-step:24463	 l-p:0.10100768506526947
epoch£º1223	 i:4 	 global-step:24464	 l-p:0.07683075219392776
epoch£º1223	 i:5 	 global-step:24465	 l-p:0.32144612073898315
epoch£º1223	 i:6 	 global-step:24466	 l-p:0.11981818825006485
epoch£º1223	 i:7 	 global-step:24467	 l-p:0.10056038945913315
epoch£º1223	 i:8 	 global-step:24468	 l-p:0.20831383764743805
epoch£º1223	 i:9 	 global-step:24469	 l-p:0.13010849058628082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6588, 3.5065, 3.6084],
        [3.6588, 3.6588, 3.6588],
        [3.6588, 3.1803, 2.5632],
        [3.6588, 2.8864, 2.5624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.14135602116584778 
model_pd.l_d.mean(): -22.680971145629883 
model_pd.lagr.mean(): -22.539615631103516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2770], device='cuda:0')), ('power', tensor([-22.9580], device='cuda:0'))])
epoch£º1224	 i:0 	 global-step:24480	 l-p:0.14135602116584778
epoch£º1224	 i:1 	 global-step:24481	 l-p:-0.11900930106639862
epoch£º1224	 i:2 	 global-step:24482	 l-p:0.13103146851062775
epoch£º1224	 i:3 	 global-step:24483	 l-p:0.2806982100009918
epoch£º1224	 i:4 	 global-step:24484	 l-p:0.15112532675266266
epoch£º1224	 i:5 	 global-step:24485	 l-p:0.0885547623038292
epoch£º1224	 i:6 	 global-step:24486	 l-p:0.13664701581001282
epoch£º1224	 i:7 	 global-step:24487	 l-p:0.08409692347049713
epoch£º1224	 i:8 	 global-step:24488	 l-p:0.17864324152469635
epoch£º1224	 i:9 	 global-step:24489	 l-p:0.13196325302124023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6634, 3.6221, 3.6581],
        [3.6634, 3.6631, 3.6634],
        [3.6634, 3.6298, 3.6596],
        [3.6634, 3.6634, 3.6634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): -0.27084171772003174 
model_pd.l_d.mean(): -23.443241119384766 
model_pd.lagr.mean(): -23.714082717895508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1799], device='cuda:0')), ('power', tensor([-23.6232], device='cuda:0'))])
epoch£º1225	 i:0 	 global-step:24500	 l-p:-0.27084171772003174
epoch£º1225	 i:1 	 global-step:24501	 l-p:0.11068383604288101
epoch£º1225	 i:2 	 global-step:24502	 l-p:0.136320099234581
epoch£º1225	 i:3 	 global-step:24503	 l-p:0.1250896006822586
epoch£º1225	 i:4 	 global-step:24504	 l-p:0.05778210982680321
epoch£º1225	 i:5 	 global-step:24505	 l-p:0.12359220534563065
epoch£º1225	 i:6 	 global-step:24506	 l-p:0.143958181142807
epoch£º1225	 i:7 	 global-step:24507	 l-p:0.2510037422180176
epoch£º1225	 i:8 	 global-step:24508	 l-p:0.14641247689723969
epoch£º1225	 i:9 	 global-step:24509	 l-p:0.14286673069000244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6671, 3.4123, 3.5399],
        [3.6671, 2.8429, 2.3531],
        [3.6671, 3.6671, 3.6671],
        [3.6671, 3.6670, 3.6671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.18144363164901733 
model_pd.l_d.mean(): -22.66078758239746 
model_pd.lagr.mean(): -22.47934341430664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3383], device='cuda:0')), ('power', tensor([-22.9991], device='cuda:0'))])
epoch£º1226	 i:0 	 global-step:24520	 l-p:0.18144363164901733
epoch£º1226	 i:1 	 global-step:24521	 l-p:0.1562914103269577
epoch£º1226	 i:2 	 global-step:24522	 l-p:0.10857995599508286
epoch£º1226	 i:3 	 global-step:24523	 l-p:0.15240222215652466
epoch£º1226	 i:4 	 global-step:24524	 l-p:-0.6487383842468262
epoch£º1226	 i:5 	 global-step:24525	 l-p:0.13868793845176697
epoch£º1226	 i:6 	 global-step:24526	 l-p:0.03728204965591431
epoch£º1226	 i:7 	 global-step:24527	 l-p:0.23084956407546997
epoch£º1226	 i:8 	 global-step:24528	 l-p:0.14354208111763
epoch£º1226	 i:9 	 global-step:24529	 l-p:0.07401353865861893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6659, 3.6561, 3.6654],
        [3.6659, 3.6638, 3.6659],
        [3.6659, 3.4948, 3.6039],
        [3.6659, 3.6659, 3.6659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.1746653914451599 
model_pd.l_d.mean(): -23.29408073425293 
model_pd.lagr.mean(): -23.119415283203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2605], device='cuda:0')), ('power', tensor([-23.5546], device='cuda:0'))])
epoch£º1227	 i:0 	 global-step:24540	 l-p:0.1746653914451599
epoch£º1227	 i:1 	 global-step:24541	 l-p:0.1557462513446808
epoch£º1227	 i:2 	 global-step:24542	 l-p:0.11169885098934174
epoch£º1227	 i:3 	 global-step:24543	 l-p:0.13767291605472565
epoch£º1227	 i:4 	 global-step:24544	 l-p:0.02588607557117939
epoch£º1227	 i:5 	 global-step:24545	 l-p:0.24002954363822937
epoch£º1227	 i:6 	 global-step:24546	 l-p:0.08421426266431808
epoch£º1227	 i:7 	 global-step:24547	 l-p:0.1367066353559494
epoch£º1227	 i:8 	 global-step:24548	 l-p:0.13312332332134247
epoch£º1227	 i:9 	 global-step:24549	 l-p:7.955281734466553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6706, 2.8544, 2.2242],
        [3.6706, 3.6506, 3.6690],
        [3.6706, 3.6650, 3.6703],
        [3.6706, 2.8478, 2.2269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.18876200914382935 
model_pd.l_d.mean(): -23.481712341308594 
model_pd.lagr.mean(): -23.292949676513672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1966], device='cuda:0')), ('power', tensor([-23.6783], device='cuda:0'))])
epoch£º1228	 i:0 	 global-step:24560	 l-p:0.18876200914382935
epoch£º1228	 i:1 	 global-step:24561	 l-p:0.18579189479351044
epoch£º1228	 i:2 	 global-step:24562	 l-p:0.08078420162200928
epoch£º1228	 i:3 	 global-step:24563	 l-p:0.12321286648511887
epoch£º1228	 i:4 	 global-step:24564	 l-p:0.12306594848632812
epoch£º1228	 i:5 	 global-step:24565	 l-p:0.1564064770936966
epoch£º1228	 i:6 	 global-step:24566	 l-p:0.5500255227088928
epoch£º1228	 i:7 	 global-step:24567	 l-p:0.15482892096042633
epoch£º1228	 i:8 	 global-step:24568	 l-p:0.10441607981920242
epoch£º1228	 i:9 	 global-step:24569	 l-p:0.12587349116802216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6742, 3.5780, 3.6516],
        [3.6742, 3.6601, 3.6733],
        [3.6742, 2.9372, 2.6869],
        [3.6742, 2.9813, 2.8107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.16599303483963013 
model_pd.l_d.mean(): -22.806032180786133 
model_pd.lagr.mean(): -22.640039443969727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2362], device='cuda:0')), ('power', tensor([-23.0423], device='cuda:0'))])
epoch£º1229	 i:0 	 global-step:24580	 l-p:0.16599303483963013
epoch£º1229	 i:1 	 global-step:24581	 l-p:0.11116335541009903
epoch£º1229	 i:2 	 global-step:24582	 l-p:0.12496694177389145
epoch£º1229	 i:3 	 global-step:24583	 l-p:0.1359662562608719
epoch£º1229	 i:4 	 global-step:24584	 l-p:0.14063706994056702
epoch£º1229	 i:5 	 global-step:24585	 l-p:0.11270307004451752
epoch£º1229	 i:6 	 global-step:24586	 l-p:0.12848860025405884
epoch£º1229	 i:7 	 global-step:24587	 l-p:13.222609519958496
epoch£º1229	 i:8 	 global-step:24588	 l-p:0.13543052971363068
epoch£º1229	 i:9 	 global-step:24589	 l-p:0.059850387275218964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6674, 3.5648, 3.6421],
        [3.6674, 3.6516, 3.6663],
        [3.6674, 3.6664, 3.6673],
        [3.6674, 2.8636, 2.2232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.1955934464931488 
model_pd.l_d.mean(): -23.749265670776367 
model_pd.lagr.mean(): -23.553672790527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1421], device='cuda:0')), ('power', tensor([-23.8914], device='cuda:0'))])
epoch£º1230	 i:0 	 global-step:24600	 l-p:0.1955934464931488
epoch£º1230	 i:1 	 global-step:24601	 l-p:0.14220508933067322
epoch£º1230	 i:2 	 global-step:24602	 l-p:0.10583943128585815
epoch£º1230	 i:3 	 global-step:24603	 l-p:0.1243797093629837
epoch£º1230	 i:4 	 global-step:24604	 l-p:-0.5060062408447266
epoch£º1230	 i:5 	 global-step:24605	 l-p:0.07050244510173798
epoch£º1230	 i:6 	 global-step:24606	 l-p:0.13223238289356232
epoch£º1230	 i:7 	 global-step:24607	 l-p:0.26094484329223633
epoch£º1230	 i:8 	 global-step:24608	 l-p:0.06313943862915039
epoch£º1230	 i:9 	 global-step:24609	 l-p:0.1392526924610138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6669, 2.8348, 2.2940],
        [3.6669, 3.6668, 3.6669],
        [3.6669, 3.6669, 3.6669],
        [3.6669, 2.8612, 2.2220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.17406275868415833 
model_pd.l_d.mean(): -23.580928802490234 
model_pd.lagr.mean(): -23.4068660736084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1866], device='cuda:0')), ('power', tensor([-23.7675], device='cuda:0'))])
epoch£º1231	 i:0 	 global-step:24620	 l-p:0.17406275868415833
epoch£º1231	 i:1 	 global-step:24621	 l-p:0.13116714358329773
epoch£º1231	 i:2 	 global-step:24622	 l-p:0.14833885431289673
epoch£º1231	 i:3 	 global-step:24623	 l-p:0.13605153560638428
epoch£º1231	 i:4 	 global-step:24624	 l-p:0.16525258123874664
epoch£º1231	 i:5 	 global-step:24625	 l-p:0.1446988433599472
epoch£º1231	 i:6 	 global-step:24626	 l-p:0.12128160148859024
epoch£º1231	 i:7 	 global-step:24627	 l-p:0.24868978559970856
epoch£º1231	 i:8 	 global-step:24628	 l-p:0.08954538404941559
epoch£º1231	 i:9 	 global-step:24629	 l-p:-0.49485811591148376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6653, 2.8365, 2.3239],
        [3.6653, 3.6358, 3.6622],
        [3.6653, 2.8512, 2.4054],
        [3.6653, 3.0723, 3.0384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.151187464594841 
model_pd.l_d.mean(): -23.67756462097168 
model_pd.lagr.mean(): -23.526376724243164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1798], device='cuda:0')), ('power', tensor([-23.8574], device='cuda:0'))])
epoch£º1232	 i:0 	 global-step:24640	 l-p:0.151187464594841
epoch£º1232	 i:1 	 global-step:24641	 l-p:0.10893045365810394
epoch£º1232	 i:2 	 global-step:24642	 l-p:0.1306106001138687
epoch£º1232	 i:3 	 global-step:24643	 l-p:0.20209139585494995
epoch£º1232	 i:4 	 global-step:24644	 l-p:-0.4777475595474243
epoch£º1232	 i:5 	 global-step:24645	 l-p:0.13642443716526031
epoch£º1232	 i:6 	 global-step:24646	 l-p:0.1156756728887558
epoch£º1232	 i:7 	 global-step:24647	 l-p:0.1336800903081894
epoch£º1232	 i:8 	 global-step:24648	 l-p:0.13541415333747864
epoch£º1232	 i:9 	 global-step:24649	 l-p:0.10883039236068726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6666, 3.0334, 2.9512],
        [3.6666, 2.8676, 2.2248],
        [3.6666, 3.6666, 3.6666],
        [3.6666, 3.6503, 3.6654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.056574173271656036 
model_pd.l_d.mean(): -23.535179138183594 
model_pd.lagr.mean(): -23.478605270385742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1721], device='cuda:0')), ('power', tensor([-23.7073], device='cuda:0'))])
epoch£º1233	 i:0 	 global-step:24660	 l-p:0.056574173271656036
epoch£º1233	 i:1 	 global-step:24661	 l-p:0.1147443950176239
epoch£º1233	 i:2 	 global-step:24662	 l-p:0.19648094475269318
epoch£º1233	 i:3 	 global-step:24663	 l-p:0.0866038054227829
epoch£º1233	 i:4 	 global-step:24664	 l-p:-0.5217033624649048
epoch£º1233	 i:5 	 global-step:24665	 l-p:0.1526608020067215
epoch£º1233	 i:6 	 global-step:24666	 l-p:0.16516871750354767
epoch£º1233	 i:7 	 global-step:24667	 l-p:0.11500557512044907
epoch£º1233	 i:8 	 global-step:24668	 l-p:0.14055994153022766
epoch£º1233	 i:9 	 global-step:24669	 l-p:0.17994236946105957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6662, 3.6633, 3.6661],
        [3.6662, 3.6662, 3.6662],
        [3.6662, 3.6662, 3.6662],
        [3.6662, 3.0819, 3.0573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.12369149178266525 
model_pd.l_d.mean(): -23.607730865478516 
model_pd.lagr.mean(): -23.484039306640625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1250], device='cuda:0')), ('power', tensor([-23.7328], device='cuda:0'))])
epoch£º1234	 i:0 	 global-step:24680	 l-p:0.12369149178266525
epoch£º1234	 i:1 	 global-step:24681	 l-p:0.1524503231048584
epoch£º1234	 i:2 	 global-step:24682	 l-p:0.11911813169717789
epoch£º1234	 i:3 	 global-step:24683	 l-p:0.14815650880336761
epoch£º1234	 i:4 	 global-step:24684	 l-p:0.10003730654716492
epoch£º1234	 i:5 	 global-step:24685	 l-p:0.1614263355731964
epoch£º1234	 i:6 	 global-step:24686	 l-p:0.16556432843208313
epoch£º1234	 i:7 	 global-step:24687	 l-p:0.07798762619495392
epoch£º1234	 i:8 	 global-step:24688	 l-p:-0.28981083631515503
epoch£º1234	 i:9 	 global-step:24689	 l-p:0.2602752447128296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6645, 3.5843, 3.6480],
        [3.6645, 3.6590, 3.6643],
        [3.6645, 3.5999, 3.6531],
        [3.6645, 3.6495, 3.6635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.13027127087116241 
model_pd.l_d.mean(): -23.5819149017334 
model_pd.lagr.mean(): -23.451642990112305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1438], device='cuda:0')), ('power', tensor([-23.7257], device='cuda:0'))])
epoch£º1235	 i:0 	 global-step:24700	 l-p:0.13027127087116241
epoch£º1235	 i:1 	 global-step:24701	 l-p:0.13627490401268005
epoch£º1235	 i:2 	 global-step:24702	 l-p:0.16213542222976685
epoch£º1235	 i:3 	 global-step:24703	 l-p:0.14159847795963287
epoch£º1235	 i:4 	 global-step:24704	 l-p:-0.3026546537876129
epoch£º1235	 i:5 	 global-step:24705	 l-p:0.0843365490436554
epoch£º1235	 i:6 	 global-step:24706	 l-p:0.19702230393886566
epoch£º1235	 i:7 	 global-step:24707	 l-p:0.1742248833179474
epoch£º1235	 i:8 	 global-step:24708	 l-p:0.12231794744729996
epoch£º1235	 i:9 	 global-step:24709	 l-p:0.11624445021152496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6663, 3.0733, 3.0394],
        [3.6663, 3.3304, 3.4549],
        [3.6663, 3.3437, 3.4701],
        [3.6663, 3.6663, 3.6663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.25472018122673035 
model_pd.l_d.mean(): -23.39040756225586 
model_pd.lagr.mean(): -23.13568687438965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2586], device='cuda:0')), ('power', tensor([-23.6490], device='cuda:0'))])
epoch£º1236	 i:0 	 global-step:24720	 l-p:0.25472018122673035
epoch£º1236	 i:1 	 global-step:24721	 l-p:0.13257835805416107
epoch£º1236	 i:2 	 global-step:24722	 l-p:0.150361567735672
epoch£º1236	 i:3 	 global-step:24723	 l-p:0.12130561470985413
epoch£º1236	 i:4 	 global-step:24724	 l-p:0.13233567774295807
epoch£º1236	 i:5 	 global-step:24725	 l-p:0.09866081923246384
epoch£º1236	 i:6 	 global-step:24726	 l-p:0.1296440064907074
epoch£º1236	 i:7 	 global-step:24727	 l-p:0.07115529477596283
epoch£º1236	 i:8 	 global-step:24728	 l-p:0.16062013804912567
epoch£º1236	 i:9 	 global-step:24729	 l-p:-0.55326908826828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6663, 3.6663, 3.6663],
        [3.6663, 3.6663, 3.6663],
        [3.6663, 3.6654, 3.6663],
        [3.6663, 3.5823, 3.6484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.07390185445547104 
model_pd.l_d.mean(): -23.192520141601562 
model_pd.lagr.mean(): -23.11861801147461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1983], device='cuda:0')), ('power', tensor([-23.3908], device='cuda:0'))])
epoch£º1237	 i:0 	 global-step:24740	 l-p:0.07390185445547104
epoch£º1237	 i:1 	 global-step:24741	 l-p:0.1606273651123047
epoch£º1237	 i:2 	 global-step:24742	 l-p:0.13379837572574615
epoch£º1237	 i:3 	 global-step:24743	 l-p:0.06715668737888336
epoch£º1237	 i:4 	 global-step:24744	 l-p:0.13606177270412445
epoch£º1237	 i:5 	 global-step:24745	 l-p:0.13520099222660065
epoch£º1237	 i:6 	 global-step:24746	 l-p:0.08775586634874344
epoch£º1237	 i:7 	 global-step:24747	 l-p:0.1733105480670929
epoch£º1237	 i:8 	 global-step:24748	 l-p:-0.4334253966808319
epoch£º1237	 i:9 	 global-step:24749	 l-p:0.2909725606441498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6667, 2.9275, 2.6748],
        [3.6667, 3.0824, 3.0580],
        [3.6667, 3.5472, 3.6338],
        [3.6667, 3.5823, 3.6487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.1295074075460434 
model_pd.l_d.mean(): -23.840253829956055 
model_pd.lagr.mean(): -23.71074676513672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0848], device='cuda:0')), ('power', tensor([-23.9251], device='cuda:0'))])
epoch£º1238	 i:0 	 global-step:24760	 l-p:0.1295074075460434
epoch£º1238	 i:1 	 global-step:24761	 l-p:0.17406177520751953
epoch£º1238	 i:2 	 global-step:24762	 l-p:0.144402876496315
epoch£º1238	 i:3 	 global-step:24763	 l-p:0.1082252487540245
epoch£º1238	 i:4 	 global-step:24764	 l-p:-1.6071348190307617
epoch£º1238	 i:5 	 global-step:24765	 l-p:0.08810855448246002
epoch£º1238	 i:6 	 global-step:24766	 l-p:0.16544058918952942
epoch£º1238	 i:7 	 global-step:24767	 l-p:0.11941565573215485
epoch£º1238	 i:8 	 global-step:24768	 l-p:0.22895072400569916
epoch£º1238	 i:9 	 global-step:24769	 l-p:0.06242619454860687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6730, 3.6721, 3.6730],
        [3.6730, 3.6589, 3.6721],
        [3.6730, 3.0151, 2.3675],
        [3.6730, 3.6491, 3.6708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.2220141589641571 
model_pd.l_d.mean(): -23.765769958496094 
model_pd.lagr.mean(): -23.54375648498535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1258], device='cuda:0')), ('power', tensor([-23.8916], device='cuda:0'))])
epoch£º1239	 i:0 	 global-step:24780	 l-p:0.2220141589641571
epoch£º1239	 i:1 	 global-step:24781	 l-p:0.13693052530288696
epoch£º1239	 i:2 	 global-step:24782	 l-p:0.6636366248130798
epoch£º1239	 i:3 	 global-step:24783	 l-p:0.07703058421611786
epoch£º1239	 i:4 	 global-step:24784	 l-p:0.10259786993265152
epoch£º1239	 i:5 	 global-step:24785	 l-p:0.1722492128610611
epoch£º1239	 i:6 	 global-step:24786	 l-p:0.05281674116849899
epoch£º1239	 i:7 	 global-step:24787	 l-p:0.12283607572317123
epoch£º1239	 i:8 	 global-step:24788	 l-p:0.1717001348733902
epoch£º1239	 i:9 	 global-step:24789	 l-p:0.06850773096084595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6849, 3.2327, 2.6172],
        [3.6849, 3.1543, 2.5239],
        [3.6849, 3.4541, 3.5785],
        [3.6849, 2.8549, 2.2644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.11865317821502686 
model_pd.l_d.mean(): -23.274763107299805 
model_pd.lagr.mean(): -23.156110763549805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2531], device='cuda:0')), ('power', tensor([-23.5279], device='cuda:0'))])
epoch£º1240	 i:0 	 global-step:24800	 l-p:0.11865317821502686
epoch£º1240	 i:1 	 global-step:24801	 l-p:0.06531395763158798
epoch£º1240	 i:2 	 global-step:24802	 l-p:0.291177898645401
epoch£º1240	 i:3 	 global-step:24803	 l-p:0.06796171516180038
epoch£º1240	 i:4 	 global-step:24804	 l-p:0.12329834699630737
epoch£º1240	 i:5 	 global-step:24805	 l-p:-0.02714640647172928
epoch£º1240	 i:6 	 global-step:24806	 l-p:0.12767980992794037
epoch£º1240	 i:7 	 global-step:24807	 l-p:0.17412304878234863
epoch£º1240	 i:8 	 global-step:24808	 l-p:0.17602595686912537
epoch£º1240	 i:9 	 global-step:24809	 l-p:0.13253143429756165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6993, 3.4549, 3.5811],
        [3.6993, 3.0535, 2.9507],
        [3.6993, 3.2811, 3.3791],
        [3.6993, 3.5731, 3.6630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): -0.09298055619001389 
model_pd.l_d.mean(): -22.330150604248047 
model_pd.lagr.mean(): -22.423131942749023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3493], device='cuda:0')), ('power', tensor([-22.6795], device='cuda:0'))])
epoch£º1241	 i:0 	 global-step:24820	 l-p:-0.09298055619001389
epoch£º1241	 i:1 	 global-step:24821	 l-p:0.14994500577449799
epoch£º1241	 i:2 	 global-step:24822	 l-p:0.03639453276991844
epoch£º1241	 i:3 	 global-step:24823	 l-p:0.12532247602939606
epoch£º1241	 i:4 	 global-step:24824	 l-p:0.1539389044046402
epoch£º1241	 i:5 	 global-step:24825	 l-p:0.14258727431297302
epoch£º1241	 i:6 	 global-step:24826	 l-p:0.12026919424533844
epoch£º1241	 i:7 	 global-step:24827	 l-p:0.24010078608989716
epoch£º1241	 i:8 	 global-step:24828	 l-p:-0.06947451084852219
epoch£º1241	 i:9 	 global-step:24829	 l-p:0.14729183912277222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7060, 2.9142, 2.2663],
        [3.7060, 2.9789, 2.3210],
        [3.7060, 3.7060, 3.7060],
        [3.7060, 3.6983, 3.7056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.17023372650146484 
model_pd.l_d.mean(): -23.282394409179688 
model_pd.lagr.mean(): -23.112159729003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2639], device='cuda:0')), ('power', tensor([-23.5463], device='cuda:0'))])
epoch£º1242	 i:0 	 global-step:24840	 l-p:0.17023372650146484
epoch£º1242	 i:1 	 global-step:24841	 l-p:0.10710526257753372
epoch£º1242	 i:2 	 global-step:24842	 l-p:-0.008191299624741077
epoch£º1242	 i:3 	 global-step:24843	 l-p:0.22375953197479248
epoch£º1242	 i:4 	 global-step:24844	 l-p:0.12125295400619507
epoch£º1242	 i:5 	 global-step:24845	 l-p:0.13417744636535645
epoch£º1242	 i:6 	 global-step:24846	 l-p:-0.7907325029373169
epoch£º1242	 i:7 	 global-step:24847	 l-p:0.13377033174037933
epoch£º1242	 i:8 	 global-step:24848	 l-p:-0.10506509244441986
epoch£º1242	 i:9 	 global-step:24849	 l-p:0.13420046865940094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7092, 3.7086, 3.7092],
        [3.7092, 3.5575, 3.6589],
        [3.7092, 3.4606, 3.5872],
        [3.7092, 3.0392, 2.9001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): -1.1036298274993896 
model_pd.l_d.mean(): -23.36265754699707 
model_pd.lagr.mean(): -24.46628761291504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2066], device='cuda:0')), ('power', tensor([-23.5693], device='cuda:0'))])
epoch£º1243	 i:0 	 global-step:24860	 l-p:-1.1036298274993896
epoch£º1243	 i:1 	 global-step:24861	 l-p:-0.027825674042105675
epoch£º1243	 i:2 	 global-step:24862	 l-p:0.17157135903835297
epoch£º1243	 i:3 	 global-step:24863	 l-p:0.10509105771780014
epoch£º1243	 i:4 	 global-step:24864	 l-p:0.13103625178337097
epoch£º1243	 i:5 	 global-step:24865	 l-p:-0.18192151188850403
epoch£º1243	 i:6 	 global-step:24866	 l-p:0.13036726415157318
epoch£º1243	 i:7 	 global-step:24867	 l-p:0.17242373526096344
epoch£º1243	 i:8 	 global-step:24868	 l-p:0.18764662742614746
epoch£º1243	 i:9 	 global-step:24869	 l-p:0.13504169881343842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7134, 3.5811, 3.6739],
        [3.7134, 3.4211, 3.5492],
        [3.7134, 3.2384, 3.3045],
        [3.7134, 3.7134, 3.7134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.1430629938840866 
model_pd.l_d.mean(): -23.345956802368164 
model_pd.lagr.mean(): -23.20289421081543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1497], device='cuda:0')), ('power', tensor([-23.4957], device='cuda:0'))])
epoch£º1244	 i:0 	 global-step:24880	 l-p:0.1430629938840866
epoch£º1244	 i:1 	 global-step:24881	 l-p:0.17506541311740875
epoch£º1244	 i:2 	 global-step:24882	 l-p:-0.2594909369945526
epoch£º1244	 i:3 	 global-step:24883	 l-p:0.12979242205619812
epoch£º1244	 i:4 	 global-step:24884	 l-p:0.11870651692152023
epoch£º1244	 i:5 	 global-step:24885	 l-p:1.0192949771881104
epoch£º1244	 i:6 	 global-step:24886	 l-p:0.1313420683145523
epoch£º1244	 i:7 	 global-step:24887	 l-p:-0.09637285023927689
epoch£º1244	 i:8 	 global-step:24888	 l-p:0.1839207410812378
epoch£º1244	 i:9 	 global-step:24889	 l-p:0.13229522109031677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6075,  0.5145,  1.0000,  0.4357,
          1.0000,  0.8469, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[3.7149, 2.9217, 2.2743],
        [3.7149, 2.9094, 2.2709],
        [3.7149, 3.0250, 2.3677],
        [3.7149, 2.8904, 2.3603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): -0.6004036068916321 
model_pd.l_d.mean(): -23.5339412689209 
model_pd.lagr.mean(): -24.13434410095215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2046], device='cuda:0')), ('power', tensor([-23.7385], device='cuda:0'))])
epoch£º1245	 i:0 	 global-step:24900	 l-p:-0.6004036068916321
epoch£º1245	 i:1 	 global-step:24901	 l-p:0.1407463103532791
epoch£º1245	 i:2 	 global-step:24902	 l-p:0.09575606882572174
epoch£º1245	 i:3 	 global-step:24903	 l-p:0.1727370172739029
epoch£º1245	 i:4 	 global-step:24904	 l-p:0.1405745893716812
epoch£º1245	 i:5 	 global-step:24905	 l-p:0.1408340483903885
epoch£º1245	 i:6 	 global-step:24906	 l-p:0.11839013546705246
epoch£º1245	 i:7 	 global-step:24907	 l-p:1.3273698091506958
epoch£º1245	 i:8 	 global-step:24908	 l-p:0.13926032185554504
epoch£º1245	 i:9 	 global-step:24909	 l-p:0.12086842209100723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7114, 2.8864, 2.3565],
        [3.7114, 3.7114, 3.7114],
        [3.7114, 3.6310, 3.6948],
        [3.7114, 3.7114, 3.7114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.14239177107810974 
model_pd.l_d.mean(): -23.2923526763916 
model_pd.lagr.mean(): -23.149961471557617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2964], device='cuda:0')), ('power', tensor([-23.5888], device='cuda:0'))])
epoch£º1246	 i:0 	 global-step:24920	 l-p:0.14239177107810974
epoch£º1246	 i:1 	 global-step:24921	 l-p:0.13400226831436157
epoch£º1246	 i:2 	 global-step:24922	 l-p:-0.09847300499677658
epoch£º1246	 i:3 	 global-step:24923	 l-p:0.13145312666893005
epoch£º1246	 i:4 	 global-step:24924	 l-p:0.1217513307929039
epoch£º1246	 i:5 	 global-step:24925	 l-p:0.19359630346298218
epoch£º1246	 i:6 	 global-step:24926	 l-p:0.11502144485712051
epoch£º1246	 i:7 	 global-step:24927	 l-p:-0.3028374910354614
epoch£º1246	 i:8 	 global-step:24928	 l-p:-0.0015334223862737417
epoch£º1246	 i:9 	 global-step:24929	 l-p:0.13817471265792847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7055, 3.0000, 2.8037],
        [3.7055, 3.7055, 3.7055],
        [3.7055, 3.3652, 3.4880],
        [3.7055, 3.0553, 2.4035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): -0.0088112261146307 
model_pd.l_d.mean(): -23.191999435424805 
model_pd.lagr.mean(): -23.2008113861084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2166], device='cuda:0')), ('power', tensor([-23.4086], device='cuda:0'))])
epoch£º1247	 i:0 	 global-step:24940	 l-p:-0.0088112261146307
epoch£º1247	 i:1 	 global-step:24941	 l-p:0.1044699102640152
epoch£º1247	 i:2 	 global-step:24942	 l-p:-0.2898828387260437
epoch£º1247	 i:3 	 global-step:24943	 l-p:0.16476674377918243
epoch£º1247	 i:4 	 global-step:24944	 l-p:0.2189863920211792
epoch£º1247	 i:5 	 global-step:24945	 l-p:0.11987458914518356
epoch£º1247	 i:6 	 global-step:24946	 l-p:0.13212238252162933
epoch£º1247	 i:7 	 global-step:24947	 l-p:0.13142962753772736
epoch£º1247	 i:8 	 global-step:24948	 l-p:0.13528163731098175
epoch£º1247	 i:9 	 global-step:24949	 l-p:0.013119425624608994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7053, 3.5346, 3.6434],
        [3.7053, 3.7053, 3.7053],
        [3.7053, 3.0771, 2.9973],
        [3.7053, 2.9820, 2.7531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.1331215798854828 
model_pd.l_d.mean(): -23.80687713623047 
model_pd.lagr.mean(): -23.673755645751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1097], device='cuda:0')), ('power', tensor([-23.9166], device='cuda:0'))])
epoch£º1248	 i:0 	 global-step:24960	 l-p:0.1331215798854828
epoch£º1248	 i:1 	 global-step:24961	 l-p:0.14068220555782318
epoch£º1248	 i:2 	 global-step:24962	 l-p:0.12384556233882904
epoch£º1248	 i:3 	 global-step:24963	 l-p:0.18412978947162628
epoch£º1248	 i:4 	 global-step:24964	 l-p:0.1181948184967041
epoch£º1248	 i:5 	 global-step:24965	 l-p:0.07878853380680084
epoch£º1248	 i:6 	 global-step:24966	 l-p:0.12822993099689484
epoch£º1248	 i:7 	 global-step:24967	 l-p:0.1389436274766922
epoch£º1248	 i:8 	 global-step:24968	 l-p:0.13391394913196564
epoch£º1248	 i:9 	 global-step:24969	 l-p:-0.21589164435863495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7013, 2.9339, 2.6123],
        [3.7013, 3.7013, 3.7013],
        [3.7013, 2.8869, 2.4232],
        [3.7013, 3.7013, 3.7013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.12090285867452621 
model_pd.l_d.mean(): -23.361133575439453 
model_pd.lagr.mean(): -23.240230560302734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2283], device='cuda:0')), ('power', tensor([-23.5894], device='cuda:0'))])
epoch£º1249	 i:0 	 global-step:24980	 l-p:0.12090285867452621
epoch£º1249	 i:1 	 global-step:24981	 l-p:0.14731068909168243
epoch£º1249	 i:2 	 global-step:24982	 l-p:0.1869957447052002
epoch£º1249	 i:3 	 global-step:24983	 l-p:0.1249581128358841
epoch£º1249	 i:4 	 global-step:24984	 l-p:-0.036314502358436584
epoch£º1249	 i:5 	 global-step:24985	 l-p:0.14097994565963745
epoch£º1249	 i:6 	 global-step:24986	 l-p:-0.29164135456085205
epoch£º1249	 i:7 	 global-step:24987	 l-p:0.15169109404087067
epoch£º1249	 i:8 	 global-step:24988	 l-p:0.19831408560276031
epoch£º1249	 i:9 	 global-step:24989	 l-p:0.13669098913669586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1828,  0.1038,  1.0000,  0.0589,
          1.0000,  0.5675, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1353,  0.0695,  1.0000,  0.0357,
          1.0000,  0.5134, 31.6228]], device='cuda:0')
 pt:tensor([[3.7044, 3.2862, 3.3842],
        [3.7044, 3.2276, 3.2930],
        [3.7044, 3.4053, 3.5333],
        [3.7044, 3.4197, 3.5481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.13015085458755493 
model_pd.l_d.mean(): -23.417280197143555 
model_pd.lagr.mean(): -23.287128448486328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1891], device='cuda:0')), ('power', tensor([-23.6063], device='cuda:0'))])
epoch£º1250	 i:0 	 global-step:25000	 l-p:0.13015085458755493
epoch£º1250	 i:1 	 global-step:25001	 l-p:0.13269001245498657
epoch£º1250	 i:2 	 global-step:25002	 l-p:0.13306869566440582
epoch£º1250	 i:3 	 global-step:25003	 l-p:-0.01193316001445055
epoch£º1250	 i:4 	 global-step:25004	 l-p:0.1663038730621338
epoch£º1250	 i:5 	 global-step:25005	 l-p:0.15271306037902832
epoch£º1250	 i:6 	 global-step:25006	 l-p:0.22644151747226715
epoch£º1250	 i:7 	 global-step:25007	 l-p:0.10739665478467941
epoch£º1250	 i:8 	 global-step:25008	 l-p:-0.1238793358206749
epoch£º1250	 i:9 	 global-step:25009	 l-p:0.04100380465388298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7017, 3.0729, 2.9928],
        [3.7017, 3.7002, 3.7017],
        [3.7017, 3.7017, 3.7017],
        [3.7017, 3.2835, 3.3815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.14745087921619415 
model_pd.l_d.mean(): -23.223356246948242 
model_pd.lagr.mean(): -23.075904846191406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2386], device='cuda:0')), ('power', tensor([-23.4620], device='cuda:0'))])
epoch£º1251	 i:0 	 global-step:25020	 l-p:0.14745087921619415
epoch£º1251	 i:1 	 global-step:25021	 l-p:0.12950094044208527
epoch£º1251	 i:2 	 global-step:25022	 l-p:0.13862372934818268
epoch£º1251	 i:3 	 global-step:25023	 l-p:0.02222188003361225
epoch£º1251	 i:4 	 global-step:25024	 l-p:0.14605708420276642
epoch£º1251	 i:5 	 global-step:25025	 l-p:0.10741814970970154
epoch£º1251	 i:6 	 global-step:25026	 l-p:0.1931285411119461
epoch£º1251	 i:7 	 global-step:25027	 l-p:0.14735578000545502
epoch£º1251	 i:8 	 global-step:25028	 l-p:0.1772056519985199
epoch£º1251	 i:9 	 global-step:25029	 l-p:-0.26665183901786804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7021, 3.4174, 3.5459],
        [3.7021, 2.8842, 2.2597],
        [3.7021, 3.7021, 3.7021],
        [3.7021, 2.8845, 2.4067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.13088668882846832 
model_pd.l_d.mean(): -23.251680374145508 
model_pd.lagr.mean(): -23.12079429626465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2632], device='cuda:0')), ('power', tensor([-23.5149], device='cuda:0'))])
epoch£º1252	 i:0 	 global-step:25040	 l-p:0.13088668882846832
epoch£º1252	 i:1 	 global-step:25041	 l-p:0.22164103388786316
epoch£º1252	 i:2 	 global-step:25042	 l-p:0.13393564522266388
epoch£º1252	 i:3 	 global-step:25043	 l-p:-0.014018477872014046
epoch£º1252	 i:4 	 global-step:25044	 l-p:-0.15938062965869904
epoch£º1252	 i:5 	 global-step:25045	 l-p:0.15235869586467743
epoch£º1252	 i:6 	 global-step:25046	 l-p:0.1624847799539566
epoch£º1252	 i:7 	 global-step:25047	 l-p:0.14672164618968964
epoch£º1252	 i:8 	 global-step:25048	 l-p:-0.0023109721951186657
epoch£º1252	 i:9 	 global-step:25049	 l-p:0.11929389834403992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7074, 3.7075, 3.7075],
        [3.7074, 3.7064, 3.7074],
        [3.7074, 3.7075, 3.7075],
        [3.7074, 3.7075, 3.7075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.12285073846578598 
model_pd.l_d.mean(): -23.27850914001465 
model_pd.lagr.mean(): -23.155658721923828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1486], device='cuda:0')), ('power', tensor([-23.4272], device='cuda:0'))])
epoch£º1253	 i:0 	 global-step:25060	 l-p:0.12285073846578598
epoch£º1253	 i:1 	 global-step:25061	 l-p:0.20452167093753815
epoch£º1253	 i:2 	 global-step:25062	 l-p:0.1271974891424179
epoch£º1253	 i:3 	 global-step:25063	 l-p:0.13926143944263458
epoch£º1253	 i:4 	 global-step:25064	 l-p:-0.05634796991944313
epoch£º1253	 i:5 	 global-step:25065	 l-p:0.11116090416908264
epoch£º1253	 i:6 	 global-step:25066	 l-p:-0.03621598333120346
epoch£º1253	 i:7 	 global-step:25067	 l-p:-0.5014713406562805
epoch£º1253	 i:8 	 global-step:25068	 l-p:0.1095828115940094
epoch£º1253	 i:9 	 global-step:25069	 l-p:0.19400107860565186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7087, 3.1645, 2.5283],
        [3.7087, 3.3684, 3.4912],
        [3.7087, 3.7031, 3.7085],
        [3.7087, 2.9881, 2.7640]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.11033239960670471 
model_pd.l_d.mean(): -22.740291595458984 
model_pd.lagr.mean(): -22.629959106445312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2777], device='cuda:0')), ('power', tensor([-23.0180], device='cuda:0'))])
epoch£º1254	 i:0 	 global-step:25080	 l-p:0.11033239960670471
epoch£º1254	 i:1 	 global-step:25081	 l-p:0.22671516239643097
epoch£º1254	 i:2 	 global-step:25082	 l-p:-0.8754916787147522
epoch£º1254	 i:3 	 global-step:25083	 l-p:-0.27224287390708923
epoch£º1254	 i:4 	 global-step:25084	 l-p:0.09801379591226578
epoch£º1254	 i:5 	 global-step:25085	 l-p:0.1193251758813858
epoch£º1254	 i:6 	 global-step:25086	 l-p:0.15972524881362915
epoch£º1254	 i:7 	 global-step:25087	 l-p:0.16737334430217743
epoch£º1254	 i:8 	 global-step:25088	 l-p:0.15674631297588348
epoch£º1254	 i:9 	 global-step:25089	 l-p:0.12628817558288574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7109, 3.3537, 3.4727],
        [3.7109, 3.0207, 2.8497],
        [3.7109, 3.4569, 3.5840],
        [3.7109, 3.7109, 3.7109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.17351828515529633 
model_pd.l_d.mean(): -22.687053680419922 
model_pd.lagr.mean(): -22.513534545898438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3051], device='cuda:0')), ('power', tensor([-22.9921], device='cuda:0'))])
epoch£º1255	 i:0 	 global-step:25100	 l-p:0.17351828515529633
epoch£º1255	 i:1 	 global-step:25101	 l-p:0.13016124069690704
epoch£º1255	 i:2 	 global-step:25102	 l-p:-0.12792252004146576
epoch£º1255	 i:3 	 global-step:25103	 l-p:0.15738359093666077
epoch£º1255	 i:4 	 global-step:25104	 l-p:-2.188807725906372
epoch£º1255	 i:5 	 global-step:25105	 l-p:0.14351825416088104
epoch£º1255	 i:6 	 global-step:25106	 l-p:0.14580939710140228
epoch£º1255	 i:7 	 global-step:25107	 l-p:0.16498973965644836
epoch£º1255	 i:8 	 global-step:25108	 l-p:0.1305495798587799
epoch£º1255	 i:9 	 global-step:25109	 l-p:-0.061459045857191086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7114, 3.5191, 3.6347],
        [3.7114, 3.1064, 3.0547],
        [3.7114, 3.1815, 2.5474],
        [3.7114, 3.6959, 3.7103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.1393294334411621 
model_pd.l_d.mean(): -23.49762535095215 
model_pd.lagr.mean(): -23.358295440673828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1684], device='cuda:0')), ('power', tensor([-23.6660], device='cuda:0'))])
epoch£º1256	 i:0 	 global-step:25120	 l-p:0.1393294334411621
epoch£º1256	 i:1 	 global-step:25121	 l-p:0.20549702644348145
epoch£º1256	 i:2 	 global-step:25122	 l-p:0.14053447544574738
epoch£º1256	 i:3 	 global-step:25123	 l-p:-0.18518389761447906
epoch£º1256	 i:4 	 global-step:25124	 l-p:0.13545121252536774
epoch£º1256	 i:5 	 global-step:25125	 l-p:0.13829608261585236
epoch£º1256	 i:6 	 global-step:25126	 l-p:5.188840866088867
epoch£º1256	 i:7 	 global-step:25127	 l-p:0.1273333728313446
epoch£º1256	 i:8 	 global-step:25128	 l-p:0.13436652719974518
epoch£º1256	 i:9 	 global-step:25129	 l-p:-0.010642938315868378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7111, 3.0409, 2.9017],
        [3.7111, 2.9208, 2.2717],
        [3.7111, 3.7104, 3.7111],
        [3.7111, 3.7111, 3.7111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): -0.05283466354012489 
model_pd.l_d.mean(): -23.431365966796875 
model_pd.lagr.mean(): -23.484201431274414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2118], device='cuda:0')), ('power', tensor([-23.6432], device='cuda:0'))])
epoch£º1257	 i:0 	 global-step:25140	 l-p:-0.05283466354012489
epoch£º1257	 i:1 	 global-step:25141	 l-p:0.13497115671634674
epoch£º1257	 i:2 	 global-step:25142	 l-p:0.16794371604919434
epoch£º1257	 i:3 	 global-step:25143	 l-p:0.14496076107025146
epoch£º1257	 i:4 	 global-step:25144	 l-p:0.1507277488708496
epoch£º1257	 i:5 	 global-step:25145	 l-p:-0.20377105474472046
epoch£º1257	 i:6 	 global-step:25146	 l-p:0.11720658093690872
epoch£º1257	 i:7 	 global-step:25147	 l-p:0.17862269282341003
epoch£º1257	 i:8 	 global-step:25148	 l-p:1.3704001903533936
epoch£º1257	 i:9 	 global-step:25149	 l-p:0.13055923581123352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7148, 3.7141, 3.7148],
        [3.7148, 3.7148, 3.7148],
        [3.7148, 3.1663, 3.1741],
        [3.7148, 2.9097, 2.4755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.1319798082113266 
model_pd.l_d.mean(): -23.691097259521484 
model_pd.lagr.mean(): -23.559118270874023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1514], device='cuda:0')), ('power', tensor([-23.8425], device='cuda:0'))])
epoch£º1258	 i:0 	 global-step:25160	 l-p:0.1319798082113266
epoch£º1258	 i:1 	 global-step:25161	 l-p:0.17526809871196747
epoch£º1258	 i:2 	 global-step:25162	 l-p:0.14342880249023438
epoch£º1258	 i:3 	 global-step:25163	 l-p:0.36799710988998413
epoch£º1258	 i:4 	 global-step:25164	 l-p:0.11749589443206787
epoch£º1258	 i:5 	 global-step:25165	 l-p:0.15855401754379272
epoch£º1258	 i:6 	 global-step:25166	 l-p:-0.03747616335749626
epoch£º1258	 i:7 	 global-step:25167	 l-p:0.10689576715230942
epoch£º1258	 i:8 	 global-step:25168	 l-p:0.12118014693260193
epoch£º1258	 i:9 	 global-step:25169	 l-p:0.12022987008094788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7139, 3.7060, 3.7135],
        [3.7139, 3.1085, 3.0562],
        [3.7139, 3.7063, 3.7135],
        [3.7139, 2.8979, 2.2719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.1717589944601059 
model_pd.l_d.mean(): -23.197595596313477 
model_pd.lagr.mean(): -23.025836944580078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2686], device='cuda:0')), ('power', tensor([-23.4662], device='cuda:0'))])
epoch£º1259	 i:0 	 global-step:25180	 l-p:0.1717589944601059
epoch£º1259	 i:1 	 global-step:25181	 l-p:0.13828535377979279
epoch£º1259	 i:2 	 global-step:25182	 l-p:0.11466728895902634
epoch£º1259	 i:3 	 global-step:25183	 l-p:-0.06490179151296616
epoch£º1259	 i:4 	 global-step:25184	 l-p:0.205606147646904
epoch£º1259	 i:5 	 global-step:25185	 l-p:-0.20194551348686218
epoch£º1259	 i:6 	 global-step:25186	 l-p:2.622062921524048
epoch£º1259	 i:7 	 global-step:25187	 l-p:0.12872205674648285
epoch£º1259	 i:8 	 global-step:25188	 l-p:0.13719013333320618
epoch£º1259	 i:9 	 global-step:25189	 l-p:0.1258063018321991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7133, 3.7054, 3.7129],
        [3.7133, 3.1493, 3.1423],
        [3.7133, 3.7133, 3.7133],
        [3.7133, 3.5520, 3.6573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): -0.06064426898956299 
model_pd.l_d.mean(): -23.457271575927734 
model_pd.lagr.mean(): -23.517915725708008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1913], device='cuda:0')), ('power', tensor([-23.6485], device='cuda:0'))])
epoch£º1260	 i:0 	 global-step:25200	 l-p:-0.06064426898956299
epoch£º1260	 i:1 	 global-step:25201	 l-p:0.13251657783985138
epoch£º1260	 i:2 	 global-step:25202	 l-p:0.14255499839782715
epoch£º1260	 i:3 	 global-step:25203	 l-p:0.11441456526517868
epoch£º1260	 i:4 	 global-step:25204	 l-p:0.13097934424877167
epoch£º1260	 i:5 	 global-step:25205	 l-p:0.12309367954730988
epoch£º1260	 i:6 	 global-step:25206	 l-p:0.14426152408123016
epoch£º1260	 i:7 	 global-step:25207	 l-p:0.1460525393486023
epoch£º1260	 i:8 	 global-step:25208	 l-p:-0.6203776597976685
epoch£º1260	 i:9 	 global-step:25209	 l-p:-0.08803844451904297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7082, 3.7082, 3.7082],
        [3.7082, 3.7080, 3.7082],
        [3.7082, 3.6777, 3.7050],
        [3.7082, 3.6996, 3.7078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): -0.5531884431838989 
model_pd.l_d.mean(): -23.67837142944336 
model_pd.lagr.mean(): -24.23155975341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1502], device='cuda:0')), ('power', tensor([-23.8286], device='cuda:0'))])
epoch£º1261	 i:0 	 global-step:25220	 l-p:-0.5531884431838989
epoch£º1261	 i:1 	 global-step:25221	 l-p:-0.05880645290017128
epoch£º1261	 i:2 	 global-step:25222	 l-p:0.1547534316778183
epoch£º1261	 i:3 	 global-step:25223	 l-p:0.0999012365937233
epoch£º1261	 i:4 	 global-step:25224	 l-p:0.16416560113430023
epoch£º1261	 i:5 	 global-step:25225	 l-p:0.14334236085414886
epoch£º1261	 i:6 	 global-step:25226	 l-p:0.19607554376125336
epoch£º1261	 i:7 	 global-step:25227	 l-p:0.11391215771436691
epoch£º1261	 i:8 	 global-step:25228	 l-p:0.13066110014915466
epoch£º1261	 i:9 	 global-step:25229	 l-p:-0.044496603310108185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7113, 2.9943, 2.3357],
        [3.7113, 3.6668, 3.7052],
        [3.7113, 2.9932, 2.3346],
        [3.7113, 2.9194, 2.2707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.14669561386108398 
model_pd.l_d.mean(): -22.955965042114258 
model_pd.lagr.mean(): -22.809268951416016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2314], device='cuda:0')), ('power', tensor([-23.1874], device='cuda:0'))])
epoch£º1262	 i:0 	 global-step:25240	 l-p:0.14669561386108398
epoch£º1262	 i:1 	 global-step:25241	 l-p:0.14093616604804993
epoch£º1262	 i:2 	 global-step:25242	 l-p:-0.11844827979803085
epoch£º1262	 i:3 	 global-step:25243	 l-p:0.12921223044395447
epoch£º1262	 i:4 	 global-step:25244	 l-p:0.10176866501569748
epoch£º1262	 i:5 	 global-step:25245	 l-p:-0.04042370244860649
epoch£º1262	 i:6 	 global-step:25246	 l-p:0.20032496750354767
epoch£º1262	 i:7 	 global-step:25247	 l-p:0.18493536114692688
epoch£º1262	 i:8 	 global-step:25248	 l-p:-1.4281792640686035
epoch£º1262	 i:9 	 global-step:25249	 l-p:0.13854338228702545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7114, 3.0746, 2.4234],
        [3.7114, 3.6222, 3.6916],
        [3.7114, 3.5299, 3.6422],
        [3.7114, 3.3943, 3.5207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): -0.2936844825744629 
model_pd.l_d.mean(): -22.538860321044922 
model_pd.lagr.mean(): -22.832544326782227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2668], device='cuda:0')), ('power', tensor([-22.8057], device='cuda:0'))])
epoch£º1263	 i:0 	 global-step:25260	 l-p:-0.2936844825744629
epoch£º1263	 i:1 	 global-step:25261	 l-p:19.830448150634766
epoch£º1263	 i:2 	 global-step:25262	 l-p:0.13079680502414703
epoch£º1263	 i:3 	 global-step:25263	 l-p:0.10065137594938278
epoch£º1263	 i:4 	 global-step:25264	 l-p:0.15276159346103668
epoch£º1263	 i:5 	 global-step:25265	 l-p:0.15753202140331268
epoch£º1263	 i:6 	 global-step:25266	 l-p:0.1359390914440155
epoch£º1263	 i:7 	 global-step:25267	 l-p:0.09985807538032532
epoch£º1263	 i:8 	 global-step:25268	 l-p:0.19472245872020721
epoch£º1263	 i:9 	 global-step:25269	 l-p:0.14141643047332764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7125, 2.9409, 2.6071],
        [3.7125, 3.2701, 3.3559],
        [3.7125, 3.7125, 3.7125],
        [3.7125, 3.7125, 3.7125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.16262614727020264 
model_pd.l_d.mean(): -22.698768615722656 
model_pd.lagr.mean(): -22.536142349243164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2676], device='cuda:0')), ('power', tensor([-22.9663], device='cuda:0'))])
epoch£º1264	 i:0 	 global-step:25280	 l-p:0.16262614727020264
epoch£º1264	 i:1 	 global-step:25281	 l-p:0.21166808903217316
epoch£º1264	 i:2 	 global-step:25282	 l-p:0.12716397643089294
epoch£º1264	 i:3 	 global-step:25283	 l-p:0.13632416725158691
epoch£º1264	 i:4 	 global-step:25284	 l-p:0.12204355001449585
epoch£º1264	 i:5 	 global-step:25285	 l-p:-0.07301405817270279
epoch£º1264	 i:6 	 global-step:25286	 l-p:0.14648501574993134
epoch£º1264	 i:7 	 global-step:25287	 l-p:-1.5768013000488281
epoch£º1264	 i:8 	 global-step:25288	 l-p:0.12644261121749878
epoch£º1264	 i:9 	 global-step:25289	 l-p:0.15820686519145966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7093, 3.7000, 3.7088],
        [3.7093, 3.2846, 3.3795],
        [3.7093, 3.6102, 3.6855],
        [3.7093, 3.7093, 3.7093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.10224149376153946 
model_pd.l_d.mean(): -22.809049606323242 
model_pd.lagr.mean(): -22.70680809020996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2713], device='cuda:0')), ('power', tensor([-23.0804], device='cuda:0'))])
epoch£º1265	 i:0 	 global-step:25300	 l-p:0.10224149376153946
epoch£º1265	 i:1 	 global-step:25301	 l-p:0.12961414456367493
epoch£º1265	 i:2 	 global-step:25302	 l-p:0.13558058440685272
epoch£º1265	 i:3 	 global-step:25303	 l-p:0.13500119745731354
epoch£º1265	 i:4 	 global-step:25304	 l-p:0.1589631885290146
epoch£º1265	 i:5 	 global-step:25305	 l-p:-0.03639878332614899
epoch£º1265	 i:6 	 global-step:25306	 l-p:0.027116430923342705
epoch£º1265	 i:7 	 global-step:25307	 l-p:0.15901680290699005
epoch£º1265	 i:8 	 global-step:25308	 l-p:-0.16637565195560455
epoch£º1265	 i:9 	 global-step:25309	 l-p:0.19081923365592957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1995,  0.1166,  1.0000,  0.0681,
          1.0000,  0.5843, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2321,  0.1426,  1.0000,  0.0876,
          1.0000,  0.6145, 31.6228]], device='cuda:0')
 pt:tensor([[3.7046, 3.2421, 3.3169],
        [3.7046, 3.4054, 3.5334],
        [3.7046, 3.2815, 3.3774],
        [3.7046, 3.1625, 3.1771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.14228889346122742 
model_pd.l_d.mean(): -23.13096809387207 
model_pd.lagr.mean(): -22.988679885864258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2242], device='cuda:0')), ('power', tensor([-23.3551], device='cuda:0'))])
epoch£º1266	 i:0 	 global-step:25320	 l-p:0.14228889346122742
epoch£º1266	 i:1 	 global-step:25321	 l-p:0.13122068345546722
epoch£º1266	 i:2 	 global-step:25322	 l-p:0.12998656928539276
epoch£º1266	 i:3 	 global-step:25323	 l-p:0.11256842315196991
epoch£º1266	 i:4 	 global-step:25324	 l-p:-0.02920667640864849
epoch£º1266	 i:5 	 global-step:25325	 l-p:0.046569306403398514
epoch£º1266	 i:6 	 global-step:25326	 l-p:0.1575309783220291
epoch£º1266	 i:7 	 global-step:25327	 l-p:-0.06476998329162598
epoch£º1266	 i:8 	 global-step:25328	 l-p:0.12546242773532867
epoch£º1266	 i:9 	 global-step:25329	 l-p:0.18729670345783234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7037, 3.1130, 3.0789],
        [3.7037, 3.7027, 3.7037],
        [3.7037, 3.6922, 3.7030],
        [3.7037, 3.7037, 3.7037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.11424411833286285 
model_pd.l_d.mean(): -23.681861877441406 
model_pd.lagr.mean(): -23.567617416381836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1665], device='cuda:0')), ('power', tensor([-23.8483], device='cuda:0'))])
epoch£º1267	 i:0 	 global-step:25340	 l-p:0.11424411833286285
epoch£º1267	 i:1 	 global-step:25341	 l-p:0.11979036033153534
epoch£º1267	 i:2 	 global-step:25342	 l-p:0.12405256181955338
epoch£º1267	 i:3 	 global-step:25343	 l-p:0.054013580083847046
epoch£º1267	 i:4 	 global-step:25344	 l-p:-0.13077548146247864
epoch£º1267	 i:5 	 global-step:25345	 l-p:0.13482943177223206
epoch£º1267	 i:6 	 global-step:25346	 l-p:0.13465441763401031
epoch£º1267	 i:7 	 global-step:25347	 l-p:0.11497237533330917
epoch£º1267	 i:8 	 global-step:25348	 l-p:0.21332763135433197
epoch£º1267	 i:9 	 global-step:25349	 l-p:0.03407461941242218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7041, 3.7011, 3.7040],
        [3.7041, 3.3089, 3.4166],
        [3.7041, 3.7041, 3.7041],
        [3.7041, 3.1662, 3.1847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.12408019602298737 
model_pd.l_d.mean(): -22.88443946838379 
model_pd.lagr.mean(): -22.760358810424805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1937], device='cuda:0')), ('power', tensor([-23.0781], device='cuda:0'))])
epoch£º1268	 i:0 	 global-step:25360	 l-p:0.12408019602298737
epoch£º1268	 i:1 	 global-step:25361	 l-p:0.12513437867164612
epoch£º1268	 i:2 	 global-step:25362	 l-p:-0.1679678112268448
epoch£º1268	 i:3 	 global-step:25363	 l-p:0.1427055448293686
epoch£º1268	 i:4 	 global-step:25364	 l-p:0.16280564665794373
epoch£º1268	 i:5 	 global-step:25365	 l-p:0.10144694894552231
epoch£º1268	 i:6 	 global-step:25366	 l-p:0.14008289575576782
epoch£º1268	 i:7 	 global-step:25367	 l-p:0.14491470158100128
epoch£º1268	 i:8 	 global-step:25368	 l-p:0.1287544071674347
epoch£º1268	 i:9 	 global-step:25369	 l-p:-0.0431753434240818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7053, 3.3520, 3.4723],
        [3.7053, 3.6912, 3.7044],
        [3.7053, 3.7053, 3.7053],
        [3.7053, 3.7052, 3.7053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.05910611152648926 
model_pd.l_d.mean(): -23.338863372802734 
model_pd.lagr.mean(): -23.279756546020508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2053], device='cuda:0')), ('power', tensor([-23.5441], device='cuda:0'))])
epoch£º1269	 i:0 	 global-step:25380	 l-p:0.05910611152648926
epoch£º1269	 i:1 	 global-step:25381	 l-p:0.11856380105018616
epoch£º1269	 i:2 	 global-step:25382	 l-p:0.17242321372032166
epoch£º1269	 i:3 	 global-step:25383	 l-p:-0.5005640387535095
epoch£º1269	 i:4 	 global-step:25384	 l-p:-0.026716308668255806
epoch£º1269	 i:5 	 global-step:25385	 l-p:0.14256839454174042
epoch£º1269	 i:6 	 global-step:25386	 l-p:0.13314937055110931
epoch£º1269	 i:7 	 global-step:25387	 l-p:0.1329113394021988
epoch£º1269	 i:8 	 global-step:25388	 l-p:0.12433155626058578
epoch£º1269	 i:9 	 global-step:25389	 l-p:0.12844982743263245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7127, 3.0633, 2.4103],
        [3.7127, 3.7106, 3.7126],
        [3.7127, 2.9532, 2.6485],
        [3.7127, 2.9222, 2.5375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.16817858815193176 
model_pd.l_d.mean(): -23.47816276550293 
model_pd.lagr.mean(): -23.30998420715332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1946], device='cuda:0')), ('power', tensor([-23.6728], device='cuda:0'))])
epoch£º1270	 i:0 	 global-step:25400	 l-p:0.16817858815193176
epoch£º1270	 i:1 	 global-step:25401	 l-p:0.13350656628608704
epoch£º1270	 i:2 	 global-step:25402	 l-p:0.18174417316913605
epoch£º1270	 i:3 	 global-step:25403	 l-p:0.13755393028259277
epoch£º1270	 i:4 	 global-step:25404	 l-p:-11.155105590820312
epoch£º1270	 i:5 	 global-step:25405	 l-p:-0.14571958780288696
epoch£º1270	 i:6 	 global-step:25406	 l-p:0.14303411543369293
epoch£º1270	 i:7 	 global-step:25407	 l-p:0.1225200816988945
epoch£º1270	 i:8 	 global-step:25408	 l-p:0.10370181500911713
epoch£º1270	 i:9 	 global-step:25409	 l-p:0.12216547876596451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7104, 2.9831, 2.3243],
        [3.7104, 2.8892, 2.2717],
        [3.7104, 3.6133, 3.6874],
        [3.7104, 3.0195, 2.8485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.1250859797000885 
model_pd.l_d.mean(): -22.819072723388672 
model_pd.lagr.mean(): -22.693986892700195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2499], device='cuda:0')), ('power', tensor([-23.0689], device='cuda:0'))])
epoch£º1271	 i:0 	 global-step:25420	 l-p:0.1250859797000885
epoch£º1271	 i:1 	 global-step:25421	 l-p:0.13800965249538422
epoch£º1271	 i:2 	 global-step:25422	 l-p:0.13259634375572205
epoch£º1271	 i:3 	 global-step:25423	 l-p:0.11598897725343704
epoch£º1271	 i:4 	 global-step:25424	 l-p:-0.033047035336494446
epoch£º1271	 i:5 	 global-step:25425	 l-p:0.13911396265029907
epoch£º1271	 i:6 	 global-step:25426	 l-p:0.15531587600708008
epoch£º1271	 i:7 	 global-step:25427	 l-p:0.21457239985466003
epoch£º1271	 i:8 	 global-step:25428	 l-p:0.2168327271938324
epoch£º1271	 i:9 	 global-step:25429	 l-p:-0.16526788473129272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228]], device='cuda:0')
 pt:tensor([[3.6978, 2.8725, 2.2618],
        [3.6978, 2.9090, 2.5342],
        [3.6978, 2.8779, 2.3953],
        [3.6978, 3.1188, 2.4782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): -0.05534342676401138 
model_pd.l_d.mean(): -23.238420486450195 
model_pd.lagr.mean(): -23.293764114379883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2945], device='cuda:0')), ('power', tensor([-23.5329], device='cuda:0'))])
epoch£º1272	 i:0 	 global-step:25440	 l-p:-0.05534342676401138
epoch£º1272	 i:1 	 global-step:25441	 l-p:-0.06037519499659538
epoch£º1272	 i:2 	 global-step:25442	 l-p:0.1203678697347641
epoch£º1272	 i:3 	 global-step:25443	 l-p:0.19356711208820343
epoch£º1272	 i:4 	 global-step:25444	 l-p:0.2012556791305542
epoch£º1272	 i:5 	 global-step:25445	 l-p:0.1110081821680069
epoch£º1272	 i:6 	 global-step:25446	 l-p:0.13111215829849243
epoch£º1272	 i:7 	 global-step:25447	 l-p:0.16846659779548645
epoch£º1272	 i:8 	 global-step:25448	 l-p:0.13193313777446747
epoch£º1272	 i:9 	 global-step:25449	 l-p:0.15272113680839539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6999, 2.9412, 2.6417],
        [3.6999, 3.5975, 3.6747],
        [3.6999, 3.6993, 3.6999],
        [3.6999, 3.6916, 3.6996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): -0.07570092380046844 
model_pd.l_d.mean(): -23.105329513549805 
model_pd.lagr.mean(): -23.1810302734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2956], device='cuda:0')), ('power', tensor([-23.4009], device='cuda:0'))])
epoch£º1273	 i:0 	 global-step:25460	 l-p:-0.07570092380046844
epoch£º1273	 i:1 	 global-step:25461	 l-p:0.15616880357265472
epoch£º1273	 i:2 	 global-step:25462	 l-p:0.15165221691131592
epoch£º1273	 i:3 	 global-step:25463	 l-p:0.1405215561389923
epoch£º1273	 i:4 	 global-step:25464	 l-p:0.10845956206321716
epoch£º1273	 i:5 	 global-step:25465	 l-p:0.19173762202262878
epoch£º1273	 i:6 	 global-step:25466	 l-p:0.18961173295974731
epoch£º1273	 i:7 	 global-step:25467	 l-p:0.12362401932477951
epoch£º1273	 i:8 	 global-step:25468	 l-p:-0.08500266075134277
epoch£º1273	 i:9 	 global-step:25469	 l-p:0.1383339911699295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7004, 3.7004, 3.7004],
        [3.7004, 3.3469, 3.4673],
        [3.7004, 3.2751, 3.3702],
        [3.7004, 3.6964, 3.7002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.05067965388298035 
model_pd.l_d.mean(): -23.21746253967285 
model_pd.lagr.mean(): -23.16678237915039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2084], device='cuda:0')), ('power', tensor([-23.4259], device='cuda:0'))])
epoch£º1274	 i:0 	 global-step:25480	 l-p:0.05067965388298035
epoch£º1274	 i:1 	 global-step:25481	 l-p:-0.08365508168935776
epoch£º1274	 i:2 	 global-step:25482	 l-p:0.011859149672091007
epoch£º1274	 i:3 	 global-step:25483	 l-p:0.0967361256480217
epoch£º1274	 i:4 	 global-step:25484	 l-p:0.1334703415632248
epoch£º1274	 i:5 	 global-step:25485	 l-p:0.12430920451879501
epoch£º1274	 i:6 	 global-step:25486	 l-p:0.2643386721611023
epoch£º1274	 i:7 	 global-step:25487	 l-p:0.1328296661376953
epoch£º1274	 i:8 	 global-step:25488	 l-p:0.15245045721530914
epoch£º1274	 i:9 	 global-step:25489	 l-p:0.125319242477417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7026, 3.4580, 3.5843],
        [3.7026, 3.6991, 3.7025],
        [3.7026, 3.7014, 3.7026],
        [3.7026, 3.2256, 2.6026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.14864444732666016 
model_pd.l_d.mean(): -23.31349754333496 
model_pd.lagr.mean(): -23.164852142333984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2351], device='cuda:0')), ('power', tensor([-23.5486], device='cuda:0'))])
epoch£º1275	 i:0 	 global-step:25500	 l-p:0.14864444732666016
epoch£º1275	 i:1 	 global-step:25501	 l-p:0.0020146083552390337
epoch£º1275	 i:2 	 global-step:25502	 l-p:0.19485202431678772
epoch£º1275	 i:3 	 global-step:25503	 l-p:0.12859728932380676
epoch£º1275	 i:4 	 global-step:25504	 l-p:0.11442024260759354
epoch£º1275	 i:5 	 global-step:25505	 l-p:-0.15975001454353333
epoch£º1275	 i:6 	 global-step:25506	 l-p:0.0183612909168005
epoch£º1275	 i:7 	 global-step:25507	 l-p:0.13224907219409943
epoch£º1275	 i:8 	 global-step:25508	 l-p:0.17437560856342316
epoch£º1275	 i:9 	 global-step:25509	 l-p:0.1312880516052246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7062, 3.7053, 3.7062],
        [3.7062, 2.8832, 2.2684],
        [3.7062, 2.8978, 2.2597],
        [3.7062, 3.0126, 2.8379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): -0.0033640670590102673 
model_pd.l_d.mean(): -23.508241653442383 
model_pd.lagr.mean(): -23.511606216430664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2230], device='cuda:0')), ('power', tensor([-23.7313], device='cuda:0'))])
epoch£º1276	 i:0 	 global-step:25520	 l-p:-0.0033640670590102673
epoch£º1276	 i:1 	 global-step:25521	 l-p:0.1305294632911682
epoch£º1276	 i:2 	 global-step:25522	 l-p:0.13319991528987885
epoch£º1276	 i:3 	 global-step:25523	 l-p:0.2098003327846527
epoch£º1276	 i:4 	 global-step:25524	 l-p:-0.7878811359405518
epoch£º1276	 i:5 	 global-step:25525	 l-p:0.1102784126996994
epoch£º1276	 i:6 	 global-step:25526	 l-p:0.10965313762426376
epoch£º1276	 i:7 	 global-step:25527	 l-p:0.149382546544075
epoch£º1276	 i:8 	 global-step:25528	 l-p:-0.17150333523750305
epoch£º1276	 i:9 	 global-step:25529	 l-p:0.14644721150398254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7139, 2.8859, 2.3276],
        [3.7139, 3.7113, 3.7139],
        [3.7139, 3.1736, 2.5371],
        [3.7139, 3.7130, 3.7139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.14723730087280273 
model_pd.l_d.mean(): -23.46133804321289 
model_pd.lagr.mean(): -23.31410026550293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2811], device='cuda:0')), ('power', tensor([-23.7425], device='cuda:0'))])
epoch£º1277	 i:0 	 global-step:25540	 l-p:0.14723730087280273
epoch£º1277	 i:1 	 global-step:25541	 l-p:0.11632850021123886
epoch£º1277	 i:2 	 global-step:25542	 l-p:0.13526496291160583
epoch£º1277	 i:3 	 global-step:25543	 l-p:0.1602841019630432
epoch£º1277	 i:4 	 global-step:25544	 l-p:-0.2370937168598175
epoch£º1277	 i:5 	 global-step:25545	 l-p:1.6039680242538452
epoch£º1277	 i:6 	 global-step:25546	 l-p:-0.09561330825090408
epoch£º1277	 i:7 	 global-step:25547	 l-p:0.15115195512771606
epoch£º1277	 i:8 	 global-step:25548	 l-p:0.19106759130954742
epoch£º1277	 i:9 	 global-step:25549	 l-p:0.14110732078552246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7196, 3.7196, 3.7196],
        [3.7196, 3.0027, 2.3427],
        [3.7196, 2.9419, 2.2871],
        [3.7196, 3.6930, 3.7170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.21934764087200165 
model_pd.l_d.mean(): -23.48917579650879 
model_pd.lagr.mean(): -23.26982879638672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1632], device='cuda:0')), ('power', tensor([-23.6524], device='cuda:0'))])
epoch£º1278	 i:0 	 global-step:25560	 l-p:0.21934764087200165
epoch£º1278	 i:1 	 global-step:25561	 l-p:0.1346179097890854
epoch£º1278	 i:2 	 global-step:25562	 l-p:0.10935508459806442
epoch£º1278	 i:3 	 global-step:25563	 l-p:0.3969717025756836
epoch£º1278	 i:4 	 global-step:25564	 l-p:0.1281425654888153
epoch£º1278	 i:5 	 global-step:25565	 l-p:0.14021317660808563
epoch£º1278	 i:6 	 global-step:25566	 l-p:1.5458523035049438
epoch£º1278	 i:7 	 global-step:25567	 l-p:0.125202476978302
epoch£º1278	 i:8 	 global-step:25568	 l-p:0.1325112283229828
epoch£º1278	 i:9 	 global-step:25569	 l-p:-0.854788064956665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7242, 3.5493, 3.6595],
        [3.7242, 3.7238, 3.7242],
        [3.7242, 2.9047, 2.2866],
        [3.7242, 3.7241, 3.7242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.11575303971767426 
model_pd.l_d.mean(): -22.031309127807617 
model_pd.lagr.mean(): -21.915555953979492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4258], device='cuda:0')), ('power', tensor([-22.4571], device='cuda:0'))])
epoch£º1279	 i:0 	 global-step:25580	 l-p:0.11575303971767426
epoch£º1279	 i:1 	 global-step:25581	 l-p:0.12633734941482544
epoch£º1279	 i:2 	 global-step:25582	 l-p:1.1525070667266846
epoch£º1279	 i:3 	 global-step:25583	 l-p:0.1170007660984993
epoch£º1279	 i:4 	 global-step:25584	 l-p:0.2321792095899582
epoch£º1279	 i:5 	 global-step:25585	 l-p:0.08958864212036133
epoch£º1279	 i:6 	 global-step:25586	 l-p:0.17983144521713257
epoch£º1279	 i:7 	 global-step:25587	 l-p:-1.2186815738677979
epoch£º1279	 i:8 	 global-step:25588	 l-p:0.12980274856090546
epoch£º1279	 i:9 	 global-step:25589	 l-p:0.12342170625925064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7250, 2.9591, 2.6370],
        [3.7250, 3.4261, 3.5539],
        [3.7250, 3.7199, 3.7248],
        [3.7250, 3.7220, 3.7249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): -1.6614201068878174 
model_pd.l_d.mean(): -23.060949325561523 
model_pd.lagr.mean(): -24.722370147705078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2520], device='cuda:0')), ('power', tensor([-23.3129], device='cuda:0'))])
epoch£º1280	 i:0 	 global-step:25600	 l-p:-1.6614201068878174
epoch£º1280	 i:1 	 global-step:25601	 l-p:0.15112902224063873
epoch£º1280	 i:2 	 global-step:25602	 l-p:0.12360614538192749
epoch£º1280	 i:3 	 global-step:25603	 l-p:0.12015654146671295
epoch£º1280	 i:4 	 global-step:25604	 l-p:0.10834087431430817
epoch£º1280	 i:5 	 global-step:25605	 l-p:1.7984826564788818
epoch£º1280	 i:6 	 global-step:25606	 l-p:0.13131050765514374
epoch£º1280	 i:7 	 global-step:25607	 l-p:0.16915187239646912
epoch£º1280	 i:8 	 global-step:25608	 l-p:0.43820253014564514
epoch£º1280	 i:9 	 global-step:25609	 l-p:0.11909092962741852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7201, 3.6815, 3.7153],
        [3.7201, 3.1302, 3.0959],
        [3.7201, 3.4585, 3.5863],
        [3.7201, 3.0918, 3.0114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.20490401983261108 
model_pd.l_d.mean(): -23.491436004638672 
model_pd.lagr.mean(): -23.286531448364258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1544], device='cuda:0')), ('power', tensor([-23.6459], device='cuda:0'))])
epoch£º1281	 i:0 	 global-step:25620	 l-p:0.20490401983261108
epoch£º1281	 i:1 	 global-step:25621	 l-p:0.12248870730400085
epoch£º1281	 i:2 	 global-step:25622	 l-p:0.12479574233293533
epoch£º1281	 i:3 	 global-step:25623	 l-p:0.13314403593540192
epoch£º1281	 i:4 	 global-step:25624	 l-p:0.587860107421875
epoch£º1281	 i:5 	 global-step:25625	 l-p:0.12225601822137833
epoch£º1281	 i:6 	 global-step:25626	 l-p:0.17073611915111542
epoch£º1281	 i:7 	 global-step:25627	 l-p:0.12187675386667252
epoch£º1281	 i:8 	 global-step:25628	 l-p:-0.6005721092224121
epoch£º1281	 i:9 	 global-step:25629	 l-p:0.12943512201309204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7151, 3.7149, 3.7151],
        [3.7151, 3.7110, 3.7149],
        [3.7151, 3.2074, 3.2507],
        [3.7151, 2.9163, 2.2705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.08882606029510498 
model_pd.l_d.mean(): -23.403364181518555 
model_pd.lagr.mean(): -23.314538955688477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2075], device='cuda:0')), ('power', tensor([-23.6109], device='cuda:0'))])
epoch£º1282	 i:0 	 global-step:25640	 l-p:0.08882606029510498
epoch£º1282	 i:1 	 global-step:25641	 l-p:-0.05563894286751747
epoch£º1282	 i:2 	 global-step:25642	 l-p:0.14091594517230988
epoch£º1282	 i:3 	 global-step:25643	 l-p:-0.1715179830789566
epoch£º1282	 i:4 	 global-step:25644	 l-p:0.18857894837856293
epoch£º1282	 i:5 	 global-step:25645	 l-p:0.12557870149612427
epoch£º1282	 i:6 	 global-step:25646	 l-p:0.15629354119300842
epoch£º1282	 i:7 	 global-step:25647	 l-p:239.4292449951172
epoch£º1282	 i:8 	 global-step:25648	 l-p:0.1550789475440979
epoch£º1282	 i:9 	 global-step:25649	 l-p:0.12467017769813538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7128, 3.0193, 2.8443],
        [3.7128, 3.7115, 3.7128],
        [3.7128, 3.7086, 3.7127],
        [3.7128, 3.7128, 3.7128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.20783281326293945 
model_pd.l_d.mean(): -23.270292282104492 
model_pd.lagr.mean(): -23.06245994567871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1978], device='cuda:0')), ('power', tensor([-23.4681], device='cuda:0'))])
epoch£º1283	 i:0 	 global-step:25660	 l-p:0.20783281326293945
epoch£º1283	 i:1 	 global-step:25661	 l-p:10.568744659423828
epoch£º1283	 i:2 	 global-step:25662	 l-p:0.1414945125579834
epoch£º1283	 i:3 	 global-step:25663	 l-p:-0.048054806888103485
epoch£º1283	 i:4 	 global-step:25664	 l-p:0.13475221395492554
epoch£º1283	 i:5 	 global-step:25665	 l-p:-0.18009166419506073
epoch£º1283	 i:6 	 global-step:25666	 l-p:0.12098728865385056
epoch£º1283	 i:7 	 global-step:25667	 l-p:0.10528571158647537
epoch£º1283	 i:8 	 global-step:25668	 l-p:0.13533371686935425
epoch£º1283	 i:9 	 global-step:25669	 l-p:0.1321103870868683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7138, 3.0332, 2.3765],
        [3.7138, 3.7084, 3.7136],
        [3.7138, 3.5561, 3.6600],
        [3.7138, 3.7137, 3.7138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.19018739461898804 
model_pd.l_d.mean(): -23.686689376831055 
model_pd.lagr.mean(): -23.496501922607422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1592], device='cuda:0')), ('power', tensor([-23.8458], device='cuda:0'))])
epoch£º1284	 i:0 	 global-step:25680	 l-p:0.19018739461898804
epoch£º1284	 i:1 	 global-step:25681	 l-p:2.3180785179138184
epoch£º1284	 i:2 	 global-step:25682	 l-p:0.12153373658657074
epoch£º1284	 i:3 	 global-step:25683	 l-p:0.12098304182291031
epoch£º1284	 i:4 	 global-step:25684	 l-p:-0.201979398727417
epoch£º1284	 i:5 	 global-step:25685	 l-p:0.14231076836585999
epoch£º1284	 i:6 	 global-step:25686	 l-p:-0.029445290565490723
epoch£º1284	 i:7 	 global-step:25687	 l-p:0.13252118229866028
epoch£º1284	 i:8 	 global-step:25688	 l-p:0.14984390139579773
epoch£º1284	 i:9 	 global-step:25689	 l-p:0.10763601213693619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7132, 2.8866, 2.2878],
        [3.7132, 3.7033, 3.7127],
        [3.7132, 3.1308, 3.1055],
        [3.7132, 2.9206, 2.2714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.12894193828105927 
model_pd.l_d.mean(): -23.595857620239258 
model_pd.lagr.mean(): -23.466915130615234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1856], device='cuda:0')), ('power', tensor([-23.7815], device='cuda:0'))])
epoch£º1285	 i:0 	 global-step:25700	 l-p:0.12894193828105927
epoch£º1285	 i:1 	 global-step:25701	 l-p:-0.03352191671729088
epoch£º1285	 i:2 	 global-step:25702	 l-p:0.16913944482803345
epoch£º1285	 i:3 	 global-step:25703	 l-p:4.338931560516357
epoch£º1285	 i:4 	 global-step:25704	 l-p:-0.21506331861019135
epoch£º1285	 i:5 	 global-step:25705	 l-p:0.12244180589914322
epoch£º1285	 i:6 	 global-step:25706	 l-p:0.1395045667886734
epoch£º1285	 i:7 	 global-step:25707	 l-p:0.20041820406913757
epoch£º1285	 i:8 	 global-step:25708	 l-p:0.13602709770202637
epoch£º1285	 i:9 	 global-step:25709	 l-p:0.11568546295166016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7182, 3.0896, 3.0092],
        [3.7182, 3.6377, 3.7016],
        [3.7182, 3.2218, 2.5928],
        [3.7182, 3.0821, 2.4298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): -0.25297844409942627 
model_pd.l_d.mean(): -22.927167892456055 
model_pd.lagr.mean(): -23.180147171020508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2616], device='cuda:0')), ('power', tensor([-23.1887], device='cuda:0'))])
epoch£º1286	 i:0 	 global-step:25720	 l-p:-0.25297844409942627
epoch£º1286	 i:1 	 global-step:25721	 l-p:0.1091049388051033
epoch£º1286	 i:2 	 global-step:25722	 l-p:0.15516668558120728
epoch£º1286	 i:3 	 global-step:25723	 l-p:0.1527114063501358
epoch£º1286	 i:4 	 global-step:25724	 l-p:0.12929695844650269
epoch£º1286	 i:5 	 global-step:25725	 l-p:0.13549570739269257
epoch£º1286	 i:6 	 global-step:25726	 l-p:0.13285653293132782
epoch£º1286	 i:7 	 global-step:25727	 l-p:0.11595655232667923
epoch£º1286	 i:8 	 global-step:25728	 l-p:0.11780194193124771
epoch£º1286	 i:9 	 global-step:25729	 l-p:-0.02496582455933094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7155, 2.9504, 2.2929],
        [3.7155, 2.9942, 2.7695],
        [3.7155, 3.7154, 3.7155],
        [3.7155, 3.7155, 3.7155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.1872287541627884 
model_pd.l_d.mean(): -23.466419219970703 
model_pd.lagr.mean(): -23.279190063476562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2552], device='cuda:0')), ('power', tensor([-23.7216], device='cuda:0'))])
epoch£º1287	 i:0 	 global-step:25740	 l-p:0.1872287541627884
epoch£º1287	 i:1 	 global-step:25741	 l-p:0.11311555653810501
epoch£º1287	 i:2 	 global-step:25742	 l-p:0.1301935315132141
epoch£º1287	 i:3 	 global-step:25743	 l-p:0.14089427888393402
epoch£º1287	 i:4 	 global-step:25744	 l-p:-0.17339880764484406
epoch£º1287	 i:5 	 global-step:25745	 l-p:76.75366973876953
epoch£º1287	 i:6 	 global-step:25746	 l-p:0.10897065699100494
epoch£º1287	 i:7 	 global-step:25747	 l-p:0.12038854509592056
epoch£º1287	 i:8 	 global-step:25748	 l-p:0.15943355858325958
epoch£º1287	 i:9 	 global-step:25749	 l-p:-0.01852288655936718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7114, 3.7114, 3.7114],
        [3.7114, 3.7112, 3.7114],
        [3.7114, 3.0802, 2.9973],
        [3.7114, 2.9155, 2.2678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.14120136201381683 
model_pd.l_d.mean(): -23.15648078918457 
model_pd.lagr.mean(): -23.01527976989746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2268], device='cuda:0')), ('power', tensor([-23.3833], device='cuda:0'))])
epoch£º1288	 i:0 	 global-step:25760	 l-p:0.14120136201381683
epoch£º1288	 i:1 	 global-step:25761	 l-p:-0.10343457758426666
epoch£º1288	 i:2 	 global-step:25762	 l-p:0.12500125169754028
epoch£º1288	 i:3 	 global-step:25763	 l-p:0.1385200321674347
epoch£º1288	 i:4 	 global-step:25764	 l-p:0.09130284935235977
epoch£º1288	 i:5 	 global-step:25765	 l-p:0.19570250809192657
epoch£º1288	 i:6 	 global-step:25766	 l-p:0.125114306807518
epoch£º1288	 i:7 	 global-step:25767	 l-p:0.12406764924526215
epoch£º1288	 i:8 	 global-step:25768	 l-p:-0.40640345215797424
epoch£º1288	 i:9 	 global-step:25769	 l-p:0.14345788955688477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7068, 3.0663, 2.4152],
        [3.7068, 3.5315, 3.6419],
        [3.7068, 3.7068, 3.7068],
        [3.7068, 3.6266, 3.6903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): -0.2965984344482422 
model_pd.l_d.mean(): -23.34101676940918 
model_pd.lagr.mean(): -23.637615203857422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1548], device='cuda:0')), ('power', tensor([-23.4958], device='cuda:0'))])
epoch£º1289	 i:0 	 global-step:25780	 l-p:-0.2965984344482422
epoch£º1289	 i:1 	 global-step:25781	 l-p:0.06242571771144867
epoch£º1289	 i:2 	 global-step:25782	 l-p:0.010723104700446129
epoch£º1289	 i:3 	 global-step:25783	 l-p:0.14889174699783325
epoch£º1289	 i:4 	 global-step:25784	 l-p:0.12058675289154053
epoch£º1289	 i:5 	 global-step:25785	 l-p:0.16183029115200043
epoch£º1289	 i:6 	 global-step:25786	 l-p:0.12054920196533203
epoch£º1289	 i:7 	 global-step:25787	 l-p:0.15345212817192078
epoch£º1289	 i:8 	 global-step:25788	 l-p:0.13434426486492157
epoch£º1289	 i:9 	 global-step:25789	 l-p:0.10234922170639038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7066, 3.7066, 3.7066],
        [3.7066, 3.2476, 3.3247],
        [3.7066, 3.3033, 3.4081],
        [3.7066, 3.5377, 3.6459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.14770184457302094 
model_pd.l_d.mean(): -22.975854873657227 
model_pd.lagr.mean(): -22.828153610229492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2769], device='cuda:0')), ('power', tensor([-23.2528], device='cuda:0'))])
epoch£º1290	 i:0 	 global-step:25800	 l-p:0.14770184457302094
epoch£º1290	 i:1 	 global-step:25801	 l-p:0.15544936060905457
epoch£º1290	 i:2 	 global-step:25802	 l-p:0.1357426792383194
epoch£º1290	 i:3 	 global-step:25803	 l-p:0.12078139930963516
epoch£º1290	 i:4 	 global-step:25804	 l-p:0.16156397759914398
epoch£º1290	 i:5 	 global-step:25805	 l-p:0.12790364027023315
epoch£º1290	 i:6 	 global-step:25806	 l-p:0.0003270339802838862
epoch£º1290	 i:7 	 global-step:25807	 l-p:0.1566466987133026
epoch£º1290	 i:8 	 global-step:25808	 l-p:-0.07628190517425537
epoch£º1290	 i:9 	 global-step:25809	 l-p:0.1383211463689804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6972, 3.2305, 3.3034],
        [3.6972, 3.2055, 3.2619],
        [3.6972, 2.9894, 2.7927],
        [3.6972, 3.6972, 3.6972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.12867343425750732 
model_pd.l_d.mean(): -23.737524032592773 
model_pd.lagr.mean(): -23.608850479125977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1413], device='cuda:0')), ('power', tensor([-23.8788], device='cuda:0'))])
epoch£º1291	 i:0 	 global-step:25820	 l-p:0.12867343425750732
epoch£º1291	 i:1 	 global-step:25821	 l-p:0.12790243327617645
epoch£º1291	 i:2 	 global-step:25822	 l-p:0.12576518952846527
epoch£º1291	 i:3 	 global-step:25823	 l-p:0.28078392148017883
epoch£º1291	 i:4 	 global-step:25824	 l-p:0.19373439252376556
epoch£º1291	 i:5 	 global-step:25825	 l-p:0.13483072817325592
epoch£º1291	 i:6 	 global-step:25826	 l-p:0.11861995607614517
epoch£º1291	 i:7 	 global-step:25827	 l-p:0.12172168493270874
epoch£º1291	 i:8 	 global-step:25828	 l-p:-0.04378883168101311
epoch£º1291	 i:9 	 global-step:25829	 l-p:0.06404534727334976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6903, 3.6896, 3.6903],
        [3.6903, 3.2376, 2.6210],
        [3.6903, 3.6881, 3.6902],
        [3.6903, 3.5379, 3.6398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.17963752150535583 
model_pd.l_d.mean(): -22.537504196166992 
model_pd.lagr.mean(): -22.357866287231445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2725], device='cuda:0')), ('power', tensor([-22.8100], device='cuda:0'))])
epoch£º1292	 i:0 	 global-step:25840	 l-p:0.17963752150535583
epoch£º1292	 i:1 	 global-step:25841	 l-p:0.1982937902212143
epoch£º1292	 i:2 	 global-step:25842	 l-p:0.13845115900039673
epoch£º1292	 i:3 	 global-step:25843	 l-p:0.26353996992111206
epoch£º1292	 i:4 	 global-step:25844	 l-p:0.03219422325491905
epoch£º1292	 i:5 	 global-step:25845	 l-p:-0.007532300893217325
epoch£º1292	 i:6 	 global-step:25846	 l-p:0.05259665846824646
epoch£º1292	 i:7 	 global-step:25847	 l-p:0.12680687010288239
epoch£º1292	 i:8 	 global-step:25848	 l-p:0.11752620339393616
epoch£º1292	 i:9 	 global-step:25849	 l-p:0.12638947367668152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6980, 3.6904, 3.6976],
        [3.6980, 2.9936, 2.8031],
        [3.6980, 3.3821, 3.5092],
        [3.6980, 3.4732, 3.5967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.20641738176345825 
model_pd.l_d.mean(): -23.38568878173828 
model_pd.lagr.mean(): -23.179271697998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2241], device='cuda:0')), ('power', tensor([-23.6098], device='cuda:0'))])
epoch£º1293	 i:0 	 global-step:25860	 l-p:0.20641738176345825
epoch£º1293	 i:1 	 global-step:25861	 l-p:0.02285277284681797
epoch£º1293	 i:2 	 global-step:25862	 l-p:-0.02131638489663601
epoch£º1293	 i:3 	 global-step:25863	 l-p:0.14455504715442657
epoch£º1293	 i:4 	 global-step:25864	 l-p:0.13624435663223267
epoch£º1293	 i:5 	 global-step:25865	 l-p:0.1554398536682129
epoch£º1293	 i:6 	 global-step:25866	 l-p:0.1157691478729248
epoch£º1293	 i:7 	 global-step:25867	 l-p:0.04553386569023132
epoch£º1293	 i:8 	 global-step:25868	 l-p:0.14728520810604095
epoch£º1293	 i:9 	 global-step:25869	 l-p:0.12873029708862305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7018, 3.7001, 3.7017],
        [3.7018, 3.6101, 3.6810],
        [3.7018, 2.9814, 2.7615],
        [3.7018, 3.7009, 3.7018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): -0.18365180492401123 
model_pd.l_d.mean(): -22.792545318603516 
model_pd.lagr.mean(): -22.9761962890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2885], device='cuda:0')), ('power', tensor([-23.0810], device='cuda:0'))])
epoch£º1294	 i:0 	 global-step:25880	 l-p:-0.18365180492401123
epoch£º1294	 i:1 	 global-step:25881	 l-p:0.13486750423908234
epoch£º1294	 i:2 	 global-step:25882	 l-p:0.11012853682041168
epoch£º1294	 i:3 	 global-step:25883	 l-p:0.14813338220119476
epoch£º1294	 i:4 	 global-step:25884	 l-p:0.12196458876132965
epoch£º1294	 i:5 	 global-step:25885	 l-p:0.23249022662639618
epoch£º1294	 i:6 	 global-step:25886	 l-p:0.04748038202524185
epoch£º1294	 i:7 	 global-step:25887	 l-p:0.1203412413597107
epoch£º1294	 i:8 	 global-step:25888	 l-p:0.14067232608795166
epoch£º1294	 i:9 	 global-step:25889	 l-p:0.16191332042217255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6962, 3.6920, 3.6961],
        [3.6962, 3.6711, 3.6939],
        [3.6962, 3.6026, 3.6746],
        [3.6962, 3.0169, 2.3629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): -0.009854611940681934 
model_pd.l_d.mean(): -23.45486831665039 
model_pd.lagr.mean(): -23.464723587036133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2064], device='cuda:0')), ('power', tensor([-23.6613], device='cuda:0'))])
epoch£º1295	 i:0 	 global-step:25900	 l-p:-0.009854611940681934
epoch£º1295	 i:1 	 global-step:25901	 l-p:0.04398301988840103
epoch£º1295	 i:2 	 global-step:25902	 l-p:0.20362113416194916
epoch£º1295	 i:3 	 global-step:25903	 l-p:0.12417314201593399
epoch£º1295	 i:4 	 global-step:25904	 l-p:0.11652059108018875
epoch£º1295	 i:5 	 global-step:25905	 l-p:0.10450643301010132
epoch£º1295	 i:6 	 global-step:25906	 l-p:0.14878085255622864
epoch£º1295	 i:7 	 global-step:25907	 l-p:0.23957030475139618
epoch£º1295	 i:8 	 global-step:25908	 l-p:0.058158330619335175
epoch£º1295	 i:9 	 global-step:25909	 l-p:0.1338197886943817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6953, 3.5335, 3.6391],
        [3.6953, 3.5687, 3.6589],
        [3.6953, 3.6953, 3.6953],
        [3.6953, 3.6595, 3.6911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.059437096118927 
model_pd.l_d.mean(): -22.844032287597656 
model_pd.lagr.mean(): -22.784595489501953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2775], device='cuda:0')), ('power', tensor([-23.1216], device='cuda:0'))])
epoch£º1296	 i:0 	 global-step:25920	 l-p:0.059437096118927
epoch£º1296	 i:1 	 global-step:25921	 l-p:0.07283470779657364
epoch£º1296	 i:2 	 global-step:25922	 l-p:0.24427825212478638
epoch£º1296	 i:3 	 global-step:25923	 l-p:0.028198013082146645
epoch£º1296	 i:4 	 global-step:25924	 l-p:0.14860765635967255
epoch£º1296	 i:5 	 global-step:25925	 l-p:0.1489396095275879
epoch£º1296	 i:6 	 global-step:25926	 l-p:0.11848684400320053
epoch£º1296	 i:7 	 global-step:25927	 l-p:0.11801724135875702
epoch£º1296	 i:8 	 global-step:25928	 l-p:0.11642274260520935
epoch£º1296	 i:9 	 global-step:25929	 l-p:0.10990389436483383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4712,  0.3667,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2304,  0.1412,  1.0000,  0.0866,
          1.0000,  0.6130, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2326,  0.1431,  1.0000,  0.0880,
          1.0000,  0.6150, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[3.6978, 2.8687, 2.3379],
        [3.6978, 3.1584, 3.1767],
        [3.6978, 3.1532, 3.1668],
        [3.6978, 2.9609, 2.7100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.1202065721154213 
model_pd.l_d.mean(): -22.509050369262695 
model_pd.lagr.mean(): -22.388843536376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2876], device='cuda:0')), ('power', tensor([-22.7966], device='cuda:0'))])
epoch£º1297	 i:0 	 global-step:25940	 l-p:0.1202065721154213
epoch£º1297	 i:1 	 global-step:25941	 l-p:-0.024202918633818626
epoch£º1297	 i:2 	 global-step:25942	 l-p:0.04845442995429039
epoch£º1297	 i:3 	 global-step:25943	 l-p:0.1401185542345047
epoch£º1297	 i:4 	 global-step:25944	 l-p:0.1562461405992508
epoch£º1297	 i:5 	 global-step:25945	 l-p:0.13289032876491547
epoch£º1297	 i:6 	 global-step:25946	 l-p:0.15348508954048157
epoch£º1297	 i:7 	 global-step:25947	 l-p:0.20275212824344635
epoch£º1297	 i:8 	 global-step:25948	 l-p:0.13429231941699982
epoch£º1297	 i:9 	 global-step:25949	 l-p:0.11004304885864258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6934, 3.6930, 3.6934],
        [3.6934, 2.8759, 2.4113],
        [3.6934, 2.8672, 2.2529],
        [3.6934, 2.8945, 2.4949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.23767593502998352 
model_pd.l_d.mean(): -23.486364364624023 
model_pd.lagr.mean(): -23.248687744140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2361], device='cuda:0')), ('power', tensor([-23.7224], device='cuda:0'))])
epoch£º1298	 i:0 	 global-step:25960	 l-p:0.23767593502998352
epoch£º1298	 i:1 	 global-step:25961	 l-p:0.07206124067306519
epoch£º1298	 i:2 	 global-step:25962	 l-p:0.15545561909675598
epoch£º1298	 i:3 	 global-step:25963	 l-p:0.031843990087509155
epoch£º1298	 i:4 	 global-step:25964	 l-p:0.09249385446310043
epoch£º1298	 i:5 	 global-step:25965	 l-p:0.033833179622888565
epoch£º1298	 i:6 	 global-step:25966	 l-p:0.1635459065437317
epoch£º1298	 i:7 	 global-step:25967	 l-p:0.1304968297481537
epoch£º1298	 i:8 	 global-step:25968	 l-p:0.1449352651834488
epoch£º1298	 i:9 	 global-step:25969	 l-p:0.14165280759334564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6946, 3.6936, 3.6946],
        [3.6946, 2.9401, 2.6532],
        [3.6946, 3.6946, 3.6946],
        [3.6946, 3.6356, 3.6848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.05712619796395302 
model_pd.l_d.mean(): -23.755844116210938 
model_pd.lagr.mean(): -23.69871711730957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1456], device='cuda:0')), ('power', tensor([-23.9014], device='cuda:0'))])
epoch£º1299	 i:0 	 global-step:25980	 l-p:0.05712619796395302
epoch£º1299	 i:1 	 global-step:25981	 l-p:0.03132382407784462
epoch£º1299	 i:2 	 global-step:25982	 l-p:0.1507011204957962
epoch£º1299	 i:3 	 global-step:25983	 l-p:0.13479556143283844
epoch£º1299	 i:4 	 global-step:25984	 l-p:0.1657264679670334
epoch£º1299	 i:5 	 global-step:25985	 l-p:0.11911638081073761
epoch£º1299	 i:6 	 global-step:25986	 l-p:0.11994193494319916
epoch£º1299	 i:7 	 global-step:25987	 l-p:0.24731747806072235
epoch£º1299	 i:8 	 global-step:25988	 l-p:-0.015885815024375916
epoch£º1299	 i:9 	 global-step:25989	 l-p:0.18470607697963715
