
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.22045108675956726 
model_pd.l_d.mean(): -22.75084114074707 
model_pd.lagr.mean(): -22.5303897857666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6202], device='cuda:0')), ('power', tensor([-23.3711], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.22045108675956726
epoch£º0	 i:1 	 global-step:1	 l-p:0.1639728546142578
epoch£º0	 i:2 	 global-step:2	 l-p:0.20958468317985535
epoch£º0	 i:3 	 global-step:3	 l-p:0.18547755479812622
epoch£º0	 i:4 	 global-step:4	 l-p:0.1604577600955963
epoch£º0	 i:5 	 global-step:5	 l-p:0.13323137164115906
epoch£º0	 i:6 	 global-step:6	 l-p:0.07205234467983246
epoch£º0	 i:7 	 global-step:7	 l-p:0.15227164328098297
epoch£º0	 i:8 	 global-step:8	 l-p:0.12444312125444412
epoch£º0	 i:9 	 global-step:9	 l-p:0.1525035947561264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5290, 3.5294, 3.5290],
        [3.5290, 3.7409, 3.6980],
        [3.5290, 3.5896, 3.5508],
        [3.5290, 4.5221, 5.2100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.03510548174381256 
model_pd.l_d.mean(): -22.22926139831543 
model_pd.lagr.mean(): -22.194156646728516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1471], device='cuda:0')), ('power', tensor([-22.3764], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.03510548174381256
epoch£º1	 i:1 	 global-step:21	 l-p:0.0948038250207901
epoch£º1	 i:2 	 global-step:22	 l-p:0.1295902580022812
epoch£º1	 i:3 	 global-step:23	 l-p:0.12658867239952087
epoch£º1	 i:4 	 global-step:24	 l-p:0.08247102797031403
epoch£º1	 i:5 	 global-step:25	 l-p:0.10980378836393356
epoch£º1	 i:6 	 global-step:26	 l-p:0.14614053070545197
epoch£º1	 i:7 	 global-step:27	 l-p:0.1311836540699005
epoch£º1	 i:8 	 global-step:28	 l-p:0.11177258938550949
epoch£º1	 i:9 	 global-step:29	 l-p:0.09871816635131836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0404, 4.8941, 5.3155],
        [4.0404, 4.1189, 4.0708],
        [4.0404, 4.6872, 4.8975],
        [4.0404, 4.1062, 4.0631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.1317572444677353 
model_pd.l_d.mean(): -22.885700225830078 
model_pd.lagr.mean(): -22.753942489624023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0763], device='cuda:0')), ('power', tensor([-22.8094], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.1317572444677353
epoch£º2	 i:1 	 global-step:41	 l-p:0.08083644509315491
epoch£º2	 i:2 	 global-step:42	 l-p:0.13022764027118683
epoch£º2	 i:3 	 global-step:43	 l-p:0.11213386058807373
epoch£º2	 i:4 	 global-step:44	 l-p:0.12493521720170975
epoch£º2	 i:5 	 global-step:45	 l-p:0.1048060804605484
epoch£º2	 i:6 	 global-step:46	 l-p:0.10844957828521729
epoch£º2	 i:7 	 global-step:47	 l-p:0.11355183273553848
epoch£º2	 i:8 	 global-step:48	 l-p:0.1038631796836853
epoch£º2	 i:9 	 global-step:49	 l-p:0.12123554199934006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6315, 4.3173, 4.6195],
        [3.6315, 3.6643, 3.6394],
        [3.6315, 3.6995, 3.6575],
        [3.6315, 4.6156, 5.2709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.2710484564304352 
model_pd.l_d.mean(): -21.906286239624023 
model_pd.lagr.mean(): -21.635238647460938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1621], device='cuda:0')), ('power', tensor([-22.0684], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.2710484564304352
epoch£º3	 i:1 	 global-step:61	 l-p:0.2700676918029785
epoch£º3	 i:2 	 global-step:62	 l-p:0.19370616972446442
epoch£º3	 i:3 	 global-step:63	 l-p:0.12462924420833588
epoch£º3	 i:4 	 global-step:64	 l-p:0.11662869900465012
epoch£º3	 i:5 	 global-step:65	 l-p:0.11948098987340927
epoch£º3	 i:6 	 global-step:66	 l-p:0.14479847252368927
epoch£º3	 i:7 	 global-step:67	 l-p:0.15526235103607178
epoch£º3	 i:8 	 global-step:68	 l-p:0.11071538925170898
epoch£º3	 i:9 	 global-step:69	 l-p:0.11260285973548889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5472, 4.0326, 4.1544],
        [3.5472, 4.1388, 4.3576],
        [3.5472, 4.5717, 5.3009],
        [3.5472, 3.6072, 3.5688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.15943466126918793 
model_pd.l_d.mean(): -22.146488189697266 
model_pd.lagr.mean(): -21.98705291748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1523], device='cuda:0')), ('power', tensor([-22.2988], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.15943466126918793
epoch£º4	 i:1 	 global-step:81	 l-p:0.12336204946041107
epoch£º4	 i:2 	 global-step:82	 l-p:0.4048042595386505
epoch£º4	 i:3 	 global-step:83	 l-p:0.11930128931999207
epoch£º4	 i:4 	 global-step:84	 l-p:0.12040645629167557
epoch£º4	 i:5 	 global-step:85	 l-p:0.14234109222888947
epoch£º4	 i:6 	 global-step:86	 l-p:0.1375732719898224
epoch£º4	 i:7 	 global-step:87	 l-p:0.32293879985809326
epoch£º4	 i:8 	 global-step:88	 l-p:0.13615918159484863
epoch£º4	 i:9 	 global-step:89	 l-p:0.09223649650812149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8067, 3.8069, 3.8067],
        [3.8067, 3.8116, 3.8071],
        [3.8067, 3.8805, 3.8356],
        [3.8067, 3.9124, 3.8589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.12029188126325607 
model_pd.l_d.mean(): -22.832006454467773 
model_pd.lagr.mean(): -22.711713790893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0265], device='cuda:0')), ('power', tensor([-22.8585], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.12029188126325607
epoch£º5	 i:1 	 global-step:101	 l-p:6.543997764587402
epoch£º5	 i:2 	 global-step:102	 l-p:0.11233146488666534
epoch£º5	 i:3 	 global-step:103	 l-p:0.12942193448543549
epoch£º5	 i:4 	 global-step:104	 l-p:0.07733573764562607
epoch£º5	 i:5 	 global-step:105	 l-p:0.12809385359287262
epoch£º5	 i:6 	 global-step:106	 l-p:-1.780015468597412
epoch£º5	 i:7 	 global-step:107	 l-p:0.11401432007551193
epoch£º5	 i:8 	 global-step:108	 l-p:0.12316784262657166
epoch£º5	 i:9 	 global-step:109	 l-p:0.11050176620483398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8387, 3.8597, 3.8424],
        [3.8387, 3.8387, 3.8387],
        [3.8387, 4.8962, 5.6053],
        [3.8387, 3.8423, 3.8389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.1407427042722702 
model_pd.l_d.mean(): -23.183931350708008 
model_pd.lagr.mean(): -23.043188095092773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0408], device='cuda:0')), ('power', tensor([-23.1431], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.1407427042722702
epoch£º6	 i:1 	 global-step:121	 l-p:0.11582386493682861
epoch£º6	 i:2 	 global-step:122	 l-p:0.10622213035821915
epoch£º6	 i:3 	 global-step:123	 l-p:0.13054077327251434
epoch£º6	 i:4 	 global-step:124	 l-p:0.10712095350027084
epoch£º6	 i:5 	 global-step:125	 l-p:0.009322552010416985
epoch£º6	 i:6 	 global-step:126	 l-p:0.12978658080101013
epoch£º6	 i:7 	 global-step:127	 l-p:0.6742181181907654
epoch£º6	 i:8 	 global-step:128	 l-p:0.09854267537593842
epoch£º6	 i:9 	 global-step:129	 l-p:0.1127580776810646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6214, 4.0770, 4.1697],
        [3.6214, 3.6655, 3.6344],
        [3.6214, 3.6214, 3.6214],
        [3.6214, 3.6364, 3.6236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.6807875037193298 
model_pd.l_d.mean(): -23.140625 
model_pd.lagr.mean(): -22.459836959838867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0511], device='cuda:0')), ('power', tensor([-23.1917], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.6807875037193298
epoch£º7	 i:1 	 global-step:141	 l-p:0.12045778334140778
epoch£º7	 i:2 	 global-step:142	 l-p:0.10574367642402649
epoch£º7	 i:3 	 global-step:143	 l-p:0.1348562389612198
epoch£º7	 i:4 	 global-step:144	 l-p:0.14500750601291656
epoch£º7	 i:5 	 global-step:145	 l-p:0.10494811832904816
epoch£º7	 i:6 	 global-step:146	 l-p:0.11308515071868896
epoch£º7	 i:7 	 global-step:147	 l-p:0.11202001571655273
epoch£º7	 i:8 	 global-step:148	 l-p:0.10918053984642029
epoch£º7	 i:9 	 global-step:149	 l-p:0.11247791349887848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7490, 3.7490, 3.7490],
        [3.7490, 3.7491, 3.7490],
        [3.7490, 4.3568, 4.5700],
        [3.7490, 3.8584, 3.8052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.15561503171920776 
model_pd.l_d.mean(): -21.972936630249023 
model_pd.lagr.mean(): -21.81732177734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0890], device='cuda:0')), ('power', tensor([-22.0620], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.15561503171920776
epoch£º8	 i:1 	 global-step:161	 l-p:0.12010689824819565
epoch£º8	 i:2 	 global-step:162	 l-p:0.08678356558084488
epoch£º8	 i:3 	 global-step:163	 l-p:0.13845521211624146
epoch£º8	 i:4 	 global-step:164	 l-p:0.1076338142156601
epoch£º8	 i:5 	 global-step:165	 l-p:0.12447142601013184
epoch£º8	 i:6 	 global-step:166	 l-p:0.1498439610004425
epoch£º8	 i:7 	 global-step:167	 l-p:0.13415363430976868
epoch£º8	 i:8 	 global-step:168	 l-p:0.08105454593896866
epoch£º8	 i:9 	 global-step:169	 l-p:0.1400149017572403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7623, 3.7637, 3.7624],
        [3.7623, 3.7638, 3.7624],
        [3.7623, 3.8714, 3.8183],
        [3.7623, 3.7777, 3.7646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.0920049175620079 
model_pd.l_d.mean(): -21.241451263427734 
model_pd.lagr.mean(): -21.149446487426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1146], device='cuda:0')), ('power', tensor([-21.3561], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.0920049175620079
epoch£º9	 i:1 	 global-step:181	 l-p:0.11670834571123123
epoch£º9	 i:2 	 global-step:182	 l-p:0.11232365667819977
epoch£º9	 i:3 	 global-step:183	 l-p:0.08288081735372543
epoch£º9	 i:4 	 global-step:184	 l-p:0.12121415138244629
epoch£º9	 i:5 	 global-step:185	 l-p:0.11359873414039612
epoch£º9	 i:6 	 global-step:186	 l-p:0.1159854382276535
epoch£º9	 i:7 	 global-step:187	 l-p:0.1259697824716568
epoch£º9	 i:8 	 global-step:188	 l-p:0.06590704619884491
epoch£º9	 i:9 	 global-step:189	 l-p:0.12519225478172302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7771, 4.6898, 5.2332],
        [3.7771, 4.4007, 4.6276],
        [3.7771, 3.7809, 3.7774],
        [3.7771, 3.7771, 3.7771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.1788807213306427 
model_pd.l_d.mean(): -22.927595138549805 
model_pd.lagr.mean(): -22.748714447021484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0218], device='cuda:0')), ('power', tensor([-22.9494], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.1788807213306427
epoch£º10	 i:1 	 global-step:201	 l-p:0.09280984848737717
epoch£º10	 i:2 	 global-step:202	 l-p:0.06468692421913147
epoch£º10	 i:3 	 global-step:203	 l-p:0.12416329979896545
epoch£º10	 i:4 	 global-step:204	 l-p:-0.4928756654262543
epoch£º10	 i:5 	 global-step:205	 l-p:0.12360355257987976
epoch£º10	 i:6 	 global-step:206	 l-p:0.09681139886379242
